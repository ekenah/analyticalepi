<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Maximum Likelihood Estimation – Analytical Epidemiology</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./bayes.html" rel="next">
<link href="./condprob.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-747bc48f972f7c4036e4d8e8c6d9e55a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script>
  window.MathJax = {
    tex: {
      macros: {
        approxsim: "\\stackrel\{\\text\{approx\}\}\{\\sim\}",
        Bernoulli: ["\\operatorname\{Bernoulli\}"],
        comp: "\\mathsf\{C\}",
        Cov: ["\\operatorname\{Cov\}"],
        data: "\\text\{data\}",
        dif: "\\text\{d\}",
        E: ["\\operatorname\{\\mathbb\{E\}\}"],
        expit: ["\\operatorname\{expit\}"],
        given: ["\\, #1| \\, ", 1],
        indicator: "\\mathbb\{1\}",
        logit: ["\\operatorname\{logit\}"],
        ptrue: "p_\{\\text\{true\}\}",
        sens: "\\text\{sens\}",
        spec: "\\text\{spec\}",
        supp: ["\\operatorname\{supp\}"],
        tonset: "t^\\text\{ons\}",
        trec: "t^\\text\{rec\}",
        true: "\\text\{true\}",
        twobytwo: "2 \\times 2",
        vand: "\\text\{ and \}",
        Var: ["\\operatorname\{Var\}"]
      }
    }
  };
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./probability.html">One-Sample Inference for Risks and Rates</a></li><li class="breadcrumb-item"><a href="./mlestimation.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Analytical Epidemiology</a> 
        <div class="sidebar-tools-main">
    <a href="./Analytical-Epidemiology.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">One-Sample Inference for Risks and Rates</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability, Random Variables, and Disease Occurrence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./condprob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Probability and Diagnostic Tests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mlestimation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bayesian Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./longitudinal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Longitudinal Data and Rates</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Two-Sample Inference and Study Design</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Principles of Causal Inference</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Epidemologic and Statistical Methods for Causal Inference</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Calculus</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#binomial-likelihood" id="toc-binomial-likelihood" class="nav-link active" data-scroll-target="#binomial-likelihood"><span class="header-section-number">3.1</span> Binomial likelihood</a>
  <ul class="collapse">
  <li><a href="#sec-binomial" id="toc-sec-binomial" class="nav-link" data-scroll-target="#sec-binomial"><span class="header-section-number">3.1.1</span> Binomial distribution</a></li>
  <li><a href="#likelihood-and-log-likelihood" id="toc-likelihood-and-log-likelihood" class="nav-link" data-scroll-target="#likelihood-and-log-likelihood"><span class="header-section-number">3.1.2</span> Likelihood and log likelihood</a></li>
  <li><a href="#score-function" id="toc-score-function" class="nav-link" data-scroll-target="#score-function"><span class="header-section-number">3.1.3</span> Score function</a></li>
  <li><a href="#sec-information" id="toc-sec-information" class="nav-link" data-scroll-target="#sec-information"><span class="header-section-number">3.1.4</span> Expected and observed information*</a></li>
  </ul></li>
  <li><a href="#large-sample-theory" id="toc-large-sample-theory" class="nav-link" data-scroll-target="#large-sample-theory"><span class="header-section-number">3.2</span> Large-sample theory</a>
  <ul class="collapse">
  <li><a href="#sample-mean-average" id="toc-sample-mean-average" class="nav-link" data-scroll-target="#sample-mean-average"><span class="header-section-number">3.2.1</span> Sample mean (average)</a></li>
  <li><a href="#law-of-large-numbers-and-consistency" id="toc-law-of-large-numbers-and-consistency" class="nav-link" data-scroll-target="#law-of-large-numbers-and-consistency"><span class="header-section-number">3.2.2</span> Law of large numbers and consistency</a></li>
  <li><a href="#sec-cltnorm" id="toc-sec-cltnorm" class="nav-link" data-scroll-target="#sec-cltnorm"><span class="header-section-number">3.2.3</span> Central limit theorem and the normal distribution</a></li>
  <li><a href="#efficiency-of-maximum-likelihood-estimators" id="toc-efficiency-of-maximum-likelihood-estimators" class="nav-link" data-scroll-target="#efficiency-of-maximum-likelihood-estimators"><span class="header-section-number">3.2.4</span> Efficiency of maximum likelihood estimators*</a></li>
  </ul></li>
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing"><span class="header-section-number">3.3</span> Hypothesis testing</a>
  <ul class="collapse">
  <li><a href="#hypothesis-tests-and-diagnostic-tests" id="toc-hypothesis-tests-and-diagnostic-tests" class="nav-link" data-scroll-target="#hypothesis-tests-and-diagnostic-tests"><span class="header-section-number">3.3.1</span> Hypothesis tests and diagnostic tests</a></li>
  <li><a href="#sec-wslr" id="toc-sec-wslr" class="nav-link" data-scroll-target="#sec-wslr"><span class="header-section-number">3.3.2</span> Wald, score, and likelihood ratio tests</a></li>
  <li><a href="#critical-values-and-p-values" id="toc-critical-values-and-p-values" class="nav-link" data-scroll-target="#critical-values-and-p-values"><span class="header-section-number">3.3.3</span> Critical values and p-values</a></li>
  </ul></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals"><span class="header-section-number">3.4</span> Confidence intervals</a>
  <ul class="collapse">
  <li><a href="#wald-confidence-intervals-and-the-delta-method" id="toc-wald-confidence-intervals-and-the-delta-method" class="nav-link" data-scroll-target="#wald-confidence-intervals-and-the-delta-method"><span class="header-section-number">3.4.1</span> Wald confidence intervals and the delta method</a></li>
  <li><a href="#score-wilson-confidence-intervals" id="toc-score-wilson-confidence-intervals" class="nav-link" data-scroll-target="#score-wilson-confidence-intervals"><span class="header-section-number">3.4.2</span> Score (Wilson) confidence intervals</a></li>
  </ul></li>
  <li><a href="#small-sample-estimation" id="toc-small-sample-estimation" class="nav-link" data-scroll-target="#small-sample-estimation"><span class="header-section-number">3.5</span> Small-sample estimation*</a>
  <ul class="collapse">
  <li><a href="#median-unbiased-estimate" id="toc-median-unbiased-estimate" class="nav-link" data-scroll-target="#median-unbiased-estimate"><span class="header-section-number">3.5.1</span> Median unbiased estimate</a></li>
  <li><a href="#exact-clopper-pearson-and-mid-p-confidence-intervals" id="toc-exact-clopper-pearson-and-mid-p-confidence-intervals" class="nav-link" data-scroll-target="#exact-clopper-pearson-and-mid-p-confidence-intervals"><span class="header-section-number">3.5.2</span> Exact (Clopper-Pearson) and mid-p confidence intervals</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./probability.html">One-Sample Inference for Risks and Rates</a></li><li class="breadcrumb-item"><a href="./mlestimation.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>Far better an approximate answer to the <em>right</em> question, which is often vague, than an <em>exact</em> answer to the wrong question, which can always be made precise. <span class="citation" data-cites="tukey1962future">(<a href="references.html#ref-tukey1962future" role="doc-biblioref">Tukey 1962</a>)</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
</blockquote>
<p>In probability, we are told the rules of the game and then we predict what it will look like. In statistics, we watch the game and try to figure out the rules. Roughly speaking, statistics (game to rules) is the reverse of probability (rules to game). When done well, statistics helps us learn from observations while accounting honestly for uncertainty. An outstanding early example statistics applied to public health is the work of <a href="https://en.wikipedia.org/wiki/Florence_Nightingale">Florence Nightingale</a> (1820-1910), who collected data and developed statistical graphics to demonstrate the need for public health reforms in the British Army in the 1850s <span class="citation" data-cites="cohen1984florence winkelstein2009florence">(<a href="references.html#ref-cohen1984florence" role="doc-biblioref">Cohen 1984</a>; <a href="references.html#ref-winkelstein2009florence" role="doc-biblioref">Winkelstein Jr 2009</a>)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>Here, we will use estimation of a probability as an example of maximum likelihood estimation, which is used for parameter estimation throughout frequentist statistics. It gives us a way to find point estimates of parameters that are optimal in large samples in a sense that we will explain below. It is also the foundation for hypothesis tests and confidence intervals, which give us an accurate way to account for uncertainty in statistical inference.</p>
<section id="binomial-likelihood" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="binomial-likelihood"><span class="header-section-number">3.1</span> Binomial likelihood</h2>
<p>In <a href="#sec-binomial" class="quarto-xref"><span>Section 3.1.1</span></a>, we used the prevalence <span class="math inline">\(p\)</span> in our population to figure out the distribution of the number <span class="math inline">\(X\)</span> of diseased individuals in a sample of size <span class="math inline">\(n\)</span>. This is probability. The corresponding statistical problem would be to estimate the prevalence <span class="math inline">\(p\)</span> after seeing <span class="math inline">\(X = x\)</span> infected individuals in a sample of size <span class="math inline">\(n\)</span>.</p>
<p>When our experiment is to sample multiple individuals from a population, the analogy between the outcomes <span class="math inline">\(\omega \in \Omega\)</span> and the individuals in the population breaks down. Recall that when we flip a coin twice, each <span class="math inline">\(\omega \in \Omega\)</span> must specify the outcomes of both flips. When the experiment is to sample <span class="math inline">\(n\)</span> individuals from a population, the entire sample is a single outcome <span class="math inline">\(\omega\)</span> and <span class="math inline">\(\Omega\)</span> contains all possible samples of <span class="math inline">\(n\)</span> individuals from the population. If the population size is <span class="math inline">\(N\)</span>, then the number of possible samples of size <span class="math inline">\(n\)</span> is given by the <em>binomial coefficient</em> <span class="math display">\[
  \binom{N}{n} = \frac{N!}{n! (N - n)!},
\]</span> where <span class="math inline">\(k!\)</span> denotes <span class="math inline">\(k\)</span> factorial. <a href="https://en.wikipedia.org/wiki/Factorial">Factorials</a> are defined by <span class="math inline">\(0! = 1\)</span> and <span class="math inline">\(k! = k \cdot (k - 1)!\)</span> for any integer <span class="math inline">\(k &gt; 0\)</span>. For example, <span class="math inline">\(1! = 1\)</span>, <span class="math inline">\(2! = 2\)</span>, <span class="math inline">\(3! = 6\)</span>, <span class="math inline">\(4! = 24\)</span>, <span class="math inline">\(5! = 120\)</span>, and so on. For <span class="math inline">\(k &gt; 0\)</span>, <span class="math inline">\(k!\)</span> is the product of all positive integers up to and including <span class="math inline">\(k\)</span>, which grows extremely fast as <span class="math inline">\(k\)</span> increases.</p>
<section id="sec-binomial" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="sec-binomial"><span class="header-section-number">3.1.1</span> Binomial distribution</h3>
<p>Suppose we sample <span class="math inline">\(n\)</span> individuals from a population <span class="math inline">\(\Omega\)</span> and test them for disease. For simplicity, we assume that the diagnostic test has perfect sensitivity and specificity. Let <span class="math inline">\(Y_i\)</span> denote whether person <span class="math inline">\(i\)</span> in the sample has disease, and let <span class="math inline">\(X\)</span> be the total number who have disease. Then <span class="math display">\[
  X = \sum_{i = 1}^n Y_i,
\]</span> so it is a linear combination of the <span class="math inline">\(Y_i\)</span>. Each <span class="math inline">\(Y_i\)</span> is a Bernoulli(<span class="math inline">\(p\)</span>) random variable, where <span class="math inline">\(p\)</span> is the prevalence of disease in the population. When <span class="math inline">\(N\)</span> is much larger than <span class="math inline">\(n\)</span> (for which we write <span class="math inline">\(N \gg n\)</span>), the test results for each person in the sample are approximately independent.</p>
<p>The distribution of a sum of <span class="math inline">\(n\)</span> independent Bernoulli(<span class="math inline">\(p\)</span>) random variables is called a <strong>binomial(n, p) distribution</strong>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> The probability <span class="math inline">\(Y_1 = 1\)</span> is <span class="math inline">\(p\)</span>, and the probability that <span class="math inline">\(Y_1 = 0\)</span> is <span class="math inline">\((1 - p)\)</span>, so we can handle both cases by writing <span class="math display">\[
  \Pr(Y_1 = y_1) = p^{y_1} (1 - p)^{1 - y_1}.
\]</span> When the <span class="math inline">\(Y_i\)</span> are independent, each <span class="math inline">\(Y_i\)</span> has a Bernoulli(p) distribution (see <a href="probability.html#sec-Bernoulli" class="quarto-xref"><span>Section 1.3.5</span></a>) and <span class="math display">\[
  \Pr(Y_1 = y_1, Y_2 = y_2, \ldots, Y_n = y_n)
  = \prod_{i = 1}^n \Pr(Y_i = y_i)
  = \prod_{i = 1}^n p^{y_i} (1 - p)^{1 - y_i}
\]</span> by the multiplication rule for independent events. Substituting <span class="math inline">\(x = \sum_{i = 1}^n y_i\)</span>, we get <span class="math display">\[
  \Pr(Y_1 = y_1, Y_2 = y_2, \ldots, Y_n = y_n)
  = p^x (1 - p)^{n - x}.
\]</span> The value of <span class="math inline">\(x\)</span> depends only on the sum of the <span class="math inline">\(y_i\)</span>, and there are <span class="math inline">\(\binom{n}{x}\)</span> different ways to get <span class="math inline">\(x\)</span> cases of disease out of <span class="math inline">\(n\)</span> sampled individuals. By the addition rule for disjoint events, we get <span id="eq-binomPMF"><span class="math display">\[
  \Pr(X = x) = \binom{n}{x} p^x (1 - p)^x.
\tag{3.1}\]</span></span> This is the probability mass function (PMF) of the binomial distribution. The set of possible values of a binomial(n, p) random variable <span class="math inline">\(X\)</span> is <span class="math inline">\(\supp(X) = \{0, 1, \ldots, n\}\)</span>.</p>
<p><a href="probability.html#sec-Bernoulli" class="quarto-xref"><span>Section 1.3.5</span></a> showed that a Bernoulli(<span class="math inline">\(p\)</span>) random variable has expected value <span class="math inline">\(p\)</span> and variance <span class="math inline">\(p(1 - p)\)</span>. Because a binomial(<span class="math inline">\(n\)</span>, <span class="math inline">\(p\)</span>) random variable is the sum of <span class="math inline">\(n\)</span> independent Bernoulli(<span class="math inline">\(p\)</span>) random variables, its expected value is <span class="math display">\[
  \E(X) = n p.
\]</span> by the rule for expectations of linear combinations in <a href="probability.html#eq-lincomE" class="quarto-xref">Equation&nbsp;<span>1.11</span></a>. Its variance is <span class="math display">\[
  \Var(X) = n p (1 - p)
\]</span> by the rule for variances of linear combinations in <a href="probability.html#eq-lincomV" class="quarto-xref">Equation&nbsp;<span>1.12</span></a>. The covariances are all zero because the <span class="math inline">\(Y_i\)</span> are independent.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" aria-current="page">R</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>binomdist.R</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="binomdist.R"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="do">## binomial distribution</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># binomial PMF</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The second and third arguments are n ("size") and p ("prob").</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">dbinom</span>(<span class="dv">2</span>, <span class="dv">10</span>, <span class="fl">0.4</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="fl">0.4</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="fl">0.4</span>))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># binomial CDF</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">pbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="fl">0.4</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cumsum</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="fl">0.4</span>))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># binomial quantiles</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">qbinom</span>(<span class="fu">c</span>(<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="dv">1</span>), <span class="dv">10</span>, <span class="fl">0.4</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># random samples</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">rbinom</span>(<span class="dv">20</span>, <span class="dv">10</span>, <span class="fl">0.4</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1000</span>, <span class="dv">10</span>, <span class="fl">0.4</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(x)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="likelihood-and-log-likelihood" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="likelihood-and-log-likelihood"><span class="header-section-number">3.1.2</span> Likelihood and log likelihood</h3>
<p>In probability, we know the prevalence of disease <span class="math inline">\(p\)</span> and we deduce the distribution of the number of diseased individuals <span class="math inline">\(X\)</span> in a sample of size <span class="math inline">\(n\)</span>. In statistics, we observe <span class="math inline">\(X = x\)</span> and use this to estimate <span class="math inline">\(p\)</span>. To do this, we rewrite the binomial PMF <a href="#eq-binomPMF" class="quarto-xref">Equation&nbsp;<span>3.1</span></a> as a function of <span class="math inline">\(p\)</span> instead of <span class="math inline">\(x\)</span>: <span id="eq-binomL"><span class="math display">\[
  L(p) = \binom{n}{x} p^x (1 - p)^{n - x}.
\tag{3.2}\]</span></span> This is the binomial <strong>likelihood function</strong>. The right-hand sides of <a href="#eq-binomPMF" class="quarto-xref">Equation&nbsp;<span>3.1</span></a> and <a href="#eq-binomL" class="quarto-xref">Equation&nbsp;<span>3.2</span></a> are identical, and they produce exactly the same value given the same <span class="math inline">\(x\)</span> and <span class="math inline">\(p\)</span>. However, the two equations define different functions. In binomial PMF in <a href="#eq-binomPMF" class="quarto-xref">Equation&nbsp;<span>3.1</span></a>, the prevalence <span class="math inline">\(p\)</span> is fixed and the number of diseased individuals <span class="math inline">\(x\)</span> is the argument of the function. In the binomial likelihood function in <a href="#eq-binomL" class="quarto-xref">Equation&nbsp;<span>3.2</span></a>, the number of diseased individuals <span class="math inline">\(x\)</span> is fixed and the prevalence <span class="math inline">\(p\)</span> is the argument of the function. The PMF belongs to probability, and the likelihood belongs to statistics.</p>
<p>The <strong>log likelihood</strong> is the natural logarithm (i.e., the logarithm to base <span class="math inline">\(e = 2.718281828\ldots\)</span>)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> of the likelihood function. For binomial log likelihood is <span class="math display">\[
  \ell(p) = \ln \binom{n}{x} + x \ln p + (n - x) \ln (1 - p).
\]</span> Because the logarithm turns products into sums, it is generally much easier to handle the log likelihood than the likelihood itself. The term <span class="math inline">\(\ln \binom{n}{x}\)</span> does not depend on <span class="math inline">\(p\)</span>, so it can be ignored. Intuitively, this tells us that the total number <span class="math inline">\(x = y_1 + y_2 + \cdots + y_n\)</span> of individuals with disease in our sample contains the same information about the prevalence of disease as the sequence <span class="math inline">\(y_1, y_2, \ldots, y_n\)</span> of disease indicators.</p>
<p>For any given <span class="math inline">\(p\)</span>, we can think of <span class="math inline">\(\ell(p)\)</span> as a random variable whose value is determined by our sample of size <span class="math inline">\(n\)</span>. Let <span class="math inline">\(\ptrue\)</span> be the true prevalence of disease. By <em>Gibb’s inequality</em>,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <span class="math display">\[
  \E[\ell(\ptrue)] &gt; \E[\ell(p)]
\]</span> for all <span class="math inline">\(p \neq \ptrue\)</span>. This inequality is about the expected value of the log likelihood over all possible samples of size <span class="math inline">\(n\)</span>. For any given sample, it is possible that <span class="math inline">\(\ell(\ptrue)\)</span> is not the maximum of the log likelihood. However, this inequality is an important part of the justification for estimating <span class="math inline">\(p\)</span> by maximizing the log likelihood <span class="citation" data-cites="boos2013essential">(<a href="references.html#ref-boos2013essential" role="doc-biblioref">Boos and Stefanski 2013</a>)</span>. Because function <span class="math inline">\(v \mapsto \ln(v)\)</span> is strictly increasing in <span class="math inline">\(v\)</span>, the likelihood <span class="math inline">\(L(p)\)</span> and the log likelihood <span class="math inline">\(\ell(p)\)</span> are maximized at exactly the same value of <span class="math inline">\(p\)</span>.</p>
</section>
<section id="score-function" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="score-function"><span class="header-section-number">3.1.3</span> Score function</h3>
<p>To find the maximum of the log likelihood, we find the value of <span class="math inline">\(p\)</span> where its slope is zero. This is the mathematical version of the insight that the ground at the top of a hill is level. The <strong>score function</strong> is the first derivative of the log likelihood <span class="math display">\[
  U(p)
  = \frac{\text{d}}{\text{d} p} \ell(p)
  = \frac{x}{p} - \frac{n - x}{1 - p},
\]</span> which is the slope of <span class="math inline">\(\ell(p)\)</span> at <span class="math inline">\(p\)</span>. To find where the slope equals zero, we solve the <em>score equation</em> <span id="eq-U"><span class="math display">\[
  U(\hat{p})
  = \frac{x}{\hat{p}} - \frac{n - x}{1 - \hat{p}}
  = 0
\tag{3.3}\]</span></span> where <span class="math inline">\(\hat{p}\)</span> denotes the maximum likelihood estimate (MLE) of <span class="math inline">\(\ptrue\)</span>. When the dust settles, we get <span class="math display">\[
  \hat{p} = \frac{x}{n}
\]</span> so our MLE of the prevalence is just the proportion of our sample who has disease.</p>
<p>To confirm that this is a maximum instead of a minimum, we need to look at the second derivative of <span class="math inline">\(\ell\)</span>. When we walk across the top of a hill, we go from walking uphill to walking downhill so the slope is decreasing. If <span class="math inline">\(\ell(p)\)</span> is maximized at <span class="math inline">\(\hat{p}\)</span>, then the slope of the slope (i.e., the second derivative) should be negative. The second derivative of <span class="math inline">\(\ell(p)\)</span> at <span class="math inline">\(\hat{p}\)</span> is <span id="eq-lsecderiv"><span class="math display">\[
  \frac{\text{d}}{\text{d} p} U(p) = \frac{\text{d}^2}{\text{d} p^2} \ell(p)
  = -\frac{x}{p^2} - \frac{n - x}{(1 - p)^2}.
\tag{3.4}\]</span></span> This is negative for any <span class="math inline">\(p \in (0, 1)\)</span>. Thus, the log likelihood is maximized at <span class="math inline">\(\hat{p}\)</span> if <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(x &lt; n\)</span>.</p>
<p>When <span class="math inline">\(x = 0\)</span> or <span class="math inline">\(x = n\)</span>, the log likelihood <span class="math inline">\(\ell(p)\)</span> has no maximum at any <span class="math inline">\(p \in (0, 1)\)</span>. Instead, the maximum occurs at one of the boundaries of the set of possible <span class="math inline">\(p\)</span>. When <span class="math inline">\(x = 0\)</span>, our MLE of <span class="math inline">\(\ptrue\)</span> is <span class="math inline">\(\hat{p} = 0\)</span>. When <span class="math inline">\(x = n\)</span>, our maximum likelihood estimate is <span class="math inline">\(\hat{p} = 1\)</span>.</p>
</section>
<section id="sec-information" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="sec-information"><span class="header-section-number">3.1.4</span> Expected and observed information*</h3>
<p>For any given <span class="math inline">\(p\)</span>, we can think of the score <span class="math inline">\(U(p)\)</span> as a random variable that has an expected value and a variance. If <span class="math inline">\(\ptrue = p\)</span>, the expected value of the score is always zero: <span class="math display">\[
  \E_p[U(p)]
  = \E_p\bigg[\frac{X}{p} - \frac{n - X}{1 - p}\bigg]
  = \frac{\E_p(X)}{p} - \frac{\E_p(n - X)}{1 - p}
  = \frac{n p}{p} - \frac{n (1 - p)}{1 - p}
  = 0
\]</span> where we use the subscript <span class="math inline">\(p\)</span> to indicate that the expected value is calculated assuming that <span class="math inline">\(\ptrue = p\)</span>. Because <span class="math inline">\(\E_p[U(p)] = 0\)</span>, the corresponding variance of the score is <span class="math display">\[
  \mathcal{I}(p) = \Var_p[U(p)] = \E_p[U(p)^2],
\]</span> by <a href="probability.html#eq-var2" class="quarto-xref">Equation&nbsp;<span>1.10</span></a>. This is called the <strong>expected Fisher information</strong> or <strong>expected information</strong>.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> It can be used to calculate confidence limits for <span class="math inline">\(\ptrue\)</span>.</p>
<p>Under <em>regularity conditions</em> that are met when <span class="math inline">\(\ptrue \in (0, 1)\)</span>, the Fisher information <span class="math inline">\(\mathcal{I}(p)\)</span> can be calculated using the second derivative of the log likelihood <span class="math inline">\(\ell(p)\)</span> from <a href="#eq-lsecderiv" class="quarto-xref">Equation&nbsp;<span>3.4</span></a>.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> Specifically, <span class="math inline">\(\mathcal{I}(p)\)</span> is the expected value of the negative second derivative of <span class="math inline">\(\ell(p)\)</span>: <span id="eq-expinf"><span class="math display">\[
  \mathcal{I}(p)
  = \E_p\Bigg[-\frac{\text{d}^2}{\text{d} p^2} \ell(p)\Bigg]
  = \E_p\bigg[\frac{X}{p^2} + \frac{n - X}{(1 - p)^2}\bigg],
\tag{3.5}\]</span></span> where the subscript <span class="math inline">\(p\)</span> indicates that the expected value is calculated assuming that <span class="math inline">\(\ptrue = p\)</span>. Using <a href="probability.html#eq-lincomE" class="quarto-xref">Equation&nbsp;<span>1.11</span></a> and the binomial(<span class="math inline">\(n\)</span>, <span class="math inline">\(p\)</span>) distribution for <span class="math inline">\(X\)</span>, this simplifies to <span class="math display">\[
  \mathcal{I}(p)
  = \frac{\E(X)}{p^2} + \frac{\E(n - X)}{(1 - p)^2}
  = \frac{n}{p} + \frac{n}{1 - p}
  = \frac{n}{p (1 - p)}.
\]</span> Because <span class="math inline">\(\ptrue\)</span> is unknown, the expected information is often evaluated at <span class="math inline">\(\hat{p}\)</span>. In some models, the expected information can be difficult to calculate.</p>
<p>The negative second derivative of <span class="math inline">\(\ell(p)\)</span> inside the expectation in <a href="#eq-expinf" class="quarto-xref">Equation&nbsp;<span>3.5</span></a> evaluated is the <strong>observed Fisher information</strong> or <strong>observed information</strong> <span id="eq-obsinf"><span class="math display">\[
I(p) = -\frac{\text{d}^2}{\text{d} p^2} \ell(p)
= \frac{x}{p^2} + \frac{n - x}{(1 - p)^2}.
\tag{3.6}\]</span></span> For the binomial distribution <span class="math inline">\(I(\hat{p}) = \mathcal{I}(\hat{p})\)</span> but this equality does not hold at other values of <span class="math inline">\(p\)</span>. The observed information is an unbiased estimator of the expected information, and it can always be calculated from the data. It often produces more accurate variance estimates than the expected information <span class="citation" data-cites="efron1978assessing kenward1998likelihood reid2003asymptotics">(<a href="references.html#ref-efron1978assessing" role="doc-biblioref">Efron and Hinkley 1978</a>; <a href="references.html#ref-kenward1998likelihood" role="doc-biblioref">Kenward and Molenberghs 1998</a>; <a href="references.html#ref-reid2003asymptotics" role="doc-biblioref">Reid 2003</a>)</span>. However, it is generally safe to use whichever is most convenient <span class="citation" data-cites="boos2013essential">(<a href="references.html#ref-boos2013essential" role="doc-biblioref">Boos and Stefanski 2013</a>)</span>.</p>
</section>
</section>
<section id="large-sample-theory" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="large-sample-theory"><span class="header-section-number">3.2</span> Large-sample theory</h2>
<p>The log likelihood, the score function, and the Fisher and observed information give us all of the pieces we need to calculate point and interval estimates of <span class="math inline">\(\ptrue\)</span>. To put them together, we use two fundamental results from probability theory about the behavior of sample means. The law of large numbers justifies point estimates and the central limit theorem justifies hypothesis tests and interval estimates, which can be obtained in three standard ways.</p>
<section id="sample-mean-average" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="sample-mean-average"><span class="header-section-number">3.2.1</span> Sample mean (average)</h3>
<p>If <span class="math inline">\(Y_1, Y_2, \ldots, Y_n\)</span> are random variables, then the <strong>sample mean</strong> or <strong>average</strong> is <span class="math display">\[
  \hat{\mu}_n = \frac{1}{n} \sum_{i = 1}^n Y_i.
\]</span> This sample mean can be thought of as a random variable whose value is determined when we observe <span class="math inline">\(Y_1 = y_1, Y_2 = y_2, \ldots, Y_n = y_n\)</span>. If each <span class="math inline">\(Y_i\)</span> has <span class="math inline">\(\E(Y_i) = \mu\)</span>, then <span id="eq-muE"><span class="math display">\[
  \E[\hat{\mu}_n]
  = \frac{1}{n} \sum_{i = 1}^n \E[Y_i]
  = \frac{1}{n} n \mu
  = \mu
\tag{3.7}\]</span></span> by <a href="probability.html#eq-lincomE" class="quarto-xref">Equation&nbsp;<span>1.11</span></a>. Thus, the sample mean <span class="math inline">\(\hat{\mu}_n\)</span> is an <strong>unbiased</strong> estimate of <span class="math inline">\(\mu\)</span> for any sample size <span class="math inline">\(n\)</span>. When the <span class="math inline">\(Y_i\)</span> are indicator variables, <span class="math inline">\(\hat{\mu}_n\)</span> is just the proportion of the sample with <span class="math inline">\(Y_i = 1\)</span>.</p>
</section>
<section id="law-of-large-numbers-and-consistency" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="law-of-large-numbers-and-consistency"><span class="header-section-number">3.2.2</span> Law of large numbers and consistency</h3>
<p>If the <span class="math inline">\(Y_i\)</span> are independent and each has <span class="math inline">\(\Var(Y_i) = \sigma^2\)</span>, then <span id="eq-muVar"><span class="math display">\[
  \Var(\hat{\mu}_n)
  = \frac{1}{n^2} \sum_{i = 1}^n \Var(Y_i)
  = \frac{1}{n^2} n \sigma^2
  = \frac{\sigma^2}{n}
\tag{3.8}\]</span></span> by <a href="probability.html#eq-lincomV" class="quarto-xref">Equation&nbsp;<span>1.12</span></a>. Thus, the variance of <span class="math inline">\(\hat{\mu}_n\)</span> decreases as the sample size <span class="math inline">\(n\)</span> increases. The standard deviation of <span class="math inline">\(\hat{\mu}_n\)</span> is proportional to <span class="math inline">\(1 / \sqrt{n}\)</span>. As <span class="math inline">\(n \rightarrow \infty\)</span>, we should have <span class="math inline">\(\hat{\mu}_n \rightarrow \mu\)</span>. This is called the <strong>law of large numbers</strong>, and it holds even when <span class="math inline">\(\sigma^2 = \infty\)</span>.</p>
<div id="thm-lln" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 (Law of Large Numbers)</strong></span> If <span class="math inline">\(Y_1, Y_2, \ldots\)</span> is an infinite sequence of independent and identically-distributed (IID) random variables with mean <span class="math inline">\(\mu &lt; \infty\)</span> and variance <span class="math inline">\(\sigma^2 \leq \infty\)</span>, then<br>
<span class="math display">\[
    \hat{\mu}_n \rightarrow \mu
\]</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
</div>
<p>Our maximum likelihood estimate <span class="math inline">\(\hat{p}_n\)</span> is a sample mean: <span class="math display">\[
  \hat{p}_n = \frac{X}{n} = \frac{1}{n} \sum_{i = 1}^n Y_i.
\]</span> where each <span class="math inline">\(Y_i \sim \text{Bernoulli}(\ptrue)\)</span> and the <span class="math inline">\(Y_i\)</span> are independent. Therefore, the LLN implies that <span class="math display">\[
  \hat{p}_n \rightarrow \ptrue
\]</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. This convergence is shown in <a href="#fig-lln" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>. An estimate that converges to its true value as <span class="math inline">\(n \rightarrow \infty\)</span> is called <strong>consistent</strong>. Intuitively, this means that <span class="math inline">\(\hat{p}_n\)</span> is guaranteed to be close to <span class="math inline">\(\ptrue\)</span> in a large sample. However, the LLN does not specify how close or how large a sample we need.</p>
<div class="cell">
<div class="code-with-filename">
<details class="code-fold">
<summary>Code</summary>
<div class="code-with-filename-file">
<pre><strong>lln.R</strong></pre>
</div>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Law of large numbers</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(n)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">cumsum</span>(<span class="fu">rbinom</span>(n, <span class="dv">1</span>, .<span class="dv">5</span>)) <span class="sc">/</span> x, <span class="at">type =</span> <span class="st">"n"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Number of samples"</span>, <span class="at">ylab =</span> <span class="st">"Sample mean"</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">cumsum</span>(<span class="fu">rbinom</span>(n, <span class="dv">1</span>, .<span class="dv">5</span>)) <span class="sc">/</span> x, <span class="at">lty =</span> <span class="st">"solid"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">cumsum</span>(<span class="fu">rbinom</span>(n, <span class="dv">1</span>, .<span class="dv">5</span>)) <span class="sc">/</span> x, <span class="at">lty =</span> <span class="st">"dashed"</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">cumsum</span>(<span class="fu">rbinom</span>(n, <span class="dv">1</span>, .<span class="dv">5</span>)) <span class="sc">/</span> x, <span class="at">lty =</span> <span class="st">"dotted"</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> .<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell-output-display">
<div id="fig-lln" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lln-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="mlestimation_files/figure-html/fig-lln-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;3.1: The LLN at work. Each line traces the sample means calculated from a sequence of random samples x_1, x_2, x_3, \ldots from a Bernoulli(0.5) distribution. For each sequence, the y-coordinate above n is the sample mean from the first n random samples in the sequence. The true mean of 0.5 is marked by a solid horizontal line."><img src="mlestimation_files/figure-html/fig-lln-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lln-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: The LLN at work. Each line traces the sample means calculated from a sequence of random samples <span class="math inline">\(x_1, x_2, x_3, \ldots\)</span> from a Bernoulli(0.5) distribution. For each sequence, the y-coordinate above <span class="math inline">\(n\)</span> is the sample mean from the first <span class="math inline">\(n\)</span> random samples in the sequence. The true mean of 0.5 is marked by a solid horizontal line.
</figcaption>
</figure>
</div>
</div>
</div>
<!-- Let $U_n(p)$ be the score function from @eq-U based on a sample of size $n$, and let $\hat{p}_n = x / n$ be the corresponding maximum likelihood estimate of $\ptrue$.
For each $p \in (0, 1)$, $U_n(p)$ can be written as a sum:
$$
  U_n(p) = \sum_{i = 1}^n \bigg(\frac{y_i}{p} - \frac{1 - y_i}{1 - p}\bigg).
$$
The $Y_i$ are independent Bernoulli($\ptrue$) random variables, so the terms in the sum are independent and each have mean
$$
  \E\bigg(\frac{Y_i}{\ptrue} - \frac{1 - Y_i}{1 - \ptrue}\bigg)
  = \frac{\ptrue}{\ptrue} - \frac{1 - \ptrue}{1 - \ptrue}
  = 1 - 1
  = 0.
$$
Therefore, the LLN implies that
$$
  \frac{1}{n} U_n(\ptrue) \rightarrow 0
$$
as $n \rightarrow \infty$.
For any other value of $p \in (0, 1)$, the LLN implies that
and
$$
  \frac{1}{n} U_n(p) \rightarrow \E\bigg(\frac{Y_i}{p} - \frac{1 - Y_i}{1 - p}\bigg) \neq 0.
$$ {#eq-pfalse}
Because $U_n(\hat{p}_n) = 0$ for each $n$, we have $U_n(\hat{p}_n) \rightarrow 0$ as $n \rightarrow \infty$.
By @eq-pfalse, this can only happen if $\hat{p}_n \rightarrow \ptrue$.
An estimate that converges to its true value as $n \rightarrow \infty$ is called **consistent**.
^[
  This argument implicitly assumes that the sequence of estimates $\hat{p}_n$ converges to something.
  This is guaranteed by Gibbs' inequality, our regularity conditions, and the fact that the interval $[0, 1]$ (which contains all possible $\hat{p}_n$) is *compact*.
  Any infinite sequence in a compact set has a convergent subsequence.
] -->
</section>
<section id="sec-cltnorm" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="sec-cltnorm"><span class="header-section-number">3.2.3</span> Central limit theorem and the normal distribution</h3>
<p>When both the mean and variance of the <span class="math inline">\(Y_i\)</span> are finite, the <strong>central limit theorem</strong> (CLT) allows us to say something about how far away our sample mean <span class="math inline">\(\hat{\mu}_n\)</span> is from the true value <span class="math inline">\(\mu\)</span>. It is the most important result in all of probability and statistics.</p>
<div id="thm-clt" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.2 (Central Limit Theorem)</strong></span> If <span class="math inline">\(Y_1, Y_2, \ldots\)</span> is an infinite sequence of IID random variables with finite mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2 &lt; \infty\)</span>, then <span class="math display">\[
  Z_n
  = \frac{\hat{\mu}_n - \E(\hat{\mu_n})}{\sqrt{\Var(\hat{\mu}_n)}}
  = \frac{\sqrt{n} (\hat{\mu}_n - \mu)}{\sqrt{\sigma^2}}
\]</span> has a distribution that converges to a <strong>normal distribution</strong> or <strong>Gaussian distribution</strong> with mean zero and variance one as <span class="math inline">\(n \rightarrow \infty\)</span>.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> Because of this, we say that <span class="math inline">\(\hat{\mu}_n\)</span> is <strong>asymptotically normal</strong>.</p>
</div>
<p>The normal distribution is a distribution for a <strong>continuous random variable</strong>, which can take any value on an interval or even on all of <span class="math inline">\(\mathbb{R}\)</span>. Instead of a PMF, a continuous random variable <span class="math inline">\(Z\)</span> has a <strong>probability density function</strong> (PDF). If <span class="math inline">\(Z\)</span> is a continuous random variable with PDF <span class="math inline">\(f(z)\)</span> and <span class="math inline">\([a, b]\)</span> is an interval, then <span class="math display">\[
  \Pr\bigl(Z \in [a, b]\bigr) = \int_a^b f(z) \,\text{d} z.
\]</span> The integral on the right-hand side represents the area under <span class="math inline">\(f(z)\)</span> over the interval <span class="math inline">\([a, b]\)</span>. The cumulative distribution function of <span class="math inline">\(Z\)</span> is <span class="math display">\[
  F(z) = \int_{-\infty}^z f(u) \,\text{d} u,
\]</span> where the integral on the right-hand side represents the area under <span class="math inline">\(f(z)\)</span> over the interval <span class="math inline">\((-\infty, u]\)</span>. For the same reason that the values of the PMF for any discrete random variable add up to one, we have <span class="math display">\[
\Pr(Z \in \mathbb{R})
= \int_{-\infty}^\infty f(z) \,\text{d} z
= 1
\]</span> for any continuous random variable <span class="math inline">\(Z\)</span>. Like the PMF and CDF of a discrete random variable, the PDF and CDF of a continuous random variable contain the same information about the distribution of <span class="math inline">\(Z\)</span>.</p>
<p>The PDF of the normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> is <span class="math display">\[
  f(z, \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(z - \mu)^2}{2 \sigma^2}}.
\]</span> The <strong>standard normal distribution</strong> has <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span>. It is such an important distribution that its PDF and CDF have special notation. The standard normal PDF is <span class="math display">\[
  \phi(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{z^2}{2}},
\]</span> and its CDF is <span class="math inline">\(\Phi(z)\)</span>. These functions and the relationship between them are illustrated in <a href="#fig-normdist" class="quarto-xref">Figure&nbsp;<span>3.2</span></a>. A normal distribution is denoted <span class="math inline">\(N(\mu, \sigma^2)\)</span>, so the standard normal distribution is written <span class="math inline">\(N(0, 1)\)</span>.</p>
<div class="cell">
<div class="code-with-filename">
<details class="code-fold">
<summary>Code</summary>
<div class="code-with-filename-file">
<pre><strong>normplots.R</strong></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Normal distribution PDF and CDF</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># set grid of plots</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">2</span>) <span class="sc">+</span> <span class="fl">0.1</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># define variables</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># plot of PDF</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">dnorm</span>(x), <span class="at">type =</span> <span class="st">"n"</span>,</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"PDF "</span>, <span class="fu">phi1</span>(z))))</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">dnorm</span>(x))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(<span class="at">x =</span> <span class="fu">c</span>(b, a, <span class="fu">seq</span>(a, b, <span class="at">by =</span> <span class="fl">0.01</span>)),</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="at">y =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="fu">dnorm</span>(<span class="fu">seq</span>(a, b, <span class="at">by =</span> <span class="fl">0.01</span>))),</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="at">lty =</span> <span class="st">"dashed"</span>, <span class="at">col =</span> <span class="st">"darkgray"</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">0.4</span>, <span class="fl">0.18</span>, <span class="at">labels =</span> <span class="st">"Area = Pr(0 &lt; Z &lt; 2)"</span>, <span class="at">srt =</span> <span class="dv">90</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># plot of CDF</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">pnorm</span>(x), <span class="at">type =</span> <span class="st">"n"</span>,</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"CDF "</span>, <span class="fu">Phi</span>(z))))</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">pnorm</span>(x))</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="sc">-</span><span class="dv">4</span>), <span class="fu">pnorm</span>(<span class="fu">c</span>(a, b)), <span class="fu">c</span>(a, b), <span class="fu">pnorm</span>(<span class="fu">c</span>(a, b)),</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>         <span class="at">lty =</span> <span class="st">"dashed"</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="fu">c</span>(a, b), <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>), <span class="fu">c</span>(a, b), <span class="fu">pnorm</span>(<span class="fu">c</span>(a, b)), <span class="at">lty =</span> <span class="st">"dashed"</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="fu">pnorm</span>(a), <span class="sc">-</span><span class="dv">3</span>, <span class="fu">pnorm</span>(b), <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> <span class="fl">0.1</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="sc">-</span><span class="fl">1.7</span>, <span class="fu">sum</span>(<span class="fu">pnorm</span>(<span class="fu">c</span>(a, b))) <span class="sc">/</span> <span class="dv">2</span>, <span class="at">labels =</span> <span class="st">"Change = Pr(0 &lt; Z &lt; 2)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell-output-display">
<div id="fig-normdist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normdist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="mlestimation_files/figure-html/fig-normdist-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;3.2: The PDF (top) and CDF (bottom) of a standard normal random variable Z. If X \sim N(0, 1), then \Pr(0 < X < 2) equals the shaded area under the PDF as well as the change in the CDF from 0 to 2. This same relationship between the CDF and the PDF holds for all continuous random variables and any interval (a, b)."><img src="mlestimation_files/figure-html/fig-normdist-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normdist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: The PDF (top) and CDF (bottom) of a standard normal random variable <span class="math inline">\(Z\)</span>. If <span class="math inline">\(X \sim N(0, 1)\)</span>, then <span class="math inline">\(\Pr(0 &lt; X &lt; 2)\)</span> equals the shaded area under the PDF as well as the change in the CDF from <span class="math inline">\(0\)</span> to <span class="math inline">\(2\)</span>. This same relationship between the CDF and the PDF holds for all continuous random variables and any interval <span class="math inline">\((a, b)\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">R</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>normdist.R</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="normdist.R"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="do">## normal (Gaussian) distribution</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># normal PDF</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Second and third arguments are mean and SD (not variance).</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># The defaults are mean = 0 and SD = 1.</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="dv">2</span>, <span class="fl">1.2</span>, <span class="dv">5</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># normal CDF (using default mean and variance)</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.96</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.96</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="fl">1.96</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># normal quantiles</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.975</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fu">qnorm</span>(<span class="fl">0.975</span>))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># random samples (using named arguments)</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean =</span> <span class="fl">2.3</span>, <span class="at">sd =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>For our estimated probability <span class="math inline">\(\hat{p}_n\)</span> is a sample mean of IID <span class="math inline">\(Y_i\)</span> with <span class="math inline">\(\E(Y_i) = \ptrue\)</span> and <span class="math inline">\(\Var(Y_i) = \ptrue (1 - \ptrue)\)</span>. When <span class="math inline">\(n\)</span> is large, <span id="eq-cltp"><span class="math display">\[
  Z_n
  = \frac{\sqrt{n} (\hat{p}_n - \ptrue)}{\sqrt{\ptrue (1 - \ptrue)}}
  = \frac{\hat{p}_n - \ptrue}{\sqrt{\mathcal{I}(\ptrue)^{-1}}}
\tag{3.9}\]</span></span> has a distribution that is close to a standard normal distribution. <a href="#fig-clt" class="quarto-xref">Figure&nbsp;<span>3.3</span></a> shows this convergence is shown for sample means where <span class="math inline">\(Y_i \sim \Bernoulli(0.1)\)</span>. The CLT does not guarantee that the distribution of <span class="math inline">\(Z_n\)</span> is approximately normal in any given sample. It only guarantees that the normal approximation holds eventually as <span class="math inline">\(n\)</span> increases. When the <span class="math inline">\(Y_i \sim \Bernoulli(p)\)</span>, the normal approximation is typically good when <span class="math inline">\(n p (1 - p) &gt; 5\)</span>.</p>
<div class="cell">
<div class="code-with-filename">
<details class="code-fold">
<summary>Code</summary>
<div class="code-with-filename-file">
<pre><strong>clt.R</strong></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Central limit theorem</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># probability mass function for sample mean</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>dbline <span class="ot">&lt;-</span> <span class="cf">function</span>(n, <span class="at">p=</span>.<span class="dv">5</span>, ...) {</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> (<span class="fu">seq</span>(<span class="sc">-</span>.<span class="dv">5</span>, n <span class="sc">+</span> .<span class="dv">5</span>) <span class="sc">/</span> n <span class="sc">-</span> p) <span class="sc">*</span> <span class="fu">sqrt</span>(n <span class="sc">/</span> (p <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> p)))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span>n, n, p), <span class="dv">0</span>) <span class="sc">*</span> <span class="fu">sqrt</span>(p <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> p) <span class="sc">*</span> n)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="fu">stepfun</span>(x, y), <span class="at">pch =</span> <span class="cn">NA</span>, ...)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># define grid of plots</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">by =</span> .<span class="dv">01</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># n = 20</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">dnorm</span>(x), <span class="at">type =</span> <span class="st">"n"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">5</span>),</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"n = 20"</span>, <span class="at">xlab =</span> <span class="st">"Z score"</span>, <span class="at">ylab =</span> <span class="st">"Probability density"</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="fu">dbline</span>(<span class="dv">20</span>, <span class="at">p =</span> .<span class="dv">1</span>, <span class="at">lty =</span> <span class="st">"dashed"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">dnorm</span>(x), <span class="at">col =</span> <span class="st">"darkgray"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># n = 50</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">dnorm</span>(x), <span class="at">type =</span> <span class="st">"n"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">5</span>),</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"n = 50"</span>, <span class="at">xlab =</span> <span class="st">"Z score"</span>, <span class="at">ylab =</span> <span class="st">"Probability density"</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="fu">dbline</span>(<span class="dv">50</span>, <span class="at">p =</span> .<span class="dv">1</span>, <span class="at">lty =</span> <span class="st">"dashed"</span>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">dnorm</span>(x), <span class="at">col =</span> <span class="st">"darkgray"</span>)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co"># n = 100</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">dnorm</span>(x), <span class="at">type =</span> <span class="st">"n"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">5</span>),</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"n = 100"</span>, <span class="at">xlab =</span> <span class="st">"Z score"</span>, <span class="at">ylab =</span> <span class="st">"Probability density"</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="fu">dbline</span>(<span class="dv">100</span>, <span class="at">p =</span> .<span class="dv">1</span>, <span class="at">lty =</span> <span class="st">"dashed"</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">dnorm</span>(x), <span class="at">col =</span> <span class="st">"darkgray"</span>)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="co"># n = 250</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">dnorm</span>(x), <span class="at">type =</span> <span class="st">"n"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">5</span>),</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"n = 250"</span>, <span class="at">xlab =</span> <span class="st">"Z score"</span>, <span class="at">ylab =</span> <span class="st">"Probability density"</span>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="fu">dbline</span>(<span class="dv">250</span>, <span class="at">p =</span> .<span class="dv">1</span>, <span class="at">lty =</span> <span class="st">"dashed"</span>)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">dnorm</span>(x), <span class="at">col =</span> <span class="st">"darkgray"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell-output-display">
<div id="fig-clt" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="mlestimation_files/figure-html/fig-clt-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3.3: The CLT at work. The dashed lines show the PMF of the distribution of the average from a sample of size n from a Bernoulli(0.1) distribution. The solid line is the standard normal PDF."><img src="mlestimation_files/figure-html/fig-clt-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: The CLT at work. The dashed lines show the PMF of the distribution of the average from a sample of size <span class="math inline">\(n\)</span> from a Bernoulli(0.1) distribution. The solid line is the standard normal PDF.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="efficiency-of-maximum-likelihood-estimators" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="efficiency-of-maximum-likelihood-estimators"><span class="header-section-number">3.2.4</span> Efficiency of maximum likelihood estimators*</h3>
<p>We have used the LLN and the CLT to show that <span class="math inline">\(\hat{p}_n\)</span> is consistent and asymptotically normal, which are both wonderful properties for an estimator to have. However, they do not prove that <span class="math inline">\(\hat{p}_n\)</span> is the best estimator of <span class="math inline">\(\ptrue\)</span> in any particular sense. In <a href="#eq-cltp" class="quarto-xref">Equation&nbsp;<span>3.9</span></a>, the variance of <span class="math inline">\(\hat{p}_n\)</span> was <span class="math display">\[
  \mathcal{I}(\ptrue)^{-1} = \frac{\ptrue (1 - \ptrue)}{n},
\]</span> which is the inverse of the Fisher information. It turns out that no other unbiased estimator of <span class="math inline">\(\ptrue\)</span> can have lower variance, so <span class="math inline">\(\hat{p}_n\)</span> is the minimum-variance unbiased estimator of <span class="math inline">\(\ptrue\)</span>.</p>
<p>Suppose <span class="math inline">\(\theta\)</span> is a parameter for a family of PMFs or PDFs <span class="math inline">\(f(y, \theta)\)</span> such that the true PMF or PDF is <span class="math inline">\(f(y, \theta_\true)\)</span>. When we observe <span class="math inline">\(Y_1 = y_1, Y_2 = y_2, \ldots Y_n = y_n\)</span>, the likelihood is <span class="math display">\[
  L(\theta) = \prod_{i = 1} f(y_i, \theta),
\]</span> and the log likelihood is <span class="math display">\[
  \ell(\theta) = \ln L(\theta) = \sum_{i = 1}^n \ln f(y_i, \theta).
\]</span> The score function is <span class="math display">\[
  U(\theta) = \frac{\text{d}}{\text{d} \theta} \ell(\theta),
\]</span> and the MLE is the solution of the score equation <span class="math inline">\(U(\hat{\theta}) = 0\)</span>. The Fisher information is <span class="math display">\[
  \mathcal{I}(\theta) = \E_\theta\bigg[\frac{\text{d}^2}{\text{d} \theta^2} \ell(\theta)\bigg],
\]</span> and <span class="math inline">\(\Var(\hat{\theta}) = \mathcal{I}(\theta)^{-1}\)</span>. If <span class="math inline">\(\bar{\theta}\)</span> is any unbiased estimator of the true value <span class="math inline">\(\theta_\text{true}\)</span>, then <span class="math display">\[
  \Var(\bar{\theta}) \geq \mathcal{I}(\theta_\text{true})^{-1}.
\]</span> This result is called the <em>Cramér-Rao lower bound</em> <span class="citation" data-cites="rao1945information cramer1946mathematical">(<a href="references.html#ref-rao1945information" role="doc-biblioref">Rao 1945</a>; <a href="references.html#ref-cramer1946mathematical" role="doc-biblioref">Cramér 1946</a>)</span>,<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> No unbiased estimator of <span class="math inline">\(\theta_\true\)</span> can have smaller variance than the MLE <span class="math inline">\(\hat{\theta}\)</span>. Maximum likelihood estimates are consistent, asymptotically normal, and asymptotically efficient when the likelihood is correct <span class="citation" data-cites="boos2013essential">(<a href="references.html#ref-boos2013essential" role="doc-biblioref">Boos and Stefanski 2013</a>)</span>.</p>
</section>
</section>
<section id="hypothesis-testing" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="hypothesis-testing"><span class="header-section-number">3.3</span> Hypothesis testing</h2>
<p>In a <strong>hypothesis test</strong>, we specify a <strong>null hypothesis</strong> and then decide whether to reject it based on the value of a <strong>test statistic</strong>. A null hypothesis often takes the form <span id="eq-H0"><span class="math display">\[
  H_0: \theta_\text{true} = \theta_0.
\tag{3.10}\]</span></span> We reject <span class="math inline">\(H_0\)</span> if the test statistic appears inconsistent with its distribution under <span class="math inline">\(H_0\)</span>. Otherwise, we <em>fail to reject</em> <span class="math inline">\(H_0\)</span>. It is traditional to avoid saying that <span class="math inline">\(H_0\)</span> was accepted.</p>
<section id="hypothesis-tests-and-diagnostic-tests" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="hypothesis-tests-and-diagnostic-tests"><span class="header-section-number">3.3.1</span> Hypothesis tests and diagnostic tests</h3>
<p>If we think of <span class="math inline">\(H_0\)</span> as not having the disease and rejecting <span class="math inline">\(H_0\)</span> as testing positive for the disease, a hypothesis test is analogous to a diagnostic test. <a href="#tbl-htesting" class="quarto-xref">Table&nbsp;<span>3.1</span></a> shows the possible outcomes of a hypothesis test, and its margins show the correspondence to diagnostic testing <span class="citation" data-cites="diamond1983clinical">(<a href="references.html#ref-diamond1983clinical" role="doc-biblioref">Diamond and Forrester 1983</a>)</span>. A false positive occurs when we reject <span class="math inline">\(H_0\)</span> when it is true, which is called a <strong>type I error</strong>. A false negative occurs when we fail to reject <span class="math inline">\(H_0\)</span> when it is false, which is called <strong>type II error</strong>.</p>
<div id="tbl-htesting" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-htesting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Truth of <span class="math inline">\(H_0\)</span> and hypothesis test results.
</figcaption>
<div aria-describedby="tbl-htesting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Reject <span class="math inline">\(H_0\)</span> (<span class="math inline">\(T^+\)</span>)</th>
<th style="text-align: center;">Fail to reject <span class="math inline">\(H_0\)</span> (<span class="math inline">\(T^-\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(H_0\)</span> false (<span class="math inline">\(D^+\)</span>)</td>
<td style="text-align: center;">True positive</td>
<td style="text-align: center;">False negative = type II error</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(H_0\)</span> true (<span class="math inline">\(D^-\)</span>)</td>
<td style="text-align: center;">False positive = type I error</td>
<td style="text-align: center;">True negative</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>A hypothesis test has analogs of sensitivity and specificity. The equivalent of specificity is <span class="math inline">\(1 - \alpha\)</span> where <span class="math display">\[
  \alpha
  = \Pr(\text{reject } H_0 \given{} H_0 \text{ true})
\]</span> is the probability of a type I error. This is also called the <strong>significance level</strong> of the test. The equivalent of sensitivity is the <strong>power</strong> of the test, which is <span class="math inline">\(1 - \beta\)</span> where <span class="math display">\[
  \beta
  = \Pr(\text{fail to reject } H_0 \given{} H_0 \text{ false})
\]</span> is the probability of a type II error.</p>
<p>A hypothesis test also has analogs of positive and negative predictive values (PPV and NPV). Just like the PPV and NPV of a dignostic test depend on the prevalence of disease, the PPV and NPV of a hypothesis test depend on the <strong>prior probability</strong> that <span class="math inline">\(H_0\)</span> is true, which is the probability that <span class="math inline">\(H_0\)</span> is true based on what we know before we see the test result. For a hypothesis test, the PPV is <span id="eq-htestppv"><span class="math display">\[
  \Pr(H_0 \text{ false} \given{} H_0 \text{ rejected})
  = \frac{(1 - \beta) \Pr(H_0 \text{ false})}
  {(1 - \beta) \Pr(H_0 \text{ false}) + \alpha \Pr(H_0 \text{ true})}
\tag{3.11}\]</span></span> by Bayes’ rule. Similarly, the NPV of the hypothesis test is <span id="eq-htestnpv"><span class="math display">\[
  \Pr(H_0 \text{ true} \given H_0 \text{ not rejected})
  = \frac{(1 - \alpha) \Pr(H_0 \text{ true})}
  {(1 - \alpha) \Pr(H_0 \text{ true}) + \beta \Pr(H_0 \text{ false})}.
\tag{3.12}\]</span></span> The conditional probability that <span class="math inline">\(H_0\)</span> is true given the result of the hypothesis test is called the <strong>posterior probability</strong> of <span class="math inline">\(H_0\)</span>.</p>
</section>
<section id="sec-wslr" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="sec-wslr"><span class="header-section-number">3.3.2</span> Wald, score, and likelihood ratio tests</h3>
<p>In a maximum likelihood framework, there are three classical tests for a null hypothesis of the form <span class="math display">\[
  H_0: \ptrue = p_0.
\]</span> These tests are asymptotically equivalent, which means that they produce similar results in large samples. The best way to visualize the different tests is to look at a graph of the log likelihood function. <a href="#fig-htests" class="quarto-xref">Figure&nbsp;<span>3.4</span></a> shows the log likelihood function for a binary outcome with <span class="math inline">\(x = 60\)</span> events out of <span class="math inline">\(n = 100\)</span> trials and a null hypothesis <span class="math inline">\(H_0: p_\true = 0.5\)</span>. All three tests generalize to null hypotheses involving multiple parameters <span class="citation" data-cites="boos2013essential">(<a href="references.html#ref-boos2013essential" role="doc-biblioref">Boos and Stefanski 2013</a>)</span>.</p>
<p>The <strong>Wald test</strong> <span class="citation" data-cites="wald1943tests">(<a href="references.html#ref-wald1943tests" role="doc-biblioref">Wald 1943</a>)</span> of <span class="math inline">\(H_0\)</span> looks at the distance between the MLE <span class="math inline">\(\hat{p}\)</span> and the hypothesized value <span class="math inline">\(p_0\)</span><span class="citation" data-cites="wald1943tests">(<a href="references.html#ref-wald1943tests" role="doc-biblioref">Wald 1943</a>)</span>, rejecting <span class="math inline">\(H_0\)</span> when this distance is sufficiently large.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> An example is shown in <a href="#fig-htests" class="quarto-xref">Figure&nbsp;<span>3.4</span></a>. The Wald test statistic is <span id="eq-Waldtest"><span class="math display">\[
  W = \frac{(\hat{p} - p_0)^2}{I(\hat{p})}
  = \frac{n (\hat{p} - p_0)^2}{\hat{p} (1 - \hat{p})}
  \approxsim \chi^2_1
\tag{3.13}\]</span></span> under <span class="math inline">\(H_0\)</span>, where <span class="math inline">\(I(\hat{p})\)</span> is the observed information from <a href="#eq-obsinf" class="quarto-xref">Equation&nbsp;<span>3.6</span></a>. The <span class="math inline">\(\chi^2_1\)</span> distribution is the distribution of <span class="math inline">\(Z^2\)</span> if <span class="math inline">\(Z \sim N(0, 1)\)</span>.</p>
<p>The <strong>score test</strong> looks at the slope of the log likelihood at <span class="math inline">\(p_0\)</span>, rejecting <span class="math inline">\(H_0\)</span> if this slope is sufficiently far from zero <span class="citation" data-cites="rao1948large aitchison1958maximum">(<a href="references.html#ref-rao1948large" role="doc-biblioref">Rao 1948</a>; <a href="references.html#ref-aitchison1958maximum" role="doc-biblioref">Aitchison and Silvey 1958</a>)</span>. An example is shown in <a href="#fig-htests" class="quarto-xref">Figure&nbsp;<span>3.4</span></a>. It score test statistic is <span id="eq-scoretest"><span class="math display">\[
  S = \frac{U(p_0)^2}{\mathcal{I}(p_0)}
  = \frac{n (\hat{p} - p_0)^2}{p_0 (1 - p_0)}
  \approxsim \chi^2_1
\tag{3.14}\]</span></span> under <span class="math inline">\(H_0\)</span>, where <span class="math inline">\(\mathcal{I}(p_0)\)</span> is the expected information from <a href="#eq-expinf" class="quarto-xref">Equation&nbsp;<span>3.5</span></a>. The numerator of the score statistic is the same as for the Wald statistic in <a href="#eq-Waldtest" class="quarto-xref">Equation&nbsp;<span>3.13</span></a>, but the denominator uses the expected information at <span class="math inline">\(p_0\)</span> instead of the observed information at <span class="math inline">\(\hat{p}\)</span>. In score tests, it generally better to use the expected information than the observed information <span class="citation" data-cites="freedman2007can">(<a href="references.html#ref-freedman2007can" role="doc-biblioref">Freedman 2007</a>)</span>. The most important advantage of the score test is that it only needs the hypothesized null value <span class="math inline">\(p_0\)</span>, so it can be done without finding the maximum likelihood estimate <span class="math inline">\(\hat{p}\)</span>.</p>
<p>The <strong>likelihood ratio test</strong> looks at the vertical distance between <span class="math inline">\(\ell(\hat{p})\)</span> (which is the maximum) and <span class="math inline">\(\ell(p_0)\)</span>, rejecting <span class="math inline">\(H_0\)</span> if this distance is sufficiently large <span class="citation" data-cites="wilks1938large">Wilks (<a href="references.html#ref-wilks1938large" role="doc-biblioref">1938</a>)</span>.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> An example is shown in <a href="#fig-htests" class="quarto-xref">Figure&nbsp;<span>3.4</span></a>. The likelihood ratio test statistic is <span id="eq-lrt"><span class="math display">\[
  L = 2\big(\ell(\hat{p}) - \ell(p_0)\big)
  \approxsim \chi^2_1
\tag{3.15}\]</span></span> under <span class="math inline">\(H_0\)</span>. The <em>Neyman-Pearson lemma</em> <span class="citation" data-cites="neyman1933ix">(<a href="references.html#ref-neyman1933ix" role="doc-biblioref">Neyman and Pearson 1933</a>)</span> shows that the likelihood ratio test is the most powerful of all hypothesis test for comparing two hypotheses <span class="math inline">\(H_0: \ptrue = p_0\)</span> and <span class="math inline">\(H_1: \ptrue = p_1\)</span> at a fixed significance level.</p>
<div class="cell">
<div class="code-with-filename">
<details class="code-fold">
<summary>Code</summary>
<div class="code-with-filename-file">
<pre><strong>htests.R</strong></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Hypothesis tests based on the log likelihood</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># binomial log likelihood, score, and information functions</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>bin_loglik <span class="ot">&lt;-</span> <span class="cf">function</span>(p, <span class="at">k=</span><span class="dv">60</span>, <span class="at">n=</span><span class="dv">100</span>) {</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  k <span class="sc">*</span> <span class="fu">log</span>(p) <span class="sc">+</span> (n <span class="sc">-</span> k) <span class="sc">*</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> p)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>bin_score <span class="ot">&lt;-</span> <span class="cf">function</span>(p, <span class="at">k=</span><span class="dv">60</span>, <span class="at">n=</span><span class="dv">100</span>) {</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  k <span class="sc">/</span> p <span class="sc">-</span> (n <span class="sc">-</span> k) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> p)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>bin_information <span class="ot">&lt;-</span> <span class="cf">function</span>(p, <span class="at">k=</span><span class="dv">60</span>, <span class="at">n=</span><span class="dv">100</span>) {</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  k <span class="sc">/</span> p<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (n <span class="sc">-</span> k) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> p)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># plot showing Wald, score, and likelihood ratio tests</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.4</span>, <span class="fl">0.8</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p, <span class="fu">bin_loglik</span>(p), <span class="at">type =</span> <span class="st">"n"</span>,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(<span class="fl">0.40</span>, <span class="fl">0.70</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">72</span>, <span class="sc">-</span><span class="dv">66</span>),</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Tests of the null hypothesis p = 0.5"</span>,</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"p"</span>, <span class="at">ylab =</span> <span class="st">"ln L(p)"</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(p, <span class="fu">bin_loglik</span>(p))</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.6</span>), <span class="at">lty =</span> <span class="st">"dotted"</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">c</span>(<span class="fu">bin_loglik</span>(<span class="fl">0.5</span>), <span class="fu">bin_loglik</span>(<span class="fl">0.6</span>)), <span class="at">lty =</span> <span class="st">"dashed"</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="fu">bin_loglik</span>(<span class="fl">0.5</span>) <span class="sc">-</span> <span class="fu">bin_score</span>(<span class="fl">0.5</span>) <span class="sc">*</span> <span class="fl">0.5</span>, <span class="at">b =</span> <span class="fu">bin_score</span>(<span class="fl">0.5</span>),</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">"darkgray"</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.6</span>), <span class="fu">c</span>(<span class="sc">-</span><span class="fl">67.05</span>, <span class="sc">-</span><span class="dv">67</span>),</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">c</span>(<span class="fu">expression</span>(p[<span class="dv">0</span>]), <span class="fu">expression</span>(<span class="fu">hat</span>(p))))</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">0.55</span>, <span class="sc">-</span><span class="fl">70.7</span>, <span class="at">labels =</span> <span class="st">"Wald test"</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="fl">0.5</span>, <span class="sc">-</span><span class="fl">70.5</span>, <span class="fl">0.6</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> <span class="fl">0.1</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="fl">0.475</span>, <span class="fu">bin_loglik</span>(<span class="fl">0.5</span>), <span class="at">y1 =</span> <span class="fu">bin_loglik</span>(<span class="fl">0.6</span>),</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>       <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> <span class="fl">0.1</span>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">0.45</span>, <span class="sc">-</span><span class="fl">68.3</span>, <span class="at">labels =</span> <span class="st">"LRT"</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="co"># The slope is the tangent of the angle to the x-axis.</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="co"># We also must account for the different scales on the x- and y-axes.</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.3 / 6 is xdist / ydist (see xlim and ylim above)</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>score_angle <span class="ot">&lt;-</span> <span class="fu">atan</span>(<span class="fu">bin_score</span>(<span class="fl">0.5</span>) <span class="sc">*</span> <span class="fl">0.3</span> <span class="sc">/</span> <span class="dv">6</span>)</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>angles <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, score_angle, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>score_x <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="sc">+</span> <span class="fl">0.04</span> <span class="sc">*</span> <span class="fu">cos</span>(angles)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>score_y <span class="ot">&lt;-</span> <span class="fu">bin_loglik</span>(<span class="fl">0.5</span>) <span class="sc">+</span> <span class="fl">0.04</span> <span class="sc">*</span> (<span class="dv">6</span> <span class="sc">/</span> <span class="fl">0.3</span>) <span class="sc">*</span> <span class="fu">sin</span>(angles)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(score_x, score_y)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">0.56</span>, <span class="sc">-</span><span class="fl">68.8</span>, <span class="st">"Score test"</span>)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(score_x[<span class="dv">2</span>], score_y[<span class="dv">2</span>], score_x[<span class="dv">1</span>], score_y[<span class="dv">1</span>], <span class="at">length =</span> <span class="fl">0.1</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="fu">rev</span>(score_x)[<span class="dv">2</span>], <span class="fu">rev</span>(score_y)[<span class="dv">2</span>], <span class="fu">rev</span>(score_x)[<span class="dv">1</span>], <span class="fu">rev</span>(score_y)[<span class="dv">1</span>],</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>       <span class="at">length =</span> <span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell-output-display">
<div id="fig-htests" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-htests-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="mlestimation_files/figure-html/fig-htests-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;3.4: Binomial log likelihood function for x = 60 and n = 100. The null value of p is p_0 = 0.5 and the maximum likelihood estimate is \hat{p} = 0.6."><img src="mlestimation_files/figure-html/fig-htests-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-htests-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: Binomial log likelihood function for <span class="math inline">\(x = 60\)</span> and <span class="math inline">\(n = 100\)</span>. The null value of <span class="math inline">\(p\)</span> is <span class="math inline">\(p_0 = 0.5\)</span> and the maximum likelihood estimate is <span class="math inline">\(\hat{p} = 0.6\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="critical-values-and-p-values" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="critical-values-and-p-values"><span class="header-section-number">3.3.3</span> Critical values and p-values</h3>
<p>The Neyman-Pearson approach to hypothesis testing fixes the significance level <span class="math inline">\(\alpha\)</span> before calculating the test statistic and deciding whether to reject <span class="math inline">\(H_0\)</span>.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> The decision to reject the null hypothesis depends on the value of the test statistic, which is compared to a <strong>critical value</strong> calculated based on the distribution of the test statistic under <span class="math inline">\(H_0\)</span>. If <span class="math inline">\(Z \sim N(0, 1)\)</span> under <span class="math inline">\(H_0\)</span> <span class="math display">\[
  \Pr\big(|Z| \geq z_{1 - \frac{\alpha}{2}} \given{} H_0 \text{ true}\big)
  = 1 - \alpha.
\]</span> Because <span class="math inline">\(Z^2 \sim \chi^2_1\)</span> when <span class="math inline">\(Z \sim N(0, 1)\)</span>, this is equivalent to <span class="math display">\[
  \Pr\big(Z^2 \geq z_{1 - \frac{\alpha}{2}}^2 \given{} H_0 \text{ true}\big)
  = 1 - \alpha.
\]</span> In the Wald, score, and likelihood ratio tests above, <span class="math inline">\(H_0\)</span> is rejected if the test statistic is larger than the critical value <span class="math inline">\(z_{1 - \frac{\alpha}{2}}^2\)</span>. For <span class="math inline">\(\alpha = 0.05\)</span>, we have <span class="math inline">\(z_{0.975} \approx 1.96\)</span> so critical value for the <span class="math inline">\(\chi^2_1\)</span> distribution is <span class="math inline">\(1.96^2 \approx 3.84\)</span>. The test statistic and critical value in a hypothesis test are analogous to the clinical measurement and cutoff in a diagnostic test.</p>
<p>Instead of making a binary decision, it is more informative to calculate a measure of the evidence against <span class="math inline">\(H_0\)</span>. The <strong>p-value</strong> for a given test statistic is the lowest value of <span class="math inline">\(\alpha\)</span> at which the test would still fail to reject <span class="math inline">\(H_0\)</span>. A hypothesis test with significance level <span class="math inline">\(\alpha\)</span> rejects <span class="math inline">\(H_0\)</span> if the p-value is <span class="math inline">\(\leq \alpha\)</span>. For the Wald, score, or likelihood ratio tests above, <span class="math display">\[
  \text{p-value} = 1 - F_{\chi^2_1}(\text{test statistic})
\]</span> where <span class="math inline">\(F_{\chi^2_1}\)</span> is the CDF of the <span class="math inline">\(\chi^2_1\)</span> distribution If we think of the test statistic as the clinical measurement underlying a diagnostic test, the p-value equals <span class="math inline">\(1 - \text{spec}_\text{max}\)</span> where <span class="math inline">\(\text{spec}_\text{max}\)</span> is the highest specificity under which we would still get a positive test (i.e., reject <span class="math inline">\(H_0\)</span>).</p>
</section>
</section>
<section id="confidence-intervals" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="confidence-intervals"><span class="header-section-number">3.4</span> Confidence intervals</h2>
<p>A p-value is more informative than a binary decision whether to reject <span class="math inline">\(H_0\)</span>, but it is still more useful to know what values of <span class="math inline">\(p\)</span> are plausibly consistent with the data we observed <span class="citation" data-cites="rothman1978show">(<a href="references.html#ref-rothman1978show" role="doc-biblioref">Rothman 1978</a>)</span>. The <span class="math inline">\(1 - \alpha\)</span> <strong>confidence interval</strong> for <span class="math inline">\(\ptrue\)</span> is the set of all possible null values <span class="math inline">\(p_0\)</span> such that we would fail to reject <span class="math inline">\(H_0: \ptrue = p_0\)</span> in a hypothesis test with significance level <span class="math inline">\(\alpha\)</span>. The endpoints of the confidence interval are called are called <em>confidence limits</em>. Just as different clinical measurements lead to different diagnostic tests, different hypothesis tests lead to different confidence intervals.</p>
<p>If we calculate a confidence interval many times with independent data sets, the <span class="math inline">\(1 - \alpha\)</span> confidence interval should contain <span class="math inline">\(\ptrue\)</span> with probability <span class="math inline">\(1 - \alpha\)</span>. The actual probability that the confidence interval contains <span class="math inline">\(\ptrue\)</span> is called the <strong>coverage probability</strong>. A good confidence interval should have a coverage probability close to <span class="math inline">\(1 - \alpha\)</span> while being as narrow as possible. The Wald, score, and likelihood ratio tests from <a href="#sec-wslr" class="quarto-xref"><span>Section 3.3.2</span></a> are large-sample tests because they rely on consistency and asymptotic normality of the maximum likelihood estimate <span class="math inline">\(\hat{p}\)</span>. All three tests can be inverted to produce confidence intervals that perform well in large samples. In smaller samples, the score and likelihood ratio confidence intervals often have better coverage probability and width than the Wald confidence interval <span class="citation" data-cites="agresti1998approximate brown2001interval">(<a href="references.html#ref-agresti1998approximate" role="doc-biblioref">Agresti and Coull 1998</a>; <a href="references.html#ref-brown2001interval" role="doc-biblioref">Brown, Cai, and DasGupta 2001</a>)</span>.</p>
<section id="wald-confidence-intervals-and-the-delta-method" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="wald-confidence-intervals-and-the-delta-method"><span class="header-section-number">3.4.1</span> Wald confidence intervals and the delta method</h3>
<p>The Wald confidence limits come from solving the equation <span id="eq-WaldCIeq"><span class="math display">\[
  \frac{(\hat{p} - p)^2}{\hat{p} (1 - \hat{p}) / n}
  = z_{1 - \frac{\alpha}{2}}^2.
\tag{3.16}\]</span></span> for <span class="math inline">\(p\)</span>, which gives us <span id="eq-WaldCI"><span class="math display">\[
  \hat{p} \pm z_{1 - \frac{\alpha}{2}} \sqrt{\frac{\hat{p} (1 - \hat{p})}{n}}.
\tag{3.17}\]</span></span> The coverage probabilities of Wald confidence intervals can be much lower than <span class="math inline">\(1 - \alpha\)</span>, especially when <span class="math inline">\(p_\true\)</span> is close to zero or one <span class="citation" data-cites="agresti1998approximate brown2001interval">(<a href="references.html#ref-agresti1998approximate" role="doc-biblioref">Agresti and Coull 1998</a>; <a href="references.html#ref-brown2001interval" role="doc-biblioref">Brown, Cai, and DasGupta 2001</a>)</span>.</p>
<p>Another problem with the Wald confidence interval for <span class="math inline">\(\ptrue\)</span> is that it can have bounds outside <span class="math inline">\([0, 1]\)</span>. One way to avoid this is to calculate confidence limits for a transformation of <span class="math inline">\(\hat{p}\)</span> using the <strong>delta method</strong>. A good transformation <span class="math inline">\(g(p)\)</span> should have continuous first derivatives and be strictly increasing or decreasing, so each value of <span class="math inline">\(g(p)\)</span> corresponds to a single value of <span class="math inline">\(p\)</span> (i.e., <span class="math inline">\(g\)</span> is <em>one-to-one</em>). The delta method derives the approximate normal distribution <span class="math inline">\(g(\hat{p})\)</span> using the approximation <span class="math display">\[
  g(\hat{p}) \approx g(\ptrue) + g'(\ptrue) (\hat{p} - \ptrue).
\]</span> where <span class="math inline">\(g'(\ptrue)\)</span> is the slope of <span class="math inline">\(g\)</span> at <span class="math inline">\(\ptrue\)</span>. An example of this approximation is shown in <a href="#fig-delta" class="quarto-xref">Figure&nbsp;<span>3.5</span></a>. The key insight is that <span class="math display">\[
  \Var[g(\hat{p})] \approx g'(\ptrue)^2 \Var(\hat{p}),
\]</span> which is a generalization of the fact that <span class="math inline">\(\Var(c \hat{p}) = c^2 \Var(\hat{p})\)</span> for any constant <span class="math inline">\(c\)</span>. If <span class="math inline">\(\hat{p}\)</span> has an approximate <span class="math inline">\(N\bigl(\ptrue, \Var(\hat{p})\bigr)\)</span> distribution in large samples, then <span class="math display">\[
  g(\hat{p}) \approxsim N\bigl(g(\ptrue), g'(\ptrue)^2 \Var(\hat{p})\bigr).
\]</span> in large samples. Because our estimator <span class="math inline">\(\hat{p}\)</span> is consistent, we can replace the unknown <span class="math inline">\(\ptrue\)</span> with <span class="math inline">\(\hat{p}\)</span>. Because <span class="math inline">\(g\)</span> is one-to-one, we can calculate confidence limits for <span class="math inline">\(\ptrue\)</span> using the confidence limits for <span class="math inline">\(g(\ptrue)\)</span>.</p>
<div class="cell">
<div class="code-with-filename">
<details class="code-fold">
<summary>Code</summary>
<div class="code-with-filename-file">
<pre><strong>delta.R</strong></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Approximation used by the delta method</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.02</span>, <span class="fl">0.98</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>logit <span class="ot">&lt;-</span> <span class="cf">function</span>(p) <span class="fu">log</span>(p) <span class="sc">-</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> p)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p, <span class="fu">logit</span>(p), <span class="at">type =</span> <span class="st">"n"</span>,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"p"</span>, <span class="at">ylab =</span> <span class="st">"logit(p)"</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(p, <span class="fu">logit</span>(p))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fl">0.6</span>, <span class="fu">logit</span>(<span class="fl">0.6</span>))</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">logit</span>(<span class="fl">0.6</span>) <span class="sc">-</span> <span class="fl">2.5</span>, <span class="dv">1</span> <span class="sc">/</span> <span class="fl">0.24</span>, <span class="at">lty =</span> <span class="st">"dashed"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">0.6</span>, <span class="sc">-</span><span class="dv">1</span>,</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"logit(p) - "</span>, <span class="fu">logit</span>(<span class="fl">0.6</span>) <span class="sc">%~~%</span> logit,</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>                               <span class="st">"'(0.6) (p - 0.6)"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell-output-display">
<div id="fig-delta" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-delta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="mlestimation_files/figure-html/fig-delta-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;3.5: The approximation used by the delta method using the logistic transformation for a binomial confidence interval near \hat{p} = 0.6. The black curve is \logit(p), and the dashed line shows the tangent line at p = 0.6."><img src="mlestimation_files/figure-html/fig-delta-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-delta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: The approximation used by the delta method using the logistic transformation for a binomial confidence interval near <span class="math inline">\(\hat{p} = 0.6\)</span>. The black curve is <span class="math inline">\(\logit(p)\)</span>, and the dashed line shows the tangent line at <span class="math inline">\(p = 0.6\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<!-- \begin{figure}
    \centering
    \includegraphics[width = \textwidth]{Delta.pdf}
    \caption{
      The approximation used by the delta method using the logistic transformation for a binomial confidence interval near $p_\true = 0.6$.
      The black curve is $\logit(p)$, and the dashed line shows the tangent line at $p = 0.6$.}
    \label{fig:delta}
\end{figure} -->
<p>A widely used transformation for probabilities is the <strong>logit transformation</strong> <span class="math display">\[
  \logit(p) = \ln\Bigl(\frac{p}{1 - p}\Bigr).
\]</span> The <strong>odds</strong> corresponding to the probability <span class="math inline">\(p\)</span> is <span class="math inline">\(\frac{p}{1 - p}\)</span>, so the logit is the natural logarithm of the odds. The logit transformation maps the interval <span class="math inline">\((0, 1)\)</span> onto all of <span class="math inline">\(\mathbb{R}\)</span>:</p>
<ul>
<li>As <span class="math inline">\(p \rightarrow 0\)</span>, the odds <span class="math inline">\(p / (1 - p) \rightarrow 0\)</span> and <span class="math inline">\(\logit(p) \rightarrow -\infty\)</span>.</li>
<li>When <span class="math inline">\(p = 1 / 2\)</span>, the odds <span class="math inline">\(p / (1 - p) = 1\)</span> and <span class="math inline">\(\logit(p) = 0\)</span>.</li>
<li>As <span class="math inline">\(p \rightarrow 1\)</span>, the odds <span class="math inline">\(p / (1 - p) \rightarrow \infty\)</span> and <span class="math inline">\(\logit(p) \rightarrow \infty\)</span>.</li>
</ul>
<p>To use the delta method, we need to calculate the derivative of <span class="math inline">\(\logit(p)\)</span>. By the chain rule, <span class="math display">\[
  \logit'(p)
  = \frac{1 - p}{p} \frac{1}{(1 - p)^2}
  = \frac{1}{p (1 - p)},
\]</span> which is continuous and strictly positive for all <span class="math inline">\(p \in (0, 1)\)</span>. By the delta method, the variance of <span class="math inline">\(\logit(\hat{p})\)</span> is approximately <span class="math display">\[
  \logit'(\ptrue)^2 \frac{\ptrue (1 - \ptrue)}{n}
  = \frac{1}{\ptrue^2 (1 - \ptrue)^2} \frac{\ptrue (1 - \ptrue)}{n}
  = \frac{1}{n \ptrue (1 - \ptrue)}.
\]</span> When we replace the unknown <span class="math inline">\(\ptrue\)</span> with our MLE <span class="math inline">\(\hat{p}\)</span>, we get the following confidence limits for <span class="math inline">\(\logit(\ptrue)\)</span>: <span class="math display">\[
  \logit(\hat{p}) \pm z_{1 - \frac{\alpha}{2}} \sqrt{\frac{1}{n \hat{p} (1 - \hat{p})}}.
\]</span> To get confidence limits for <span class="math inline">\(\ptrue\)</span>, we use the inverse function for the logit, which is <span class="math display">\[
  \expit(v) = \frac{e^v}{1 + e^v} = \frac{1}{1 + e^{-v}}.
\]</span> This is called the <em>logistic function</em>. If the confidence limits for <span class="math inline">\(\logit(\ptrue)\)</span> are <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, then the confidence limits for <span class="math inline">\(\ptrue\)</span> are <span class="math inline">\(\expit(a)\)</span> and <span class="math inline">\(\expit(b)\)</span>. These are guaranteed to be in <span class="math inline">\((0, 1)\)</span> because <span class="math inline">\(\expit(v) \in (0, 1)\)</span> for any <span class="math inline">\(v \in \mathbb{R}\)</span>. The logit-transformed confidence interval can have narrower width and a coverage probability closer to <span class="math inline">\(1 - \alpha\)</span> than the untransformed Wald confidence interval <span class="citation" data-cites="agresti2012categorical">(<a href="references.html#ref-agresti2012categorical" role="doc-biblioref">Agresti 2013</a>)</span>.</p>
</section>
<section id="score-wilson-confidence-intervals" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="score-wilson-confidence-intervals"><span class="header-section-number">3.4.2</span> Score (Wilson) confidence intervals</h3>
<p>The <strong>score</strong> or <strong>Wilson</strong> confidence limits come from solving the equation <span id="eq-scoreCIeq"><span class="math display">\[
  \frac{(\hat{p} - p)^2}{p (1 - p) / n}
  = z_{1 - \frac{\alpha}{2}}^2.
\tag{3.18}\]</span></span> for <span class="math inline">\(p\)</span> <span class="citation" data-cites="wilson1927probable">(<a href="references.html#ref-wilson1927probable" role="doc-biblioref">Wilson 1927</a>)</span>. This differs from <a href="#eq-WaldCIeq" class="quarto-xref">Equation&nbsp;<span>3.16</span></a> for the Wald confidence interval because it uses <span class="math inline">\(p\)</span> instead of <span class="math inline">\(\hat{p}\)</span> in the denominator. It is a quadratic equation in <span class="math inline">\(p\)</span>, so it has two solutions. The center of the resulting confidence interval is <span id="eq-ptilde"><span class="math display">\[
  \tilde{p}
  = \hat{p} \Bigg(\frac{n}{n + z_{1 - \frac{\alpha}{2}}^2}\Bigg)
    + \frac{1}{2} \Bigg(\frac{z_{1 - \frac{\alpha}{2}}^2}{n + z_{1 - \frac{\alpha}{2}}^2}\Bigg)
  = \frac{x + \frac{1}{2} z_{1 - \frac{\alpha}{2}}^2}{n + z_{1 - \frac{\alpha}{2}}^2},
\tag{3.19}\]</span></span> where <span class="math inline">\(x\)</span> is the number of diseased individuals in our sample. This is a weighted average of <span class="math inline">\(\hat{p}\)</span> and <span class="math inline">\(1 / 2\)</span> with weights proportional to <span class="math inline">\(n\)</span> and <span class="math inline">\(z_{1 - \frac{\alpha}{2}}^2\)</span>, respectively. The resulting confidence interval is <span class="math display">\[
  \tilde{p} \pm z_{1 - \frac{\alpha}{2}} \sqrt{\tilde{V}}
\]</span> where <span class="math display">\[
  \tilde{V}
  = \frac{\hat{p} (1 - \hat{p})}{n + z_{1 - \frac{\alpha}{2}}^2} \Bigg(\frac{n}{n + z_{1 - \frac{\alpha}{2}}^2}\Bigg)
    + \frac{\big(\frac{1}{2}\big)^2}{n + z_{1 - \frac{\alpha}{2}}^2} \Bigg(\frac{z_{1 - \frac{\alpha}{2}}^2}{n + z_{1 - \frac{\alpha}{2}}^2}\Bigg).
\]</span> This variance is a weighted average of the variances of sample proportions equal to <span class="math inline">\(\hat{p}\)</span> and <span class="math inline">\(1 / 2\)</span> with the same weights as in <span class="math inline">\(\tilde{p}\)</span> and with <span class="math inline">\(n + z_{1 - \frac{\alpha}{2}}^2\)</span> instead of <span class="math inline">\(n\)</span> in the denominator. Wilson confidence intervals are narrower than the corresponding Wald intervals, and they have coverage probabilities much closer to <span class="math inline">\(1 - \alpha\)</span> <span class="citation" data-cites="agresti1998approximate brown2001interval">(<a href="references.html#ref-agresti1998approximate" role="doc-biblioref">Agresti and Coull 1998</a>; <a href="references.html#ref-brown2001interval" role="doc-biblioref">Brown, Cai, and DasGupta 2001</a>)</span>.</p>
<p>The <em>Agresti-Coull confidence interval</em> is a simplification of the Wilson confidence interval that replaces <span class="math inline">\(\hat{p}\)</span> with <span class="math inline">\(\tilde{p}\)</span> in the Wald confidence interval to get the confidence limits <span class="math display">\[
  \tilde{p} \pm z_{1 - \frac{\alpha}{2}} \sqrt{\frac{\tilde{p} (1 - \tilde{p})}{n}}.
\]</span> Because <span class="math inline">\(z_{0.975} \approx 1.96\)</span>, we have <span class="math inline">\(\tilde{p} \approx \frac{k + 2}{n + 4}\)</span> for a 95% confidence interval. In this case, the Agresti-Coull interval is often implemented as follows: ``Add two successes and two failures and then use the Wald formula’’ <span class="citation" data-cites="agresti1998approximate">(<a href="references.html#ref-agresti1998approximate" role="doc-biblioref">Agresti and Coull 1998</a>)</span>. This interval is only slightly wider than the score confidence interval, and the two intervals are nearly identical for <span class="math inline">\(n &gt; 40\)</span> <span class="citation" data-cites="brown2001interval">(<a href="references.html#ref-brown2001interval" role="doc-biblioref">Brown, Cai, and DasGupta 2001</a>)</span>.</p>
<p>The likelihood ratio test can also be inverted to get confidence intervals, but these can only be calculated numerically. For the binomial model, the likelihood ratio and score confidence intervals are nearly identical <span class="citation" data-cites="agresti1998approximate brown2001interval">(<a href="references.html#ref-agresti1998approximate" role="doc-biblioref">Agresti and Coull 1998</a>; <a href="references.html#ref-brown2001interval" role="doc-biblioref">Brown, Cai, and DasGupta 2001</a>)</span>. The score intervals are more common in practice because they are easier to calculate.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">R</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>binconf.R</strong></pre>
</div>
<div class="sourceCode" id="cb8" data-filename="binconf.R"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Binomial confidence intervals</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># using BinomCI() function from the DescTools package</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DescTools)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu">BinomCI</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">method =</span> <span class="st">"wald"</span>)            <span class="co"># Wald confidence interval</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="fu">BinomCI</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">method =</span> <span class="st">"logit"</span>)           <span class="co"># logit-transformed Wald CI</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="fu">BinomCI</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">method =</span> <span class="st">"wilson"</span>)          <span class="co"># score CI (default)</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="fu">BinomCI</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">method =</span> <span class="st">"agresti-coull"</span>)   <span class="co"># Agresti-Coull CI</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="fu">BinomCI</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">method =</span> <span class="st">"lik"</span>)             <span class="co"># likelihood ratio CI</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># using binconf() function from the Hmisc package</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Hmisc)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="fu">binconf</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">method =</span> <span class="st">"asymptotic"</span>)  <span class="co"># Wald CI</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="fu">binconf</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">method =</span> <span class="st">"wilson"</span>)      <span class="co"># score CI (default)</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co"># using prop.test in base R (stats package)</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Wilson confidence interval with continuity correction by default</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co"># The continuity correction is not generally recommended. Like the exact CI,</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="co"># it can be too wide and have a coverage probability greater than 1 - \alpha.</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.test</span>(<span class="dv">15</span>, <span class="dv">22</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">prop.test</span>(<span class="dv">15</span>, <span class="dv">22</span>))</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.test</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">correct =</span> <span class="cn">FALSE</span>)      <span class="co"># score CI</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="co"># using binom.test (exact confidence interval)</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(<span class="dv">15</span>, <span class="dv">22</span>)    <span class="co"># same as binconf with method = "exact"</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">binom.test</span>(<span class="dv">15</span>, <span class="dv">22</span>))</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="co"># changing the confidence level (1 - alpha) to 80%</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co"># All are score (Wilson) confidence intervals by default.</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="fu">BinomCI</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">conf.level =</span> <span class="fl">0.8</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="fu">binconf</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">alpha =</span> <span class="fl">0.2</span>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.test</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">conf.level =</span> <span class="fl">0.8</span>, <span class="at">correct =</span> <span class="cn">FALSE</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co"># writing a function to get Wald confidence limits</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>bconf_wald <span class="ot">&lt;-</span> <span class="cf">function</span>(x, n, <span class="at">level=</span><span class="fl">0.95</span>) {</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>  <span class="co"># x is number of successes out of n trials</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>  p_hat <span class="ot">&lt;-</span> x <span class="sc">/</span> n</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>  alpha <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> level</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>  pvar <span class="ot">&lt;-</span> p_hat <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> p_hat) <span class="sc">/</span> n</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>  p_int <span class="ot">&lt;-</span> p_hat <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="sc">*</span> <span class="fu">qnorm</span>(<span class="dv">1</span> <span class="sc">-</span> alpha <span class="sc">/</span> <span class="dv">2</span>) <span class="sc">*</span> <span class="fu">sqrt</span>(pvar)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>  <span class="co"># return named vector (names do not need quotes)</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(<span class="at">point =</span> p_hat, <span class="at">lower =</span> p_int[<span class="dv">1</span>], <span class="at">upper =</span> p_int[<span class="dv">2</span>]))</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a><span class="fu">bconf_wald</span>(<span class="dv">15</span>, <span class="dv">22</span>)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a><span class="fu">bconf_wald</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">level =</span> <span class="fl">0.80</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="small-sample-estimation" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="small-sample-estimation"><span class="header-section-number">3.5</span> Small-sample estimation*</h2>
<p>Maximum likelihood estimates are consistent, asymptotically normal, and asymptotically efficient. However, they are not guaranteed to perform well in any finite sample. For a sample of <span class="math inline">\(n\)</span> independent Bernoulli(<span class="math inline">\(p\)</span>) random variables, the sum has a binomial(<span class="math inline">\(n, p\)</span>) distribution and this can be used to find the finite-sample distribution of the sample mean. This distribution can be used directly to calculate point estimates, p-values, and confidence limits.</p>
<p>Confidence limits calculated using the finite-sample distribution of a test statistic under <span class="math inline">\(H_0\)</span> are called <strong>exact confidence limits</strong>. They can often be constructed to have a coverage probability of at least <span class="math inline">\(1 - \alpha\)</span>. However, their coverage probabilities are often higher than <span class="math inline">\(1 - \alpha\)</span>, and they can be much wider than approximate <span class="math inline">\(1 - \alpha\)</span> confidence intervals for the same parameter <span class="citation" data-cites="agresti1998approximate">(<a href="references.html#ref-agresti1998approximate" role="doc-biblioref">Agresti and Coull 1998</a>)</span>.</p>
<p>If the finite-sample distribution of the test statistic is not known exactly, it is possible to calculate point estimates, p-values, or confidence limits using simulations. This is the basic idea behind the <em>bootstrap</em> <span class="citation" data-cites="efron1994introduction">(<a href="references.html#ref-efron1994introduction" role="doc-biblioref">Efron and Tibshirani 1994</a>)</span> and <em>Monte Carlo methods</em> <span class="citation" data-cites="robert2004monte">(<a href="references.html#ref-robert2004monte" role="doc-biblioref">Robert and Casella 2004</a>)</span>.</p>
<section id="median-unbiased-estimate" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="median-unbiased-estimate"><span class="header-section-number">3.5.1</span> Median unbiased estimate</h3>
<p>The <strong>median unbiased estimate</strong> of <span class="math inline">\(\ptrue\)</span> is the value of <span class="math inline">\(p\)</span> that makes <span class="math display">\[
  \Pr\nolimits_p(X &lt; x) = \Pr\nolimits_p(X &gt; x)
\]</span> where we use the subscript <span class="math inline">\(p\)</span> to indicate that these probabilities are calculated assuming <span class="math inline">\(\ptrue = p\)</span>. If <span class="math inline">\(p_\text{med}\)</span> is the median unbiased estimate, then <span class="math display">\[
  \sum_{k = 0}^{x - 1} \binom{n}{k} p_\text{med}^k \big(1 - p_\text{med}\big)^{n - k}
  + \frac{1}{2} \binom{n}{x} p_\text{med}^k \big(1 - p_\text{med}\big)^{n - x}
  = \frac{1}{2},
\]</span> and <span class="math display">\[
  \frac{1}{2} \binom{n}{x} p_\text{med}^x \big(1 - p_\text{med}\big)^{n - x}
  + \sum_{k = x + 1}^n \binom{n}{k} p_\text{med}^k \big(1 - p_\text{med}\big)^{n - k}
  = \frac{1}{2}.
\]</span> The median of the distribution of <span class="math inline">\(p_\text{med}\)</span> is always <span class="math inline">\(\ptrue\)</span> <span class="citation" data-cites="birnbaum1964median">(<a href="references.html#ref-birnbaum1964median" role="doc-biblioref">Birnbaum 1964</a>)</span>, which is a slightly different notion of unbiasedness than the unbiasedness of <span class="math inline">\(\hat{p}\)</span> where <span class="math inline">\(\E(\hat{p}) = \ptrue\)</span>.</p>
</section>
<section id="exact-clopper-pearson-and-mid-p-confidence-intervals" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="exact-clopper-pearson-and-mid-p-confidence-intervals"><span class="header-section-number">3.5.2</span> Exact (Clopper-Pearson) and mid-p confidence intervals</h3>
<p>The <strong>exact</strong> or <strong>Clopper-Pearson</strong> confidence limits for <span class="math inline">\(\ptrue\)</span> use the finite-sample distribution of the sample mean <span class="math inline">\(\hat{p}\)</span> <span class="citation" data-cites="clopper1934use">Clopper and Pearson (<a href="references.html#ref-clopper1934use" role="doc-biblioref">1934</a>)</span>. When <span class="math inline">\(x &gt; 0\)</span>, the lower <span class="math inline">\(1 - \alpha\)</span> confidence limit is the solution to <span id="eq-uppertail"><span class="math display">\[
  \sum_{k = x}^n \binom{n}{k} p_\text{lower}^k (1 - p_\text{lower})^{n - k}
  = \frac{\alpha}{2},
\tag{3.20}\]</span></span> so the <em>upper tail</em> of the binomial(<span class="math inline">\(n\)</span>, <span class="math inline">\(p_\text{lower}\)</span>) distribution has probability <span class="math inline">\(\alpha / 2\)</span>. When <span class="math inline">\(x = 0\)</span>, we set <span class="math inline">\(p_\text{lower} = 0\)</span>. When <span class="math inline">\(x &lt; n\)</span>, the upper confidence limit is the solution to <span id="eq-lowertail"><span class="math display">\[
  \sum_{k = 0}^x \binom{n}{k} p_\text{upper}^k (1 - p_\text{upper})^{n - k}
  = \frac{\alpha}{2},
\tag{3.21}\]</span></span> so the <em>lower tail</em> of the binomial(<span class="math inline">\(n\)</span>, <span class="math inline">\(p_\text{upper}\)</span>) distribution has probability <span class="math inline">\(\alpha / 2\)</span>. When <span class="math inline">\(x = n\)</span>, we set <span class="math inline">\(p_\text{upper} = 1\)</span>. This interval is guaranteed to have a coverage probability of at least <span class="math inline">\(1 - \alpha\)</span>, but the price for this is that it is always wider than the Wald and Wilson confidence intervals <span class="citation" data-cites="agresti1998approximate brown2001interval">(<a href="references.html#ref-agresti1998approximate" role="doc-biblioref">Agresti and Coull 1998</a>; <a href="references.html#ref-brown2001interval" role="doc-biblioref">Brown, Cai, and DasGupta 2001</a>)</span>. In general, the score or likelihood ratio confidence intervals have better combinations of coverage probability and width.</p>
<p>To make exact confidence limits less conservative, we can include only <span class="math inline">\(\frac{1}{2} \Pr(X = x)\)</span> instead of <span class="math inline">\(\Pr(X = x)\)</span> in the calculation of the tail probabilities in <a href="#eq-lowertail" class="quarto-xref">Equation&nbsp;<span>3.21</span></a> and <a href="#eq-uppertail" class="quarto-xref">Equation&nbsp;<span>3.20</span></a>. The resulting confidence intervals are called <strong>mid-p exact confidence intervals</strong> <span class="citation" data-cites="lancaster1961significance">(<a href="references.html#ref-lancaster1961significance" role="doc-biblioref">Lancaster 1961, berry1995mid</a>)</span>. The lower <span class="math inline">\(1 - \alpha\)</span> mid-p exact confidence limit is the solution to <span class="math display">\[
  \frac{1}{2} \binom{n}{x} p_\text{lower}^x (1 - p_\text{lower})^{n - x}
    + \sum_{k = x + 1}^n \binom{n}{k} p_\text{lower}^k (1 - p_\text{lower})^{n - k}
  = \frac{\alpha}{2}.
\]</span> and the upper limit is the solution to <span class="math display">\[
  \sum_{k = 0}^{x - 1} \binom{n}{k} p_\text{upper}^k (1 - p_\text{upper})^{n - k}
    + \frac{1}{2} \binom{n}{x} p_\text{upper}^x (1 - p_\text{upper})^{n - x}
  = \frac{\alpha}{2}.
\]</span> The mid-p exact confidence limits are have good combinations of coverage probability and width as well as good perfomance in small samples <span class="citation" data-cites="brown2001interval">(<a href="references.html#ref-brown2001interval" role="doc-biblioref">Brown, Cai, and DasGupta 2001</a>)</span>.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true">R</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>binconf-smallsamp.R</strong></pre>
</div>
<div class="sourceCode" id="cb9" data-filename="binconf-smallsamp.R"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Small-sample binomial point and interval estimates</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># median unbiased estimate</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>medp.binom <span class="ot">&lt;-</span> <span class="cf">function</span>(k, n) {</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># k = number of successes, n = number of trials</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># binomial lower tail probability</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  lower.tail <span class="ot">&lt;-</span> <span class="cf">function</span>(p) <span class="fu">pbinom</span>(k, n, p) <span class="sc">-</span> <span class="fu">dbinom</span>(k, n, p) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># median unbiased estimate</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  med <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(<span class="cf">function</span>(p) <span class="fu">lower.tail</span>(p) <span class="sc">-</span> <span class="dv">1</span> <span class="sc">/</span> <span class="dv">2</span>, <span class="at">interval =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(med<span class="sc">$</span>root)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="fu">medp.binom</span>(<span class="dv">15</span>, <span class="dv">22</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># exact (Clopper-Pearson) confidence intervals</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(<span class="dv">15</span>, <span class="dv">22</span>)                              <span class="co"># base R (stats)</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">binom.test</span>(<span class="dv">15</span>, <span class="dv">22</span>))</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">conf.level =</span> <span class="fl">0.8</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Hmisc)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="fu">binconf</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">method =</span> <span class="st">"exact"</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="fu">binconf</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">method =</span> <span class="st">"exact"</span>, <span class="at">alpha =</span> <span class="fl">0.2</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DescTools)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="fu">BinomCI</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">method =</span> <span class="st">"clopper-pearson"</span>)     <span class="co"># exact CI</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="fu">BinomCI</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="at">method =</span> <span class="st">"midp"</span>)                <span class="co"># mid-p exact CI</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-agresti2012categorical" class="csl-entry" role="listitem">
Agresti, Alan. 2013. <em>Categorical Data Analysis</em>. Third. Vol. 792. John Wiley &amp; Sons.
</div>
<div id="ref-agresti1998approximate" class="csl-entry" role="listitem">
Agresti, Alan, and Brent A Coull. 1998. <span>“Approximate Is Better Than <span>‘Exact’</span> for Interval Estimation of Binomial Proportions.”</span> <em>The American Statistician</em> 52 (2): 119–26.
</div>
<div id="ref-aitchison1958maximum" class="csl-entry" role="listitem">
Aitchison, John, and SD Silvey. 1958. <span>“Maximum-Likelihood Estimation of Parameters Subject to Restraints.”</span> <em>The Annals of Mathematical Statistics</em> 29: 813–28.
</div>
<div id="ref-birnbaum1964median" class="csl-entry" role="listitem">
Birnbaum, Allan. 1964. <span>“Median-Unbiased Estimators.”</span> <em>Bulletin of Mathematical Statistics</em> 11: 25–34.
</div>
<div id="ref-boos2013essential" class="csl-entry" role="listitem">
Boos, Dennis D, and Leonard A Stefanski. 2013. <em>Essential Statistical Inference</em>. Springer.
</div>
<div id="ref-brown2001interval" class="csl-entry" role="listitem">
Brown, Lawrence D, T Tony Cai, and Anirban DasGupta. 2001. <span>“Interval Estimation for a Binomial Proportion.”</span> <em>Statistical Science</em> 16 (2): 101–17.
</div>
<div id="ref-clopper1934use" class="csl-entry" role="listitem">
Clopper, Charles J, and Egon S Pearson. 1934. <span>“The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial.”</span> <em>Biometrika</em> 26 (4): 404–13.
</div>
<div id="ref-cohen1984florence" class="csl-entry" role="listitem">
Cohen, I Bernard. 1984. <span>“Florence Nightingale.”</span> <em>Scientific American</em> 250 (3): 128–37.
</div>
<div id="ref-cramer1946mathematical" class="csl-entry" role="listitem">
Cramér, Harald. 1946. <em>Mathematical Methods of Statistics</em>. Princeton University Press.
</div>
<div id="ref-diamond1983clinical" class="csl-entry" role="listitem">
Diamond, George A, and James S Forrester. 1983. <span>“Clinical Trials and Statistical Verdicts: Probable Grounds for Appeal.”</span> <em>Annals of Internal Medicine</em> 98 (3): 385–94.
</div>
<div id="ref-efron1978assessing" class="csl-entry" role="listitem">
Efron, Bradley, and David V Hinkley. 1978. <span>“Assessing the Accuracy of the Maximum Likelihood Estimator: Observed Versus Expected <span>F</span>isher Information.”</span> <em>Biometrika</em> 65 (3): 457–83.
</div>
<div id="ref-efron1994introduction" class="csl-entry" role="listitem">
Efron, Bradley, and Robert J Tibshirani. 1994. <em>An Introduction to the Bootstrap</em>. Chapman &amp; Hall/CRC.
</div>
<div id="ref-freedman2007can" class="csl-entry" role="listitem">
Freedman, David A. 2007. <span>“How Can the Score Test Be Inconsistent?”</span> <em>The American Statistician</em> 61 (4): 291–95.
</div>
<div id="ref-kenward1998likelihood" class="csl-entry" role="listitem">
Kenward, Michael G, and Geert Molenberghs. 1998. <span>“Likelihood Based Frequentist Inference When Data Are Missing at Random.”</span> <em>Statistical Science</em> 13 (3): 236–47.
</div>
<div id="ref-lancaster1961significance" class="csl-entry" role="listitem">
Lancaster, H Oliver. 1961. <span>“Significance Tests in Discrete Distributions.”</span> <em>Journal of the American Statistical Association</em> 56 (294): 223–34.
</div>
<div id="ref-neyman1933ix" class="csl-entry" role="listitem">
Neyman, Jerzy, and Egon Sharpe Pearson. 1933. <span>“On the Problem of the Most Efficient Tests of Statistical Hypotheses.”</span> <em>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character</em> 231 (694-706): 289–337.
</div>
<div id="ref-rao1945information" class="csl-entry" role="listitem">
Rao, C Radhakrishna. 1945. <span>“Information and Accuracy Attainable in the Estimation of Statistical Parameters.”</span> <em>Bulletin of the Calcutta Mathematical Society</em> 37 (3): 81–91.
</div>
<div id="ref-rao1948large" class="csl-entry" role="listitem">
———. 1948. <span>“Large Sample Tests of Statistical Hypotheses Concerning Several Parameters with Applications to Problems of Estimation.”</span> In <em>Mathematical Proceedings of the Cambridge Philosophical Society</em>, 44:50–57. Cambridge University Press.
</div>
<div id="ref-reid2003asymptotics" class="csl-entry" role="listitem">
Reid, Nancy. 2003. <span>“Asymptotics and the Theory of Inference.”</span> <em>The Annals of Statistics</em> 31 (6): 1695–2095.
</div>
<div id="ref-robert2004monte" class="csl-entry" role="listitem">
Robert, Christian P, and George Casella. 2004. <em>Monte Carlo Statistical Methods</em>. Second edition. Springer.
</div>
<div id="ref-rothman1978show" class="csl-entry" role="listitem">
Rothman, Kenneth J. 1978. <span>“A Show of Confidence.”</span> <em>New England Journal of Medicine</em> 299 (24): 1362–63.
</div>
<div id="ref-tukey1962future" class="csl-entry" role="listitem">
Tukey, John W. 1962. <span>“The Future of Data Analysis.”</span> <em>The Annals of Mathematical Statistics</em> 33 (1): 1–67.
</div>
<div id="ref-wald1943tests" class="csl-entry" role="listitem">
Wald, Abraham. 1943. <span>“Tests of Statistical Hypotheses Concerning Several Parameters When the Number of Observations Is Large.”</span> <em>Transactions of the American Mathematical Society</em> 54 (3): 426–82.
</div>
<div id="ref-wilks1938large" class="csl-entry" role="listitem">
Wilks, Samuel S. 1938. <span>“The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses.”</span> <em>The Annals of Mathematical Statistics</em> 9 (1): 60–62.
</div>
<div id="ref-wilson1927probable" class="csl-entry" role="listitem">
Wilson, Edwin B. 1927. <span>“Probable Inference, the Law of Succession, and Statistical Inference.”</span> <em>Journal of the American Statistical Association</em> 22 (158): 209–12.
</div>
<div id="ref-winkelstein2009florence" class="csl-entry" role="listitem">
Winkelstein Jr, Warren. 2009. <span>“Florence Nightingale: Founder of Modern Nursing and Hospital Epidemiology.”</span> <em>Epidemiology</em> 20 (2): 311.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p> <a href="https://en.wikipedia.org/wiki/John_Tukey">John Tukey</a> (1915-2000) was an American mathematician and statistician who worked at Bell Labs and Princeton University. He developed the box plot, Tukey’s range test for multiple comparisons, and the <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">fast Fourier transform</a>. In 1947, he coined the term “bit” as shorthand for “binary digit”.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p> She was elected a member of the Royal Statistical Society in 1859, where she was the first woman. In 1860, she founded the world’s first modern nursing school at St.&nbsp;Thomas Hospital in London.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p> For finite <span class="math inline">\(N\)</span>, <span class="math inline">\(X\)</span> actually has a <em>hypergeometric distribution</em> because the test results are not exactly independent. If the first person in our sample has disease, the probability that the next person we sample has disease is slightly less than <span class="math inline">\(p\)</span>. If the first person in our sample does not have disease, the probability that the next person we sample has disease is slightly greater than <span class="math inline">\(p\)</span>. When <span class="math inline">\(N \gg n\)</span>, this hypergeometric distribution is approximately binomial(<span class="math inline">\(n\)</span>, <span class="math inline">\(p\)</span>).<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p> <a href="https://en.wikipedia.org/wiki/E_(mathematical_constant)">Euler’s number</a> <span class="math inline">\(e\)</span> is named after <a href="https://en.wikipedia.org/wiki/Leonhard_Euler">Leonhard Euler</a> (1707–1783), a Swiss mathematician who introduced the notation <span class="math inline">\(f(x)\)</span> for mathematical functions and the letter <span class="math inline">\(i\)</span> to denote the imaginary unit <span class="math inline">\(\sqrt{-1}\)</span>. He spent most of his life in Berlin and St.&nbsp;Petersburg, and he is widely considered the greatest mathematician of the 18th century. The number <span class="math inline">\(e\)</span> was first discovered in 1683 by Jacob Bernoulli (the namesake of the Bernoulli distribution) when studying compound interest, where <span class="math inline">\(e = \lim_{n \rightarrow \infty} (1 + 1 / n)^n\)</span>. In 1748, Euler proved that <span class="math inline">\(e = \frac{1}{0!} + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \cdots\)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p> This is named for <a href="https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs">Josiah Willard Gibbs</a> (1839–1903), an American scientist who earned the first American doctorate in engineering in 1863 and went on to work on statistical mechanics, thermodynamics, optics, and vector calculus as a professor of physics at Yale. Albert Einstein called him the greatest mind in American history.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p> Named after <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a> (1890–1962), who established the foundations of maximum likelihood inference between 1912 and 1922. He was the most important statistician of the 20th century, and he was one of the founders of population genetics. He had poor eyesight for his entire life, which led him to develop a formidable sense of geometry in his head. However, he was also a leading eugenicist and one of the most vocal opponents of the hypothesis that smoking causes lung cancer.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p> For estimating a parameter <span class="math inline">\(\theta\)</span>, the conditions are these: (1) The set of possible values of the observed data <span class="math inline">\(X\)</span> does not depend on <span class="math inline">\(\theta\)</span>. (2) Each <span class="math inline">\(\theta\)</span> produces a different distribution of <span class="math inline">\(X\)</span>. (3) The true value of <span class="math inline">\(\theta\)</span> is in the interior of the set of possible values. (4) The log likelihood <span class="math inline">\(\ell(\theta)\)</span> has continuous first and second derivatives with respect to <span class="math inline">\(\theta\)</span> in a neighborhood of <span class="math inline">\(\theta_\true\)</span>. These conditions are met by the binomial likelihood when <span class="math inline">\(\ptrue \in (0, 1)\)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p> For simplicity, we are being vague about what we mean by <span class="math inline">\(\hat{\mu}_n \rightarrow \mu\)</span>. Probability has several different notions of convergence/. The <em>weak</em> LLN guarantees convergence <em>in probability</em>, which means that <span class="math inline">\(\lim_{n \rightarrow \infty} \Pr\big(|\hat{\mu}_n - \mu| &gt; \varepsilon\big) = 0\)</span> for any <span class="math inline">\(\varepsilon &gt; 0\)</span>. The <em>strong</em> LLN guarantees convergence <em>almost surely</em>, which means that <span class="math inline">\(\Pr\big(\lim_{n \rightarrow \infty} \hat{\mu}_n = \mu\big) = 1\)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p> Named after <a href="https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss">Carl Friedrich Gauss</a> (1777-1855), a German mathematician who is widely considered one of the greatest mathematicians of all time. He discovered the normal distribution in 1809, but the CLT itself was first proved by Laplace in 1810 (see <a href="probability.html" class="quarto-xref"><span>Chapter 1</span></a>).<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p> Named after Swedish statistician <a href="https://en.wikipedia.org/wiki/Harald_Cramér">Harald Cramér</a> (1893–1985), who was a professor at Stockholm University, and Indian-American statistician <a href="https://en.wikipedia.org/wiki/C._R._Rao">Calyampudi Radhakrishna (C. R.) Rao</a> (1920–2023), who was a professor at the Indian Statistical Institute, the University of Cambridge, the University of Pittsburgh, and Pennsylvania State University.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p> Named after <a href="https://en.wikipedia.org/wiki/Abraham_Wald">Abraham Wald</a> (1902–1950), a Jewish Hungarian mathematician who was invited to move from Vienna to the United States in 1938 after Nazi Germany annexed Austria. He worked at the Statistical Research Group at Columbia University during World War II. In 1950, he and his wife were killed in a plane crash in India, where he was visiting the Indian Statistical Institute.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p> <a href="https://en.wikipedia.org/wiki/Samuel_S._Wilks">Samuel S. Wilks</a> (1906–1964) was an American mathematician and statistician who grew up on a farm in Texas, got a Ph.D.&nbsp;at the University of Iowa, and went on to be a professor at Princeton University.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p> This approach to hypothesis testing was pioneered in the 1929s by <a href="https://en.wikipedia.org/wiki/Jerzy_Neyman">Jerzy Neyman</a> (1894–1981), a Polish mathematician and statistician who founded the first department of statistics in the United States at the University of California, Berkeley in 1938, and <a href="https://en.wikipedia.org/wiki/Egon_Pearson">Egon Pearson</a> (1895–1980), a British statistician who was a professor at University College London like his father Karl Pearson. <a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./condprob.html" class="pagination-link" aria-label="Conditional Probability and Diagnostic Tests">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Probability and Diagnostic Tests</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./bayes.html" class="pagination-link" aria-label="Bayesian Estimation">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bayesian Estimation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>