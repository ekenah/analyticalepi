[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analytical Epidemiology",
    "section": "",
    "text": "Preface\nOne day at lunch at the Harvard School of Public Health, I overheard Professor Murray Mittleman say: “I love epidemiology. It all fits together like a diamond.” As a second-year doctoral student in epidemiology, I was surprised to hear the subject described with such unstrained enthusiasm. It has taken years of study and experience for me to understand what he meant. On the way, I too have fallen in love.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "Analytical Epidemiology",
    "section": "Who this book is for",
    "text": "Who this book is for\nThis book is intended primarily for two audiences:\n\nEpidemiologists are often protected from the mathematical foundations of their field. The long-term price of this is “dogmatism, that is, a tendency to rigidly protect a partially understood theoretical heritage” (Morabia 2004). The mathematics needed for a deeper understanding of epidemiologic methods is within reach of anyone who has come far enough to need it. Whether you master this material or just learn to approach it with more patience than fear, you will be doing a service to epidemiology and to public health.\nBiostatisticians are familiar with probability and statistical inference, but applying statistics to solve scientific problems in public health requires skills different from those needed to prove that a method works under given assumptions. Epidemiology is a living example of the interplay between theory and applications in statistics, and epidemiologists have shown integrity, courage, and ingenuity in confronting causal questions with statistical tools.\n\nBeyond these audiences, I hope to explain the logic of epidemiology to any interested reader. It is possible that epidemiologic research has already helped save your life.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Analytical Epidemiology",
    "section": "How to use this book",
    "text": "How to use this book\nDifficult chapters, sections, subsections, and exercises are marked with an asterisk (*). These can be skipped without harming the logical flow of the book, but none of them is beyond the reach of a determined reader. The starring is recursive: Starred sections can be skipped within a starred chapter, starred subsections can be skipped within a starred section, and so on. Footnotes offer context or hint at more advanced material. All of them can be ignored if they do not seem useful or interesting.\nThis is a work in progress. You may find that some parts are unfinished or just bad. Please report errors (including typos) or submit suggestions (especially good examples) at:\nhttps://github.com/ekenah/analyticalepi/issues.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Analytical Epidemiology",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis book is written in LaTeX and Quarto with calculations and figures generated in R, Python, and Inkscape. I have also included many links to Wikipedia. These are free, open-source, and publicly available thanks to the work of many contributors.\nTony Barry, Devesh Kapur, Paul Farmer, and James H. Maguire guided me to a career in public health when I was an undergraduate. James Robins, Miguel A. Hernán, Marc Lipsitch, and Stephen P. Luby helped me become an epidemiologist, biostatistician, and epidemic modeler in graduate school. My career began under the mentorship of Ira M. Longini, Jr., and M. Elizabeth Halloran as a postdoctoral fellow at the Univerity of Washington and an assistant professor at the University of Florida. My colleagues Yang Yang, Grzegorz Rempała, Forrest Crawford, and Patrick Schnell have all provided useful comments. For their patience with early versions of this material, I am grateful to the students of STA 6177/PHC 6937 (Applied Survival Analysis) at the University of Florida from 2013 to 2016 and PUBHEPI 8430 (Epidemiology 4) at The Ohio State University from 2019 to the present.\nMy parents, Chris and Kate Kenah, courageously allowed me to travel to places they had never been to and do things I had been told to avoid. These experiences in the United States, India, South Africa, and especially Bangladesh opened my eyes to the terrible importance of clear thinking in public health. My wife, Asma Aktar, and our sons Rafi, Rayhan, and Rabi remind me every day how important it is to destroy everything that stifles humanity. To that end, I hope this book is useful.\nAny mistakes are my own, and God knows best (الله أعلم).\n\n\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological Perspective.” In A History of Epidemiologic Methods and Concepts, edited by Alfredo Morabia, 3–125. Springer.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera. Second edition. John Churchill. https://wellcomecollection.org/works/uqa27qrt.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "1.1 Sets, experiments, and events\nTo begin at the beginning, we will start with probability. Morabia (2004) accurately observed that “Epidemiology came late in human history because it had to wait for the emergence of probability.” This is probably the most difficult chapter of the book, but it will make all subsequent chapters easier. You can use it as a reference and come back to the difficult parts when you need them. Learning to think clearly about probability will give you a compass to find your way through difficult terrain in epidemiology.\nTo speak clearly about probabilities, we need some basic notation for sets. If \\(A\\) is a set that contains an element \\(a\\), we write \\[a \\in A.\\] If \\(A\\) and \\(B\\) are sets such that every element of \\(A\\) is also an element of \\(B\\), we write \\[A \\subseteq B.\\] to indicate that \\(A\\) is a subset of \\(B\\). Sets \\(A\\) and \\(B\\) are equal if and only if \\(A \\subseteq B\\) and \\(B \\subseteq A\\), which means they contain exactly the same elements. The empty set with no elements is denoted \\(\\varnothing\\). For any set \\(A\\), it is true that \\(A \\subseteq A\\) and \\(\\varnothing \\subseteq A\\).\nWe use \\(\\mathbb{R}\\) to denote the real numbers. Intervals are subsets of \\(\\mathbb{R}\\) that take one of the following forms: \\[\\begin{aligned}\n  (a, b) &= \\{x \\in \\mathbb{R}: a &lt; x &lt; b\\}, \\\\\n  (a, b] &= \\{x \\in \\mathbb{R}: a &lt; x \\leq b\\}, \\\\\n  [a, b) &= \\{x \\in \\mathbb{R}: a \\leq x &lt; b\\}, \\\\\n  [a, b] &= \\{x \\in \\mathbb{R}: a \\leq x \\leq b\\}. \\\\\n\\end{aligned}\\] An endpoint with a square bracket is included in the interval; an endpoint with a round bracket is not. We can have \\(a = -\\infty\\) or \\(b = \\infty\\) as long as we use a round bracket for the corresponding endpoint. For example, it is true that \\(\\mathbb{R} = (-\\infty, \\infty)\\). However, \\(\\mathbb{R} \\neq [-\\infty, \\infty]\\) because \\(\\pm \\infty\\) are not real numbers.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#sets-experiments-and-events",
    "href": "probability.html#sets-experiments-and-events",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "1.1.1 Experiments and events\nIn probability, an experiment is any process that will produce one outcome out of a set possible outcomes. The set of possible outcomes is called the sample space and is traditionally denoted \\(\\Omega\\). An experiment produces a single outcome \\(\\omega \\in \\Omega\\). For example, the sample space for a single coin flip is \\[\\Omega = \\{H, T\\},\\] where \\(\\omega = H\\) if we get heads and \\(\\omega = T\\) if we get tails.\nThe outcomes in the sample space must determine everything about the random outcome of the experiment. If we flip a coin twice, the sample space cannot be \\(\\{H, T\\}\\) because each \\(\\omega \\in \\Omega\\) must specify the outcome of both coin flips. Instead, \\[\\Omega = \\{HH, HT, TH, TT\\}\\] where \\(\\omega = XY\\) if we get \\(X\\) on the first flip and \\(Y\\) on the second. This helps us see, for example, that there are two ways to get one \\(H\\) and one \\(T\\) in two coin flips.\nThe purpose of probability is to summarize uncertainty about the outcomes of experiments. However, the outcomes themselves do not have probabilities. Probabilities are assigned to events, which are subsets of the sample space \\(\\Omega\\). If \\(A\\) is an event, then \\(A\\) occurs if and only if the outcome \\(\\omega\\) produced by our experiment is an element of \\(A\\) (i.e., if and only if \\(\\omega \\in A\\)). If we flip a coin twice, the event that we get two heads is \\(\\{HH\\}\\), the event that we get one head is \\(\\{HT, TH\\}\\), and the event that we get zero heads is \\(\\{TT\\}\\). By definition, the event \\(\\Omega\\) always occurs and the event \\(\\varnothing\\) never occurs.\nIn experiments with a finite or countably infinite sample space,2 the distinction between the outcome \\(\\omega\\) and the event \\(\\{\\omega\\}\\) can be safely ignored. In more complex experiments (e.g., taking a random sample from a standard normal distribution), this distinction is important.3 In all cases, experiments have outcomes and events have probabilities.\nIn epidemiology, it is often useful to think of the sample space \\(\\Omega\\) as being a population and each \\(\\omega \\in \\Omega\\) as an individual in this population. In this context, our experiment is to sample a person from \\(\\Omega\\) and ask them questions, take measurements, or follow them over time to ascertain disease occurrence. Events would be subpopulations of \\(\\Omega\\), such as \\(\\{\\omega \\in \\Omega: \\omega \\text{ lives in Ohio}\\}\\). This event occurs if the sampled individual \\(\\omega\\) lives in Ohio, and it does not occur if they live somewhere else.\n\n\n1.1.2 Set operations and logic\nThere are three basic set operations that take one or more sets and define another set: complement, intersection, and union. Each operation has a simple interpretation in terms of logic.\n\nThe complement of a set \\(A\\) is \\[A^\\comp = \\{\\omega \\in \\Omega : \\omega \\not\\in A\\},\\] which can be interpreted logically as not \\(A\\). If \\(A\\) is an event, then the event \\(A^\\comp\\) occurs if \\(\\omega \\not\\in A\\). For the same reason that “not not A” means “A”, we have \\((A^\\comp)^\\comp = A\\).\nThe intersection of two sets \\(A\\) and \\(B\\) is \\[A \\cap B = \\{\\omega \\in \\Omega : \\omega \\in A \\text{ and } \\omega \\in B\\},\\] which can be interpreted logically as \\(A\\) and \\(B\\). If \\(A\\) and \\(B\\) are events, then the event \\(A \\cap B\\) occurs if \\(\\omega \\in A\\) and \\(\\omega \\in B\\).\nThe union of two sets \\(A\\) and \\(B\\) is \\[A \\cup B = \\{\\omega \\in \\Omega : \\omega \\in A \\text{ or } \\omega \\in B\\},\\] which can be interpreted logically as \\(A\\) or \\(B\\) as long as we use an inclusive “or” (i.e., and/or). If \\(A\\) and \\(B\\) are events, then the event \\(A \\cup B\\) occurs if \\(\\omega \\in A\\) or \\(\\omega \\in B\\).\n\nIf \\(A \\subseteq B\\), then \\(A \\cap B = A\\) and \\(A \\cup B = B\\). An important special case is that \\[\n  A \\cap A = A \\cup A = A.\n\\tag{1.1}\\] For the empty set \\(\\varnothing\\), we get \\(A \\cap \\varnothing = \\varnothing\\) and \\(A \\cup \\varnothing = A\\). For the sample space \\(\\Omega\\), we get \\(A \\cap \\Omega = A\\) and \\(A \\cup \\Omega = \\Omega\\).\nUnion and intersection are commutative operations like addition and multiplication, so the order of \\(A\\) and \\(B\\) does not matter: \\[\n  A \\cup B = B \\cup A\n\\] and \\[\n  A \\cap B = B \\cap A.\n\\] Events \\(A\\) and \\(B\\) are disjoint or mutually exclusive when \\(A \\cap B = \\varnothing\\). If \\(A\\) and \\(B\\) are disjoint, then at most of one of them can occur in a single experiment. Any set and its complement are disjoint, and the empty set \\(\\varnothing\\) is disjoint with itself and all other sets.\nIf \\(\\Omega\\) is a population, these set operations allow us to define subpopulations in terms of multiple traits. If the event \\(A = \\{\\omega \\in \\Omega: \\omega \\text{ lives in Ohio}\\}\\), then its complement \\(A^\\comp\\) contains all individuals in \\(\\Omega\\) who live outside Ohio. If the event \\(B = \\{\\omega \\in \\Omega: \\omega \\text{ is 42 years old}\\}\\), then the intersection \\(A \\cap B\\) contains everyone in \\(\\Omega\\) who is 42 years old and lives in Ohio. If \\(\\Omega\\) does not contain any 42-year-old Ohio residents, then \\(A\\) and \\(B\\) are disjoint. The union \\(A \\cup B\\) contains everyone in \\(\\Omega\\) who lives in Ohio or is 42 years old. This could include both a 24-year-old who lives Ohio and a 42-year-old who lives Michigan.\n\n\n1.1.3 Venn diagrams\nA useful tool for understanding events and set operations is the Venn diagram.4 An example is shown in Figure 1.1. The rectangle represents \\(\\Omega\\), and the circles \\(A\\) and \\(B\\) represent events. \\(A^\\comp\\) is everything in \\(\\Omega\\) outside the circle \\(A\\), and \\(B^\\comp\\) is everything outside the circle \\(B\\). Their intersection \\(A \\cap B\\) is the area where the two circles overlap. Their union \\(A \\cup B\\) is everything contained in at least one of \\(A\\) or \\(B\\).\n\n\n\n\n\n\nFigure 1.1: Venn diagram showing events \\(A\\) and \\(B\\). The area contained in both events is their intersection \\(A \\cap B\\). The union \\(A \\cup B\\) is all area contained in at least one of \\(A\\) and \\(B\\), including \\(A \\cap B\\).\n\n\n\n\n\n1.1.4 Sequences of events*\nIntersections can be written for more than two events. The intersection of \\(A_1, A_2, \\ldots, A_n\\) is \\[\n  I_n = \\bigcap_{i = 1}^n A_i.\n\\tag{1.2}\\] Because set intersection is commutative and associative, any ordering of \\(A_1, \\ldots, A_n\\) produces the same intersection. The event \\(I_n\\) occurs if and only if all of the events \\(A_1, \\ldots, A_n\\) occur. Each new event makes the intersection smaller (i.e., never larger) in the sense that \\[\n  \\bigcap_{i = 1}^{n + 1} A_i \\subseteq I_n.\n\\] whenever \\(A_{n + 1}\\) is another event.\nSimilarly, unions can be written for more than two events. If \\(A_1, A_2, \\ldots, A_n\\) is a set of events, then their union is \\[\n  U_n = \\bigcup_{i = 1}^n A_i.\n\\tag{1.3}\\] Because set union is commutative and associative, any ordering of \\(A_1, \\ldots, A_n\\) produces the same union. The event \\(U_n\\) occurs if and only if at least one of the events \\(A_i\\) occurs. Each new event makes the union bigger (i.e., never smaller) in the sense that \\[\n  U_n \\subseteq \\bigcup_{i = 1}^{n + 1} A_i\n\\] whenever \\(A_{n + 1}\\) is another event.\nBoth unions and intersections can be defined for infinite sequences of events.5 To describe this, we let \\(n = \\infty\\) in the notation from Equation 1.2 or Equation 1.3. The union of any finite sequence of events can be turned into the union of an infinite sequence of events by adding an endless sequence of empty sets to the finite sequence. The new sequence is still a sequence of disjoint events, and each empty set \\(\\varnothing\\) leaves the union unchanged. If \\((A_1, A_2, \\ldots)\\) is an infinite sequence of events such that \\(A_i = \\varnothing\\) for all \\(i &gt; n\\), then \\[\n  \\bigcup_{i = 1}^\\infty A_i = \\bigcup_{i = 1}^n A_i.\n\\] This turns out to be useful when we try to give a mathematically rigorous definition of probability.\n\n\n1.1.5 Algebra of sets\\(^*\\)\nUnions, intersections, and complements can be combined in complex ways. Fortunately, there are a few basic principles that can be used to simplify these calculations. We have already seen that unions and intersections are commutative. Unions and intersections are also associative, so \\[\n  A \\cup (B \\cup C)\n  = (A \\cup B) \\cup C\n\\] and \\[\n  A \\cap (B \\cap C)\n  = (A \\cap B) \\cap C\n\\] for any sets \\(A\\), \\(B\\), and \\(C\\).\nDe Morgan’s laws describe how complements affect unions and intersections. If \\(A\\) and \\(B\\) are sets, then \\[\n  (A \\cap B)^\\comp\n  = A^\\comp \\cup B^\\comp\n\\tag{1.4}\\] because you are outside \\(A \\cap B\\) if and only f you are outside \\(A\\) or outside \\(B\\). Similarly, \\[\n  (A \\cup B)^\\comp\n  = A^\\comp \\cap B^\\comp.\n\\tag{1.5}\\] because you are outside \\(A \\cup B\\) if and only if you are outside \\(A\\) and outside \\(B\\). Note that each of these equations implies the other if we replace \\(A = (A^\\comp)^\\comp\\) with \\(A^\\comp\\) and replace \\(B = (B^\\comp)^\\comp\\) with \\(B^\\comp\\). They are two sides of the same coin, but it is helpful to remember them both.\nThe distributive properties describe how unions and intersections interact with each other. Recall that multiplication distributes over addition, so \\(a (b + c) = ab + ac\\). For any sets \\(A\\), \\(B\\), and \\(C\\), we have the following distributive properties:\n\nIntersections distribute over unions, so \\[\n  A \\cap (B \\cup C)\n  = (A \\cap B) \\cup (A \\cap C).\n\\]\nUnions distribute over intersections, so \\[\n  A \\cup (B \\cap C)\n  = (A \\cup B) \\cap (A \\cup C).\n\\]\n\nIntersections and unions also distribute over themselves. However, this is a consequence of commutativity, associativity, and Equation 1.1, not a separate property like the distributive rules above.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#probability",
    "href": "probability.html#probability",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.2 Probability",
    "text": "1.2 Probability\nA probability measure is a function that takes an event \\(A \\subseteq \\Omega\\) and returns a number \\(\\Pr(A) \\in [0, 1]\\) in any way that conforms to the following rules:\n\n\\(\\Pr(\\Omega) = 1\\).\n\\(\\Pr(A) \\in [0, 1]\\) for any event \\(A \\subseteq \\Omega\\).6\nThe addition rule: If \\((A_1, A_2, \\ldots)\\) is any sequence of disjoint events, then \\[\n  \\Pr\\Biggl( \\bigcup_{i = 1}^\\infty A_i\\Biggr)\n  = \\sum_{i = 1}^\\infty \\Pr(A_i).\n\\] The addition rule is stated in terms of an infinite sequence of disjoint events because this implies the addition rule for any finite sequence of disjoint events (see Section 1.1.4).\n\nIt is useful to think of probability as a generalization of our intuitions about area or volume. When there is no overlap in a set of two-dimensional shapes, we can get the total area they cover by adding up the areas of the individual shapes. Similarly, we can get the total volume taken up by a set of bowling balls by adding up their individual volumes.\nThere is a lot of debate about the meaning of probability, but its definition does not assume any particular interpretation. Probability calculations are based on the rules above no matter what we think it all means, and any interpretation consistent with these rules is valid.\n\n1.2.1 Probability calculations\nSeveral useful properties of probability follow immediately from the definition above. A short proof follows each result. To follow the proofs, it helps to draw Venn diagrams.\n\nTheorem 1.1 If \\(A\\) is an event, \\(\\Pr\\bigl(A^\\comp\\bigr) = 1 - \\Pr(A)\\).\n\n\nProof. Because \\(\\Omega = A \\cup A^\\comp\\) and \\(A\\) and \\(A^\\comp\\) are disjoint, we have \\[\n    \\Pr(A) + \\Pr\\bigl(A^\\comp\\bigr) = \\Pr(\\Omega) = 1\n  \\] by the addition rule. The result follows when we subtract \\(\\Pr(A)\\) from both sides.\n\n\nTheorem 1.2 If \\(A\\) and \\(B\\) are events such that \\(A \\subseteq B\\), then \\(\\Pr(A) = \\Pr(B) - \\Pr\\bigl(B \\cap A^\\comp\\bigr)\\). This implies that \\(\\Pr(A) \\leq \\Pr(B)\\).\n\n\nProof. Each element of \\(B\\) either is or is not in \\(A\\), so \\[\n    B = (B \\cap A) \\cup \\big(B \\cap A^\\comp\\big)\n    = A \\cup \\big(B \\cap A^\\comp\\big).\n  \\] where the second equality follows from the fact that \\(B \\cap A = A\\) because \\(A \\subseteq B\\). The two sets on the right-hand side are disjoint, so we have \\[\n    \\Pr(B) = \\Pr(A) + \\Pr\\bigl(B \\cap A^\\comp\\bigr)\n  \\] by the addition rule. The result follows if we subtract \\(\\Pr\\bigl(B \\cap A^\\comp\\bigr)\\) from both sides. This implies that \\(\\Pr(A) \\leq \\Pr(B)\\) because \\(\\Pr\\bigl(B \\cap A^\\comp\\bigr) \\geq 0\\).\n\n\nTheorem 1.3 If \\(A\\) and \\(B\\) are events, \\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\).\n\n\nProof. We can break \\(A \\cup B\\) into three disjoint sets: elements of \\(A\\) and not \\(B\\), elements of \\(B\\) and not \\(A\\), and elements of both \\(A\\) and \\(B\\). In set notation, this is \\[\n    A \\cup B = \\big(A \\cap B^\\comp\\big) \\cup \\big(B \\cap A^\\comp\\big) \\cup (A \\cap B).\n  \\] By the addition rule, \\[\n    \\Pr(A \\cup B) = \\Pr\\bigl(A \\cap B^\\comp\\bigr) + \\Pr\\bigl(B \\cap A^\\comp\\bigr) + \\Pr(A \\cap B).\n   \\tag{1.6}\\] By Theorem 1.2, we have \\[\n      \\Pr\\bigl(A \\cap B^\\comp\\bigr)\n      = \\Pr(A) - \\Pr(A \\cap B), \\\\\n  \\] because \\(A \\cap B \\subseteq A\\) and \\[\n      \\Pr\\bigl(B \\cap A^\\comp\\bigr)\n      = \\Pr(B) - \\Pr(A \\cap B).\n  \\] because \\(A \\cap B \\subseteq B\\). The result follows from substituting these back into Equation 1.6 and collecting terms involving \\(\\Pr(A \\cap B)\\). Intuitively, \\(\\Pr(A) + \\Pr(B)\\) includes the overlap \\(\\Pr(A \\cap B)\\) twice, so we have to subtract out one of them. This can be see clearly in Figure 1.1.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#random-variables",
    "href": "probability.html#random-variables",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.3 Random variables",
    "text": "1.3 Random variables\nThe outcomes of an experiment can be anything, not just numbers. A random variable is a real-valued function defined on a sample space \\(\\Omega\\). In other words, a random variable \\(X\\) is a function that takes an argument \\(\\omega \\in \\Omega\\) as input and returns a value \\(X(\\omega) \\in \\mathbb{R}\\). Traditionally, random variables are written as capital letters and possible values are written as lower-case letters, so \\(\\Pr(X = x)\\) denotes the probability of the event \\[\n  \\{\\omega \\in \\Omega : X(\\omega) = x\\}.\n\\] For simplicity, random variables are usually written without the argument \\(\\omega\\).\nThe distinction between outcomes and random variables is useful because we can define multiple random variables on the same sample space. For example, the height, weight, and age of an individual \\(\\omega\\) sampled from a population \\(\\Omega\\) are different random variables defined on the same sample space.\n\n1.3.1 Indicator variables\nThe simplest random variables are indicator variables. For an event \\(A\\), the indicator variable \\[\n  \\indicator_A(\\omega)\n  = \\begin{cases}\n    1 & \\text{ if } \\omega \\in A, \\\\\n    0 & \\text{ if } \\omega \\not\\in A.\n  \\end{cases}\n\\] Indicator variables are binary random variables, which take exactly two values. In practice, these values should be zero and one unless there is a specific reason to do otherwise. When sampling from a population, we can define indicator variables for membership in different subpopulations.\nAll of the basic set operations above can be expressed in terms of indicator variables for sets.\n\nThe indicator function for the complement of \\(A\\) is \\[\n  \\indicator_{A^\\comp} = 1 - \\indicator_A.\n\\tag{1.7}\\]\nIf \\(B\\) is another event and \\(\\indicator_B\\) is its indicator variable, then the indicator variable for the intersection \\(A\\) and \\(B\\) is the product of their indicator variables: \\[\n  \\indicator_{A \\cap B}\n  = \\indicator_A \\indicator_B.\n\\tag{1.8}\\]\nThe indicator variable for the union \\(A \\cup B\\) is \\[\n  \\indicator_{A \\cup B}\n  = 1 - (1 - \\indicator_A) (1 - \\indicator_B)\n  = \\indicator_A + \\indicator_B - \\indicator_{A \\cap B}.\n\\tag{1.9}\\] This follows from Equation 1.5 because \\(A \\cup B = (A^\\comp \\cap B^\\comp)^\\comp\\).\n\n\nR\n\n\n\n\nindicators.R\n\n## Indicator variables for events A and B, etc.\n\n# Setting the seed ensures that everyone gets the same random samples.\n# Functions are called using parentheses (round brackets).\n# The function rbinom() is a random sample from a binomial distribution.\nset.seed(42)\nn &lt;- 100\ndat &lt;- data.frame(A = rbinom(n, 1, 0.3))\ndat$B &lt;- rbinom(n, 1, 0.6)\n\n# inspecting a data frame\nnames(dat)  # variables in the data frame\nnrow(dat)   # number of rows (individuals)\nncol(dat)   # number of columns (variables)\ndim(dat)    # rows and columns in the data frame\nstr(dat)    # summary of the data frame structure (variables and types)\n\n# inspecting columns of a data frame (or vectors)\n# Our sample space or population consists of 100 individuals.\n# Square brackets are used for indices, which can be numbers or TRUE/FALSE.\ndat$A                 # indicator for A for all 100 individuals\ndat$A[10]             # indicator for A in individual 10\ndat$A[2:6]            # indicator variables for individuals 2 to 6\ndat$A[c(10, 20, 30)]  # A indicators for individuals 10, 20, and 30\nwhich(dat$A == 1)     # which individuals are in event A\nwhich(dat$A == 0)     # which individuals are not in event A\n\n# indicator variable for A complement\n# In R (and many other languages), \"!\" means \"not\".\n# The function as.integer() changes TRUE/FALSE to 1/0.\ndat$Acomp &lt;- as.integer(!dat$A)\n\n# indicator variable for A intersection B\n# In R (and many other languages), \"&\" means \"and\".\ndat$ABintersect &lt;- as.integer(dat$A & dat$B)\n\n# indicator variable for A union B\n# In R (and many other languages), \"|\" means \"or\".\ndat$ABunion &lt;- as.integer(dat$A | dat$B)\n\n# save the data frame as a CSV file\n# The file argument can be a path (e.g., \"./data/indicators.csv\" in Linux).\nwrite.csv(dat, file = \"indicators.csv\", row.names = FALSE)\n\n\n\n\n\n\n1.3.2 Probability distributions\nThe set of possible values of a random variable \\(X\\) is called the support of \\(X\\) and denoted \\(\\supp(X)\\).7 For example, the support of an indicator variable is \\(\\{0, 1\\}\\). In this section, we will focus on discrete random variables, which have a support on a finite or countably infinite set. There are two standard ways to describe the distribution of a discrete random variable:\n\nThe probability mass function (PMF) of a discrete random variable \\(X\\) is \\[\n  f(x) =\n  \\begin{cases}\n    \\Pr(X = x) &gt; 0  & \\text{ if } x \\in \\supp(X), \\\\\n    0               & \\text{ if } x \\not \\in \\supp(X).\n  \\end{cases}\n\\] Because \\(\\Pr(\\Omega) = 1\\), we always have \\[\n  \\sum_{x \\in \\supp(X)} f(x) = 1.\n\\]\nThe cumulative distribution function (CDF) of \\(X\\) is \\[\n  F(x)\n  = \\Pr(X \\leq x).\n\\] \\(F(x)\\) is monotonically increasing in \\(x\\), which means that \\(F(a) \\leq F(b)\\) whenever \\(a &lt; b\\). It has a jump upward of size \\(f(x)\\) at each \\(x \\in \\supp(X)\\), and its value at each such \\(x\\) is the value that it jumps to—not the value that it jumps up from. For sufficiently small \\(x\\), \\(F(x)\\) can be made arbitrarily close to zero. For sufficiently large \\(x\\), \\(F(x)\\) can be made arbitrarily close to one. More formally, we say that \\(\\lim_{x \\downarrow -\\infty} F(x) = 0\\) and \\(\\lim_{x \\uparrow \\infty} F(x) = 1\\).\n\nThe PMF and CDF provide equivalent descriptions of the distribution of \\(X\\) in the sense that either of these functions can be used to calculate the other. Given the PMF \\(f\\), the CDF is defined by \\[\n  F(x) = \\sum_{\\substack{v \\in \\supp(X): \\\\ v \\leq x}} f(v).\n\\] where the sum is taken over all \\(u \\in \\supp(X)\\) such that \\(u \\leq x\\). Given the CDF \\(F\\), the PMF is defined by \\[\n  f(x) = F(x) - \\max_{v \\leq x} F(v)\n\\] where the maximum is \\(F(v)\\) for the largest \\(v \\in \\supp(X)\\) such that \\(v &lt; x\\).\n\n\n1.3.3 Mean\nThe mean or expected value of a random variable \\(X\\) is \\[\n  \\E(X)\n  = \\sum_{x \\in \\supp(X)} x \\Pr(X = x)\n  = \\sum_{x \\in \\supp(X)} x f(x),\n\\] where \\(f\\) is the PMF of \\(X\\). The mean is often written \\(\\mu\\), and it is often described as a measure of the “location” or “central tendency” of \\(X\\).\nIndicators are an extremely useful for calculating probabilities using means. For any event \\(A\\), its probability is the mean of the indicator variable \\(\\indicator_A\\): \\[\n  \\Pr(A)\n  = 0 \\Pr(\\indicator_A = 0) + 1 \\Pr(\\indicator_A = 1)\n  = \\E(\\indicator_A).\n\\] This is a common way to calculate probabilities in data analyses.\n\nR\n\n\n\n\nprobabilities.R\n\n## Indicator variables and probability calculations\n\n# read in CSV file with indicator variables using the function read.csv()\n# The argument can be a path (e.g., \"./data/indicators.csv\" in Linux).\ndat &lt;- read.csv(\"indicators.csv\")\n\n# calculate probabilities from indicator variables using the function mean()\n# This will also work with TRUE/FALSE (i.e., logical) variables, which are\n# converted to TRUE = 1 and FALSE = 0 in calculations.\nprob_A &lt;- mean(dat$A)\nprob_B &lt;- mean(dat$B)\nprob_Acomp &lt;- mean(dat$Acomp)\nprob_ABintersect &lt;- mean(dat$ABintersect)\nprob_ABunion &lt;- mean(dat$ABunion)\n\n# Pr(A complement) = 1 - Pr(A)\nprob_Acomp\n1 - prob_A\n\n# Pr(A union B) = Pr(A) + Pr(B) - Pr(A intersect B)\nprob_ABunion\nprob_A + prob_B - prob_ABintersect\n\n# Beware of numerical error when comparing floating-point numbers!\n# This example is from The R Inferno by Patrick Burns.\n# https://www.burns-stat.com/pages/Tutor/R_inferno.pdf\n0.1 == 0.3 / 3\nsprintf(\"%.20f\", 0.1)\nsprintf(\"%.20f\", 0.3 / 3)\n\n# math can be more accurate than computers (which is not their fault)\nprob_ABunion == prob_A + prob_B - probABintersect\nsprintf(\"%.20f\", prob_ABunion)\nsprintf(\"%.20f\", prob_A + prob_B - prob_ABintersect)\n\n\n\n\n\n\n1.3.4 Variance\nIf \\(X\\) has \\(\\E(X) = \\mu\\), then \\((X - \\mu)^2\\) is another random variable. The variance of \\(X\\) is the expected value of \\((X - \\mu)^2\\): \\[\n  \\Var(X)\n  = \\E\\big[(X - \\mu)^2\\big]\n  = \\sum_{x \\in \\supp(X)} (x - \\mu)^2 f(x).\n\\] {eq-Var} Because \\((x - \\mu)^2 \\geq 0\\) with equality if and only if \\(x = \\mu\\), we always have \\(\\Var(X) \\geq 0\\). We have \\(\\Var(X) = 0\\) if and only if \\(X = \\mu\\) with probability one. An equivalent expression for the variance that is often easier to use is: \\[\n  \\Var(X) = \\E(X^2) - \\mu^2\n\\tag{1.10}\\] where \\(\\E(X^2)\\) is the expected value of the random variable \\(X^2\\). The variance is often written \\(\\sigma^2\\), and it is often described as a measure of the dispersion of \\(X\\) around the mean.\nThe square root of the variance is called the standard devation, which is often written \\(\\sigma\\). If a random variable \\(X\\) has units (e.g., length, weight, or time), the mean and the standard deviation have the same units as \\(X\\). For example, the mean and standard deviation of a length in meters both have units of \\(\\text{meters}\\) but the variance has units of \\(\\text{meters}^2\\).\n\n\n1.3.5 Bernoulli distribution\nThe distribution of an indicator variable is called the Bernoulli distribution.8 A random variable with the Bernoulli(\\(p\\)) distribution has the PMF \\[\n  f(x)\n  = p^x (1 - p)^{1 - x}\n  = \\begin{cases}\n    1 - p &\\text{if } x = 0\\\\\n    p     &\\text{if } x = 1.\n  \\end{cases}\n\\] Equivalently, it has the CDF \\[\n  F(x)\n  = \\begin{cases}\n    0     &\\text{if } x &lt; 0 \\\\\n    1 - p &\\text{if } x \\in [0, 1) \\\\\n    1     &\\text{if } x \\geq 1.\n  \\end{cases}\n\\] If a random variable \\(X\\) has a Bernoulli(\\(p\\)) distribution, we write \\(X \\sim \\text{Bernoulli}(p)\\). The indicator variable for an event \\(A\\) has a Bernoulli distribution with \\(p = \\Pr(A)\\).\nIf \\(X \\sim \\Bernoulli(p)\\), then it has mean \\[\n  \\E(X)\n  = 0 \\times (1 - p) + 1 \\times p \\\\\n  = p\n\\] and variance \\[\n  \\Var(X)\n  = (0 - p)^2(1 - p) + (1 - p)^2 p \\\\\n  = p (1 - p).\n\\] Its standard deviation is \\(\\sqrt{p (1 - p)}\\), which is greater than zero unless \\(p = 0\\) or \\(p = 1\\). If \\(p = 0\\), then \\(X = 0\\) with probability one. If \\(p = 1\\), then \\(X = 1\\) with probability one.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#joint-and-marginal-distributions",
    "href": "probability.html#joint-and-marginal-distributions",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.4 Joint and marginal distributions",
    "text": "1.4 Joint and marginal distributions\nIf \\(X\\) and \\(Y\\) are random variables defined on the same probability space, then their joint probability mass function is \\[\n  f(x, y)\n  = \\Pr(X = x \\text{ and } Y = y)\n  = \\Pr\\big(\\{\\omega: X(\\omega) = x \\text{ and } Y(\\omega) = y\\}\\big).\n\\] The marginal probability mass functions are the PMFs of \\(X\\) or \\(Y\\) individually, which can be calculated from the joint PMF. The marginal PMF of \\(X\\) is \\[\n  f_X(x) = \\sum_{y \\in \\supp(Y)} f(x, y),\n\\] and the marginal PMF of \\(Y\\) is \\[\n  f_Y(y) = \\sum_{x \\in \\supp(X)} f(x, y).\n\\] These are called marginal distributions by analogy to the margins of a table. The distinction between joint and marginal distributions is extremely important in epidemiology and other applications of probability.\nFor example, Table 1.1 shows the joint and marginal PMFs for two binary random variables \\(X\\) and \\(Y\\). By definition, \\[\n  f(0, 0) + f(0, 1) + f(1, 0) + f(1, 1) = 1.\n\\] In the table, it is clear that the joint distribution determines the marginal distributions. However, there are many different joint distributions that are consistent with the same marginal distributions. Thus, the marginal distributions do not determine the joint distribution.9\n\n\n\nTable 1.1: Joint and marginal PMFs for binary random variables \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Y = 0\\)\n\\(Y = 1\\)\n\\(X\\) margin\n\n\n\n\n\\(X = 0\\)\n\\(f(0, 0)\\)\n\\(f(0, 1)\\)\n\\(f_X(0) = f(0, 0) + f(0, 1)\\)\n\n\n\\(X = 1\\)\n\\(f(1, 0)\\)\n\\(f(1, 1)\\)\n\\(f_X(1) = f(1, 0) + f(1, 1)\\)\n\n\n\\(Y\\) margin\n\\(f_Y(0) = f(0, 0) + f(1, 0)\\)\n\\(f_Y(1) = f(0, 1) + f(1, 1)\\)\n1\n\n\n\n\n\n\n\nR\n\n\n\n\njointdist.R\n\n## Joint and marginal distributions of indicators for events A and B\n\n# read indicator variable data from the CSV file\ndat &lt;- read.csv(\"indicators.csv\")\nn &lt;- nrow(dat)\n\n# tables of counts\n# Putting \"&lt;name&gt; = \" before the vector creates a label.\ntable(A = dat$A)\ntable(B = dat$B)\n\n# joint table of counts\n# In table(), the first argument defines rows and the second defines columns.\n# The addmargins() functions adds the row, column, and overall sums.\ntable(A = dat$A, B = dat$B)\naddmargins(table(A = dat$A, B = dat$B))\n\n# tables of probabilities\n# Table margins match the distributions of A (rows) and B (columns).\ntable(Adist = dat$A) / n    # marginal distribution of A indicator\ntable(Bdist = dat$B) / n    # marginal distribution of B indicator\naddmargins(table(A = dat$A, B = dat$B)) / n   # joint distribution\n\n\n\n\nJoint distributions can be defined for more than two random variables. If \\(X_1, X_2, \\ldots, X_n\\) are random variables defined on the same sample space, then their joint PMF is \\[\n  f(x_1, x_2, \\ldots, x_n) = \\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n).\n\\] The marginal distribution of each \\(X_i\\) can be found by adding up the PMF over the support of all the other random variables. For example, \\[\n  f_{X_2}(x_2) = \\sum_{x_1 \\in \\supp(X_1)} \\sum_{x_3 \\in \\supp(X_3)} f(x_1, x_2, x_3).\n\\] when \\(n = 3\\). In this same case, we can talk about the joint distribution of any two variables marginalized over the third. For example, \\[\n  f_{X_2, X_3}(x_2, x_3) = \\sum_{x_1 \\in \\supp(X_1)} f(x_1, x_2, x_3).\n\\] For larger \\(n\\), the formulas gets uglier but the ideas are the same.\n\n1.4.1 Linear combinations*\nIf \\(a\\) and \\(b\\) are constants, then \\(a X + b Y\\) is another random variable on \\(\\Omega\\). It is called a linear combination of \\(X\\) and \\(Y\\). Linear combinations can be defined for more than two random variables. If \\(X_1, \\ldots, X_n\\) are random variables defined on a sample space and \\(a_1, \\ldots, a_n\\) are constants, then \\[\n  \\sum_{i = 1}^n a_i X_i = a_1 X_1 + a_2 X_2 + \\cdots + a_n X_n\n\\] is a linear combination of \\(X_1, \\ldots, X_n\\). The constants can be any real numbers, including one and zero.\nSection 1.3.1 contains both examples and non-examples of linear combinations of random variables.\n\nThe indicator function for \\(A^\\comp\\) in Equation 1.7 is a linear combination of \\(\\indicator_A\\) and the random variable \\(\\indicator_\\Omega\\), which equals one for all \\(\\omega \\in \\Omega\\).\nThe indicator function for \\(A \\cup B\\) in Equation 1.9 is linear combination of the indicator variables \\(\\indicator_A\\), \\(\\indicator_B\\), and \\(\\indicator_{A \\cap B}\\).\nThe indicator function for \\(A \\cap B\\) in Equation 1.8 is not a linear combination of \\(\\indicator_A\\) and \\(\\indicator_B\\) because we have to multiply these two variables.\n\nIf \\(X\\) and \\(Y\\) are random variables defined on the same sample space and \\(a\\) and \\(b\\) are constants, the mean of the linear combination \\(a X + b Y\\) is \\[\n  \\E(a X + b Y) = a \\E(X) + b \\E(Y).\n\\tag{1.11}\\] This is a direct consequence of the definition of expected value: \\[\n  \\begin{aligned}\n    \\E(a X + b Y)\n    &= \\sum_{x \\in \\supp(X)} \\sum_{y \\in \\supp(Y)} (a x + b y) f(x, y) \\\\\n    &= a \\sum_{x \\in \\supp(X)} \\bigg(x \\sum_{y \\in \\supp(Y)} f(x, y)\\bigg)\n      + b \\sum_{y \\in \\supp(Y)} \\bigg(y \\sum_{x \\in \\supp(X)} f(x, y)\\bigg) \\\\\n    &= a \\sum_{x \\in \\supp(X)} x f_X(x) + b \\sum_{y \\in \\supp(Y)} y f_Y(y).\n  \\end{aligned}\n\\] The algebra is not pretty, but the logic is straightforward. We split up the sum into parts depending only on \\(x\\) and only on \\(y\\) outside the joint PMF. In each part, we factor out a constant and find the marginal PMF. This same logic extends to a linear combination of any number of random variables.\n\n\n1.4.2 Variance and covariance*\nThe variance of \\(a X + b Y\\) is \\[\n  \\Var(a X + b Y) = a^2 \\Var(X) + b^2 \\Var(Y) + 2 a b \\Cov(X, Y)\n\\tag{1.12}\\] where \\[\n  \\Cov(X, Y) = \\E\\bigl[\\big(X - \\E(X)\\big) \\big(Y - \\E(Y)\\big)\\bigr]\n\\] is called the covariance of \\(X\\) and \\(Y\\). Note that \\(\\Cov(X, Y) = \\Cov(Y, X)\\). Because \\(\\Var(X) = \\Cov(X, X)\\), variance is a special case of covariance. When \\(X\\) and \\(Y\\) are independent in the sense that the value of one tells us nothing about the value of the other, then \\(\\Cov(X, Y) = 0\\) and \\(\\Var(a X + b Y) = a^2 \\Var(X) + b^2 Var(Y)\\).10\nThe joint distribution of \\(X\\) and \\(Y\\) has a covariance matrix which is \\[\n  \\begin{bmatrix}\n    \\Var(X)     & \\Cov(X, Y) \\\\\n    \\Cov(X, Y)  & \\Var(Y)\n  \\end{bmatrix}\n\\] The variances are along the diagonal of the matrix, and the covariances appear off the diagonal. Because \\(\\Cov(X, Y) = \\Cov(Y, X)\\), covariance matrices are always symmetric (i.e., symmetric across the diagonal). Covariance matrices are an extremely useful tool for calculating the variances of linear combinations of random variables. For example: \\[\n  \\Var(a X + b Y)\n  = \\begin{pmatrix}\n      a & b\n    \\end{pmatrix}\n    \\begin{bmatrix}\n      \\Var(X)     & \\Cov(X, Y) \\\\\n      \\Cov(X, Y)  & \\Var(Y)\n    \\end{bmatrix}\n    \\begin{pmatrix}\n      a \\\\ b\n    \\end{pmatrix}\n\\] in matrix and vector notation from linear algebra. This logic extends to linear combinations of any number of random variables.\nThe covariance is the numerator of the Pearson correlation coefficient,11 which is \\[\n  \\rho_{XY} = \\rho_{YX}\n  = \\frac{\\Cov(X, Y)}{\\sqrt{\\Var(X) \\Var(Y)}}.\n\\] Because of the Cauchy-Schwarz inequality, it turns out that \\(\\rho_{XY} \\in [-1, 1]\\).\n\nWe get \\(\\rho_{XY} = -1\\) if and only if \\(Y = c X\\) for some negative constant \\(c\\).\nWe get \\(\\rho_{XY} = 1\\) if and only if \\(Y = c X\\) for some positive constant \\(c\\). For example, \\(\\rho_{XX} = 1\\) for any random variable \\(X\\).\nWe get \\(\\rho_{XY} = 0\\) if (but not only if) \\(X\\) and \\(Y\\) are independent. However, it is possible to have \\(\\rho_{XY} = 0\\) when \\(X\\) and \\(Y\\) are not independent.\n\nIf we divide each entry \\(\\Cov(X, Y)\\) in a covariance matrix by \\(\\sqrt{\\Var(X) \\Var(Y)}\\), when we get a correlation matrix. Any correlation matrix is symmetric, and the entries along its diagonals are all ones.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#probability-and-disease-occurrence",
    "href": "probability.html#probability-and-disease-occurrence",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.5 Probability and disease occurrence",
    "text": "1.5 Probability and disease occurrence\nIn epidemiology, there are two fundamental measures of disease occurrence that are probabilities: prevalence and risk. In both cases, our experiment is to sample an individual \\(\\omega\\) from a population \\(\\Omega\\). The disease outcome is a binary random variable \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has the disease outcome}, \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] The set of individuals in \\(\\Omega\\) who have \\(D(\\omega) = 1\\) is an event in \\(\\Omega\\), and our measure of disease occurrence is \\[\n  \\Pr(\\{\\omega \\in \\Omega: D(\\omega) = 1\\}).\n\\] The most important difference between prevalence and risk is the role of time in the definition of \\(D\\).\nThere is an important technical detail to remember when we talk about disease onset and recovery. When a person has disease onset at time \\(\\tonset\\) and recovers at time \\(\\trec\\), they have disease for each \\(t \\in [\\tonset, \\trec)\\). We assume that \\(\\trec &gt; \\tonset\\) so this interval is nonempty. We let the onset and recovery times for person \\(i\\) be \\(\\tonset_i\\) and \\(\\trec_i\\), respectively. If a person has multiple episodes of the disease, each episode has its own \\(\\tonset\\) and \\(\\trec\\). For example, the \\(j^\\text{th}\\) episode in person \\(i\\) would have onset time \\(\\tonset_{ij}\\) and recovery time \\(\\trec_{ij}\\).\nThe time scale used to define disease onset is flexible, and this flexibility is useful. The most obvious time scale is calendar time or absolute time. Another common time scale is age, which is an important determinant of the risk of many diseases. In some cases, time since an event is a useful time scale. The event that defines time scale could be a single event (e.g., exposure to contaminated food at a party) or an event that occurs at different times for different individuals (e.g., time since menopause). In general, it is wise to choose the time scale that corresponds to the most important time-varying determinant of disease onset. The chosen time scale is often called the analysis time scale.\n\n1.5.1 Prevalence\nFor prevalence, the disease outcome is defined by choosing a time \\(t\\) and letting \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has disease at time } t, \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In other words, it is the proportion of the population \\(\\Omega\\) that disease at time \\(t\\). This includes individuals who have disease onset at time \\(\\tonset = t\\) but not individuals who recover from disease at time \\(\\trec = t\\). This is often called the point prevalence at time \\(t\\).\nAnother version of prevalence is period prevalence. For period prevalence, we choose a nonempty time interval \\((t_a, t_b]\\) and define \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has disease at any time } t \\in (t_a, t_b], \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In other words, it is the proportion of the population that has disease at any time in the interval \\((t_a, t_b]\\). This includes prevalent cases at time \\(t_a\\) and cases with disease onset in \\((t_a, t_b]\\). The period prevalence in \\((t_a, t_b]\\) is the point prevalence at \\(t_a\\) plus the risk of disease onset in \\((t_a, t_b]\\), to which we now turn.\n\nR\n\n\n\n\nprevalence.R\n\n## Point and period prevalence\n\n# generate onset and recovery data for 100 individuals\n# Setting the seed ensures that everyone gets the same random numbers,\n# but it is strictly optional.\n# The function rexp() randomly samples from an exponential distribution.\nset.seed(42)\ncohort &lt;- data.frame(onset = rexp(100, rate = 0.4))\ncohort$duration &lt;- rexp(100, rate = 2)\ncohort$recovery &lt;- cohort$onset + cohort$duration\n\n# statistical summaries (mean, quartiles, range)\nsummary(cohort$onset)\nsummary(cohort$duration)\nsummary(cohort$recovery)\n\n# highest and lowest recovery times\n# The function sort() sorts the vector from lowest to highest.\n# head() returns the first 6 values of a vector; tails() returns the last 6.\nmin(cohort$onset)\nhead(sort(cohort$onset))    # lowest 6 values (first 6 in the sorted vector)\ntail(sort(cohort$onset))    # highest 6 values (last 6 in the sorted vector)\nmax(cohort$onset)\n\n# With a long vector, sorting repeatedly can be slow.\n# You can also control the number of elements returned by head() or tail().\nonset_ordered &lt;- sort(cohort$onset)\nhead(onset_ordered, n = 10)\ntail(onset_ordered, n = 10)\n\n# seeing rows and columns of the data frame\ncohort[1:10, c(\"onset\", \"duration\", \"recovery\")]\ncohort[c(10, 20, 50), c(\"onset\", \"recovery\")]\ncohort[which(cohort$recovery &lt; 1), c(\"onset\", \"recovery\")]\ncohort[, c(\"onset\", \"recovery\")]    # all rows\ncohort[c(2, 3, 5, 7, 11), ]         # all columns\n\n# point prevalence\nprev &lt;- function(t) {\n  # vector of TRUE/FALSE for prevalent cases at time t\n  prevalent &lt;- cohort$onset &lt;= t & cohort$recovery &gt; t\n  mean(prevalent)\n}\n\nprev(0)\nprev(1)\nprev(2)\nprev(6)\n\n# period prevalence\n# The parentheses around the logical tests are just for readability.\npdprev &lt;- function(ta, tb) {\n  # prevalent cases at t_a\n  prevalent_ta &lt;- (cohort$onset &lt;= ta & cohort$recovery &gt; ta)\n  # incident cases in (t_a, t_b]\n  incident_ab &lt;- (cohort$onset &gt; ta & cohort$onset &lt;= tb)\n  # mean indicator for prevalent at t_a or incident in (t_a, t_b]\n  mean(prevalent_ta | incident_ab)\n}\n\npdprev(0, 1)\npdprev(1, 2)\npdprev(0, 6)\n\n# save the data as a CSV file\nwrite.csv(cohort, \"cohort.csv\", row.names = FALSE)\n\n\n\n\n\n\n1.5.2 Risk (cumulative incidence) and the survival function\nTo define risk or cumulative incidence, we first choose an nonempty time interval \\((t_a, t_b]\\). The disease outcome is defined as \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has } \\tonset \\in (t_a, t_b], \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In the population that is disease-free and at risk of disease at time \\(t_a\\), it is the proportion who have disease onset at \\(\\tonset \\leq t_b\\). The risk is sometimes called the incidence proportion.\nThe risk depends on a specified interval \\((t_a, t_b]\\). We can always define our time scale so that \\(t_a = 0\\), so the risk in \\((t_a, t_b]\\) on the original time scale is the same as the risk in the interval \\((0, t_b - t_a]\\) on the analysis time scale. On the analysis time scale, the cumulative incidence function \\(F(t)\\) is the risk of disease in \\((0, t]\\) for any possible \\(t\\). The corresponding survival function is \\[\n  S(t)\n  = 1 - F(t),\n\\] which is the probability of no disease onset in \\((0, t]\\). In practice, it is often easier to calculate the survival function than to calculate the cumulative incidence function directly. There is only one way to survive disease-free through the interval \\((0, t]\\), but you can have disease onset at any time.\n\nR\n\n\n\n\nrisk.R\n\n## Risk, survival function, and cumulative incidence function\n\n# read data from CSV file\n# Change or remove \".R/\" in the path as needed to locate the cohort.csv file.\n# You can also re-generate the data as in prevalence.R using the same seed.\ncohort &lt;- read.csv(\"./R/cohort.csv\")\n\n# risk (cumpulative incidence)\nrisk &lt;- function(t) {\n  # vector of TRUE/FALSE for incident cases in (0, t]\n  incident &lt;- cohort$onset &lt;= t\n  mean(incident)\n}\n\nrisk(0)\nrisk(1)\nrisk(2)\nrisk(6)\n\n# cumulative incidence function\n# Vectorize() takes a function like risk() that takes a single number as input\n# and creates a function that can take a number or vector as input.\ncuminc &lt;- Vectorize(risk)\ncuminc(c(0, 1, 2, 6))\n\n# survival function\n# A simple function can be put on one line.\n# It takes the same input as cuminc(), so it can take a vector\nsurv &lt;- function(t) 1 - cuminc(t)\nsurv(c(0, 1, 2, 6))\n\n# plot the survival and cumulative incidence functions\nt &lt;- seq(0, 20, by = 0.1)\nplot(t, surv(t), type = \"l\",\n     xlab = \"Time\", ylab = \"Probability\")\nlines(t, cuminc(t), lty = \"dashed\")\ngrid()\nlegend(\"right\", bg = \"white\", lty = c(\"dashed\", \"solid\"),\n       legend = c(\"Cumulative incidence\", \"Survival\"))\n\n\n\n\nThe survival function has several important properties:\n\n\\(S(0) = 1\\) because \\((0, 0]\\) is an empty interval where no one can have disease onset.\nBecause \\(S(t)\\) is a probability, \\(S(t) \\in [0, 1]\\) for all \\(t\\).\n\\(S(t)\\) monotonically decreases (i.e., never increases) with increasing \\(t\\). If \\(t_a &lt; t_b\\), then the time interval \\((0, t_a]\\) is contained \\((0, t_b]\\). Everyone who survives disease-free through \\((0, t_b]\\) must have survived disease-free through \\((0, t_a]\\), but some people who survived through \\((0, t_a]\\) might not make it all the way through \\((0, t_b]\\). Thus, \\(S(t_a) \\geq S(t_b)\\) whenever \\(t_a &lt; t_b\\).\nIf the disease or event occurs eventually for all individuals in our population \\(\\Omega\\) (e.g., death), then \\(S(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\).\n\nEach of these probabilities follows directly from the definition of \\(S(t)\\). Similarly, the cumulative incidence function \\(F\\) has \\(F(0) = 0\\) and \\(F(t) \\in [0, 1]\\), and it is monotonically increasing (i.e., never decreasing) with increasing \\(t\\). If the disease or event occurs eventually in all individuals, then \\(F(t) \\rightarrow 1\\) as \\(t \\rightarrow \\infty\\). Figure 1.2 shows the survival and cumulative hazard curves for the data generated in the prevalence example above.\n\n\n\nCode\n\nsurv-fig.R\n\n## Plot of survival and cumulative incidence functions\n\n# read data from CSV file\n# Change or remove \".R/\" in the path as needed to locate the cohort.csv file.\n# You can also re-generate the data as in prevalence.R using the same seed.\ncohort &lt;- read.csv(\"./R/cohort.csv\")\n\n# risk (cumpulative incidence)\nrisk &lt;- function(t) {\n  # vector of TRUE/FALSE for incident cases in (0, t]\n  incident &lt;- cohort$onset &lt;= t\n  mean(incident)\n}\n\n# cumulative incidence function\ncuminc &lt;- Vectorize(risk)\n\n# survival function\nsurv &lt;- function(t) 1 - cuminc(t)\n\n# plot the survival and cumulative incidence functions\nt &lt;- seq(0, 20, by = 0.1)\nplot(t, surv(t), type = \"l\",\n     xlab = \"Time\", ylab = \"Probability\")\nlines(t, cuminc(t), lty = \"dashed\")\ngrid()\nlegend(\"right\", bg = \"white\", lty = c(\"dashed\", \"solid\"),\n       legend = c(\"Cumulative incidence\", \"Survival\"))\n\n\n\n\n\n\n\n\n\nFigure 1.2: Survival and cumulative incidence curves for the data from the prevalence example.\n\n\n\n\n\nHere, I will generally use the word “risk” to refer to the probability of disease onset in a specified interval. When there is possible confusion about the meaning of “risk”, I will use “cumulative incidence” instead. The terms “cumulative incidence function” and “survival function” are standard in survival analysis, which is the branch of statistics that studies times to events. The creative use of “risk” in public health and medicine should not make you shy away from using the word correctly.\n\n\n1.5.3 Prevalence and the duration of disease\nPoint and period prevalence are both affected by the duration of disease. Both measures will increase if the duration of disease increases. A simple illustration of this is given in Figure 1.3. For a fixed set of onset times, the point prevalence of disease at any time \\(t\\) either stays the same or increases when the duration of disease increases. The prevalence at time \\(t = 5\\) is \\(\\frac{2}{5} = 0.4\\) under the shorter duration of disease but \\(\\frac{3}{5} = 0.6\\) under the longer duration of disease. Period prevalence over any interval \\((t_a, t_b]\\) is affected by the duration of disease because it is the point prevalence at \\(t_a\\) (which is affected by disease duration) plus the risk of disease onset over \\((t_a, t_b]\\). In a given population, the relationship between prevalence, frequency of disease onset (incidence), and the duration of disease can be complex (Freeman and Hutchison 1980; Preston 1987; Keiding 1991; Alho 1992). The risk of disease in any given interval is not affected by the duration of disease.\n\n\n\nCode\n\nprevdur-fig.R\n\n## R code for prevalence and duration plot\nplot(0, 0, type = \"n\", xlim = c(0, 10), ylim = c(0, 5.5),\n     xlab = \"Time\", ylab = \"Individual\", yaxt = \"n\")\nAxis(side = 2, at = 1:5, labels = 1:5)\ngrid()\nstart &lt;- c(4, 1, 3, 2, 6)\nstop1 &lt;- c(7, 3, 6, 4, 7)\nstop2 &lt;- c(9, 4, 8, 7, 9)\narrows(x0 = start, y0 = 1:5, x1 = stop1, code = 3, length = 0.2, angle = 90)\narrows(x0 = stop1, y0 = 1:5, x1 = stop2, code = 2, length = 0.2, angle = 90,\n       col = \"darkgray\")\nabline(v = 5, lty = \"dashed\")\ntext(5.5, 0.5, label = \"t = 5\")\n\n\n\n\n\n\n\n\n\nFigure 1.3: Each black horizonal line shows the onset of disease and recovery from disease in a single individual. The gray lines show recoveries from disease if the disease duration increases.\n\n\n\n\n\n\n\n1.5.4 Descriptive and analytic epidemiology\nPrevalence is often a useful measure for descriptive epidemiology, which measures the distribution of disease over person, place, and time. Because prevalence depends on both incidence and duration of disease, a change in the prevalence of disease can generally be explained several different ways (MacMahon and Terry 1958; Dunn Jr 1962). For example, an increase in prevalence of human immunodeficiency virus (HIV) infection could be cause by an increase in the incidence of HIV infection (which is bad) or an increase in the life expectancy of HIV-infected people (which is good).\nRisk (cumulative incidence) is generally more useful than prevalence for analytic epidemiology, which attempts to identify the causes of a disease. Another advantage of risk is that it can be used for outcomes that begin and end very quickly (e.g., traffic accidents or being hit by lightning) and for outcomes that remove individuals from the population (e.g., emigration or death). Prevalence is not a useful measure of the public health impact of these events.\n\n\n\n\nAlho, Juha M. 1992. “On Prevalence, Incidence, and Duration in General Stable Populations.” Biometrics 48 (2): 587–92.\n\n\nDunn Jr, John E. 1962. “The Use of Incidence and Prevalence in the Study of Disease Development in a Population.” American Journal of Public Health 52 (7): 1107–18.\n\n\nFreeman, Jonathan, and George B Hutchison. 1980. “Prevalence, Incidence and Duration.” American Journal of Epidemiology 112 (5): 707–23.\n\n\nKeiding, Niels. 1991. “Age-Specific Incidence and Prevalence: A Statistical Perspective.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 154 (3): 371–96.\n\n\nLaplace, Pierre Simon. 1820. Théorie Analytique Des Probabilités. Vol. 7. Courcier.\n\n\nMacMahon, Brian, and William D Terry. 1958. “Application of Cohort Analysis to the Study of Time Trends in Neoplastic Disease.” Journal of Chronic Diseases 7 (1): 24–35.\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological Perspective.” In A History of Epidemiologic Methods and Concepts, edited by Alfredo Morabia, 3–125. Springer.\n\n\nPreston, Samuel H. 1987. “Relations Among Standard Epidemiologic Measures in a Population.” American Journal of Epidemiology 126 (2): 336–45.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#footnotes",
    "href": "probability.html#footnotes",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "Pierre-Simone, marquis de Laplace (1749-1827) is often called the Newton of France. He proved that the solar system is stable, developed theories of ocean tides and gravitational potential, proved one of the first general versions of the central limit theorem, and pioneered the Bayesian interpretation of probability. His is one of the 72 names on the Eiffel Tower. ↩︎\n The natural numbers \\(\\mathbb{N} = \\{0, 1, 2, \\ldots\\}\\) are countably infinite, as are the integers \\(\\mathbb{Z}\\) and the rational numbers \\(\\mathbb{Q}\\). The real numbers \\(\\mathbb{R}\\) are uncountably infinite, as are the real numbers in any nonempty interval \\((a, b)\\) and the irrational numbers. Uncountably infinite sets are infinitely larger than countably infinite sets. This distinction was discovered in the 1870s by the German mathematician Georg Cantor (1845–1918). It was considered shocking, but it has become a cornerstone of modern mathematics.↩︎\n In experiments with uncountably infinite sample spaces, the probability of an event \\(A\\) cannot always be calculated by adding up the probabilities of \\(\\{\\omega\\}\\) for all \\(\\omega \\in A\\). For example: If we choose a number at uniformly at random in \\([0, 1]\\), the probability of getting any particular number \\(\\omega\\) is zero. The sum of the probabilities of all \\(\\{\\omega\\} \\subseteq A\\) is zero (if \\(A\\) is countable) or undefined (if \\(A\\) is uncountable). By maintaining a distinction between outcomes and events and by limiting probability calculations to countable (i.e., finite or countably infinite) sums, we end up with something coherent and useful.↩︎\n Named after John Venn (1834-1923), an English logician and philosopher who was one of the pioneers of the frequentist interpretation of probability. He was ordained as an Anglican priest in 1859 but resigned from the church in 1883. He was a prize-winning gardener of roses and white carrots and a prominent supporter of women’s right to vote. From 1903 until his death, he was President of Fellows in Gonville and Caius College at the University of Cambridge, where he is commemorated with a Venn diagram in a stained glass window.↩︎\n In probability, we only consider unions and intersections of finite or countably infinite sets of events. Although unions and intersections can be defined for uncountably infinite sets of events, it can be impossible to assign probabilities to the resulting sets (see the Banach-Tarski paradox). As an epidemiologist, this should not keep you up at night.↩︎\n Technically, we assign probabilities only to events in a class \\(\\mathcal{F}\\) of subsets of \\(\\Omega\\) that is required to contain \\(\\Omega\\) and to be closed under complements and countable unions. “Closed under complements” means that \\(A^\\comp \\in \\mathcal{F}\\) whenever \\(A \\in \\mathcal{F}\\). For example, \\(\\varnothing = \\Omega^\\comp\\) must be in \\(\\mathcal{F}\\) because \\(\\Omega \\in \\mathcal{F}\\). “Closed under countable unions” means that \\(\\bigcup_{i = 1}^\\infty A_i \\in \\mathcal{F}\\) whenever \\((A_1, A_2, \\ldots)\\) is a sequence of events in \\(\\mathcal{F}\\). The class \\(\\mathcal{F}\\) is called a \\(\\sigma\\)-algebra or \\(\\sigma\\)-field, and this restriction on the domain of probability helps avoid internal contradictions like the Banach-Tarski paradox.↩︎\n Technically, the support of \\(X\\) is the smallest closed set \\(S_X\\) such that \\(\\Pr(X \\in S_X) = 1\\). For a discrete random variable with support on a finite set, it is just the set of possible values. For a discrete random variable with support on a countably infinite set, it can include points whose probability mass is zero—a pathological case that we can safely ignore. For a continuous random variable, it can include values whose probability density is zero—a case that is not unusual or pathological.↩︎\n Named after Jacob Bernoulli (1655-1705), a Swiss mathematician who derived the first version of the law of large numbers and discovered the constant \\(e \\approx 2.718281828\\), which is the base for natural logarithms. He and his younger brother Johann Bernoulli (1667-1748) were some of the first mathematicians to try to understand and apply calculus, but their relationship eventually curdled into a jealous rivalry. A lunar impact crater called Bernoulli is named jointly after them.↩︎\n This becomes a fundamental insight when we discuss hypothesis tests for independence as well as confounding and selection bias.↩︎\n Discrete random variables \\(X\\) and \\(Y\\) are independent if \\(\\Pr(X = x \\vand Y = y) = \\Pr(X = x) \\Pr(Y = y)\\) for any possible values \\(x \\in \\supp(X)\\) and \\(y \\in \\supp(Y)\\). We will discuss independence more rigorously when we discuss conditional probabilities in Chapter 2.↩︎\n Named after Karl Pearson (1857-1936), an English mathematician who founded the modern discipline of mathematical statistics. In 1911, he started the world’s first university department of statistics at University College London. He was an outspoken socialist and supporter of women’s rights, but he was also a vocal proponent of social Darwinism and eugenics who opposed Jewish immigration into Britain.↩︎",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "condprob.html",
    "href": "condprob.html",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "",
    "text": "2.1 Contingency tables\nSuppose we know that an event \\(A\\) occurred and want calculate the probability that \\(B\\) also occurred. The conditional probability of \\(B\\) given \\(A\\) is \\[\n    \\Pr(B \\given{} A) = \\frac{\\Pr(A \\cap B)}{\\Pr(A)}.\n\\tag{2.1}\\] Note that this is well-defined only if \\(\\Pr(A) &gt; 0\\). Conditional probabilities given \\(A\\) are just probabilities where the original sample space \\(\\Omega\\) has been replaced with an event \\(A \\subseteq \\Omega\\). Everything we have learned about probabilities applies to all of the conditional probabilities given the same event \\(A\\). Conditional probability is arguably the most important mathematical tool in epidemiology.\nIn statistics, a contingency table classifies individuals by two discrete variables, one that defines the rows and one that defines the columns. Each cell in the table contains the number of individuals who are in the intersection of the corresponding categories of the row and column variables. These numbers are called cell counts. The margins of the table contain row or column totals.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#contingency-tables",
    "href": "condprob.html#contingency-tables",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "",
    "text": "2.1.1 2x2 tables\nIn epidemiology, a 2x2 table is a contingency table based on a binary exposure variable and a binary disease outcome. We denote exposure by \\(X = 1\\) and no exposure by \\(X = 0\\), and we denote disease by \\(D = 1\\) and no disease by \\(D = 0\\). The precise definition of “disease” depends on context. In descriptive epidemiology, \\(D_i = 1\\) might mean that person \\(i\\) is a prevalent case of disease. In analytic epidemiology, \\(D_i = 1\\) might mean that person \\(i\\) had an onset of disease in an interval \\((t_\\text{start}, t_\\text{stop}]\\) on a relevant time scale. We put exposure in the rows and disease in the columns,2 and the exposure and disease categories are ordered so that individuals with \\(X = 1\\) and \\(D = 1\\) go in the top left corner. This is the most common arrangement in epidemiologic research, but it is not universal.\nTable 2.1 shows an example of a 2x2 table. There are \\(a\\) individuals with both exposure and disease, \\(b\\) individuals with exposure but not disease, \\(c\\) individuals with disease but no exposure, and \\(d\\) individuals with neither. In the rows, there are \\(r_1 = a + b\\) exposed individuals and \\(r_0 = c + d\\) unexposed individuals. In the columns, there are \\(k_1 = a + c\\) individuals who had a disease onset and \\(k_0 = b + d\\) individuals who did not. The row and column totals are called the margins of the table. The total number of individuals is \\(n = a + b + c + d\\).\n\n\n\nTable 2.1: 2x2 table of exposure (\\(X\\)) and disease (\\(D\\)).\n\n\n\n\n\n\n\\(D = 1\\)\n\\(D = 0\\)\nTotal\n\n\n\n\n\\(X = 1\\)\n\\(a\\)\n\\(b\\)\n\\(r_1 = a + b\\)\n\n\n\\(X = 0\\)\n\\(c\\)\n\\(d\\)\n\\(r_0 = c + d\\)\n\n\nTotal\n\\(k_1 = a + c\\)\n\\(k_0 = b + d\\)\n\\(n = a + b + c + d\\)\n\n\n\n\n\n\n\n\n2.1.2 Joint and marginal probabilities\nHere, we assume that Table 2.1 represents our entire population \\(\\Omega\\) and our experiment is to randomly sample an individual \\(\\omega \\in \\Omega\\) and measure their exposure status \\(X(\\omega)\\) and their disease status \\(D(\\omega)\\). Probabilities involving both \\(X\\) and \\(D\\) are called joint probabilities, and they can be calculated using the cell counts. In Table 2.1, the four joint probabilities are \\[\n  \\begin{aligned}\n    \\Pr(X = 1 \\text{ and } D = 1) &= a / n, \\\\\n    \\Pr(X = 1 \\text{ and } D = 0) &= b / n, \\\\\n    \\Pr(X = 0 \\text{ and } D = 1) &= c / n, \\\\\n    \\Pr(X = 0 \\text{ and } D = 0) &= d / n.\n  \\end{aligned}\n\\] Together, these probabilities defined the joint distribution of the random variables \\(X\\) and \\(D\\) via their joint probability mass function (PMF).\nProbabilities involving \\(X\\) or \\(D\\) alone are called marginal probabilities because they are calculated using the margins of the table. In Table 2.1, the marginal probabilities for exposure \\(X\\) are \\[\n  \\begin{aligned}\n    \\Pr(X = 1) &= r_1 / n, \\\\\n    \\Pr(X = 0) &= r_0 / n.\n  \\end{aligned}\n\\] Together, these define the marginal distribution of \\(X\\), which is Bernoulli(\\(r_1 / n\\)). The marginal probabilities for disease \\(D\\) are \\[\n  \\begin{aligned}\n  \\Pr(D = 1) &= k_1 / n, \\\\\n  \\Pr(D = 0) &= k_0 / n.\n  \\end{aligned}\n\\] Together, these define the marginal distribution of \\(D\\), which is Bernoulli(\\(k_1 / n\\)).\n\n\n2.1.3 Conditional probabilities\nJoint and marginal probabilities can be used to calculate conditional probabilities, which have a joint probability in the numerator and a marginal probability in the denominator. As before, we assume that Table 2.1 represents our entire population \\(\\Omega\\) and our experiment is to randomly sample an individual \\(\\omega \\in \\Omega\\) and measure \\(X(\\omega)\\) and \\(D(\\omega)\\). In Table 2.1, the conditional probability of disease given exposure is \\[\n    \\Pr(D = 1 \\given{} X = 1)\n    = \\frac{Pr(D = 1 \\vand X = 1)}{\\Pr(X = 1)}\n    = \\frac{a / n}{r_1 / n}\n    = \\frac{a}{r_1},\n\\] and the conditional probability of disease given no exposure is \\[\n  \\Pr(D = 1 \\given{} X = 0)\n  = \\frac{Pr(D = 1 \\vand X = 0)}{\\Pr(X = 0)}\n  = \\frac{c / n}{r_0 / n}\n  = \\frac{c}{r_0},\n\\] Similarly, the conditional probability of exposure given disease is \\[\n  \\Pr(X = 1 \\given{} D = 1)\n  = \\frac{\\Pr(X = 1 \\vand D = 1)}{\\Pr(D = 1)}\n  = \\frac{a / n}{k_1 / n}\n  = \\frac{a}{k_1},\n\\] and the conditional probabilty of exposure given no disease is \\[\n  \\Pr(X = 1 \\given{} D = 0)\n  = \\frac{\\Pr(X = 1 \\vand D = 0)}{\\Pr(D = 0)}\n  = \\frac{b / n}{k_0 / n}\n  = \\frac{b}{k_0}.\n\\] In all cases, the table total cancels out and we get a calculation in one row (for conditional probabilities given \\(X\\)) or one column (for conditional probabilities given \\(D\\)).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#multiplication-of-conditional-probabilities",
    "href": "condprob.html#multiplication-of-conditional-probabilities",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.2 Multiplication of conditional probabilities",
    "text": "2.2 Multiplication of conditional probabilities\nEquation 2.1 can be rearranged into \\[\n    \\Pr(A \\cap B) = \\Pr(B \\given{} A) \\Pr(A),\n\\tag{2.2}\\] exactly as described by Bayes at the beginning of this chapter (if we let \\(A\\) be the “1st event” and \\(B\\) be the “2d”). This depends only on the definition of conditional probability in Equation 2.1, not on any assumptions about the relationship between the events \\(A\\) and \\(B\\). This multiplication rule for conditional probabilities extends to any number of events. For three events \\(A\\), \\(B\\), and \\(C\\) such that \\(B \\cap C\\) and \\(C\\) have probabilities greater than zero, we have \\[\\begin{aligned}\n  \\Pr(A \\cap B \\cap C)\n  &= \\Pr(A \\given{} B \\cap C) \\Pr(B \\cap C) \\\\\n  &= \\Pr(A \\given{} B \\cap C) \\Pr(B \\given{} C) \\Pr(C).\n\\end{aligned}\\] To ensure that all of these conditional probabilities are well-defined, we need \\(B \\cap C\\) and \\(C\\) to have probabilities greater than zero. In practice, \\(\\Pr(A \\given{} B \\cap C)\\) is usually written \\(\\Pr(A \\given{} B, C)\\).\n\n2.2.1 Decision trees\nFigure 2.1 shows an example of a decision tree. The root of the tree is on the left and the leaves of the tree are on the right. Each node where two or more branches meet represents a decision. In the example, the root represents the decision \\(A\\) or \\(A^C\\) (i.e., not \\(A\\)). The two nodes connected to the root each represent the decision \\(B\\) or \\(B^C\\) (i.e., not \\(B\\)). Each branch of the tree is labeled with the conditional probability of the branch given the event that it branches out from. Because of the multiplication rule for conditional probabilities, the probability of each leaf is equal to the product of the probabilities along the branches connecting it to the root.\n\n\n\n\n\n\nFigure 2.1: A decision tree for events \\(A\\) and \\(B\\). The probability of each leaf is found by multiplying the probabilities along the branches leading from the leaf back to the root.\n\n\n\n\n\n2.2.2 Independence of events\nThe events \\(A\\) and \\(B\\) are independent if \\[\n  \\Pr(A \\cap B) = \\Pr(A) \\Pr(B).\n\\tag{2.3}\\] When two events are independent, the occurrence (or not) of one event tells us nothing about whether the other event occurred: If \\(\\Pr(A) &gt; 0\\), equation Equation 2.3 is equivalent to \\(\\Pr(B \\given{} A) = \\Pr(B)\\). If \\(\\Pr(B) &gt; 0\\), it is equivalent to \\(\\Pr(A \\given{} B) = \\Pr(A)\\). If \\(A\\) and \\(B\\) are not independent, the occurrence of \\(A\\) contains information about the occurrence of \\(B\\) and vice versa.\nIndependence of events \\(A\\) and \\(B\\) implies that the events \\(A\\) and \\(B^\\comp\\) are also independent: \\[\n  \\begin{aligned}\n    \\Pr\\bigl(A \\cap B^\\comp\\bigr)\n    &= \\Pr(A) - \\Pr(A \\cap B) \\\\\n    &= \\Pr(A) - \\Pr(A) \\Pr(B) \\\\\n    &= \\Pr(A) \\big(1 - \\Pr(B)\\big) \\\\\n    &= \\Pr(A) \\Pr\\bigl(B^\\comp\\bigr).\n  \\end{aligned}\n\\] A similar argument shows that \\(A^\\comp\\) and \\(B\\) are independent. Because \\(A^\\comp \\cap B^\\comp = (A \\cup B)^\\comp\\) by DeMorgan’s laws (see Section 1.1.5), \\[\n  \\begin{aligned}\n    \\Pr\\bigl(A^\\comp \\cap B^\\comp\\bigr)\n    &= 1 - \\Pr(A \\cup B) \\\\\n    &= 1 - \\Pr(A) - \\Pr(B) - \\Pr(A \\cap B) \\\\\\\n    &= 1 - \\Pr(A) - \\Pr(B) - \\Pr(A) \\Pr(B) \\\\\n    &= \\big(1 - \\Pr(A)\\big) \\big(1 - \\Pr(B)\\big) \\\\\n    &= \\Pr\\bigl(A^\\comp\\bigr) \\Pr\\bigl(B^\\comp\\bigr).\n  \\end{aligned}\n\\] Therefore, independence of two events implies independence between any combination of themselves or their complements.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#sensitivity-and-specificity",
    "href": "condprob.html#sensitivity-and-specificity",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.3 Sensitivity and specificity",
    "text": "2.3 Sensitivity and specificity\nIn the epidemiology of screening and diagnostic tests, several of the most important concepts are conditional probabilities. If we classify disease status into diseased (\\(D^+\\)) and nondiseased (\\(D^-\\)) and the test result into positive (\\(T^+\\)) and negative (\\(T^-\\)), we have the four possible combinations Table 2.2.\n\n\n\nTable 2.2: Disease status (\\(D^+\\)/\\(D^-\\)) and test result (\\(T^+\\)/\\(T^-\\)).\n\n\n\n\n\n\n\\(T^+\\)\n\\(T^-\\)\n\n\n\n\n\\(D^+\\)\nTrue positive\nFalse negative\n\n\n\\(D^-\\)\nFalse positive\nTrue negative\n\n\n\n\n\n\nThe sensitivity of a test is the conditional probability that the test is positive given that the individual tested has the disease: \\[\n  \\sens = \\Pr(T^+ \\given{} D^+).\n\\] The specificity of a test is the conditional probability that the test is negative given that the individual tested does not have the disease: \\[\n  \\spec = \\Pr(T^- \\given{} D^-).\n\\] In both cases, we are conditioning on the disease status of the individual being tested. These concepts were introduced by Yerushalmy (1947) in a comparison of different types of chest X-rays for tuberculosis case detection.\n\nR\n\n\n\n\nsensspec.R\n\n## Sensitivity and specificity\n\n# generate diagnostic testing data\nset.seed(42)\nn &lt;- 500\ndtdat &lt;- data.frame(disease = rbinom(n, 1, 0.5))\ndtdat$testpos &lt;- ifelse(dtdat$disease,\n                        rbinom(n, 1, 0.85), rbinom(n, 1, 0.05))\n\n# prevalence\nmean(dtdat$disease)\n# Pr(T+)\nmean(dtdat$testpos)\n\n# sensitivity\nmean(dtdat$testpos[dtdat$disease == TRUE])\nsum(dtdat$disease & dtdat$testpos) / sum(dtdat$disease)\n\n# specificity\n1 - mean(dtdat$testpos[dtdat$disease == FALSE])\nmean(!dtdat$testpos[dtdat$disease == FALSE])\n\n\n\n\nMaximizing either sensitivity or specificity alone does not necessarily lead to good screening or diagnostic test: A test where everyone tests positive has perfect sensitivity but zero specificity, and a test where everyone tests negative has perfect specificity but zero sensitivity. There is almost always a tradeoff where higher sensitivity leads to lower specificity and vice versa.\n\n2.3.1 Example: Diabetes testing\nRemein and Wilkerson (1961) describe an early study of diabetes screening conducted by the United States Public Health Service in Boston City Hospital between 1954 and 1957. They recruited early-morning patients who were not febrile or acutely ill. Those willing to participate gave urine and blood samples. Next, they were given a meal meant to approximate an average breakfast or light lunch (a sandwich, 5 grams of butter, 60 grams of cheese, and three filled cookies). After the meal, they gave further urine and blood samples at one, two, and three hours after eating. The samples were analyzed using four different blood tests and six different urine tests. Participants returned for a follow-up visit between 3 and 21 days after the screening tests, where a definitive diagnosis of diabetes was made using an oral glucose tolerance test and a physical examination according to criteria established by a group of experts.\nA total of 595 participants completed both visits. Table 2.3 is a reconstruction of the data for the Somogyi-Nelson blood test based on the 580 participants (70 with diabetes and 510 without) who took the test at all four time points. In the table, a positive test is defined as a blood glucose concentration above 130 mg/dL (milligrams per deciliter).\n\n\n\nTable 2.3: Sensitivity and specificity of the Somogyi-Nelson blood glucose test for diabetes where \\(T^+\\) corresponds to a concentration above 130 mg/dL.\n\n\n\n\n\n\n\\(T^+\\)\n\\(T^-\\)\nSensitivity and specificity\n\n\n\n\nBefore meal\n\n\n\\(D^+\\)\n31\n39\n\\(\\sens = 31 / 70 \\approx 0.443\\)\n\n\n\\(D^-\\)\n5\n505\n\\(\\spec = 505 / 510 \\approx 0.990\\)\n\n\nOne hour after meal\n\n\n\\(D^+\\)\n55\n15\n\\(\\sens = 55 / 70 \\approx 0.786\\)\n\n\n\\(D^-\\)\n48\n462\n\\(\\spec = 462 / 510 \\approx 0.906\\)\n\n\nTwo hours after meal\n\n\n\\(D^+\\)\n45\n25\n\\(\\sens = 45 / 70 \\approx 0.643\\)\n\n\n\\(D^-\\)\n16\n494\n\\(\\spec = 494 / 510 \\approx 0.969\\)\n\n\nThree hours after meal\n\n\n\\(D^+\\)\n34\n36\n\\(\\sens = 34 / 70 \\approx 0.486\\)\n\n\n\\(D^-\\)\n1\n509\n\\(\\spec = 509 / 510 \\approx 0.998\\)\n\n\n\n\n\n\n\nR\n\n\n\n\nRWtable.R\n\n## Table 2 from Remein and Wilkerson (Journal of Chronic Disease, 1961)\n\n# function to generate numbers based on sensitivity and specificity\nRWtable &lt;- function(sens, spec, n1=70, n0=510) {\n  # arguments:  sensitivity, specificity,\n  #             n1 is number of diabetics, n0 is number of nondiabetics\n  tp &lt;- round(sens * n1)\n  fp &lt;- round((1 - spec) * n0)\n  tn &lt;- round(spec * n0)\n  fn &lt;- round((1 - sens) * n1)\n  return(c(truepos = tp, falsepos = fp, trueneg = tn, falseneg = fn))\n}\n\nRWtable(0.443, 0.990)   # before meal\nRWtable(0.786, 0.906)   # one hour after\nRWtable(0.643, 0.969)   # two hours after\nRWtable(0.486, 0.998)   # three hours after\n\n\n\n\n\n\n2.3.2 Receiver operating characteristic (ROC) curves*\nThe tradeoff between sensitivity and sensitivity in choosing a clinical measurement cutoff to distinguish positive and negative tests can be seen using a receiver operating characteristic (ROC) curve (Lusted 1971a, 1971b; Swets 1988; Zweig and Campbell 1993). These curves were originally used in World War II to analyze the performance of radar systems locating ships and airplanes. They were applied to diagnostic tests in the late 1950s in the first attempt to automate the classification of Pap smears to detect cervical cancer (Bostrom, Sawyer, and Tolles 1959; Lusted 1984; Bengtsson and Malm 2014).\nEach combination of a clinical measurement and a cutoff between positive and negative tests defines a diagnostic or screening test that has a sensitivity \\(\\sens \\in [0, 1]\\) and a specificity \\(\\spec \\in [0, 1]\\). The horizontal axis of an ROC curve plots \\[\n  1 - \\spec = \\Pr(T^+ \\given{} D^-),\n\\] and its vertical axis plots \\(\\sens = \\Pr(T^+ \\given{} D^+)\\). The test corresponds to a point \\((1 - \\spec, \\sens)\\) in the unit square \\([0, 1] \\times [0, 1]\\). The best tests correspond to points close to the top left corner \\((0, 1)\\), which represents a test with perfect specificity (so \\(1 - \\spec = 0\\)) and perfect sensitivity.\nFor a sequence of cutoffs, a given clinical measurement produces a curve connecting the points produced by the tests based on it. Figure 2.2 shows four ROC curves based on data from Remein and Wilkerson (1961): one for the Somogyi-Nelson blood glucose measurement before the meal and one each for the measurements one, two, and three hours after the meal. For all four measurements, the curves are based on the combinations of sensitivity and specificity for glucose concentration cutoffs from 70 mg/dL to 200 mg/dL. In these tests, using a higher glucose concentration cutoff to define a positive test leads to lower sensitivity and higher specificity.\n\n\n\nCode\n\nROCcurve.R\n\n# data from Table 2 in Remein and Wilkerson (Journal of Chronic Disease, 1961)\nSNdat &lt;- data.frame(cutoff = seq(70, 200, by = 10))\nSNdat$sens_pre &lt;- c(95.7, 91.4, 82.9, 65.7, 54.3, 50.0, 44.3, 37.1, 30.0,\n                    25.7, 25.7, 22.9, 21.4, 17.1) / 100\nSNdat$spec_pre &lt;- c(11.0, 36.3, 65.7, 84.7, 92.7, 96.7, 99.0, 99.6, 99.8,\n                    99.8, 99.8, 99.8, 100.0, 100.0) / 100\nSNdat$sens_1hr &lt;- c(100.0, 97.1, 97.1, 95.7, 92.9, 88.6, 78.6, 68.6, 57.1,\n                    52.9, 47.1, 40.0, 34.3, 28.6) / 100\nSNdat$spec_1hr &lt;- c(8.2, 22.4, 39.0, 57.3, 70.6, 83.3, 90.6, 95.1, 97.8,\n                    99.4, 99.6, 99.8, 100.0, 100.0) / 100\nSNdat$sens_2hr &lt;- c(98.6, 97.1, 94.3, 88.6, 85.7, 71.4, 64.3, 57.1, 50.0,\n                    47.1, 42.9, 38.6, 34.3, 27.1) / 100\nSNdat$spec_2hr &lt;- c(8.8, 25.5, 47.6, 69.8, 84.1, 92.5, 96.9, 99.4, 99.6,\n                    99.8, 100.0, 100.0, 100.0, 100.0) / 100\nSNdat$sens_3hr &lt;- c(94.3, 91.4, 82.9, 70.0, 60.0, 51.4, 48.6, 41.4, 32.9,\n                    28.6, 28.6, 28.6, 24.3, 20.0) / 100\nSNdat$spec_3hr &lt;- c(8.6, 34.7, 67.5, 86.5, 95.3, 98.2, 99.8,\n                    rep(100.0, 7)) / 100\n# write.csv(SNdat, \"SNdat.csv\", row.names = FALSE)\n\n# ROC curves with labels\nplot(1 - SNdat$spec_pre, SNdat$sens_pre, type = \"n\",\n     xlim = c(0, 1), ylim = c(0, 1),\n     xlab = \"1 - Specificity = Pr(T+ | D-)\",\n     ylab = \"Sensitivity = Pr(T+ | D+)\")\ngrid()\nlines(1 - SNdat$spec_pre, SNdat$sens_pre, col = \"darkgray\")\nlines(1 - SNdat$spec_1hr, SNdat$sens_1hr, lty = \"solid\")\nlines(1 - SNdat$spec_2hr, SNdat$sens_2hr, lty = \"dashed\")\nlines(1 - SNdat$spec_3hr, SNdat$sens_3hr, lty = \"dotted\")\npoints(1 - SNdat[SNdat$cutoff == 130, c(3, 5, 7, 9)],\n       SNdat[SNdat$cutoff == 130, c(2, 4, 6, 8)])\npoints(1 - SNdat$spec_pre[seq(2, 12, by = 2)],\n       SNdat$sens_pre[seq(2, 12, by = 2)], pch = 8)\ntext(1 - SNdat$spec_pre[seq(2, 12, by = 2)] + c(0, .09, .09, .1, .1, .1),\n     SNdat$sens_pre[seq(2, 12, by = 2)] + c(-.05, -.02, -.02, 0, 0, -.01),\n     labels = c(\"80 mg/dL\", \"100 mg/dL\", \"120 mg/dL\", \"140 mg/dL\",\n                \"160 mg/dL\", \"180 mg/dL\"))\nabline(0, 1, lty = \"dotted\", col = \"darkgray\")\ntext(.51, .49, adj = c(.5, 1), srt = 42,\n     label = \"Useless tests (T and D independent)\")\npoints(c(0, 0, 1), c(0, 1, 1), pch = 3)\ntext(.01, .99, adj = c(0, 1), label = \"Perfect test\")\ntext(.01, .01, adj = c(0, 0), label = \"Everyone tests negative\")\ntext(.99, .99, adj = c(1, 0), srt = 90, label = \"Everyone tests positive\")\nlegend(\"bottomright\", bg = \"white\",\n       lty = c(\"solid\", \"solid\", \"dashed\", \"dotted\", NA),\n       col = c(\"darkgray\", rep(\"black\", 4)), pch = c(rep(NA, 4), 1),\n       legend = c(\"Before meal  (AUC = 0.825)\",\n                  \"1 hour after   (AUC = 0.923)\",\n                  \"2 hours after (AUC = 0.904)\",\n                  \"3 hours after (AUC = 0.839)\", \"130 mg/dL cutoff\"))\n\n\n\n\n\n\n\n\n\nFigure 2.2: ROC curves for Somogyi-Nelson blood tests conducted before the meal and at 1-3 hours after the meal. Cutoff values for the before-meal curve are labeled, and the points corresponding to the 130 mg/dL cutoff along the curve for each blood glucose measurement are circled.\n\n\n\n\n\nROC curves for different clinical measurements can be compared using the area under the curve (AUC), which is the area between the x-axis \\([0, 1]\\) and the ROC curve. Greater AUC corresponds to a measurement that is better able to distinguish between disease and no disease (Bamber 1975; Hanley and McNeil 1982). For a test that is positive when a clinical measurement is above a given cutoff, the AUC is the probability that a person with disease has a higher value than a person without disease.3 In this example, it is the probability that a true diabetic has a higher blood glucose concentration than a true nondiabetic at the time blood glucose concentration is measured. A measurement that was always higher (or always lower) for individuals with disease than individuals without disease would have \\(\\text{AUC} = 1\\). The AUCs in Figure 2.2 show clearly that the tests one and two hours after the meal, which have curves above and to the left of the other two curves, better distinguish between diabetics and nondiabetics than the tests before and three hours after the meal. This is biologically plausible: Before the meal, there is no glucose load. Three hours after the meal, the glucose from the meal has largely been absorbed.\n\nR\n\n\n\n\nauc.R\n\n## areas under the ROC curves\n\n# load Somogyi-Nelson test data generated for Figure 2.2 (if needed)\n# The argument can contain a path before the file name.\nSNdat &lt;- read.csv(\"SNdat.csv\")\n\nauc &lt;- function(x, y) {\n  # x is an increasing list of specificities\n  # y is a decreasing list of sensitivities\n  roc &lt;- approxfun(c(1, 1 - x, 0), c(1, y, 0), ties = \"max\")\n  area &lt;- integrate(function(x) roc(x), 0, 1)\n  return(area)\n}\nauc(SNdat$spec_pre, SNdat$sens_pre)\nauc(SNdat$spec_1hr, SNdat$sens_1hr)\nauc(SNdat$spec_2hr, SNdat$sens_2hr)\nauc(SNdat$spec_3hr, SNdat$sens_3hr)\n\n\n\n\nThe test one hour after the meal with a 130 mg/dL cutoff has a good combination of sensitivity and specificity. It is near the top left corner, where perfect tests live. If a diagnostic test was completely useless, the test results (\\(T^+\\) or \\(T^-\\)) would be independent of disease status (\\(D^+\\) or \\(D^-\\)). In that case, \\[\n  \\Pr(T^+ \\given{} D^+) = \\Pr(T^+ \\given{} D^-) = \\Pr(T^+).\n\\] Thus, the ROC curve for a useless test follows the diagonal line from the lower left corner \\((0, 0)\\) to the upper right corner \\((1, 1)\\), and it has an AUC of 0.5. Tests below the diagonal on an ROC curve are worse than useless: the definitions of positive and negative should be reversed.\nThe sensitivity and specificity of a test tell us how accurate it is with a given definition of positive and negative. The ROC curve shows us how this accuracy depends on the cutoff between positive and negative tests, and the area under the curve shows us how well the underlying clinical measurement (e.g., blood glucose concentration) can distinguish between people with and without disease. However, the best cutoff for a test depends on its purpose, the population to be tested, and the benefit of identifying a true positive or negative versus the harm of a false positive or negative (Blumberg 1957; Kessel 1962).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#law-of-total-probability",
    "href": "condprob.html#law-of-total-probability",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.4 Law of total probability",
    "text": "2.4 Law of total probability\nSuppose \\(A_1, \\ldots, A_n\\) are disjoint events such that their union is \\(\\Omega\\). This is called a partition of \\(\\Omega\\). An important special case is when we partition \\(\\Omega\\) into \\(A\\) and \\(A^\\comp\\).\nLet \\(B\\) be another event. Every \\(\\omega \\in B\\) is in exactly one of the \\(A_i\\). For each \\(i\\), \\(B \\cap A_i\\) is the part of \\(B\\) that is contained in \\(A_i\\). The event \\(B\\) is the union of these subsets: \\[\n  B = \\bigcup_{i = 1}^n (B \\cap A_i).\n\\] Because \\(A_i\\) are disjoint, so are the subsets \\(B \\cap A_i\\). By the addition rule for probabilities of disjoint sets, we have \\[\n  \\Pr(B) = \\sum_{i = 1}^n \\Pr(B \\cap A_i)\n\\] which is the sum of the \\(\\Pr(B \\cap A_i)\\).4 Using the multiplication rule for conditional probabilities in Equation 2.2 on each \\(\\Pr(B \\cap A_i)\\), we get \\[\n  \\Pr(B) = \\sum_{i = 1}^n \\Pr(B \\given{} A_i) \\Pr(A_i).\n\\] This is called the law of total probability.\n\n2.4.1 Example: probability of a positive or negative test\nWe can use the law of total probability to calculate the probability of a positive or negative test based on the sensitivity and specificity of the test and the prevalence of disease. Because all individuals either do or do not have the disease,5 we have \\[\n  T^+ = (T^+ \\cap D^+) \\cup (T^+ \\cap D^-).\n\\] These two groups are mutually exclusive, so \\[\n  \\Pr(T^+) = \\Pr(T^+ \\cap D^+) + \\Pr(T^+ \\cap D^-).\n\\] We can calculate each probability on the right-hand side using the multiplication rule in Equation 2.2: \\[\n  \\begin{aligned}\n    \\Pr(T^+ \\cap D^+)\n    &= \\Pr(T^+ \\given{} D^+) \\Pr(D^+)\n      = \\text{sensitivity} \\times \\text{prevalence},\\\\\n    \\Pr(T^+ \\cap D^-)\n    &= \\Pr(T^+ \\given{} D^-) \\Pr(D^-)\n      = (1 - \\text{specificity}) \\times (1 - \\text{prevalence}).\n  \\end{aligned}\n\\] Putting everything together, we get \\[\n  \\begin{aligned}\n    \\Pr(T^+)\n    &= \\Pr(T^+ \\given{} D^+) \\Pr(D^+) + \\Pr(T^+ \\given{} D^-) \\Pr(D^-) \\\\\n    &= \\text{sensitivity} \\times \\text{prevalence}\n      + (1 - \\text{specificity}) \\times (1 - \\text{prevalence}).\n  \\end{aligned}\n\\tag{2.4}\\] A similar chain of reasoning shows that \\[\n  \\Pr(T^-) = (1 - \\text{sensitivity}) \\times \\text{prevalence} + \\text{specificity} \\times (1 - \\text{prevalence}),\n\\] which equals \\(1 - \\Pr(T^+)\\).\nFigure 2.3 shows how the probability of a positive test depends on the prevalence of disease using the example of the Somogyi-Nelson test one hour after the meal in Table 2.3. With a cutoff of 130 mg/dL, the test has a sensitivity of 0.786 and a specificity of 0.906. At low prevalences, the test overestimates the prevalence of diabetes due to imperfect specificity. A high prevalences, it underestimates the prevalence of diabetes due to imperfect sensitivity. The errors cancel out somewhere near a prevalence of 30%.\n\n\n\nCode\n\ntestpos.R\n\n## probability of testing positive as a function of prevalence\n\n# function to generate testing data\ntdat &lt;- function(prev, sens=0.786, spec=0.906) {\n  # defaults are sensitivity and sensitivity one hour after the meal\n  truepos &lt;- sens * prev\n  falsepos &lt;- (1 - spec) * (1 - prev)\n  trueneg &lt;- spec * (1 - prev)\n  falseneg &lt;- (1 - spec) * prev\n  pos &lt;- truepos + falsepos\n  neg &lt;- 1 - pos\n  ppv &lt;- truepos / pos\n  npv &lt;- trueneg / neg\n  return(data.frame(prev = prev, sens = sens, spec = spec,\n                    truepos = truepos, falsepos = falsepos,\n                    trueneg = trueneg, falseneg = falseneg,\n                    pos = pos, neg = neg, ppv = ppv, npv = npv))\n}\ntdat_1hr &lt;- tdat(seq(0, 1, by = .01))\nwrite.csv(tdat_1hr, \"R/tdat_1hr.csv\", row.names = FALSE)\n\n# plot\nplot(tdat_1hr$prev, tdat_1hr$pos, type = \"n\", xlim = c(0, 1), ylim = c(0, 1),\n     xlab = \"Prevalence of disease = Pr(D+)\",\n     ylab = \"Probability of positive test = Pr(T+)\")\npolygon(c(tdat_1hr$prev, 1, 0), c(tdat_1hr$pos, 0, 0),\n        border = NA, col = \"gray\")\npolygon(c(tdat_1hr$prev, 1, 0), c(tdat_1hr$falsepos, 0, 0),\n        border = NA, col = \"darkgray\")\ngrid()\nlines(tdat_1hr$prev, tdat_1hr$falsepos, col = \"gray\")\nlines(tdat_1hr$prev, tdat_1hr$pos)\nabline(0, 1, lty = \"dotted\")\ntext(0.1, 0.02, adj = c(0, 0), label = \"False positives\")\ntext(0.2, 0.1, adj = c(0, 0), label = \"True positives\")\n\n\n\n\n\n\n\n\n\nFigure 2.3: The probability of a positive Somogyi-Nelson diabetes test one hour after the meal as a function of the hypothetical prevalence of diabetes. The black dotted line shows the true prevalence of diabetes.\n\n\n\n\n\n\n\n2.4.2 Standardization\nIn epidemiology, it is often useful to think of our sample space \\(\\Omega\\) as a population and the outcomes \\(\\omega \\in \\Omega\\) as individuals. The sets \\(A_1, \\ldots, A_n\\) into which we partition the sample space are disjoint subpopulations (e.g., age groups). Let \\(\\Pr(D \\given{} A_i)\\) be the prevalence of disease in subpopulation \\(A_i\\) at a given time point. Then the overall prevalence of disease is \\[\n  \\Pr(D) = \\sum_{i = 1}^n \\Pr(D \\given{} A_i) \\Pr(A_i).\n\\tag{2.5}\\] This application of the law of total probability is called standardization. By changing the \\(\\Pr(A_i)\\), we can use the subpopulation prevalences to calculate the prevalence of disease in a population with any desired composition of subpopulations. Equation 2.5 can also be used to calculate population-level risk from the subpopulation-specific risks in any given time interval. In the form of standardization, the law of total probability is one of the most important tools in epidemiology.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#bayes-rule",
    "href": "condprob.html#bayes-rule",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.5 Bayes’ rule",
    "text": "2.5 Bayes’ rule\nBayes’ rule (Bayes 1763) relates the conditional probabilities \\(\\Pr(A \\given{} B)\\) and \\(\\Pr(B \\given{} A)\\): \\[\n  \\Pr(A \\given{} B) = \\frac{\\Pr(B \\cap A)}{\\Pr(B)} = \\frac{\\Pr(B \\given{} A) \\Pr(A)}{\\Pr(B)}.\n\\tag{2.6}\\] In the denominator, the law of total probability is often used to calculate \\(\\Pr(B)\\) via partitioning \\(\\Omega\\) into \\(A\\) and \\(A^\\comp\\). This gives us \\[\n  \\Pr(A \\given{} B) = \\frac{\\Pr(B \\given{} A) \\Pr(A)}{\\Pr(B \\given{} A) \\Pr(A) + \\Pr(B \\given{} A^\\comp) \\Pr(A^\\comp)}.\n\\] Bayes’ rule is an incredibly useful application of conditional probabilities, and it forms the theoretical foundation for Bayesian statistical inference.\n\n2.5.1 Positive and negative predictive values\nSensitivity and specificity tell us how disease status predicts the result of a test, but they do not tell us how to interpret a test result. If you test positive, it is important to know the conditional probability that you truly have disease given that you tested positive. This is called the positive predictive value (PPV): \\[\n  \\text{PPV} = \\Pr(D^+ \\given{} T^+).\n\\] If you test negative, it is important to know the conditional probability that you are truly disease-free given that you tested negative. This is called the negative predictive value (NPV): \\[\n  \\text{NPV} = \\Pr(D^- \\given{} T^-).\n\\] These terms were introduced by Vecchio (1966). Table 2.4 shows the PPV and NPV for the Somogyi-Nelson diabetes tests from Table 2.3.\n\n\n\nTable 2.4: PPV and NPV of the Somogyi-Nelson blood glucose test for diabetes where \\(T^+\\) corresponds to a concentration above 130 mg/dL.\n\n\n\n\n\n\n\\(T^+\\)\n\\(T^-\\)\nPPV and NPV\n\n\n\n\nBefore meal\n\n\n\\(D^+\\)\n31\n39\n\\(\\text{PPV} = 31 / 36 \\approx 0.861\\)\n\n\n\\(D^-\\)\n5\n505\n\\(\\text{NPV} = 505 / 544 \\approx 0.928\\)\n\n\nTotal\n36\n544\n\n\n\nOne hour after meal\n\n\n\\(D^+\\)\n55\n15\n\\(\\text{PPV} = 55 / 103 \\approx 0.534\\)\n\n\n\\(D^-\\)\n48\n462\n\\(\\text{NPV} = 462 / 477 \\approx 0.969\\)\n\n\nTotal\n103\n477\n\n\n\nTwo hours after meal\n\n\n\\(D^+\\)\n45\n25\n\\(\\text{PPV} = 45 / 61 \\approx 0.738\\)\n\n\n\\(D^-\\)\n16\n494\n\\(\\text{NPV} = 494 / 519 \\approx 0.952\\)\n\n\nTotal\n61\n519\n\n\n\nThree hours after meal\n\n\n\\(D^+\\)\n34\n36\n\\(\\text{PPV} = 34 / 35 \\approx 0.971\\)\n\n\n\\(D^-\\)\n1\n509\n\\(\\text{NPV} = 509 / 545 \\approx 0.934\\)\n\n\nTotal\n35\n545\n\n\n\n\n\n\n\nVecchio (1966) showed that the PPV and NPV depend on the prevalence of disease as well as the sensitivity and specificity of the test. To calculate the PPV and NPV, we use Bayes’ rule to switch the conditional probabilities from \\(\\Pr(T \\given{} D)\\) to \\(\\Pr(D \\given{} T)\\). From the definition of PPV and Bayes’ rule, we get \\[\n  \\Pr(D^+ \\given{} T^+)\n  = \\frac{\\Pr(T^+ \\cap D^+)}{\\Pr(T^+)}\n  = \\frac{\\Pr(T^+ \\given{} D^+) \\Pr(D^+)}{\\Pr(T^+)}.\n\\] The sensitivity of the test and the prevalence of disease are in the numerator, and \\(\\Pr(T+)\\) is in Equation 2.4. Putting this all together, we get \\[\n  \\text{PPV}\n  = \\frac{\\text{sensitivity} \\times \\text{prevalence}}{\\text{sensitivity} \\times \\text{prevalence}\n    + (1 - \\text{specificity}) \\times (1 - \\text{prevalence})}.\n\\] The numerator is the probability of a true positive test, and the denominator is the probability of a (true or false) positive test. By a similar argument, \\[\n  \\text{NPV} = \\frac{\\text{specificity} \\times (1 - \\text{prevalence})}{\\text{specificity} \\times (1 - \\text{prevalence})\n    + (1 - \\text{sensitivity}) \\times \\text{prevalence}}.\n  \\label{eq:npv}\n\\] The numerator is the probability of a true negative test, and the denominator is the probability of a (true or false) negative test.\nFigure 2.4 shows how the positive and negative predictive values of a test depend on the prevalence of disease for the Somogyi-Nelson test before the meal and one hour after the meal in Remein and Wilkerson (1961). With a cutoff of 130 mg/dL, the sensitivity and specificity are \\(0.443\\) and \\(0.990\\) before the meal and \\(0.786\\) and \\(0.906\\) one hour after the meal. If prevalence equals zero, the PPV is zero and the NPV equals one because no one has disease. As prevalence increases, PPV increases and NPV decreases. If the prevalence equals one, the PPV is one and the NPV is zero because everyone has disease. A perfect test would have PPV and NPV equal to one at all prevalences.\n\n\n\nCode\n\npredval.R\n\n## Predictive values as a function of prevalence\n\n# uses tdat_1hr data and tdat() function from Figure 2.3 (testpos.R)\n# tdat_1hr &lt;- read.csv(\"tdat_1hr.csv\")\n# generate data using the sensitivity and specificity of the pre-meal test\ntdat_pre &lt;- tdat(seq(0, 1, by = .01), sens = 0.443, spec = 0.990)\n\n# plot of PPV and NPV as a function of diabetes prevalence\nplot(tdat_1hr$prev, tdat_1hr$ppv, type = \"n\", xlim = c(0, 1), ylim = c(0, 1),\n     xlab = \"Prevalence of disease = Pr(D+)\",\n     ylab = \"Predictive value = Pr(D | T)\")\ngrid()\nlines(tdat_1hr$prev, tdat_1hr$ppv)\nlines(tdat_1hr$prev, tdat_1hr$npv, lty = \"dashed\")\nlines(tdat_pre$prev, tdat_pre$ppv, col = \"darkgray\")\nlines(tdat_pre$prev, tdat_pre$npv, lty = \"dashed\", col = \"darkgray\")\nlegend(\"bottom\", lty = c(\"solid\", \"dashed\", \"solid\", \"dashed\"),\n       col = c(\"darkgray\", \"darkgray\", \"black\", \"black\"),\n       bg = \"white\", inset = 0.05,\n       legend = c(\"PPV before meal\", \"NPV before meal\",\n                  \"PPV 1 hour after\", \"NPV 1 hour after\"))\n\n\n\n\n\n\n\n\n\nFigure 2.4: Positive and negative predictive values of the Somogyi-Nelson diabetes test before the meal (gray) and one hour after the meal (black) as a function of diabetes prevalence.\n\n\n\n\n\n\n\n2.5.2 Likelihood ratios*\nFor a probability \\(p\\), the odds is \\[\n  \\theta = \\frac{p}{1 - p}.\n\\] While a probability lives in \\([0, 1]\\), the odds can go from zero (for \\(p = 0\\)) to infinity (as \\(p\\) approaches one). There is a one-to-one relationship between probabilities and odds, so we can calculate the probability of an event if we know the odds. If the odds is \\(\\theta\\), the corresponding probability is \\[\n  p = \\frac{\\theta}{1 + \\theta}.\n\\] Odds and odds ratios have an important role in epidemiology and statistical inference. In a Bayesian statistical framework, odds ratios give us a simple way to update our knowledge about the probability of an event given new information.\nSuppose we know the prevalence of a disease in a population \\(\\Omega\\). We randomly sample an individual \\(\\omega \\in \\Omega\\) and give them a diagnostic test. If we randomly sample an individual \\(\\omega\\) from a population \\(\\Omega\\), the odds that \\(\\omega\\) has disease is \\[\n  \\frac{\\Pr(D^+)}{1 - \\Pr(D^+)}\n  = \\frac{\\Pr(D^+)}{\\Pr(D^-)}.\n\\] where \\(\\Pr(D^+)\\) is the prevalence of disease. This is called the prior odds of disease. If \\(\\omega\\) tests positive for the disease, the conditional odds that they have disease is \\[\n  \\frac{PPV}{1 - PPV}\n  = \\frac{\\Pr(D^+ \\given{} T^+)}{\\Pr(D^- \\given{} T^+)}\n  = \\frac{\\Pr(D^+ \\cap T^+)}{\\Pr(D^- \\cap T^+)},\n\\] where we have cancelled out \\(\\Pr(T^+)\\) from the numerator and the denominator in the last expression. This is called the posterior odds of disease. The second expression above shows that the probability corresponding to the posterior odds is the PPV.\nUsing the multiplication rule for conditional probabilities, we get \\[\n  \\frac{\\Pr(D^+ \\cap T^+)}{\\Pr(D^- \\cap T^+)}\n  = \\frac{\\Pr(T^+ \\given D^+) \\Pr(D^+)}{\\Pr(T^+ \\given D^-) \\Pr(D^-)}\n  = \\frac{\\text{sensitivity}}{1 - \\text{specificity}}\n    \\times \\frac{\\Pr(D^+)}{\\Pr(D^-)}.\n\\] The term \\(\\text{sensitivity} / (1 - \\text{specificity})\\) is called the likelihood ratio. If our individual \\(\\omega\\) tests positive for disease, \\[\n  \\text{posterior odds of } D^+\n  = \\text{likelihood ratio} \\times \\text{prior odds of } D^+.\n\\] The likelihood ratio is a measure of how much we learn from a positive test result, and it does not depend on the prevalence of disease [Lusted (1971b); Swets (1973); Fagan (1975); Albert (1982); Zweig and Campbell (1993)}. Because an ROC curve plots sensitivity on the vertical axis and \\(1 - \\text{specificity}\\) on the horizontal axis, the likelihood ratio for a given test is the slope of the line from the point \\((0, 0)\\) to the point representing the test.\nTable 2.5 shows the prior odds, likelihood ratio, posterior odds, and PPV for the Somogyi-Nelson blood glucose tests for diabetes from 580 participants (70 with diabetes and 510 without) in Remein and Wilkerson (1961). Note that the tests with the highest likelihood ratios come from the glucose measurements that had the lowest AUCs in Figure 2.2. These tests have high likelihood ratios despite their low sensitivity because they have specificities near one. The test with the best combination of sensitivity and specificity in Table 2.3 has the lowest likelhood ratio. Like other summaries of diagnostic test performance, the likelihood ratio by itself does not determine the best test for a given purpose.\n\n\n\nTable 2.5: Prior odds, likelihood ratios, posterior odds, and PPV for the Somogyi-Nelson blood glucose test for diabetes where \\(T^+\\) corresponds to a concentration above 130 mg/dL.\n\n\n\n\n\nTest\nPrior odds\nLikelihood ratio\nPosterior odds\nPPV\n\n\n\n\nBefore meal\n\\(70 / 510 \\approx 0.137\\)\n45.171\n\\(31 / 5 = 6.200\\)\n\\(31 / 36 \\approx 0.861\\)\n\n\n1 hour after\n\\(70 / 510 \\approx 0.137\\)\n8.348\n\\(55 / 48 \\approx 1.146\\)\n\\(55 / 103 \\approx 0.534\\)\n\n\n2 hours after\n\\(70 / 510 \\approx 0.137\\)\n20.491\n\\(45 / 16 \\approx 2.813\\)\n\\(45 / 61 \\approx 0.738\\)\n\n\n3 hours after\n\\(70 / 510 \\approx 0.137\\)\n247.714\n\\(34 / 1 = 34.000\\)\n\\(34 / 35 \\approx 0.971\\)\n\n\n\n\n\n\n\n\n\n\nAlbert, Adelin. 1982. “On the Use and Computation of Likelihood Ratios in Clinical Chemistry.” Clinical Chemistry 28 (5): 1113–19.\n\n\nBamber, Donald. 1975. “The Area Above the Ordinal Dominance Graph and the Area Below the Receiver Operating Characteristic Graph.” Journal of Mathematical Psychology 12 (4): 387–415.\n\n\nBayes, Thomas. 1763. “LII. An Essay Towards Solving a Problem in the Doctrine of Chances. By the Late Rev. Mr. Bayes, FRS Communicated by Mr. Price, in a Letter to John Canton, AMFRS.” Philosophical Transactions of the Royal Society of London 53: 370–418.\n\n\nBengtsson, Ewert, and Patrik Malm. 2014. “Screening for Cervical Cancer Using Automated Analysis of PAP-Smears.” Computational and Mathematical Methods in Medicine 2014: 842037.\n\n\nBlumberg, Mark S. 1957. “Evaluating Health Screening Procedures.” Operations Research 5 (3): 351–60.\n\n\nBostrom, RC, HS Sawyer, and WE Tolles. 1959. “Instrumentation for Automatically Prescreening Cytological Smears.” Proceedings of the IRE 47 (11): 1895–1900.\n\n\nFagan, Terrence J. 1975. “Nomogram for Bayes’s Theorem.” New England Journal of Medicine 293 (5): 257.\n\n\nHanley, James A, and Barbara J McNeil. 1982. “The Meaning and Use of the Area Under a Receiver Operating Characteristic (ROC) Curve.” Radiology 143 (1): 29–36.\n\n\nKessel, Elton. 1962. “Diabetes Detection: An Improved Approach.” Journal of Chronic Diseases 15 (12): 1109–21.\n\n\nLusted, Lee B. 1971a. “Decision-Making Studies in Patient Management.” New England Journal of Medicine 284 (8): 416–24.\n\n\n———. 1971b. “Signal Detectability and Medical Decision-Making.” Science 171 (3977): 1217–19.\n\n\n———. 1984. “ROC Recollected.” Medical Decision Making 4: 131–35.\n\n\nRemein, Quentin R, and Hugh LC Wilkerson. 1961. “The Efficiency of Screening Tests for Diabetes.” Journal of Chronic Diseases 13 (1): 6–21.\n\n\nRothman, Kenneth J. 1981. “Induction and Latent Periods.” American Journal of Epidemiology 114 (2): 253–59.\n\n\nSwets, John A. 1973. “The Relative Operating Characteristic in Psychology: A Technique for Isolating Effects of Response Bias Finds Wide Use in the Study of Perception and Cognition.” Science 182 (4116): 990–1000.\n\n\n———. 1988. “Measuring the Accuracy of Diagnostic Systems.” Science 240 (4857): 1285–93.\n\n\nVecchio, Thomas J. 1966. “Predictive Value of a Single Diagnostic Test in Unselected Populations.” New England Journal of Medicine 274 (21): 1171–73.\n\n\nYerushalmy, Jacob. 1947. “Statistical Problems in Assessing Methods of Medical Diagnosis, with Special Reference to X-Ray Techniques.” Public Health Reports (1896-1970) 62 (40): 1432–49.\n\n\nZweig, Mark H, and Gregory Campbell. 1993. “Receiver-Operating Characteristic (ROC) Plots: A Fundamental Evaluation Tool in Clinical Medicine.” Clinical Chemistry 39 (4): 561–77.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#footnotes",
    "href": "condprob.html#footnotes",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "",
    "text": "Thomas Bayes (1701-1761) was an English Presbyterian minister from a family of Nonconformists (i.e., Protestants who did not observe the rules of the Church of England). He studied logic and theology at the University of Edinburgh and served as a minister in Tunbridge Wells near Kent, England. He was elected a Fellow of the Royal Society in 1742 for his defense of Newton’s calculus against a 1734 book called The Analyst: A Discourse Addressed to an Infidel Mathematician by Bishop George Berkeley (1685-1753). Late in life, Bayes became interested in probability and “inverse probability” (statistics). This essay was published posthumously, and it has had a profound effect on modern statistics.↩︎\n This is partly to respect the linear algebra convention that rows come before columns in matrix indices, so \\(M_{ij}\\) is the entry in row \\(i\\) and column \\(j\\) of the matrix \\(M\\). In analytic epidemiology, exposure must occur before any disease that it causes, so we let the exposure define the rows.↩︎\n For a test that is positive when a clinical measurement is below a given cutoff, it is the probability that a person with disease has a lower value than a person without disease. Bamber (1975) showed that the AUC is closely related to the Wilcoxon rank sum statistic for the null hypothesis that the diseased and nondiseased have the same distribution for the measurement on which the test is based.↩︎\n The symbol \\(\\Sigma\\), which is an upper-case Greek letter \\(\\sigma\\) (sigma), stands for a sum. For products, we use \\(\\Pi\\), which is an upper-case Greek letter \\(\\pi\\) (pi).↩︎\n Many diseases are complex processes (Rothman 1981), making any binary classification of disease status somewhat arbitrary. Here, we assume that we have an operational definition of disease status that allows a reasonable binary classification.↩︎",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "mlestimation.html",
    "href": "mlestimation.html",
    "title": "3  Maximum Likelihood Estimation",
    "section": "",
    "text": "3.1 Binomial likelihood\nIn probability, we are told the rules of the game and then we predict what it will look like. In statistics, we watch the game and try to figure out the rules. Roughly speaking, statistics (game to rules) is the reverse of probability (rules to game). When done well, statistics helps us learn from observations while accounting honestly for uncertainty. An outstanding early example statistics applied to public health is the work of Florence Nightingale (1820-1910), who collected data and developed statistical graphics to demonstrate the need for public health reforms in the British Army in the 1850s (Cohen 1984; Winkelstein Jr 2009).2\nHere, we will use estimation of a probability as an example of maximum likelihood estimation, which is used for parameter estimation throughout frequentist statistics. It gives us a way to find point estimates of parameters that are optimal in large samples in a sense that we will explain below. It is also the foundation for hypothesis tests and confidence intervals, which give us an accurate way to account for uncertainty in statistical inference.\nIn Section 3.1.1, we used the prevalence \\(p\\) in our population to figure out the distribution of the number \\(X\\) of diseased individuals in a sample of size \\(n\\). This is probability. The corresponding statistical problem would be to estimate the prevalence \\(p\\) after seeing \\(X = x\\) infected individuals in a sample of size \\(n\\).\nWhen our experiment is to sample multiple individuals from a population, the analogy between the outcomes \\(\\omega \\in \\Omega\\) and the individuals in the population breaks down. Recall that when we flip a coin twice, each \\(\\omega \\in \\Omega\\) must specify the outcomes of both flips. When the experiment is to sample \\(n\\) individuals from a population, the entire sample is a single outcome \\(\\omega\\) and \\(\\Omega\\) contains all possible samples of \\(n\\) individuals from the population. If the population size is \\(N\\), then the number of possible samples of size \\(n\\) is given by the binomial coefficient \\[\n  \\binom{N}{n} = \\frac{N!}{n! (N - n)!},\n\\] where \\(k!\\) denotes \\(k\\) factorial. Factorials are defined by \\(0! = 1\\) and \\(k! = k \\cdot (k - 1)!\\) for any integer \\(k &gt; 0\\). For example, \\(1! = 1\\), \\(2! = 2\\), \\(3! = 6\\), \\(4! = 24\\), \\(5! = 120\\), and so on. For \\(k &gt; 0\\), \\(k!\\) is the product of all positive integers up to and including \\(k\\), which grows extremely fast as \\(k\\) increases.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#binomial-likelihood",
    "href": "mlestimation.html#binomial-likelihood",
    "title": "3  Maximum Likelihood Estimation",
    "section": "",
    "text": "3.1.1 Binomial distribution\nSuppose we sample \\(n\\) individuals from a population \\(\\Omega\\) and test them for disease. For simplicity, we assume that the diagnostic test has perfect sensitivity and specificity. Let \\(Y_i\\) denote whether person \\(i\\) in the sample has disease, and let \\(X\\) be the total number who have disease. Then \\[\n  X = \\sum_{i = 1}^n Y_i,\n\\] so it is a linear combination of the \\(Y_i\\). Each \\(Y_i\\) is a Bernoulli(\\(p\\)) random variable, where \\(p\\) is the prevalence of disease in the population. When \\(N\\) is much larger than \\(n\\) (for which we write \\(N \\gg n\\)), the test results for each person in the sample are approximately independent.\nThe distribution of a sum of \\(n\\) independent Bernoulli(\\(p\\)) random variables is called a binomial(n, p) distribution.3 The probability \\(Y_1 = 1\\) is \\(p\\), and the probability that \\(Y_1 = 0\\) is \\((1 - p)\\), so we can handle both cases by writing \\[\n  \\Pr(Y_1 = y_1) = p^{y_1} (1 - p)^{1 - y_1}.\n\\] When the \\(Y_i\\) are independent, each \\(Y_i\\) has a Bernoulli(p) distribution (see Section 1.3.5) and \\[\n  \\Pr(Y_1 = y_1, Y_2 = y_2, \\ldots, Y_n = y_n)\n  = \\prod_{i = 1}^n \\Pr(Y_i = y_i)\n  = \\prod_{i = 1}^n p^{y_i} (1 - p)^{1 - y_i}\n\\] by the multiplication rule for independent events. Substituting \\(x = \\sum_{i = 1}^n y_i\\), we get \\[\n  \\Pr(Y_1 = y_1, Y_2 = y_2, \\ldots, Y_n = y_n)\n  = p^x (1 - p)^{n - x}.\n\\] The value of \\(x\\) depends only on the sum of the \\(y_i\\), and there are \\(\\binom{n}{x}\\) different ways to get \\(x\\) cases of disease out of \\(n\\) sampled individuals. By the addition rule for disjoint events, we get \\[\n  \\Pr(X = x) = \\binom{n}{x} p^x (1 - p)^x.\n\\tag{3.1}\\] This is the probability mass function (PMF) of the binomial distribution. The set of possible values of a binomial(n, p) random variable \\(X\\) is \\(\\supp(X) = \\{0, 1, \\ldots, n\\}\\).\nSection 1.3.5 showed that a Bernoulli(\\(p\\)) random variable has expected value \\(p\\) and variance \\(p(1 - p)\\). Because a binomial(\\(n\\), \\(p\\)) random variable is the sum of \\(n\\) independent Bernoulli(\\(p\\)) random variables, its expected value is \\[\n  \\E(X) = n p.\n\\] by the rule for expectations of linear combinations in Equation 1.11. Its variance is \\[\n  \\Var(X) = n p (1 - p)\n\\] by the rule for variances of linear combinations in Equation 1.12. The covariances are all zero because the \\(Y_i\\) are independent.\n\nR\n\n\n\n\nbinomdist.R\n\n## binomial distribution\n\n# binomial PMF\n# The second and third arguments are n (\"size\") and p (\"prob\").\ndbinom(2, 10, 0.4)\ndbinom(0:10, 10, 0.4)\nsum(dbinom(0:10, 10, 0.4))\n\n# binomial CDF\npbinom(0:10, 10, 0.4)\ncumsum(dbinom(0:10, 10, 0.4))\n\n# binomial quantiles\nqbinom(c(0.25, 0.5, 0.75, 1), 10, 0.4)\n\n# random samples\nrbinom(20, 10, 0.4)\nx &lt;- rbinom(1000, 10, 0.4)\nmean(x)\nvar(x)\n\n\n\n\n\n\n3.1.2 Likelihood and log likelihood\nIn probability, we know the prevalence of disease \\(p\\) and we deduce the distribution of the number of diseased individuals \\(X\\) in a sample of size \\(n\\). In statistics, we observe \\(X = x\\) and use this to estimate \\(p\\). To do this, we rewrite the binomial PMF Equation 3.1 as a function of \\(p\\) instead of \\(x\\): \\[\n  L(p) = \\binom{n}{x} p^x (1 - p)^{n - x}.\n\\tag{3.2}\\] This is the binomial likelihood function. The right-hand sides of Equation 3.1 and Equation 3.2 are identical, and they produce exactly the same value given the same \\(x\\) and \\(p\\). However, the two equations define different functions. In binomial PMF in Equation 3.1, the prevalence \\(p\\) is fixed and the number of diseased individuals \\(x\\) is the argument of the function. In the binomial likelihood function in Equation 3.2, the number of diseased individuals \\(x\\) is fixed and the prevalence \\(p\\) is the argument of the function. The PMF belongs to probability, and the likelihood belongs to statistics.\nThe log likelihood is the natural logarithm (i.e., the logarithm to base \\(e = 2.718281828\\ldots\\))4 of the likelihood function. For binomial log likelihood is \\[\n  \\ell(p) = \\ln \\binom{n}{x} + x \\ln p + (n - x) \\ln (1 - p).\n\\] Because the logarithm turns products into sums, it is generally much easier to handle the log likelihood than the likelihood itself. The term \\(\\ln \\binom{n}{x}\\) does not depend on \\(p\\), so it can be ignored. Intuitively, this tells us that the total number \\(x = y_1 + y_2 + \\cdots + y_n\\) of individuals with disease in our sample contains the same information about the prevalence of disease as the sequence \\(y_1, y_2, \\ldots, y_n\\) of disease indicators.\nFor any given \\(p\\), we can think of \\(\\ell(p)\\) as a random variable whose value is determined by our sample of size \\(n\\). Let \\(\\ptrue\\) be the true prevalence of disease. By Gibb’s inequality,5 \\[\n  \\E[\\ell(\\ptrue)] &gt; \\E[\\ell(p)]\n\\] for all \\(p \\neq \\ptrue\\). This inequality is about the expected value of the log likelihood over all possible samples of size \\(n\\). For any given sample, it is possible that \\(\\ell(\\ptrue)\\) is not the maximum of the log likelihood. However, this inequality is an important part of the justification for estimating \\(p\\) by maximizing the log likelihood (Boos and Stefanski 2013). Because function \\(v \\mapsto \\ln(v)\\) is strictly increasing in \\(v\\), the likelihood \\(L(p)\\) and the log likelihood \\(\\ell(p)\\) are maximized at exactly the same value of \\(p\\).\n\n\n3.1.3 Score function\nTo find the maximum of the log likelihood, we find the value of \\(p\\) where its slope is zero. This is the mathematical version of the insight that the ground at the top of a hill is level. The score function is the first derivative of the log likelihood \\[\n  U(p)\n  = \\frac{\\text{d}}{\\text{d} p} \\ell(p)\n  = \\frac{x}{p} - \\frac{n - x}{1 - p},\n\\] which is the slope of \\(\\ell(p)\\) at \\(p\\). To find where the slope equals zero, we solve the score equation \\[\n  U(\\hat{p})\n  = \\frac{x}{\\hat{p}} - \\frac{n - x}{1 - \\hat{p}}\n  = 0\n\\tag{3.3}\\] where \\(\\hat{p}\\) denotes the maximum likelihood estimate (MLE) of \\(\\ptrue\\). When the dust settles, we get \\[\n  \\hat{p} = \\frac{x}{n}\n\\] so our MLE of the prevalence is just the proportion of our sample who has disease.\nTo confirm that this is a maximum instead of a minimum, we need to look at the second derivative of \\(\\ell\\). When we walk across the top of a hill, we go from walking uphill to walking downhill so the slope is decreasing. If \\(\\ell(p)\\) is maximized at \\(\\hat{p}\\), then the slope of the slope (i.e., the second derivative) should be negative. The second derivative of \\(\\ell(p)\\) at \\(\\hat{p}\\) is \\[\n  \\frac{\\text{d}}{\\text{d} p} U(p) = \\frac{\\text{d}^2}{\\text{d} p^2} \\ell(p)\n  = -\\frac{x}{p^2} - \\frac{n - x}{(1 - p)^2}.\n\\tag{3.4}\\] This is negative for any \\(p \\in (0, 1)\\). Thus, the log likelihood is maximized at \\(\\hat{p}\\) if \\(x &gt; 0\\) and \\(x &lt; n\\).\nWhen \\(x = 0\\) or \\(x = n\\), the log likelihood \\(\\ell(p)\\) has no maximum at any \\(p \\in (0, 1)\\). Instead, the maximum occurs at one of the boundaries of the set of possible \\(p\\). When \\(x = 0\\), our MLE of \\(\\ptrue\\) is \\(\\hat{p} = 0\\). When \\(x = n\\), our maximum likelihood estimate is \\(\\hat{p} = 1\\).\n\n\n3.1.4 Expected and observed information*\nFor any given \\(p\\), we can think of the score \\(U(p)\\) as a random variable that has an expected value and a variance. If \\(\\ptrue = p\\), the expected value of the score is always zero: \\[\n  \\E_p[U(p)]\n  = \\E_p\\bigg[\\frac{X}{p} - \\frac{n - X}{1 - p}\\bigg]\n  = \\frac{\\E_p(X)}{p} - \\frac{\\E_p(n - X)}{1 - p}\n  = \\frac{n p}{p} - \\frac{n (1 - p)}{1 - p}\n  = 0\n\\] where we use the subscript \\(p\\) to indicate that the expected value is calculated assuming that \\(\\ptrue = p\\). Because \\(\\E_p[U(p)] = 0\\), the corresponding variance of the score is \\[\n  \\mathcal{I}(p) = \\Var_p[U(p)] = \\E_p[U(p)^2],\n\\] by Equation 1.10. This is called the expected Fisher information or expected information.6 It can be used to calculate confidence limits for \\(\\ptrue\\).\nUnder regularity conditions that are met when \\(\\ptrue \\in (0, 1)\\), the Fisher information \\(\\mathcal{I}(p)\\) can be calculated using the second derivative of the log likelihood \\(\\ell(p)\\) from Equation 3.4.7 Specifically, \\(\\mathcal{I}(p)\\) is the expected value of the negative second derivative of \\(\\ell(p)\\): \\[\n  \\mathcal{I}(p)\n  = \\E_p\\Bigg[-\\frac{\\text{d}^2}{\\text{d} p^2} \\ell(p)\\Bigg]\n  = \\E_p\\bigg[\\frac{X}{p^2} + \\frac{n - X}{(1 - p)^2}\\bigg],\n\\tag{3.5}\\] where the subscript \\(p\\) indicates that the expected value is calculated assuming that \\(\\ptrue = p\\). Using Equation 1.11 and the binomial(\\(n\\), \\(p\\)) distribution for \\(X\\), this simplifies to \\[\n  \\mathcal{I}(p)\n  = \\frac{\\E(X)}{p^2} + \\frac{\\E(n - X)}{(1 - p)^2}\n  = \\frac{n}{p} + \\frac{n}{1 - p}\n  = \\frac{n}{p (1 - p)}.\n\\] Because \\(\\ptrue\\) is unknown, the expected information is often evaluated at \\(\\hat{p}\\). In some models, the expected information can be difficult to calculate.\nThe negative second derivative of \\(\\ell(p)\\) inside the expectation in Equation 3.5 evaluated is the observed Fisher information or observed information \\[\nI(p) = -\\frac{\\text{d}^2}{\\text{d} p^2} \\ell(p)\n= \\frac{x}{p^2} + \\frac{n - x}{(1 - p)^2}.\n\\tag{3.6}\\] For the binomial distribution \\(I(\\hat{p}) = \\mathcal{I}(\\hat{p})\\) but this equality does not hold at other values of \\(p\\). The observed information is an unbiased estimator of the expected information, and it can always be calculated from the data. It often produces more accurate variance estimates than the expected information (Efron and Hinkley 1978; Kenward and Molenberghs 1998; Reid 2003). However, it is generally safe to use whichever is most convenient (Boos and Stefanski 2013).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#large-sample-theory",
    "href": "mlestimation.html#large-sample-theory",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.2 Large-sample theory",
    "text": "3.2 Large-sample theory\nThe log likelihood, the score function, and the Fisher and observed information give us all of the pieces we need to calculate point and interval estimates of \\(\\ptrue\\). To put them together, we use two fundamental results from probability theory about the behavior of sample means. The law of large numbers justifies point estimates and the central limit theorem justifies hypothesis tests and interval estimates, which can be obtained in three standard ways.\n\n3.2.1 Sample mean (average)\nIf \\(Y_1, Y_2, \\ldots, Y_n\\) are random variables, then the sample mean or average is \\[\n  \\hat{\\mu}_n = \\frac{1}{n} \\sum_{i = 1}^n Y_i.\n\\] This sample mean can be thought of as a random variable whose value is determined when we observe \\(Y_1 = y_1, Y_2 = y_2, \\ldots, Y_n = y_n\\). If each \\(Y_i\\) has \\(\\E(Y_i) = \\mu\\), then \\[\n  \\E[\\hat{\\mu}_n]\n  = \\frac{1}{n} \\sum_{i = 1}^n \\E[Y_i]\n  = \\frac{1}{n} n \\mu\n  = \\mu\n\\tag{3.7}\\] by Equation 1.11. Thus, the sample mean \\(\\hat{\\mu}_n\\) is an unbiased estimate of \\(\\mu\\) for any sample size \\(n\\). When the \\(Y_i\\) are indicator variables, \\(\\hat{\\mu}_n\\) is just the proportion of the sample with \\(Y_i = 1\\).\n\n\n3.2.2 Law of large numbers and consistency\nIf the \\(Y_i\\) are independent and each has \\(\\Var(Y_i) = \\sigma^2\\), then \\[\n  \\Var(\\hat{\\mu}_n)\n  = \\frac{1}{n^2} \\sum_{i = 1}^n \\Var(Y_i)\n  = \\frac{1}{n^2} n \\sigma^2\n  = \\frac{\\sigma^2}{n}\n\\tag{3.8}\\] by Equation 1.12. Thus, the variance of \\(\\hat{\\mu}_n\\) decreases as the sample size \\(n\\) increases. The standard deviation of \\(\\hat{\\mu}_n\\) is proportional to \\(1 / \\sqrt{n}\\). As \\(n \\rightarrow \\infty\\), we should have \\(\\hat{\\mu}_n \\rightarrow \\mu\\). This is called the law of large numbers, and it holds even when \\(\\sigma^2 = \\infty\\).\n\nTheorem 3.1 (Law of Large Numbers) If \\(Y_1, Y_2, \\ldots\\) is an infinite sequence of independent and identically-distributed (IID) random variables with mean \\(\\mu &lt; \\infty\\) and variance \\(\\sigma^2 \\leq \\infty\\), then\n\\[\n    \\hat{\\mu}_n \\rightarrow \\mu\n\\] as \\(n \\rightarrow \\infty\\).8\n\nOur maximum likelihood estimate \\(\\hat{p}_n\\) is a sample mean: \\[\n  \\hat{p}_n = \\frac{X}{n} = \\frac{1}{n} \\sum_{i = 1}^n Y_i.\n\\] where each \\(Y_i \\sim \\text{Bernoulli}(\\ptrue)\\) and the \\(Y_i\\) are independent. Therefore, the LLN implies that \\[\n  \\hat{p}_n \\rightarrow \\ptrue\n\\] as \\(n \\rightarrow \\infty\\). This convergence is shown in Figure 3.1. An estimate that converges to its true value as \\(n \\rightarrow \\infty\\) is called consistent. Intuitively, this means that \\(\\hat{p}_n\\) is guaranteed to be close to \\(\\ptrue\\) in a large sample. However, the LLN does not specify how close or how large a sample we need.\n\n\n\nCode\n\nlln.R\n\n## Law of large numbers\n\nn &lt;- 1000\nx &lt;- seq(n)\nplot(x, cumsum(rbinom(n, 1, .5)) / x, type = \"n\", ylim = c(0, 1),\n     xlab = \"Number of samples\", ylab = \"Sample mean\")\ngrid()\nlines(x, cumsum(rbinom(n, 1, .5)) / x, lty = \"solid\")\nlines(x, cumsum(rbinom(n, 1, .5)) / x, lty = \"dashed\")\nlines(x, cumsum(rbinom(n, 1, .5)) / x, lty = \"dotted\")\nabline(h = .5)\n\n\n\n\n\n\n\n\n\nFigure 3.1: The LLN at work. Each line traces the sample means calculated from a sequence of random samples \\(x_1, x_2, x_3, \\ldots\\) from a Bernoulli(0.5) distribution. For each sequence, the y-coordinate above \\(n\\) is the sample mean from the first \\(n\\) random samples in the sequence. The true mean of 0.5 is marked by a solid horizontal line.\n\n\n\n\n\n\n\n\n3.2.3 Central limit theorem and the normal distribution\nWhen both the mean and variance of the \\(Y_i\\) are finite, the central limit theorem (CLT) allows us to say something about how far away our sample mean \\(\\hat{\\mu}_n\\) is from the true value \\(\\mu\\). It is the most important result in all of probability and statistics.\n\nTheorem 3.2 (Central Limit Theorem) If \\(Y_1, Y_2, \\ldots\\) is an infinite sequence of IID random variables with finite mean \\(\\mu\\) and variance \\(\\sigma^2 &lt; \\infty\\), then \\[\n  Z_n\n  = \\frac{\\hat{\\mu}_n - \\E(\\hat{\\mu_n})}{\\sqrt{\\Var(\\hat{\\mu}_n)}}\n  = \\frac{\\sqrt{n} (\\hat{\\mu}_n - \\mu)}{\\sqrt{\\sigma^2}}\n\\] has a distribution that converges to a normal distribution or Gaussian distribution with mean zero and variance one as \\(n \\rightarrow \\infty\\).9 Because of this, we say that \\(\\hat{\\mu}_n\\) is asymptotically normal.\n\nThe normal distribution is a distribution for a continuous random variable, which can take any value on an interval or even on all of \\(\\mathbb{R}\\). Instead of a PMF, a continuous random variable \\(Z\\) has a probability density function (PDF). If \\(Z\\) is a continuous random variable with PDF \\(f(z)\\) and \\([a, b]\\) is an interval, then \\[\n  \\Pr\\bigl(Z \\in [a, b]\\bigr) = \\int_a^b f(z) \\,\\text{d} z.\n\\] The integral on the right-hand side represents the area under \\(f(z)\\) over the interval \\([a, b]\\). The cumulative distribution function of \\(Z\\) is \\[\n  F(z) = \\int_{-\\infty}^z f(u) \\,\\text{d} u,\n\\] where the integral on the right-hand side represents the area under \\(f(z)\\) over the interval \\((-\\infty, u]\\). For the same reason that the values of the PMF for any discrete random variable add up to one, we have \\[\n\\Pr(Z \\in \\mathbb{R})\n= \\int_{-\\infty}^\\infty f(z) \\,\\text{d} z\n= 1\n\\] for any continuous random variable \\(Z\\). Like the PMF and CDF of a discrete random variable, the PDF and CDF of a continuous random variable contain the same information about the distribution of \\(Z\\).\nThe PDF of the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) is \\[\n  f(z, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(z - \\mu)^2}{2 \\sigma^2}}.\n\\] The standard normal distribution has \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\). It is such an important distribution that its PDF and CDF have special notation. The standard normal PDF is \\[\n  \\phi(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{z^2}{2}},\n\\] and its CDF is \\(\\Phi(z)\\). These functions and the relationship between them are illustrated in Figure 3.2. A normal distribution is denoted \\(N(\\mu, \\sigma^2)\\), so the standard normal distribution is written \\(N(0, 1)\\).\n\n\n\nCode\n\nnormplots.R\n\n## Normal distribution PDF and CDF\n\n# set grid of plots\npar(mfrow = c(2, 1), mar = c(2, 5, 2, 2) + 0.1)\n\n# define variables\nx &lt;- seq(-3.5, 3.5, by = 0.01)\na &lt;- 0\nb &lt;- 2\n\n# plot of PDF\nplot(x, dnorm(x), type = \"n\",\n    ylab = expression(paste(\"PDF \", phi1(z))))\ngrid()\nlines(x, dnorm(x))\npolygon(x = c(b, a, seq(a, b, by = 0.01)),\n        y = c(0, 0, dnorm(seq(a, b, by = 0.01))),\n        lty = \"dashed\", col = \"darkgray\")\ntext(0.4, 0.18, labels = \"Area = Pr(0 &lt; Z &lt; 2)\", srt = 90)\n\n# plot of CDF\nplot(x, pnorm(x), type = \"n\",\n     ylab = expression(paste(\"CDF \", Phi(z))))\ngrid()\nlines(x, pnorm(x))\nsegments(c(-4, -4), pnorm(c(a, b)), c(a, b), pnorm(c(a, b)),\n         lty = \"dashed\")\nsegments(c(a, b), c(-1, -1), c(a, b), pnorm(c(a, b)), lty = \"dashed\")\narrows(-3, pnorm(a), -3, pnorm(b), code = 3, length = 0.1)\ntext(-1.7, sum(pnorm(c(a, b))) / 2, labels = \"Change = Pr(0 &lt; Z &lt; 2)\")\n\n\n\n\n\n\n\n\n\nFigure 3.2: The PDF (top) and CDF (bottom) of a standard normal random variable \\(Z\\). If \\(X \\sim N(0, 1)\\), then \\(\\Pr(0 &lt; X &lt; 2)\\) equals the shaded area under the PDF as well as the change in the CDF from \\(0\\) to \\(2\\). This same relationship between the CDF and the PDF holds for all continuous random variables and any interval \\((a, b)\\).\n\n\n\n\n\n\nR\n\n\n\n\nnormdist.R\n\n## normal (Gaussian) distribution\n\n# normal PDF\n# Second and third arguments are mean and SD (not variance).\n# The defaults are mean = 0 and SD = 1.\ndnorm(2, 1.2, 5)\n\n# normal CDF (using default mean and variance)\npnorm(1.96)\npnorm(1.96) - pnorm(-1.96)\n\n# normal quantiles\nqnorm(0.975)\npnorm(qnorm(0.975))\n\n# random samples (using named arguments)\nrnorm(25, mean = 2.3, sd = 3)\n\n\n\n\nFor our estimated probability \\(\\hat{p}_n\\) is a sample mean of IID \\(Y_i\\) with \\(\\E(Y_i) = \\ptrue\\) and \\(\\Var(Y_i) = \\ptrue (1 - \\ptrue)\\). When \\(n\\) is large, \\[\n  Z_n\n  = \\frac{\\sqrt{n} (\\hat{p}_n - \\ptrue)}{\\sqrt{\\ptrue (1 - \\ptrue)}}\n  = \\frac{\\hat{p}_n - \\ptrue}{\\sqrt{\\mathcal{I}(\\ptrue)^{-1}}}\n\\tag{3.9}\\] has a distribution that is close to a standard normal distribution. Figure 3.3 shows this convergence is shown for sample means where \\(Y_i \\sim \\Bernoulli(0.1)\\). The CLT does not guarantee that the distribution of \\(Z_n\\) is approximately normal in any given sample. It only guarantees that the normal approximation holds eventually as \\(n\\) increases. When the \\(Y_i \\sim \\Bernoulli(p)\\), the normal approximation is typically good when \\(n p (1 - p) &gt; 5\\).\n\n\n\nCode\n\nclt.R\n\n## Central limit theorem\n\n# probability mass function for sample mean\ndbline &lt;- function(n, p=.5, ...) {\n  x &lt;- (seq(-.5, n + .5) / n - p) * sqrt(n / (p * (1 - p)))\n  y &lt;- c(0, dbinom(0:n, n, p), 0) * sqrt(p * (1 - p) * n)\n  lines(stepfun(x, y), pch = NA, ...)\n}\n\n# define grid of plots\npar(mfrow = c(2, 2))\nx &lt;- seq(-4, 4, by = .01)\n\n# n = 20\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 20\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(20, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n# n = 50\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 50\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(50, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n# n = 100\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 100\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(100, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n# n = 250\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 250\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(250, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n\n\n\n\n\n\n\n\nFigure 3.3: The CLT at work. The dashed lines show the PMF of the distribution of the average from a sample of size \\(n\\) from a Bernoulli(0.1) distribution. The solid line is the standard normal PDF.\n\n\n\n\n\n\n\n3.2.4 Efficiency of maximum likelihood estimators*\nWe have used the LLN and the CLT to show that \\(\\hat{p}_n\\) is consistent and asymptotically normal, which are both wonderful properties for an estimator to have. However, they do not prove that \\(\\hat{p}_n\\) is the best estimator of \\(\\ptrue\\) in any particular sense. In Equation 3.9, the variance of \\(\\hat{p}_n\\) was \\[\n  \\mathcal{I}(\\ptrue)^{-1} = \\frac{\\ptrue (1 - \\ptrue)}{n},\n\\] which is the inverse of the Fisher information. It turns out that no other unbiased estimator of \\(\\ptrue\\) can have lower variance, so \\(\\hat{p}_n\\) is the minimum-variance unbiased estimator of \\(\\ptrue\\).\nSuppose \\(\\theta\\) is a parameter for a family of PMFs or PDFs \\(f(y, \\theta)\\) such that the true PMF or PDF is \\(f(y, \\theta_\\true)\\). When we observe \\(Y_1 = y_1, Y_2 = y_2, \\ldots Y_n = y_n\\), the likelihood is \\[\n  L(\\theta) = \\prod_{i = 1} f(y_i, \\theta),\n\\] and the log likelihood is \\[\n  \\ell(\\theta) = \\ln L(\\theta) = \\sum_{i = 1}^n \\ln f(y_i, \\theta).\n\\] The score function is \\[\n  U(\\theta) = \\frac{\\text{d}}{\\text{d} \\theta} \\ell(\\theta),\n\\] and the MLE is the solution of the score equation \\(U(\\hat{\\theta}) = 0\\). The Fisher information is \\[\n  \\mathcal{I}(\\theta) = \\E_\\theta\\bigg[\\frac{\\text{d}^2}{\\text{d} \\theta^2} \\ell(\\theta)\\bigg],\n\\] and \\(\\Var(\\hat{\\theta}) = \\mathcal{I}(\\theta)^{-1}\\). If \\(\\bar{\\theta}\\) is any unbiased estimator of the true value \\(\\theta_\\text{true}\\), then \\[\n  \\Var(\\bar{\\theta}) \\geq \\mathcal{I}(\\theta_\\text{true})^{-1}.\n\\] This result is called the Cramér-Rao lower bound (Rao 1945; Cramér 1946),10 No unbiased estimator of \\(\\theta_\\true\\) can have smaller variance than the MLE \\(\\hat{\\theta}\\). Maximum likelihood estimates are consistent, asymptotically normal, and asymptotically efficient when the likelihood is correct (Boos and Stefanski 2013).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#hypothesis-testing",
    "href": "mlestimation.html#hypothesis-testing",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.3 Hypothesis testing",
    "text": "3.3 Hypothesis testing\nIn a hypothesis test, we specify a null hypothesis and then decide whether to reject it based on the value of a test statistic. A null hypothesis often takes the form \\[\n  H_0: \\theta_\\text{true} = \\theta_0.\n\\tag{3.10}\\] We reject \\(H_0\\) if the test statistic appears inconsistent with its distribution under \\(H_0\\). Otherwise, we fail to reject \\(H_0\\). It is traditional to avoid saying that \\(H_0\\) was accepted.\n\n3.3.1 Hypothesis tests and diagnostic tests\nIf we think of \\(H_0\\) as not having the disease and rejecting \\(H_0\\) as testing positive for the disease, a hypothesis test is analogous to a diagnostic test. Table 3.1 shows the possible outcomes of a hypothesis test, and its margins show the correspondence to diagnostic testing (Diamond and Forrester 1983). A false positive occurs when we reject \\(H_0\\) when it is true, which is called a type I error. A false negative occurs when we fail to reject \\(H_0\\) when it is false, which is called type II error.\n\n\n\nTable 3.1: Truth of \\(H_0\\) and hypothesis test results.\n\n\n\n\n\n\nReject \\(H_0\\) (\\(T^+\\))\nFail to reject \\(H_0\\) (\\(T^-\\))\n\n\n\n\n\\(H_0\\) false (\\(D^+\\))\nTrue positive\nFalse negative = type II error\n\n\n\\(H_0\\) true (\\(D^-\\))\nFalse positive = type I error\nTrue negative\n\n\n\n\n\n\nA hypothesis test has analogs of sensitivity and specificity. The equivalent of specificity is \\(1 - \\alpha\\) where \\[\n  \\alpha\n  = \\Pr(\\text{reject } H_0 \\given{} H_0 \\text{ true})\n\\] is the probability of a type I error. This is also called the significance level of the test. The equivalent of sensitivity is the power of the test, which is \\(1 - \\beta\\) where \\[\n  \\beta\n  = \\Pr(\\text{fail to reject } H_0 \\given{} H_0 \\text{ false})\n\\] is the probability of a type II error.\nA hypothesis test also has analogs of positive and negative predictive values (PPV and NPV). Just like the PPV and NPV of a dignostic test depend on the prevalence of disease, the PPV and NPV of a hypothesis test depend on the prior probability that \\(H_0\\) is true, which is the probability that \\(H_0\\) is true based on what we know before we see the test result. For a hypothesis test, the PPV is \\[\n  \\Pr(H_0 \\text{ false} \\given{} H_0 \\text{ rejected})\n  = \\frac{(1 - \\beta) \\Pr(H_0 \\text{ false})}\n  {(1 - \\beta) \\Pr(H_0 \\text{ false}) + \\alpha \\Pr(H_0 \\text{ true})}\n\\tag{3.11}\\] by Bayes’ rule. Similarly, the NPV of the hypothesis test is \\[\n  \\Pr(H_0 \\text{ true} \\given H_0 \\text{ not rejected})\n  = \\frac{(1 - \\alpha) \\Pr(H_0 \\text{ true})}\n  {(1 - \\alpha) \\Pr(H_0 \\text{ true}) + \\beta \\Pr(H_0 \\text{ false})}.\n\\tag{3.12}\\] The conditional probability that \\(H_0\\) is true given the result of the hypothesis test is called the posterior probability of \\(H_0\\).\n\n\n3.3.2 Wald, score, and likelihood ratio tests\nIn a maximum likelihood framework, there are three classical tests for a null hypothesis of the form \\[\n  H_0: \\ptrue = p_0.\n\\] These tests are asymptotically equivalent, which means that they produce similar results in large samples. The best way to visualize the different tests is to look at a graph of the log likelihood function. Figure 3.4 shows the log likelihood function for a binary outcome with \\(x = 60\\) events out of \\(n = 100\\) trials and a null hypothesis \\(H_0: p_\\true = 0.5\\). All three tests generalize to null hypotheses involving multiple parameters (Boos and Stefanski 2013).\nThe Wald test (Wald 1943) of \\(H_0\\) looks at the distance between the MLE \\(\\hat{p}\\) and the hypothesized value \\(p_0\\)(Wald 1943), rejecting \\(H_0\\) when this distance is sufficiently large.11 An example is shown in Figure 3.4. The Wald test statistic is \\[\n  W = \\frac{(\\hat{p} - p_0)^2}{I(\\hat{p})}\n  = \\frac{n (\\hat{p} - p_0)^2}{\\hat{p} (1 - \\hat{p})}\n  \\approxsim \\chi^2_1\n\\tag{3.13}\\] under \\(H_0\\), where \\(I(\\hat{p})\\) is the observed information from Equation 3.6. The \\(\\chi^2_1\\) distribution is the distribution of \\(Z^2\\) if \\(Z \\sim N(0, 1)\\).\nThe score test looks at the slope of the log likelihood at \\(p_0\\), rejecting \\(H_0\\) if this slope is sufficiently far from zero (Rao 1948; Aitchison and Silvey 1958). An example is shown in Figure 3.4. It score test statistic is \\[\n  S = \\frac{U(p_0)^2}{\\mathcal{I}(p_0)}\n  = \\frac{n (\\hat{p} - p_0)^2}{p_0 (1 - p_0)}\n  \\approxsim \\chi^2_1\n\\tag{3.14}\\] under \\(H_0\\), where \\(\\mathcal{I}(p_0)\\) is the expected information from Equation 3.5. The numerator of the score statistic is the same as for the Wald statistic in Equation 3.13, but the denominator uses the expected information at \\(p_0\\) instead of the observed information at \\(\\hat{p}\\). In score tests, it generally better to use the expected information than the observed information (Freedman 2007). The most important advantage of the score test is that it only needs the hypothesized null value \\(p_0\\), so it can be done without finding the maximum likelihood estimate \\(\\hat{p}\\).\nThe likelihood ratio test looks at the vertical distance between \\(\\ell(\\hat{p})\\) (which is the maximum) and \\(\\ell(p_0)\\), rejecting \\(H_0\\) if this distance is sufficiently large Wilks (1938).12 An example is shown in Figure 3.4. The likelihood ratio test statistic is \\[\n  L = 2\\big(\\ell(\\hat{p}) - \\ell(p_0)\\big)\n  \\approxsim \\chi^2_1\n\\tag{3.15}\\] under \\(H_0\\). The Neyman-Pearson lemma (Neyman and Pearson 1933) shows that the likelihood ratio test is the most powerful of all hypothesis test for comparing two hypotheses \\(H_0: \\ptrue = p_0\\) and \\(H_1: \\ptrue = p_1\\) at a fixed significance level.\n\n\n\nCode\n\nhtests.R\n\n## Hypothesis tests based on the log likelihood\n\n# binomial log likelihood, score, and information functions\nbin_loglik &lt;- function(p, k=60, n=100) {\n  k * log(p) + (n - k) * log(1 - p)\n}\nbin_score &lt;- function(p, k=60, n=100) {\n  k / p - (n - k) / (1 - p)\n}\nbin_information &lt;- function(p, k=60, n=100) {\n  k / p^2 + (n - k) / (1 - p)^2\n}\n\n# plot showing Wald, score, and likelihood ratio tests\np &lt;- seq(0.4, 0.8, length.out = 200)\nplot(p, bin_loglik(p), type = \"n\",\n     xlim = c(0.40, 0.70), ylim = c(-72, -66),\n     main = \"Tests of the null hypothesis p = 0.5\",\n     xlab = \"p\", ylab = \"ln L(p)\")\ngrid()\nlines(p, bin_loglik(p))\nabline(v = c(0.5, 0.6), lty = \"dotted\")\nabline(h = c(bin_loglik(0.5), bin_loglik(0.6)), lty = \"dashed\")\nabline(a = bin_loglik(0.5) - bin_score(0.5) * 0.5, b = bin_score(0.5),\n       col = \"darkgray\")\ntext(c(0.5, 0.6), c(-67.05, -67),\n     labels = c(expression(p[0]), expression(hat(p))))\ntext(0.55, -70.7, labels = \"Wald test\")\narrows(0.5, -70.5, 0.6, code = 3, length = 0.1)\narrows(0.475, bin_loglik(0.5), y1 = bin_loglik(0.6),\n       code = 3, length = 0.1)\ntext(0.45, -68.3, labels = \"LRT\")\n\n# The slope is the tangent of the angle to the x-axis.\n# We also must account for the different scales on the x- and y-axes.\n# 0.3 / 6 is xdist / ydist (see xlim and ylim above)\nscore_angle &lt;- atan(bin_score(0.5) * 0.3 / 6)\nangles &lt;- seq(0, score_angle, by = 0.01)\nscore_x &lt;- 0.5 + 0.04 * cos(angles)\nscore_y &lt;- bin_loglik(0.5) + 0.04 * (6 / 0.3) * sin(angles)\nlines(score_x, score_y)\ntext(0.56, -68.8, \"Score test\")\narrows(score_x[2], score_y[2], score_x[1], score_y[1], length = 0.1)\narrows(rev(score_x)[2], rev(score_y)[2], rev(score_x)[1], rev(score_y)[1],\n       length = 0.1)\n\n\n\n\n\n\n\n\n\nFigure 3.4: Binomial log likelihood function for \\(x = 60\\) and \\(n = 100\\). The null value of \\(p\\) is \\(p_0 = 0.5\\) and the maximum likelihood estimate is \\(\\hat{p} = 0.6\\).\n\n\n\n\n\n\n\n3.3.3 Critical values and p-values\nThe Neyman-Pearson approach to hypothesis testing fixes the significance level \\(\\alpha\\) before calculating the test statistic and deciding whether to reject \\(H_0\\).13 The decision to reject the null hypothesis depends on the value of the test statistic, which is compared to a critical value calculated based on the distribution of the test statistic under \\(H_0\\). If \\(Z \\sim N(0, 1)\\) under \\(H_0\\) \\[\n  \\Pr\\big(|Z| \\geq z_{1 - \\frac{\\alpha}{2}} \\given{} H_0 \\text{ true}\\big)\n  = 1 - \\alpha.\n\\] Because \\(Z^2 \\sim \\chi^2_1\\) when \\(Z \\sim N(0, 1)\\), this is equivalent to \\[\n  \\Pr\\big(Z^2 \\geq z_{1 - \\frac{\\alpha}{2}}^2 \\given{} H_0 \\text{ true}\\big)\n  = 1 - \\alpha.\n\\] In the Wald, score, and likelihood ratio tests above, \\(H_0\\) is rejected if the test statistic is larger than the critical value \\(z_{1 - \\frac{\\alpha}{2}}^2\\). For \\(\\alpha = 0.05\\), we have \\(z_{0.975} \\approx 1.96\\) so critical value for the \\(\\chi^2_1\\) distribution is \\(1.96^2 \\approx 3.84\\). The test statistic and critical value in a hypothesis test are analogous to the clinical measurement and cutoff in a diagnostic test.\nInstead of making a binary decision, it is more informative to calculate a measure of the evidence against \\(H_0\\). The p-value for a given test statistic is the lowest value of \\(\\alpha\\) at which the test would still fail to reject \\(H_0\\). A hypothesis test with significance level \\(\\alpha\\) rejects \\(H_0\\) if the p-value is \\(\\leq \\alpha\\). For the Wald, score, or likelihood ratio tests above, \\[\n  \\text{p-value} = 1 - F_{\\chi^2_1}(\\text{test statistic})\n\\] where \\(F_{\\chi^2_1}\\) is the CDF of the \\(\\chi^2_1\\) distribution If we think of the test statistic as the clinical measurement underlying a diagnostic test, the p-value equals \\(1 - \\text{spec}_\\text{max}\\) where \\(\\text{spec}_\\text{max}\\) is the highest specificity under which we would still get a positive test (i.e., reject \\(H_0\\)).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#confidence-intervals",
    "href": "mlestimation.html#confidence-intervals",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.4 Confidence intervals",
    "text": "3.4 Confidence intervals\nA p-value is more informative than a binary decision whether to reject \\(H_0\\), but it is still more useful to know what values of \\(p\\) are plausibly consistent with the data we observed (Rothman 1978). The \\(1 - \\alpha\\) confidence interval for \\(\\ptrue\\) is the set of all possible null values \\(p_0\\) such that we would fail to reject \\(H_0: \\ptrue = p_0\\) in a hypothesis test with significance level \\(\\alpha\\). The endpoints of the confidence interval are called are called confidence limits. Just as different clinical measurements lead to different diagnostic tests, different hypothesis tests lead to different confidence intervals.\nIf we calculate a confidence interval many times with independent data sets, the \\(1 - \\alpha\\) confidence interval should contain \\(\\ptrue\\) with probability \\(1 - \\alpha\\). The actual probability that the confidence interval contains \\(\\ptrue\\) is called the coverage probability. A good confidence interval should have a coverage probability close to \\(1 - \\alpha\\) while being as narrow as possible. The Wald, score, and likelihood ratio tests from Section 3.3.2 are large-sample tests because they rely on consistency and asymptotic normality of the maximum likelihood estimate \\(\\hat{p}\\). All three tests can be inverted to produce confidence intervals that perform well in large samples. In smaller samples, the score and likelihood ratio confidence intervals often have better coverage probability and width than the Wald confidence interval (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001).\n\n3.4.1 Wald confidence intervals and the delta method\nThe Wald confidence limits come from solving the equation \\[\n  \\frac{(\\hat{p} - p)^2}{\\hat{p} (1 - \\hat{p}) / n}\n  = z_{1 - \\frac{\\alpha}{2}}^2.\n\\tag{3.16}\\] for \\(p\\), which gives us \\[\n  \\hat{p} \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p} (1 - \\hat{p})}{n}}.\n\\tag{3.17}\\] The coverage probabilities of Wald confidence intervals can be much lower than \\(1 - \\alpha\\), especially when \\(p_\\true\\) is close to zero or one (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001).\nAnother problem with the Wald confidence interval for \\(\\ptrue\\) is that it can have bounds outside \\([0, 1]\\). One way to avoid this is to calculate confidence limits for a transformation of \\(\\hat{p}\\) using the delta method. A good transformation \\(g(p)\\) should have continuous first derivatives and be strictly increasing or decreasing, so each value of \\(g(p)\\) corresponds to a single value of \\(p\\) (i.e., \\(g\\) is one-to-one). The delta method derives the approximate normal distribution \\(g(\\hat{p})\\) using the approximation \\[\n  g(\\hat{p}) \\approx g(\\ptrue) + g'(\\ptrue) (\\hat{p} - \\ptrue).\n\\] where \\(g'(\\ptrue)\\) is the slope of \\(g\\) at \\(\\ptrue\\). An example of this approximation is shown in Figure 3.5. The key insight is that \\[\n  \\Var[g(\\hat{p})] \\approx g'(\\ptrue)^2 \\Var(\\hat{p}),\n\\] which is a generalization of the fact that \\(\\Var(c \\hat{p}) = c^2 \\Var(\\hat{p})\\) for any constant \\(c\\). If \\(\\hat{p}\\) has an approximate \\(N\\bigl(\\ptrue, \\Var(\\hat{p})\\bigr)\\) distribution in large samples, then \\[\n  g(\\hat{p}) \\approxsim N\\bigl(g(\\ptrue), g'(\\ptrue)^2 \\Var(\\hat{p})\\bigr).\n\\] in large samples. Because our estimator \\(\\hat{p}\\) is consistent, we can replace the unknown \\(\\ptrue\\) with \\(\\hat{p}\\). Because \\(g\\) is one-to-one, we can calculate confidence limits for \\(\\ptrue\\) using the confidence limits for \\(g(\\ptrue)\\).\n\n\n\nCode\n\ndelta.R\n\n## Approximation used by the delta method\n\np &lt;- seq(0.02, 0.98, by = 0.01)\nlogit &lt;- function(p) log(p) - log(1 - p)\n\n# plot\nplot(p, logit(p), type = \"n\",\n     xlab = \"p\", ylab = \"logit(p)\")\ngrid()\nlines(p, logit(p))\npoints(0.6, logit(0.6))\nabline(logit(0.6) - 2.5, 1 / 0.24, lty = \"dashed\")\ntext(0.6, -1,\n     labels = expression(paste(\"logit(p) - \", logit(0.6) %~~% logit,\n                               \"'(0.6) (p - 0.6)\")))\n\n\n\n\n\n\n\n\n\nFigure 3.5: The approximation used by the delta method using the logistic transformation for a binomial confidence interval near \\(\\hat{p} = 0.6\\). The black curve is \\(\\logit(p)\\), and the dashed line shows the tangent line at \\(p = 0.6\\).\n\n\n\n\n\n\nA widely used transformation for probabilities is the logit transformation \\[\n  \\logit(p) = \\ln\\Bigl(\\frac{p}{1 - p}\\Bigr).\n\\] The odds corresponding to the probability \\(p\\) is \\(\\frac{p}{1 - p}\\), so the logit is the natural logarithm of the odds. The logit transformation maps the interval \\((0, 1)\\) onto all of \\(\\mathbb{R}\\):\n\nAs \\(p \\rightarrow 0\\), the odds \\(p / (1 - p) \\rightarrow 0\\) and \\(\\logit(p) \\rightarrow -\\infty\\).\nWhen \\(p = 1 / 2\\), the odds \\(p / (1 - p) = 1\\) and \\(\\logit(p) = 0\\).\nAs \\(p \\rightarrow 1\\), the odds \\(p / (1 - p) \\rightarrow \\infty\\) and \\(\\logit(p) \\rightarrow \\infty\\).\n\nTo use the delta method, we need to calculate the derivative of \\(\\logit(p)\\). By the chain rule, \\[\n  \\logit'(p)\n  = \\frac{1 - p}{p} \\frac{1}{(1 - p)^2}\n  = \\frac{1}{p (1 - p)},\n\\] which is continuous and strictly positive for all \\(p \\in (0, 1)\\). By the delta method, the variance of \\(\\logit(\\hat{p})\\) is approximately \\[\n  \\logit'(\\ptrue)^2 \\frac{\\ptrue (1 - \\ptrue)}{n}\n  = \\frac{1}{\\ptrue^2 (1 - \\ptrue)^2} \\frac{\\ptrue (1 - \\ptrue)}{n}\n  = \\frac{1}{n \\ptrue (1 - \\ptrue)}.\n\\] When we replace the unknown \\(\\ptrue\\) with our MLE \\(\\hat{p}\\), we get the following confidence limits for \\(\\logit(\\ptrue)\\): \\[\n  \\logit(\\hat{p}) \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\frac{1}{n \\hat{p} (1 - \\hat{p})}}.\n\\] To get confidence limits for \\(\\ptrue\\), we use the inverse function for the logit, which is \\[\n  \\expit(v) = \\frac{e^v}{1 + e^v} = \\frac{1}{1 + e^{-v}}.\n\\] This is called the logistic function. If the confidence limits for \\(\\logit(\\ptrue)\\) are \\(a\\) and \\(b\\), then the confidence limits for \\(\\ptrue\\) are \\(\\expit(a)\\) and \\(\\expit(b)\\). These are guaranteed to be in \\((0, 1)\\) because \\(\\expit(v) \\in (0, 1)\\) for any \\(v \\in \\mathbb{R}\\). The logit-transformed confidence interval can have narrower width and a coverage probability closer to \\(1 - \\alpha\\) than the untransformed Wald confidence interval (Agresti 2013).\n\n\n3.4.2 Score (Wilson) confidence intervals\nThe score or Wilson confidence limits come from solving the equation \\[\n  \\frac{(\\hat{p} - p)^2}{p (1 - p) / n}\n  = z_{1 - \\frac{\\alpha}{2}}^2.\n\\tag{3.18}\\] for \\(p\\) (Wilson 1927). This differs from Equation 3.16 for the Wald confidence interval because it uses \\(p\\) instead of \\(\\hat{p}\\) in the denominator. It is a quadratic equation in \\(p\\), so it has two solutions. The center of the resulting confidence interval is \\[\n  \\tilde{p}\n  = \\hat{p} \\Bigg(\\frac{n}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg)\n    + \\frac{1}{2} \\Bigg(\\frac{z_{1 - \\frac{\\alpha}{2}}^2}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg)\n  = \\frac{x + \\frac{1}{2} z_{1 - \\frac{\\alpha}{2}}^2}{n + z_{1 - \\frac{\\alpha}{2}}^2},\n\\tag{3.19}\\] where \\(x\\) is the number of diseased individuals in our sample. This is a weighted average of \\(\\hat{p}\\) and \\(1 / 2\\) with weights proportional to \\(n\\) and \\(z_{1 - \\frac{\\alpha}{2}}^2\\), respectively. The resulting confidence interval is \\[\n  \\tilde{p} \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\tilde{V}}\n\\] where \\[\n  \\tilde{V}\n  = \\frac{\\hat{p} (1 - \\hat{p})}{n + z_{1 - \\frac{\\alpha}{2}}^2} \\Bigg(\\frac{n}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg)\n    + \\frac{\\big(\\frac{1}{2}\\big)^2}{n + z_{1 - \\frac{\\alpha}{2}}^2} \\Bigg(\\frac{z_{1 - \\frac{\\alpha}{2}}^2}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg).\n\\] This variance is a weighted average of the variances of sample proportions equal to \\(\\hat{p}\\) and \\(1 / 2\\) with the same weights as in \\(\\tilde{p}\\) and with \\(n + z_{1 - \\frac{\\alpha}{2}}^2\\) instead of \\(n\\) in the denominator. Wilson confidence intervals are narrower than the corresponding Wald intervals, and they have coverage probabilities much closer to \\(1 - \\alpha\\) (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001).\nThe Agresti-Coull confidence interval is a simplification of the Wilson confidence interval that replaces \\(\\hat{p}\\) with \\(\\tilde{p}\\) in the Wald confidence interval to get the confidence limits \\[\n  \\tilde{p} \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\frac{\\tilde{p} (1 - \\tilde{p})}{n}}.\n\\] Because \\(z_{0.975} \\approx 1.96\\), we have \\(\\tilde{p} \\approx \\frac{k + 2}{n + 4}\\) for a 95% confidence interval. In this case, the Agresti-Coull interval is often implemented as follows: ``Add two successes and two failures and then use the Wald formula’’ (Agresti and Coull 1998). This interval is only slightly wider than the score confidence interval, and the two intervals are nearly identical for \\(n &gt; 40\\) (Brown, Cai, and DasGupta 2001).\nThe likelihood ratio test can also be inverted to get confidence intervals, but these can only be calculated numerically. For the binomial model, the likelihood ratio and score confidence intervals are nearly identical (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001). The score intervals are more common in practice because they are easier to calculate.\n\nR\n\n\n\n\nbinconf.R\n\n## Binomial confidence intervals\n\n# using BinomCI() function from the DescTools package\nlibrary(DescTools)\nBinomCI(15, 22, method = \"wald\")            # Wald confidence interval\nBinomCI(15, 22, method = \"logit\")           # logit-transformed Wald CI\nBinomCI(15, 22, method = \"wilson\")          # score CI (default)\nBinomCI(15, 22, method = \"agresti-coull\")   # Agresti-Coull CI\nBinomCI(15, 22, method = \"lik\")             # likelihood ratio CI\n\n# using binconf() function from the Hmisc package\nlibrary(Hmisc)\nbinconf(15, 22, method = \"asymptotic\")  # Wald CI\nbinconf(15, 22, method = \"wilson\")      # score CI (default)\n\n# using prop.test in base R (stats package)\n# Wilson confidence interval with continuity correction by default\n# The continuity correction is not generally recommended. Like the exact CI,\n# it can be too wide and have a coverage probability greater than 1 - \\alpha.\nprop.test(15, 22)\nnames(prop.test(15, 22))\nprop.test(15, 22, correct = FALSE)      # score CI\n\n# using binom.test (exact confidence interval)\nbinom.test(15, 22)    # same as binconf with method = \"exact\"\nnames(binom.test(15, 22))\n\n# changing the confidence level (1 - alpha) to 80%\n# All are score (Wilson) confidence intervals by default.\nBinomCI(15, 22, conf.level = 0.8)\nbinconf(15, 22, alpha = 0.2)\nprop.test(15, 22, conf.level = 0.8, correct = FALSE)\n\n# writing a function to get Wald confidence limits\nbconf_wald &lt;- function(x, n, level=0.95) {\n  # x is number of successes out of n trials\n  p_hat &lt;- x / n\n  alpha &lt;- 1 - level\n  pvar &lt;- p_hat * (1 - p_hat) / n\n  p_int &lt;- p_hat + c(-1, 1) * qnorm(1 - alpha / 2) * sqrt(pvar)\n\n  # return named vector (names do not need quotes)\n  return(c(point = p_hat, lower = p_int[1], upper = p_int[2]))\n}\nbconf_wald(15, 22)\nbconf_wald(15, 22, level = 0.80)",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#small-sample-estimation",
    "href": "mlestimation.html#small-sample-estimation",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.5 Small-sample estimation*",
    "text": "3.5 Small-sample estimation*\nMaximum likelihood estimates are consistent, asymptotically normal, and asymptotically efficient. However, they are not guaranteed to perform well in any finite sample. For a sample of \\(n\\) independent Bernoulli(\\(p\\)) random variables, the sum has a binomial(\\(n, p\\)) distribution and this can be used to find the finite-sample distribution of the sample mean. This distribution can be used directly to calculate point estimates, p-values, and confidence limits.\nConfidence limits calculated using the finite-sample distribution of a test statistic under \\(H_0\\) are called exact confidence limits. They can often be constructed to have a coverage probability of at least \\(1 - \\alpha\\). However, their coverage probabilities are often higher than \\(1 - \\alpha\\), and they can be much wider than approximate \\(1 - \\alpha\\) confidence intervals for the same parameter (Agresti and Coull 1998).\nIf the finite-sample distribution of the test statistic is not known exactly, it is possible to calculate point estimates, p-values, or confidence limits using simulations. This is the basic idea behind the bootstrap (Efron and Tibshirani 1994) and Monte Carlo methods (Robert and Casella 2004).\n\n3.5.1 Median unbiased estimate\nThe median unbiased estimate of \\(\\ptrue\\) is the value of \\(p\\) that makes \\[\n  \\Pr\\nolimits_p(X &lt; x) = \\Pr\\nolimits_p(X &gt; x)\n\\] where we use the subscript \\(p\\) to indicate that these probabilities are calculated assuming \\(\\ptrue = p\\). If \\(p_\\text{med}\\) is the median unbiased estimate, then \\[\n  \\sum_{k = 0}^{x - 1} \\binom{n}{k} p_\\text{med}^k \\big(1 - p_\\text{med}\\big)^{n - k}\n  + \\frac{1}{2} \\binom{n}{x} p_\\text{med}^k \\big(1 - p_\\text{med}\\big)^{n - x}\n  = \\frac{1}{2},\n\\] and \\[\n  \\frac{1}{2} \\binom{n}{x} p_\\text{med}^x \\big(1 - p_\\text{med}\\big)^{n - x}\n  + \\sum_{k = x + 1}^n \\binom{n}{k} p_\\text{med}^k \\big(1 - p_\\text{med}\\big)^{n - k}\n  = \\frac{1}{2}.\n\\] The median of the distribution of \\(p_\\text{med}\\) is always \\(\\ptrue\\) (Birnbaum 1964), which is a slightly different notion of unbiasedness than the unbiasedness of \\(\\hat{p}\\) where \\(\\E(\\hat{p}) = \\ptrue\\).\n\n\n3.5.2 Exact (Clopper-Pearson) and mid-p confidence intervals\nThe exact or Clopper-Pearson confidence limits for \\(\\ptrue\\) use the finite-sample distribution of the sample mean \\(\\hat{p}\\) Clopper and Pearson (1934). When \\(x &gt; 0\\), the lower \\(1 - \\alpha\\) confidence limit is the solution to \\[\n  \\sum_{k = x}^n \\binom{n}{k} p_\\text{lower}^k (1 - p_\\text{lower})^{n - k}\n  = \\frac{\\alpha}{2},\n\\tag{3.20}\\] so the upper tail of the binomial(\\(n\\), \\(p_\\text{lower}\\)) distribution has probability \\(\\alpha / 2\\). When \\(x = 0\\), we set \\(p_\\text{lower} = 0\\). When \\(x &lt; n\\), the upper confidence limit is the solution to \\[\n  \\sum_{k = 0}^x \\binom{n}{k} p_\\text{upper}^k (1 - p_\\text{upper})^{n - k}\n  = \\frac{\\alpha}{2},\n\\tag{3.21}\\] so the lower tail of the binomial(\\(n\\), \\(p_\\text{upper}\\)) distribution has probability \\(\\alpha / 2\\). When \\(x = n\\), we set \\(p_\\text{upper} = 1\\). This interval is guaranteed to have a coverage probability of at least \\(1 - \\alpha\\), but the price for this is that it is always wider than the Wald and Wilson confidence intervals (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001). In general, the score or likelihood ratio confidence intervals have better combinations of coverage probability and width.\nTo make exact confidence limits less conservative, we can include only \\(\\frac{1}{2} \\Pr(X = x)\\) instead of \\(\\Pr(X = x)\\) in the calculation of the tail probabilities in Equation 3.21 and Equation 3.20. The resulting confidence intervals are called mid-p exact confidence intervals (Lancaster 1961, berry1995mid). The lower \\(1 - \\alpha\\) mid-p exact confidence limit is the solution to \\[\n  \\frac{1}{2} \\binom{n}{x} p_\\text{lower}^x (1 - p_\\text{lower})^{n - x}\n    + \\sum_{k = x + 1}^n \\binom{n}{k} p_\\text{lower}^k (1 - p_\\text{lower})^{n - k}\n  = \\frac{\\alpha}{2}.\n\\] and the upper limit is the solution to \\[\n  \\sum_{k = 0}^{x - 1} \\binom{n}{k} p_\\text{upper}^k (1 - p_\\text{upper})^{n - k}\n    + \\frac{1}{2} \\binom{n}{x} p_\\text{upper}^x (1 - p_\\text{upper})^{n - x}\n  = \\frac{\\alpha}{2}.\n\\] The mid-p exact confidence limits are have good combinations of coverage probability and width as well as good perfomance in small samples (Brown, Cai, and DasGupta 2001).\n\nR\n\n\n\n\nbinconf-smallsamp.R\n\n## Small-sample binomial point and interval estimates\n\n# median unbiased estimate\nmedp.binom &lt;- function(k, n) {\n  # k = number of successes, n = number of trials\n\n  # binomial lower tail probability\n  lower.tail &lt;- function(p) pbinom(k, n, p) - dbinom(k, n, p) / 2\n\n  # median unbiased estimate\n  med &lt;- uniroot(function(p) lower.tail(p) - 1 / 2, interval = c(0, 1))\n\n  return(med$root)\n}\nmedp.binom(15, 22)\n\n\n# exact (Clopper-Pearson) confidence intervals\nbinom.test(15, 22)                              # base R (stats)\nnames(binom.test(15, 22))\nbinom.test(15, 22, conf.level = 0.8)\n\nlibrary(Hmisc)\nbinconf(15, 22, method = \"exact\")\nbinconf(15, 22, method = \"exact\", alpha = 0.2)\n\nlibrary(DescTools)\nBinomCI(15, 22, method = \"clopper-pearson\")     # exact CI\nBinomCI(15, 22, method = \"midp\")                # mid-p exact CI\n\n\n\n\n\n\n\n\nAgresti, Alan. 2013. Categorical Data Analysis. Third. Vol. 792. John Wiley & Sons.\n\n\nAgresti, Alan, and Brent A Coull. 1998. “Approximate Is Better Than ‘Exact’ for Interval Estimation of Binomial Proportions.” The American Statistician 52 (2): 119–26.\n\n\nAitchison, John, and SD Silvey. 1958. “Maximum-Likelihood Estimation of Parameters Subject to Restraints.” The Annals of Mathematical Statistics 29: 813–28.\n\n\nBirnbaum, Allan. 1964. “Median-Unbiased Estimators.” Bulletin of Mathematical Statistics 11: 25–34.\n\n\nBoos, Dennis D, and Leonard A Stefanski. 2013. Essential Statistical Inference. Springer.\n\n\nBrown, Lawrence D, T Tony Cai, and Anirban DasGupta. 2001. “Interval Estimation for a Binomial Proportion.” Statistical Science 16 (2): 101–17.\n\n\nClopper, Charles J, and Egon S Pearson. 1934. “The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial.” Biometrika 26 (4): 404–13.\n\n\nCohen, I Bernard. 1984. “Florence Nightingale.” Scientific American 250 (3): 128–37.\n\n\nCramér, Harald. 1946. Mathematical Methods of Statistics. Princeton University Press.\n\n\nDiamond, George A, and James S Forrester. 1983. “Clinical Trials and Statistical Verdicts: Probable Grounds for Appeal.” Annals of Internal Medicine 98 (3): 385–94.\n\n\nEfron, Bradley, and David V Hinkley. 1978. “Assessing the Accuracy of the Maximum Likelihood Estimator: Observed Versus Expected Fisher Information.” Biometrika 65 (3): 457–83.\n\n\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. Chapman & Hall/CRC.\n\n\nFreedman, David A. 2007. “How Can the Score Test Be Inconsistent?” The American Statistician 61 (4): 291–95.\n\n\nKenward, Michael G, and Geert Molenberghs. 1998. “Likelihood Based Frequentist Inference When Data Are Missing at Random.” Statistical Science 13 (3): 236–47.\n\n\nLancaster, H Oliver. 1961. “Significance Tests in Discrete Distributions.” Journal of the American Statistical Association 56 (294): 223–34.\n\n\nNeyman, Jerzy, and Egon Sharpe Pearson. 1933. “On the Problem of the Most Efficient Tests of Statistical Hypotheses.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 231 (694-706): 289–337.\n\n\nRao, C Radhakrishna. 1945. “Information and Accuracy Attainable in the Estimation of Statistical Parameters.” Bulletin of the Calcutta Mathematical Society 37 (3): 81–91.\n\n\n———. 1948. “Large Sample Tests of Statistical Hypotheses Concerning Several Parameters with Applications to Problems of Estimation.” In Mathematical Proceedings of the Cambridge Philosophical Society, 44:50–57. Cambridge University Press.\n\n\nReid, Nancy. 2003. “Asymptotics and the Theory of Inference.” The Annals of Statistics 31 (6): 1695–2095.\n\n\nRobert, Christian P, and George Casella. 2004. Monte Carlo Statistical Methods. Second edition. Springer.\n\n\nRothman, Kenneth J. 1978. “A Show of Confidence.” New England Journal of Medicine 299 (24): 1362–63.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\nWald, Abraham. 1943. “Tests of Statistical Hypotheses Concerning Several Parameters When the Number of Observations Is Large.” Transactions of the American Mathematical Society 54 (3): 426–82.\n\n\nWilks, Samuel S. 1938. “The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses.” The Annals of Mathematical Statistics 9 (1): 60–62.\n\n\nWilson, Edwin B. 1927. “Probable Inference, the Law of Succession, and Statistical Inference.” Journal of the American Statistical Association 22 (158): 209–12.\n\n\nWinkelstein Jr, Warren. 2009. “Florence Nightingale: Founder of Modern Nursing and Hospital Epidemiology.” Epidemiology 20 (2): 311.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#footnotes",
    "href": "mlestimation.html#footnotes",
    "title": "3  Maximum Likelihood Estimation",
    "section": "",
    "text": "John Tukey (1915-2000) was an American mathematician and statistician who worked at Bell Labs and Princeton University. He developed the box plot, Tukey’s range test for multiple comparisons, and the fast Fourier transform. In 1947, he coined the term “bit” as shorthand for “binary digit”.↩︎\n She was elected a member of the Royal Statistical Society in 1859, where she was the first woman. In 1860, she founded the world’s first modern nursing school at St. Thomas Hospital in London.↩︎\n For finite \\(N\\), \\(X\\) actually has a hypergeometric distribution because the test results are not exactly independent. If the first person in our sample has disease, the probability that the next person we sample has disease is slightly less than \\(p\\). If the first person in our sample does not have disease, the probability that the next person we sample has disease is slightly greater than \\(p\\). When \\(N \\gg n\\), this hypergeometric distribution is approximately binomial(\\(n\\), \\(p\\)).↩︎\n Euler’s number \\(e\\) is named after Leonhard Euler (1707–1783), a Swiss mathematician who introduced the notation \\(f(x)\\) for mathematical functions and the letter \\(i\\) to denote the imaginary unit \\(\\sqrt{-1}\\). He spent most of his life in Berlin and St. Petersburg, and he is widely considered the greatest mathematician of the 18th century. The number \\(e\\) was first discovered in 1683 by Jacob Bernoulli (the namesake of the Bernoulli distribution) when studying compound interest, where \\(e = \\lim_{n \\rightarrow \\infty} (1 + 1 / n)^n\\). In 1748, Euler proved that \\(e = \\frac{1}{0!} + \\frac{1}{1!} + \\frac{1}{2!} + \\frac{1}{3!} + \\cdots\\).↩︎\n This is named for Josiah Willard Gibbs (1839–1903), an American scientist who earned the first American doctorate in engineering in 1863 and went on to work on statistical mechanics, thermodynamics, optics, and vector calculus as a professor of physics at Yale. Albert Einstein called him the greatest mind in American history.↩︎\n Named after Ronald Fisher (1890–1962), who established the foundations of maximum likelihood inference between 1912 and 1922. He was the most important statistician of the 20th century, and he was one of the founders of population genetics. He had poor eyesight for his entire life, which led him to develop a formidable sense of geometry in his head. However, he was also a leading eugenicist and one of the most vocal opponents of the hypothesis that smoking causes lung cancer.↩︎\n For estimating a parameter \\(\\theta\\), the conditions are these: (1) The set of possible values of the observed data \\(X\\) does not depend on \\(\\theta\\). (2) Each \\(\\theta\\) produces a different distribution of \\(X\\). (3) The true value of \\(\\theta\\) is in the interior of the set of possible values. (4) The log likelihood \\(\\ell(\\theta)\\) has continuous first and second derivatives with respect to \\(\\theta\\) in a neighborhood of \\(\\theta_\\true\\). These conditions are met by the binomial likelihood when \\(\\ptrue \\in (0, 1)\\).↩︎\n For simplicity, we are being vague about what we mean by \\(\\hat{\\mu}_n \\rightarrow \\mu\\). Probability has several different notions of convergence/. The weak LLN guarantees convergence in probability, which means that \\(\\lim_{n \\rightarrow \\infty} \\Pr\\big(|\\hat{\\mu}_n - \\mu| &gt; \\varepsilon\\big) = 0\\) for any \\(\\varepsilon &gt; 0\\). The strong LLN guarantees convergence almost surely, which means that \\(\\Pr\\big(\\lim_{n \\rightarrow \\infty} \\hat{\\mu}_n = \\mu\\big) = 1\\).↩︎\n Named after Carl Friedrich Gauss (1777-1855), a German mathematician who is widely considered one of the greatest mathematicians of all time. He discovered the normal distribution in 1809, but the CLT itself was first proved by Laplace in 1810 (see Chapter 1).↩︎\n Named after Swedish statistician Harald Cramér (1893–1985), who was a professor at Stockholm University, and Indian-American statistician Calyampudi Radhakrishna (C. R.) Rao (1920–2023), who was a professor at the Indian Statistical Institute, the University of Cambridge, the University of Pittsburgh, and Pennsylvania State University.↩︎\n Named after Abraham Wald (1902–1950), a Jewish Hungarian mathematician who was invited to move from Vienna to the United States in 1938 after Nazi Germany annexed Austria. He worked at the Statistical Research Group at Columbia University during World War II. In 1950, he and his wife were killed in a plane crash in India, where he was visiting the Indian Statistical Institute.↩︎\n Samuel S. Wilks (1906–1964) was an American mathematician and statistician who grew up on a farm in Texas, got a Ph.D. at the University of Iowa, and went on to be a professor at Princeton University.↩︎\n This approach to hypothesis testing was pioneered in the 1929s by Jerzy Neyman (1894–1981), a Polish mathematician and statistician who founded the first department of statistics in the United States at the University of California, Berkeley in 1938, and Egon Pearson (1895–1980), a British statistician who was a professor at University College London like his father Karl Pearson. ↩︎",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html",
    "href": "bayes.html",
    "title": "4  Bayesian Estimation",
    "section": "",
    "text": "4.1 Prior and posterior distributions\nIn Bayesian inference, probability distributions are used to summarize our knowledge about the possible values of the unknown parameter \\(\\theta_\\true\\). The distribution of possible values of \\(\\theta\\) before we have seen our data is called the prior distribution, and the distribution of possible values of \\(\\theta\\) after we see the data is called the posterior distribution. Most parameters we are interested in estimating (such as probabilities) are continuous, so they have a probability density function (PDF) instead of a probability mass function (PMF). The posterior PDF is proportional to the prior PDF times the likelihood function, and the posterior distribution can be used to get point and interval estimates of an unknown parameter. The interval estimates are called credible intervals, and they have important advantages over confidence intervals. The Bayesian approach to statistics is more logically consistent and more intuitive than the frequentist approach. While large-sample theory can be useful in Bayesian inference, it does not rely on asymptotic normality to nearly the same degree that maximum likelihood estimation does.\nThe value of a PDF is not a probability (it can be greater than one), but PDFs can be handled like probabilities in terms of the addition rule and the multiplication of conditional probabilities Boos and Stefanski (2013). Let \\(\\pi(\\theta)\\) be the prior PDF of \\(\\theta\\). Before we see our data, we believe that \\(\\theta_\\true \\in [a, b]\\) with probability \\[\n  \\Pr(a \\leq \\theta \\leq b) = \\int_a^b \\pi(\\theta) \\,\\dif \\theta.\n\\] This integral is the area under the graph of \\(\\pi(\\theta)\\) between \\(\\theta = a\\) and \\(\\theta = b\\). For a given value of \\(\\theta\\), the likelihood of our data is \\(L(\\theta) = \\pi(\\data \\given{} \\theta)\\). By Bayes’ rule, the posterior PDF is \\[\n  \\pi(\\theta \\given{} \\data)\n  = \\frac{L(\\theta) \\pi(\\theta)}{\\pi(\\data)}\n  \\propto L(\\theta) \\pi(\\theta).\n\\] where \\(\\propto\\) denotes “proportional to.” Calculating \\(\\pi(\\data)\\) is difficult and almost always unnecessary. As long as we can calculate \\(\\pi(\\theta \\given{} \\data)\\) up to a constant of proportionality, we can normalize it to ensure that we have a posterior PDF whose integral over \\(\\mathbb{R}\\) equals one. After we see our data, we believe that \\(\\theta_\\true \\in [a, b]\\) with probability \\[\n  \\Pr(a \\leq \\theta \\leq b \\given{} \\data)\n  = \\int_a^b \\pi(\\theta \\given{} \\data) \\,\\dif \\theta.\n\\] Bayesian point and interval estimates of \\(\\theta_\\true\\) are based on the posterior PDF.\nBayesian methods are often simpler, easier to interpret, and more robust to small sample sizes than frequentist methods like maximum likelihood estimation. In the limit of a large sample size, Bayesian and frequentist methods almost always give equivalent results. The Bayesian approach is also valuable because it emphasizes estimation and the accumulation of knowledge rather than binary decisions (Tukey 1960). However, the adoption of Bayesian methods has been impeded by a historical lack of the computational power needed to use them and by the widespread hesitation to specify prior distributions among epidemiologists, statisticians, and other scientists. The first problem is largely solved, but the latter problem remains with us today.\nA common approach to specifying a prior distribution is to use a noninformative prior that places few or no restrictions on the value of \\(\\theta\\). This lets the data “speak for itself” at the price of ignoring existing knowledge about the underlying scientific question. The ability to incorporate an informative prior distribution in the Bayesian approach to statistical inference should be viewed as a feature, not a bug (Greenland 2006; Greenland and Poole 2013).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html#prior-and-posterior-distributions",
    "href": "bayes.html#prior-and-posterior-distributions",
    "title": "4  Bayesian Estimation",
    "section": "",
    "text": "4.1.1 Posterior point and interval estimation\nThe mean, median, or mode of the posterior distribution of \\(\\theta\\) can be used as a point estimate of \\(\\theta_\\true\\). We will use \\(\\bar{\\theta}\\) to denote the posterior mean and \\(\\tilde{\\theta}\\) to denote the posterior median. In large samples, the posterior distribution converges to a normal distribution with mean \\(\\theta_\\true\\) and variance \\(I(\\theta_\\true)^{-1}\\) Gelman et al. (2013),2 which is the same as the limiting normal distribution of the MLE \\(\\hat{\\theta}\\). In this limit, the posterior mean, median, and mode are all equal to \\(\\hat{\\theta}\\).\nA \\(1 - \\alpha\\) credible interval is an interval \\([a, b]\\) such that \\[\n  \\Pr(a \\leq \\theta \\leq b \\given{} \\data)\n  = \\int_a^b \\pi(\\theta \\given{} \\data) \\,\\dif \\theta\n  = 1 - \\alpha.\n\\] Given our data, we believe that \\(\\theta_\\true \\in [a, b]\\) with probability \\(1 - \\alpha\\). There are many different ways that a credible interval can be defined. The one that is conceptually closest to a confidence interval is the central posterior interval or equal-tailed interval, where \\[\n  \\Pr(\\theta &lt; a \\given{} \\data) = \\Pr(\\theta &gt; b \\given{} \\data)\n  = \\frac{\\alpha}{2}.\n\\] Thus, the \\(1 - \\alpha\\) confidence limits are the \\(\\alpha / 2\\) and \\(1 - \\alpha / 2\\) quantiles of the posterior distribution of \\(\\theta_\\true\\). Unlike confidence intervals, credible intervals can accurately be interpreted as containing \\(\\theta_\\true\\) with probability \\(1 - \\alpha\\). Like confidence intervals, credible intervals are only reliable if the likelihood is approximately correct. An equal-tailed credible interval is guaranteed to contain the posterior median \\(\\tilde{\\theta}\\). It will usually contain the posterior mean \\(\\bar{\\theta}\\), but this is not guaranteed in small samples.\n\n\n4.1.2 Bayesian interpretation of confidence intervals\nBayesian credible intervals actually have the properties that people intuitively but naively expect of frequentist confidence intervals. In finite samples, a \\(1 - \\alpha\\) equal-tailed credible interval and a \\(1 - \\alpha\\) confidence interval will be similar when the posterior density \\(\\pi(\\theta \\given \\data)\\) is proportional to the likelihood \\(L(\\theta) = \\pi(\\theta \\given \\data)\\). This occurs when \\(\\pi(\\theta)\\) is constant, as in some uninformative priors. In the limit of a large sample, the credible interval and the confidence interval are nearly identical. In general, a frequentist confidence interval can be interpreted as an approximation to a Bayesian credible interval when there is a large sample and a prior distribution that is flat across the range of the confidence interval (Pratt 1965; Greenland and Poole 2013). However, credible intervals do not rely on large-sample approximations, and they are able to incorporate prior knowledge about the possible values of \\(\\theta_\\true\\).\n\n\n4.1.3 Posterior probability of \\(H_0\\) and p-values\nThe idea of prior and posterior probabilities from Bayesian inference gives us a useful perspective on the interpretation of p-values, which is a source of much confusion in epidemiology (Diamond and Forrester 1983; Greenland et al. 2016; Baduashvili, Evans, and Cutler 2020). The most common error is to interpret the p-value as the posterior probability that \\(H_0\\) is true. By Bayes’ rule, \\[\n  \\Pr(H_0 \\ \\true \\given{} \\data)\n  = \\frac{\\Pr(\\data \\given{} H_0 \\ \\true) \\Pr(H_0 \\ \\true)}{\\Pr(\\data)}.\n\\] The p-value is analogous to \\(\\Pr(\\data \\given{} H_0 \\ \\true)\\), but the posterior probability \\(\\Pr(H_0 \\text{ true} \\given{} \\ \\data)\\) depends on its prior probability \\(\\Pr(H_0)\\). It should take more data to convince us of a null hypothesis that seemed very unlikely than to convince us of a null hypothesis that seemed very likely.\nFor a null hypothesis of the form \\(H_0: \\theta_\\true = \\theta_0\\) where \\(\\theta\\) ranges over an interval, it can be difficult to assign a prior probability to \\(H_0\\). For any continuous distribution of \\(\\theta\\), the probability that it takes any particular value is zero. One way around this difficulty is to assign a prior probability mass to the null value \\(\\theta_0\\). A more difficult problem is to assign a prior probability to the alternative hypothesis \\(H_1\\), which is often not clearly specified. This means that the posterior probability of the null hypothesis is often not clearly defined.\nHowever, it is not difficult to calculate a lower bound on the probability that \\(H_0\\) is true given the data (Edwards, Lindman, and Savage 1963; Berger and Sellke 1987). Let \\(\\pi_0\\) be the prior probability of \\(H_0\\), and suppose we have a single alternative hypothesis \\(H_1: \\theta_\\true = \\theta_1\\) with prior probability \\(1 - \\pi_0\\). Then the posterior probability of \\(H_0\\) is \\[\n  \\Pr(H_0 \\given{} \\data)\n  = \\frac{L(\\theta_0) \\pi_0}{L(\\theta_0) \\pi_0 + L(\\theta_1) (1 - \\pi_0)}\n  = \\bigg(1 + \\frac{1 - \\pi_0}{\\pi_0} \\frac{L(\\theta_1)}{L(\\theta_0)}\\bigg)^{-1}.\n\\] Given the data, the posterior probability of \\(H_0\\) is minimized if we happen to get the MLE \\(\\hat{\\theta} = \\theta_1\\), so \\[\n  \\Pr(H_0 \\ \\true \\given{} \\data)\n  \\geq \\bigg(1 + \\frac{1 - \\pi_0}{\\pi_0}\\frac{L(\\hat{\\theta})}{L(\\theta_0)}\\bigg)^{-1}.\n\\] From Wilk’s theorem (Wilks 1938) for the likelihood ratio test, twice the log likelihood ratio has an approximate \\(\\chi^2_1\\) distribution in large samples. To get a p-value of \\(\\alpha\\) from the likelihood ratio test, we need \\[\n  2\\big(\\ell(\\hat{\\theta}) - \\ell(\\theta_0)\\big)\n  = z_{1 - \\frac{\\alpha}{2}}^2\n  \\;\\Rightarrow\\;\n  \\frac{L(\\hat{\\theta})}{L(\\theta_0)}\n   = e^{\\frac{1}{2} z_{1 - \\frac{\\alpha}{2}}^2}.\n\\] Figure 4.1 shows the minimum posterior probability of \\(H_0\\) if \\(\\pi_0 = 0.5\\) for different p-values. The p-value is almost always much lower than the lower bound of the posterior probability of the null. For \\(\\pi_0 = 0.5\\), the lower bounds for \\(\\Pr(H_0 \\ \\true \\given{} \\data)\\) are approximately 0.205 for \\(p = 0.10\\), 0.128 for \\(p = 0.05\\), and 0.035 for \\(p = 0.01\\). In practice, the posterior probability of \\(H_0\\) can be much larger than its lower bound (Berger and Sellke 1987).\n\n\n\nCode\n\npostH0.R\n\n## Posterior probability of the null hypothesis (H0)\n\n# function to calculate lower bound\nlowerb &lt;- function(pi0=0.5, pval=0.05) {\n  # args: pi0 = prior probability of H0, pval = p-value\n  # return: lower bound on posterior probability of H0\n  z &lt;- qnorm(1 - pval / 2)\n  1 / (1 + (1 - pi0) / pi0 * exp(0.5 * z^2))\n}\n\n# plot of lower bounds for p-value = 0.01, 0.05, and 0.1\nx &lt;- seq(0, 1, by = .01)\nplot(x, lowerb(x), type = \"n\", xlim = c(0, 1), ylim = c(0, 1),\n     xlab = expression(\"Prior probability of H\"[0]),\n     ylab = expression(\"Minimum posterior probability of H\"[0]))\ngrid()\nabline(0, 1, col = \"gray\")\nlines(x, lowerb(x))\nlines(x, lowerb(x, pval = .01), lty = \"dashed\")\nlines(x, lowerb(x, pval = .1), lty = \"dotted\")\nlegend(\"topleft\", bg = \"white\", lty = c(\"dotted\", \"solid\", \"dashed\"),\n       legend = c(\"p = 0.10\", \"p = 0.05\", \"p = 0.01\"))\n\n\n\n\n\n\n\n\n\nFigure 4.1: The minimum posterior probability of \\(H_0\\) as a function of its prior probability \\(\\pi_0\\) for p-values of 0.01, 0.05, and 0.1. These posterior probabilities can be much higher than the p-value.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html#bayesian-estimation-of-a-probability",
    "href": "bayes.html#bayesian-estimation-of-a-probability",
    "title": "4  Bayesian Estimation",
    "section": "4.2 Bayesian estimation of a probability",
    "text": "4.2 Bayesian estimation of a probability\nWhen estimating a probability \\(\\ptrue\\), our likelihood will be the binomial likelihood \\[\n  L(p) = \\binom{n}{k} p^k (1 - p)^{n - x}\n\\] when \\(k\\) out of \\(n\\) samples equal one. Because we only need to calculate the likelihood up to a constant of proportionality, we can safely ignore the \\(\\binom{n}{k}\\) because it does not depend on \\(p\\). The posterior distribution of \\(p\\) will be \\[\n  \\pi(p \\given{} \\data) \\propto p^k (1 - p)^{n - k} \\pi(p)\n\\] where \\(\\pi(p)\\) is the prior distribution of \\(p\\). When \\(\\pi(p \\given{} \\data)\\) and \\(\\pi(p)\\) are from the same family of distributions, the prior \\(\\pi(p)\\) is said to be a conjugate distribution for the binomial likelihood. Conjugate distributions exist for many likelihoods used in epidemiology.\n\n4.2.1 Beta distribution\nThe conjugate distribution for the binomial likelihood is the beta distribution. It has is a support on \\([0, 1]\\) (where \\(p_\\true\\) must live) with the PDF \\[\n  f(x, \\alpha, \\beta)\n  = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}\n    x^{\\alpha - 1} (1 - x)^{\\beta - 1}\n  \\propto x^{\\alpha - 1} (1 - x)^{\\beta - 1}.\n\\] where \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\) are shape parameters and \\(\\Gamma(v)\\) is the gamma function.3 Because we will only need to calculate PDFs up to a mutiplicative constant, the gamma function term can be safely ignored. If \\(X \\sim \\text{beta}(\\alpha, \\beta)\\), then \\[\n  \\E[X] = \\frac{\\alpha}{\\alpha + \\beta}\n\\] and \\[\n  \\Var(X) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}.\n\\] The beta distribution has a number of important special cases:\n\nWhen \\(\\alpha = \\beta = 1\\), it is a uniform(\\(0, 1\\)) distribution.\nThe beta distribution with \\(\\alpha = \\beta = 1 / 2\\) is the Jeffreys prior for the binomial likelihood.4 A Jeffreys prior is proportional to the square root of the expected information \\(\\mathcal{I}_1(\\theta)\\) of a single observation(Jeffreys 1946). They are widely used as noninformative priors. For the binomial likelihood, \\(\\mathcal{I}_1(p) = \\frac{1}{p (1 - p)}\\).\n\n\n\n4.2.2 Posterior point and interval estimates\nIf the prior distribution of \\(p\\) is a beta(\\(\\alpha, \\beta\\)) distribution, then \\[\n  \\begin{aligned}\n    \\pi(p \\given \\data)\n    &\\propto p^k (1 - p)^{n - k} \\times p^{\\alpha - 1} (1 - p)^{\\beta - 1} \\\\\n    &= p^{k + \\alpha - 1} (1 - p)^{n - k + \\beta - 1}\n  \\end{aligned}\n\\] so the posterior distribution of \\(p\\) is a beta(\\(k + \\alpha, n - k + \\beta\\)) distribution. The posterior mean is \\[\n  \\bar{p} = \\frac{k + \\alpha}{(k + \\alpha) + (n - k + \\beta)}\n  = \\frac{k + \\alpha}{n + \\alpha + \\beta},\n\\] and the posterior variance is \\[\n  \\frac{(k + \\alpha) (n - k + \\beta)}{(n + \\alpha + \\beta)^2 (n + \\alpha + \\beta + 1)}\n  = \\frac{\\bar{p} (1 - \\bar{p})}{n + \\alpha + \\beta + 1}.\n\\] The endpoints of the \\(1 - \\alpha\\) central credible interval are the \\(\\alpha / 2\\) and \\(1 - \\alpha / 2\\) quantiles of the beta(\\(k + \\alpha, n - k + \\beta)\\) distribution. Figure 4.2 shows prior and posterior distributions for 15 successes out of 22 trials. Although the prior distributions are quite different, the posterior distributions are quite similar. With large samples, the prior distribution disappears into the likelihood.\n\n\n\nCode\n\nnormplots.R\n\n## Bayesian prior and posterior distributions for a probability\n\n# beta prior and posterior distributions\n# The prior is beta(1, 1), which is the uniform(0, 1) distribution.\n# Because k = 15 and n = 22, the posterior is beta(15 + 1, 22 - 15 + 1)\np &lt;- seq(0.01, 0.99, by = 0.01)\nplot(p, dbeta(p, 16, 8), type = \"n\", ylim = c(0, 5),\n     xlab = \"p\", ylab = \"Probability density\")\ngrid()\nlines(p, dbeta(p, 1, 1), lty = \"dashed\")  # prior PDF\nlines(p, dbeta(p, 16, 8))                 # posterior PDF\npostlower &lt;- qbeta(0.025, 16, 8)          # 95% credible interval lower bound\npostupper &lt;- qbeta(0.975, 16, 8)          # 95% credible interval upper bound\npolygon(c(0, seq(0, postlower, by = 0.01), postlower),\n        c(0, dbeta(seq(0, postlower, by = 0.01), 16, 8), 0),\n        col = \"darkgray\")\npolygon(c(postupper, seq(postupper, 1, by = 0.01), 1),\n        c(0, dbeta(seq(postupper, 1, by = 0.01), 16, 8), 0),\n        col = \"darkgray\")\nlegend(\"topleft\", bg = \"white\",\n       lty = c(\"dashed\", \"solid\", NA, \"dashed\", \"solid\"),\n       col = c(\"black\", \"black\", NA, \"darkgray\", \"darkgray\"),\n       fill = c(NA, NA, \"darkgray\", NA, NA),\n       border = c(NA, NA, \"black\", NA, NA),\n       legend = c(\"uniform prior\", \"posterior (uniform)\",\n                  \"2.5% tails (uniform)\", \"Jeffreys prior\",\n                  \"posterior (Jeffreys)\"))\nlines(p, dbeta(p, 0.5, 0.5), lty = \"dashed\", col = \"darkgray\")\nlines(p, dbeta(p, 15.5, 7.5), col = \"darkgray\")\n\n\n\n\n\n\n\n\n\nFigure 4.2: Prior and posterior distributions for \\(p\\) after observing 15 successes out of 22 trials. The uniform prior and the resulting posterior distribution are shown in black with the 2.5% tails shaded. The central 95% credible interval includes all values of \\(p\\) between the two tails. The Jeffreys prior and the corresponding posterior distribution are shown in dark gray.\n\n\n\n\n\nAs \\(n \\rightarrow \\infty\\), we have \\(\\bar{p} - \\hat{p} \\rightarrow 0\\) and \\[\\frac{\\bar{p} (1 - \\bar{p})}{n + \\alpha + \\beta + 1}\n    - \\frac{\\hat{p} (1 - \\hat{p})}{n} \\rightarrow 0.\\] Thus, the posterior distribution approaches approximate normal distribution of the maximum likelihood estimate \\(\\hat{p}\\) in large samples. However, the Bayesian posterior distribution is valid for any sample size, not just large samples.\n\n\n4.2.3 Jeffreys confidence interval\nThe \\(1 - \\alpha\\) Jeffreys confidence interval is the central credible interval with a beta(\\(1 / 2, 1 / 2)\\) prior, which is the Jeffreys prior for a binomial model. When \\(k &gt; 0\\) and \\(k &lt; n\\), its endpoints are the \\(\\alpha / 2\\) and \\(1 - \\alpha / 2\\) quantiles of the beta(\\(k + 1 / 2, n - k + 1 / 2\\)) posterior distribution that we get if we see \\(k\\) successes in \\(n\\) trials. When \\(k = 0\\), the lower endpoint is 0. When \\(k = n\\), the upper endpoint is 1. For a binomial proportion, the Jeffreys confidence interval has width and coverage probability similar to the score (Wilson) and mid-p exact confidence intervals (Brown, Cai, and DasGupta 2001).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html#comparison-of-binomial-confidence-intervals",
    "href": "bayes.html#comparison-of-binomial-confidence-intervals",
    "title": "4  Bayesian Estimation",
    "section": "4.3 Comparison of binomial confidence intervals",
    "text": "4.3 Comparison of binomial confidence intervals\nTable 4.1 shows eight types of confidence intervals for a probability that we have discussed. These are calculated for the sensitivity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV) of the diabetes test one hour after the meal, where a positive test was defined as a blood glucose concentration above 130 mg/dL.\n\n\n\nTable 4.1: 95% confidence intervals for the sensitivity, specificity, PPV, and NPV of diabetes test in Remein and Wilkerson (1961).\n\n\n\n\n\n\nSensitivity\nSpecificity\nPPV\nNPV\n\n\n\n\n95% CI type\n\\(55 / 70 \\approx 0.786\\)\n\\(462 / 510 \\approx 0.906\\)\n\\(55 / 103 \\approx 0.534\\)\n\\(462 / 477 \\approx 0.969\\)\n\n\nWald\n(0.690, 0.882)\n(0.881, 0.931)\n(0.438, 0.630)\n(0.953, 0.984)\n\n\nLogit\n(0.674, 0.866)\n(0.877, 0.928)\n(0.438, 0.628)\n(0.949, 0.981)\n\n\nAgresti-Coull\n(0.675, 0.867)\n(0.877, 0.928)\n(0.438, 0.627)\n(0.948, 0.981)\n\n\nScore\n(0.676, 0.866)\n(0.877, 0.928)\n(0.438, 0.627)\n(0.949, 0.981)\n\n\nLikelihood ratio\n(0.680, 0.871)\n(0.879, 0.929)\n(0.438, 0.629)\n(0.950, 0.982)\n\n\nExact\n(0.671, 0.875)\n(0.877, 0.930)\n(0.433, 0.633)\n(0.949, 0.982)\n\n\nMid-p\n(0.678, 0.870)\n(0.878, 0.929)\n(0.437, 0.629)\n(0.950, 0.982)\n\n\nJeffreys\n(0.679, 0.869)\n(0.878, 0.929)\n(0.438, 0.628)\n(0.950, 0.982)\n\n\n\n\n\n\nBecause of their good coverage probabilities and narrow widths, the score (Wilson), likelihood ratio, and Jeffreys intervals are recommended by Agresti and Coull (1998) and Brown, Cai, and DasGupta (2001). The Agresti-Coull intervals are slightly wider than these intervals, but they have slightly higher coverage probabilities and are simpler to calculate. Mid-p exact confidence intervals are similar to the Jeffreys intervals, but they are more difficult to calculate. Exact intervals are too wide, and the Wald intervals have coverage probabilities that are often too low. The logit-transformed Wald interval has good coverage probabilities, but it can be even wider than the exact interval. In large samples (with larger samples required for probabilities near zero or one), all of the intervals are similar.\n\nR\n\n\n\n\nbinconf-table.R\n\n## Comparison of binomial confidence limits\nlibrary(DescTools)\n\n# Sensitivity (55 / 70)\n# Use round() to get rounded numbers shown in the table.\nround(BinomCI(55, 70, method = \"wald\"), 3)\nround(BinomCI(55, 70, method = \"logit\"), 3)\nround(BinomCI(55, 70, method = \"agresti-coull\"), 3)\nround(BinomCI(55, 70, method = \"wilson\"), 3)\nround(BinomCI(55, 70, method = \"lik\"), 3)\nround(BinomCI(55, 70, method = \"clopper-pearson\"), 3)\nround(BinomCI(55, 70, method = \"midp\"), 3)\nround(BinomCI(55, 70, method = \"jeffreys\"), 3)\n\n# Specificity (462 / 510)\nround(BinomCI(462, 510, method = \"wald\"), 3)\nround(BinomCI(462, 510, method = \"logit\"), 3)\nround(BinomCI(462, 510, method = \"agresti-coull\"), 3)\nround(BinomCI(462, 510, method = \"wilson\"), 3)\nround(BinomCI(462, 510, method = \"lik\"), 3)\nround(BinomCI(462, 510, method = \"clopper-pearson\"), 3)\nround(BinomCI(462, 510, method = \"midp\"), 3)\nround(BinomCI(462, 510, method = \"jeffreys\"), 3)\n\n# PPV (55 / 103)\nround(BinomCI(55, 103, method = \"wald\"), 3)\nround(BinomCI(55, 103, method = \"logit\"), 3)\nround(BinomCI(55, 103, method = \"agresti-coull\"), 3)\nround(BinomCI(55, 103, method = \"wilson\"), 3)\nround(BinomCI(55, 103, method = \"lik\"), 3)\nround(BinomCI(55, 103, method = \"clopper-pearson\"), 3)\nround(BinomCI(55, 103, method = \"midp\"), 3)\nround(BinomCI(55, 103, method = \"jeffreys\"), 3)\n\n# NPV (462 / 477)\nround(BinomCI(462, 477, method = \"wald\"), 3)\nround(BinomCI(462, 477, method = \"logit\"), 3)\nround(BinomCI(462, 477, method = \"agresti-coull\"), 3)\nround(BinomCI(462, 477, method = \"wilson\"), 3)\nround(BinomCI(462, 477, method = \"lik\"), 3)\nround(BinomCI(462, 477, method = \"clopper-pearson\"), 3)\nround(BinomCI(462, 477, method = \"midp\"), 3)\nround(BinomCI(462, 477, method = \"jeffreys\"), 3)\n\n\n\n\n\n\n\n\nAgresti, Alan, and Brent A Coull. 1998. “Approximate Is Better Than ‘Exact’ for Interval Estimation of Binomial Proportions.” The American Statistician 52 (2): 119–26.\n\n\nBaduashvili, Amiran, Arthur T Evans, and Todd Cutler. 2020. “How to Understand and Teach p-Values: A Diagnostic Test Framework.” Journal of Clinical Epidemiology 122: 49–55.\n\n\nBerger, James O, and Thomas Sellke. 1987. “Testing a Point Null Hypothesis: The Irreconcilability of p Values and Evidence.” Journal of the American Statistical Association 82 (397): 112–22.\n\n\nBerkson, Joseph. 1942. “Tests of Significance Considered as Evidence.” Journal of the American Statistical Association 37 (219): 325–35.\n\n\nBoos, Dennis D, and Leonard A Stefanski. 2013. Essential Statistical Inference. Springer.\n\n\nBrown, Lawrence D, T Tony Cai, and Anirban DasGupta. 2001. “Interval Estimation for a Binomial Proportion.” Statistical Science 16 (2): 101–17.\n\n\nDiamond, George A, and James S Forrester. 1983. “Clinical Trials and Statistical Verdicts: Probable Grounds for Appeal.” Annals of Internal Medicine 98 (3): 385–94.\n\n\nEdwards, Ward, Harold Lindman, and Leonard J Savage. 1963. “Bayesian Statistical Inference for Psychological Research.” Psychological Review 70 (3): 193–242.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis. CRC press.\n\n\nGreenland, Sander. 2006. “Bayesian Perspectives for Epidemiological Research: I. Foundations and Basic Methods.” International Journal of Epidemiology 35 (3): 765–75.\n\n\nGreenland, Sander, and Charles Poole. 2013. “Living with p Values: Resurrecting a Bayesian Perspective on Frequentisi Statistics.” Epidemiology 24 (1): 62–68.\n\n\nGreenland, Sander, Stephen J Senn, Kenneth J Rothman, John B Carlin, Charles Poole, Steven N Goodman, and Douglas G Altman. 2016. “Statistical Tests, p Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31 (4): 337–50.\n\n\nJeffreys, Harold. 1946. “An Invariant Form for the Prior Probability in Estimation Problems.” Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences 186 (1007): 453–61.\n\n\nLe Cam, Lucien. 1953. “On Some Asymptotic Properties of Maximum Likelihood Estimates and Related Bayes’ Estimates.” University of California Publications in Statistics 1 (11): 277–330.\n\n\nPratt, John W. 1965. “Bayesian Interpretation of Standard Inference Statements.” Journal of the Royal Statistical Society. Series B (Methodological) 27 (2): 169–203.\n\n\nRemein, Quentin R, and Hugh LC Wilkerson. 1961. “The Efficiency of Screening Tests for Diabetes.” Journal of Chronic Diseases 13 (1): 6–21.\n\n\nTukey, John W. 1960. “Conclusions Vs Decisions.” Technometrics 2 (4): 423–33.\n\n\nWilks, Samuel S. 1938. “The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses.” The Annals of Mathematical Statistics 9 (1): 60–62.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html#footnotes",
    "href": "bayes.html#footnotes",
    "title": "4  Bayesian Estimation",
    "section": "",
    "text": "Joseph Berkson (1899–1982) was an American physician and statistician at the Mayo Clinic in Rochester, Minnesota. He helped develop and popularize the use of logistic regression for binary outcomes, coining the term “logit” for the log odds in 1944. He also pioneered the study of selection bias, a special case of which is called “Berkson’s bias”. In the late 1950s and the 1960s, he argued that scientific evidence did not establish that smoking causes lung cancer.↩︎\n This convergence follows from the Laplace approximation to the posterior distribution (Gelman et al. 2013). It occurs when the likelihood \\(L(\\theta)\\) has a continuous second derivative with respect to \\(\\theta\\) and \\(\\theta_\\true\\) is not on the boundary of the support of the prior distribution. These are similar to the regularity conditions for maximum likelihood estimation.↩︎\n The gamma function is \\(\\Gamma(v) = \\int_0^\\infty y^{v - 1} e^{-y} \\dif y\\). It is used to define the gamma distribution, of which the chi-squared distributions (including \\(\\chi^2_1\\)) are special cases. If \\(v\\) is a positive integer, then \\(\\Gamma(v) = (v - 1)!\\).↩︎\n Harold Jeffreys (1891–1989) was an English mathematician, statistician, geophysicist, and astronomer. He helped revive the Bayesian notion of probability as an expression of our knowledge about an unknown quantity, wrote a classic textbook on mathematical physics with his wife Bertha (also a mathematician and physicist), and was a prominent opponent of the theory of plate tectonics.↩︎",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agresti, Alan. 2013. Categorical Data Analysis. Third. Vol.\n792. John Wiley & Sons.\n\n\nAgresti, Alan, and Brent A Coull. 1998. “Approximate Is Better\nThan ‘Exact’ for Interval Estimation of Binomial\nProportions.” The American Statistician 52 (2): 119–26.\n\n\nAitchison, John, and SD Silvey. 1958. “Maximum-Likelihood\nEstimation of Parameters Subject to Restraints.” The Annals\nof Mathematical Statistics 29: 813–28.\n\n\nAlbert, Adelin. 1982. “On the Use and Computation of Likelihood\nRatios in Clinical Chemistry.” Clinical Chemistry 28\n(5): 1113–19.\n\n\nAlho, Juha M. 1992. “On Prevalence, Incidence, and Duration in\nGeneral Stable Populations.” Biometrics 48 (2): 587–92.\n\n\nBaduashvili, Amiran, Arthur T Evans, and Todd Cutler. 2020. “How\nto Understand and Teach p-Values: A Diagnostic Test Framework.”\nJournal of Clinical Epidemiology 122: 49–55.\n\n\nBamber, Donald. 1975. “The Area Above the Ordinal Dominance Graph\nand the Area Below the Receiver Operating Characteristic Graph.”\nJournal of Mathematical Psychology 12 (4): 387–415.\n\n\nBayes, Thomas. 1763. “LII. An Essay\nTowards Solving a Problem in the Doctrine of Chances. By the Late\nRev. Mr. Bayes, FRS\nCommunicated by Mr. Price, in a Letter to\nJohn Canton, AMFRS.”\nPhilosophical Transactions of the Royal Society of London 53:\n370–418.\n\n\nBengtsson, Ewert, and Patrik Malm. 2014. “Screening for Cervical\nCancer Using Automated Analysis of PAP-Smears.”\nComputational and Mathematical Methods in Medicine 2014:\n842037.\n\n\nBerger, James O, and Thomas Sellke. 1987. “Testing a Point Null\nHypothesis: The Irreconcilability of p Values and Evidence.”\nJournal of the American Statistical Association 82 (397):\n112–22.\n\n\nBerkson, Joseph. 1942. “Tests of Significance Considered as\nEvidence.” Journal of the American Statistical\nAssociation 37 (219): 325–35.\n\n\nBirnbaum, Allan. 1964. “Median-Unbiased Estimators.”\nBulletin of Mathematical Statistics 11: 25–34.\n\n\nBlumberg, Mark S. 1957. “Evaluating Health Screening\nProcedures.” Operations Research 5 (3): 351–60.\n\n\nBoos, Dennis D, and Leonard A Stefanski. 2013. Essential Statistical\nInference. Springer.\n\n\nBostrom, RC, HS Sawyer, and WE Tolles. 1959. “Instrumentation for\nAutomatically Prescreening Cytological Smears.” Proceedings\nof the IRE 47 (11): 1895–1900.\n\n\nBrown, Lawrence D, T Tony Cai, and Anirban DasGupta. 2001.\n“Interval Estimation for a Binomial Proportion.”\nStatistical Science 16 (2): 101–17.\n\n\nClopper, Charles J, and Egon S Pearson. 1934. “The Use of\nConfidence or Fiducial Limits Illustrated in the Case of the\nBinomial.” Biometrika 26 (4): 404–13.\n\n\nCohen, I Bernard. 1984. “Florence Nightingale.”\nScientific American 250 (3): 128–37.\n\n\nCramér, Harald. 1946. Mathematical Methods of Statistics.\nPrinceton University Press.\n\n\nDiamond, George A, and James S Forrester. 1983. “Clinical Trials\nand Statistical Verdicts: Probable Grounds for Appeal.”\nAnnals of Internal Medicine 98 (3): 385–94.\n\n\nDunn Jr, John E. 1962. “The Use of Incidence and Prevalence in the\nStudy of Disease Development in a Population.” American\nJournal of Public Health 52 (7): 1107–18.\n\n\nEdwards, Ward, Harold Lindman, and Leonard J Savage. 1963.\n“Bayesian Statistical Inference for Psychological\nResearch.” Psychological Review 70 (3): 193–242.\n\n\nEfron, Bradley, and David V Hinkley. 1978. “Assessing the Accuracy\nof the Maximum Likelihood Estimator: Observed Versus Expected\nFisher Information.” Biometrika 65 (3):\n457–83.\n\n\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to\nthe Bootstrap. Chapman & Hall/CRC.\n\n\nFagan, Terrence J. 1975. “Nomogram for Bayes’s\nTheorem.” New England Journal of Medicine 293 (5): 257.\n\n\nFreedman, David A. 2007. “How Can the Score Test Be\nInconsistent?” The American Statistician 61 (4): 291–95.\n\n\nFreeman, Jonathan, and George B Hutchison. 1980. “Prevalence,\nIncidence and Duration.” American Journal of\nEpidemiology 112 (5): 707–23.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari,\nand Donald B Rubin. 2013. Bayesian Data Analysis. CRC press.\n\n\nGreenland, Sander. 2006. “Bayesian Perspectives for\nEpidemiological Research: I. Foundations and Basic Methods.”\nInternational Journal of Epidemiology 35 (3): 765–75.\n\n\nGreenland, Sander, and Charles Poole. 2013. “Living with p Values:\nResurrecting a Bayesian Perspective on Frequentisi Statistics.”\nEpidemiology 24 (1): 62–68.\n\n\nGreenland, Sander, Stephen J Senn, Kenneth J Rothman, John B Carlin,\nCharles Poole, Steven N Goodman, and Douglas G Altman. 2016.\n“Statistical Tests, p Values, Confidence Intervals, and Power: A\nGuide to Misinterpretations.” European Journal of\nEpidemiology 31 (4): 337–50.\n\n\nHanley, James A, and Barbara J McNeil. 1982. “The Meaning and Use\nof the Area Under a Receiver Operating Characteristic (ROC)\nCurve.” Radiology 143 (1): 29–36.\n\n\nJeffreys, Harold. 1946. “An Invariant Form for the Prior\nProbability in Estimation Problems.” Proceedings of the Royal\nSociety of London. Series A. Mathematical and Physical Sciences 186\n(1007): 453–61.\n\n\nKeiding, Niels. 1991. “Age-Specific Incidence and Prevalence: A\nStatistical Perspective.” Journal of the Royal Statistical\nSociety: Series A (Statistics in Society) 154 (3): 371–96.\n\n\nKenward, Michael G, and Geert Molenberghs. 1998. “Likelihood Based\nFrequentist Inference When Data Are Missing at Random.”\nStatistical Science 13 (3): 236–47.\n\n\nKessel, Elton. 1962. “Diabetes Detection: An Improved\nApproach.” Journal of Chronic Diseases 15 (12): 1109–21.\n\n\nLancaster, H Oliver. 1961. “Significance Tests in Discrete\nDistributions.” Journal of the American Statistical\nAssociation 56 (294): 223–34.\n\n\nLaplace, Pierre Simon. 1820. Théorie Analytique Des\nProbabilités. Vol. 7. Courcier.\n\n\nLe Cam, Lucien. 1953. “On Some Asymptotic Properties of Maximum\nLikelihood Estimates and Related Bayes’ Estimates.”\nUniversity of California Publications in Statistics 1 (11):\n277–330.\n\n\nLusted, Lee B. 1971a. “Decision-Making Studies in Patient\nManagement.” New England Journal of Medicine 284 (8):\n416–24.\n\n\n———. 1971b. “Signal Detectability and Medical\nDecision-Making.” Science 171 (3977): 1217–19.\n\n\n———. 1984. “ROC Recollected.” Medical\nDecision Making 4: 131–35.\n\n\nMacMahon, Brian, and William D Terry. 1958. “Application of Cohort\nAnalysis to the Study of Time Trends in Neoplastic Disease.”\nJournal of Chronic Diseases 7 (1): 24–35.\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological\nPerspective.” In A History of Epidemiologic Methods and\nConcepts, edited by Alfredo Morabia, 3–125. Springer.\n\n\nNeyman, Jerzy, and Egon Sharpe Pearson. 1933. “On the Problem of\nthe Most Efficient Tests of Statistical Hypotheses.”\nPhilosophical Transactions of the Royal Society of London. Series A,\nContaining Papers of a Mathematical or Physical Character 231\n(694-706): 289–337.\n\n\nPratt, John W. 1965. “Bayesian Interpretation of Standard\nInference Statements.” Journal of the Royal Statistical\nSociety. Series B (Methodological) 27 (2): 169–203.\n\n\nPreston, Samuel H. 1987. “Relations Among Standard Epidemiologic\nMeasures in a Population.” American Journal of\nEpidemiology 126 (2): 336–45.\n\n\nRao, C Radhakrishna. 1945. “Information and Accuracy Attainable in\nthe Estimation of Statistical Parameters.” Bulletin of the\nCalcutta Mathematical Society 37 (3): 81–91.\n\n\n———. 1948. “Large Sample Tests of Statistical Hypotheses\nConcerning Several Parameters with Applications to Problems of\nEstimation.” In Mathematical Proceedings of the Cambridge\nPhilosophical Society, 44:50–57. Cambridge University Press.\n\n\nReid, Nancy. 2003. “Asymptotics and the Theory of\nInference.” The Annals of Statistics 31 (6): 1695–2095.\n\n\nRemein, Quentin R, and Hugh LC Wilkerson. 1961. “The Efficiency of\nScreening Tests for Diabetes.” Journal of Chronic\nDiseases 13 (1): 6–21.\n\n\nRobert, Christian P, and George Casella. 2004. Monte Carlo\nStatistical Methods. Second edition. Springer.\n\n\nRothman, Kenneth J. 1978. “A Show of Confidence.” New\nEngland Journal of Medicine 299 (24): 1362–63.\n\n\n———. 1981. “Induction and Latent Periods.” American\nJournal of Epidemiology 114 (2): 253–59.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera.\nSecond edition. John Churchill. https://wellcomecollection.org/works/uqa27qrt.\n\n\nSwets, John A. 1973. “The Relative Operating Characteristic in\nPsychology: A Technique for Isolating Effects of Response Bias Finds\nWide Use in the Study of Perception and Cognition.”\nScience 182 (4116): 990–1000.\n\n\n———. 1988. “Measuring the Accuracy of Diagnostic Systems.”\nScience 240 (4857): 1285–93.\n\n\nTukey, John W. 1960. “Conclusions Vs Decisions.”\nTechnometrics 2 (4): 423–33.\n\n\n———. 1962. “The Future of Data Analysis.” The Annals of\nMathematical Statistics 33 (1): 1–67.\n\n\nVecchio, Thomas J. 1966. “Predictive Value of a Single Diagnostic\nTest in Unselected Populations.” New England Journal of\nMedicine 274 (21): 1171–73.\n\n\nWald, Abraham. 1943. “Tests of Statistical Hypotheses Concerning\nSeveral Parameters When the Number of Observations Is Large.”\nTransactions of the American Mathematical Society 54 (3):\n426–82.\n\n\nWilks, Samuel S. 1938. “The Large-Sample Distribution of the\nLikelihood Ratio for Testing Composite Hypotheses.” The\nAnnals of Mathematical Statistics 9 (1): 60–62.\n\n\nWilson, Edwin B. 1927. “Probable Inference, the Law of Succession,\nand Statistical Inference.” Journal of the American\nStatistical Association 22 (158): 209–12.\n\n\nWinkelstein Jr, Warren. 2009. “Florence Nightingale: Founder of\nModern Nursing and Hospital Epidemiology.” Epidemiology\n20 (2): 311.\n\n\nYerushalmy, Jacob. 1947. “Statistical Problems in Assessing\nMethods of Medical Diagnosis, with Special Reference to\nX-Ray Techniques.” Public Health Reports\n(1896-1970) 62 (40): 1432–49.\n\n\nZweig, Mark H, and Gregory Campbell. 1993. “Receiver-Operating\nCharacteristic (ROC) Plots: A Fundamental Evaluation Tool\nin Clinical Medicine.” Clinical Chemistry 39 (4):\n561–77.",
    "crumbs": [
      "References"
    ]
  }
]