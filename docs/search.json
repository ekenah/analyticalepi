[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analytical Epidemiology",
    "section": "",
    "text": "Preface\nOne day at lunch at the Harvard School of Public Health, I overheard Professor Murray Mittleman say: “I love epidemiology. It all fits together like a diamond.” As a second-year doctoral student in epidemiology, I was surprised to hear the subject described with such unstrained enthusiasm. It has taken years of study and experience for me to understand what he meant. On the way, I too have fallen in love.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "Analytical Epidemiology",
    "section": "Who this book is for",
    "text": "Who this book is for\nThis book is intended primarily for two audiences:\n\nEpidemiologists are often protected from the mathematical foundations of their field. The long-term price of this is “dogmatism, that is, a tendency to rigidly protect a partially understood theoretical heritage” (Morabia 2004). The mathematics needed for a deeper understanding of epidemiologic methods is within reach of anyone who has come far enough to need it. Whether you master this material or just learn to approach it with more patience than fear, you will be doing a service to epidemiology and to public health.\nBiostatisticians are familiar with probability and statistical inference, but applying statistics to solve scientific problems in public health requires skills different from those needed to prove that a method works under given assumptions. Epidemiology is a living example of the interplay between theory and practice in applied statistics, and epidemiologists have shown integrity, courage, and ingenuity in confronting causal questions with statistical tools.\n\nBeyond these audiences, I hope to explain the logic of epidemiology to any interested reader. It is one of the most important applications of statistics, and it has quite possibly already helped save your life.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Analytical Epidemiology",
    "section": "How to use this book",
    "text": "How to use this book\nDifficult chapters, sections, subsections, and exercises are marked with an asterisk (*). These can be skipped without harming the logical flow of the book, but none of them is beyond the reach of a determined reader. The starring is recursive: Starred sections can be skipped within a starred chapter, starred subsections can be skipped within a starred section, and so on. Footnotes offer context or hint at more advanced material. All of them can be ignored if they do not seem useful or interesting.\nThis is a work in progress, and you may notice that some parts are obviously unfinished. Please report errors (including typos) or submit suggestions (especially good examples) at:\nhttps://github.com/ekenah/analyticalepi/issues.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Analytical Epidemiology",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nDevesh Kapur, Paul Farmer, and James H. Maguire guided me to a career in public health when I was an undergraduate. James Robins, Miguel A. Hernán, Marc Lipsitch, and Stephen P. Luby helped me become an epidemiologist, biostatistician, and epidemic modeler in graduate school. My career began under the mentorship of Ira M. Longini, Jr., and M. Elizabeth Halloran as a postdoctoral fellow at the Univerity of Washington and an assistant professor at the University of Florida. My colleagues Yang Yang, Grzegorz Rempała, Forrest Crawford, and Patrick Schnell have all provided useful comments. For their patience with early versions of this material, I am grateful to the students of STA 6177/PHC 6937 (Applied Survival Analysis) at the University of Florida from 2013 to 2016 and PUBHEPI 8430 (Epidemiology 4) at The Ohio State University from 2019 to the present.\nMy parents, Chris and Kate Kenah, courageously allowed me to travel to places they had never been to and do things I had been told to avoid. These experiences in the United States, India, South Africa, and especially Bangladesh opened my eyes to the terrible importance of clear thinking in public health. My wife, Asma Aktar, and our sons Rafi, Rayhan, and Rabi remind me every day how important it is to destroy everything that stifles humanity. To that end, I hope this book is useful.\nAny mistakes are my own, and God knows best (الله أعلم).\n\n\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological Perspective.” In A History of Epidemiologic Methods and Concepts, edited by Alfredo Morabia, 3–125. Springer.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera. Second edition. John Churchill. https://wellcomecollection.org/works/uqa27qrt.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "1.1 Sets, experiments, and events\nTo begin at the beginning, we will start with probability. Morabia (2004) accurately observed that “Epidemiology came late in human history because it had to wait for the emergence of probability.” This is probably the most difficult chapter of the entire book, but it will make all subsequent chapters easier. You can use it as a reference and come back to the difficult parts when you need them. Learning to think clearly about probability will give you a compass to find your way through epidemiologic methods.\nTo speak clearly about probabilities, we need some basic notation for sets. If \\(A\\) is a set that contains an object \\(a\\), we write \\[\\begin{equation}\n  a \\in A\n\\end{equation}\\] to indicate that \\(a\\) is an element of \\(A\\). If \\(A\\) and \\(B\\) are sets such that every element of \\(A\\) is also an element of \\(B\\), we write \\[\\begin{equation}\n  A \\subseteq B.\n\\end{equation}\\] to indicate that \\(A\\) is a subset of \\(B\\). Sets \\(A\\) and \\(B\\) are equal if and only if \\(A \\subseteq B\\) and \\(B \\subseteq A\\), which means they contain exactly the same elements. The empty set with no elements is denoted \\(\\varnothing\\). For any set \\(A\\), it is true that \\(A \\subseteq A\\) and \\(\\varnothing \\subseteq A\\).\nWe use \\(\\mathbb{R}\\) to denote the real numbers. Intervals are subsets of \\(\\mathbb{R}\\) that take one of the following forms: \\[\\begin{align*}\n  [a, b] &= \\{x \\in \\mathbb{R}: a \\leq x \\leq b\\}, \\\\\n  [a, b) &= \\{x \\in \\mathbb{R}: a \\leq x &lt; b\\}, \\\\\n  (a, b] &= \\{x \\in \\mathbb{R}: a &lt; x \\leq b\\}, \\\\\n  (a, b) &= \\{x \\in \\mathbb{R}: a &lt; x &lt; b\\}.\n\\end{align*}\\] An endpoint with a square bracket is included in the interval and an endpoint with a round bracket is not. We can have \\(a = -\\infty\\) or \\(b = \\infty\\) as long as we use a round bracket for the corresponding endpoint. For example, it is true that \\(\\mathbb{R} = (-\\infty, \\infty)\\). However, \\(\\mathbb{R} \\neq [-\\infty, \\infty]\\) because \\(\\pm \\infty\\) are not real numbers.",
    "crumbs": [
      "A. One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#sets-experiments-and-events",
    "href": "probability.html#sets-experiments-and-events",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "1.1.1 Experiments and events\nIn probability, an experiment is any process that will produce one outcome out of a set possible outcomes. The set of possible outcomes is called the sample space and is traditionally denoted \\(\\Omega\\). An experiment produces a single outcome \\(\\omega \\in \\Omega\\). For example, the sample space for a single coin flip is \\[\\begin{equation}\n  \\Omega = \\{H, T\\},\n\\end{equation}\\] where \\(\\omega = H\\) if we get heads and \\(\\omega = T\\) if we get tails.\nThe outcomes in the sample space must determine everything about the random outcome of the experiment. If we flip a coin twice, the sample space cannot be \\(\\{H, T\\}\\) because each \\(\\omega \\in \\Omega\\) must specify the outcome of both coin flips. Instead, \\[\\begin{equation}\n  \\Omega = \\{HH, HT, TH, TT\\}\n\\end{equation}\\] where \\(\\omega = XY\\) if we get \\(X\\) on the first flip and \\(Y\\) on the second. This helps us see, for example, that there are two ways to get one \\(H\\) and one \\(T\\) in two coin flips.\nThe purpose of probability is to summarize uncertainty about the outcomes of experiments. However, the outcomes themselves do not have probabilities. Probabilities are assigned to events, which are subsets of the sample space \\(\\Omega\\). If \\(A\\) is an event, then \\(A\\) occurs if and only if the outcome \\(\\omega\\) produced by our experiment is an element of \\(A\\) (i.e., if and only if \\(\\omega \\in A\\)). If we flip a coin twice, the event that we get two heads is \\(\\{HH\\}\\), the event that we get one head is \\(\\{HT, TH\\}\\), and the event that we get zero heads is \\(\\{TT\\}\\). By definition, the event \\(\\Omega\\) always occurs and the event \\(\\varnothing\\) never occurs.\nIn experiments with a finite or countably infinite sample space,2 the distinction between the outcome \\(\\omega\\) and the event \\(\\{\\omega\\}\\) can be safely ignored. In more complex experiments (e.g., taking a random sample from a standard normal distribution), this distinction is important.3 In all cases, experiments have outcomes and events have probabilities.\nIn epidemiology, it is often useful to think of the sample space \\(\\Omega\\) as being a population and each \\(\\omega \\in \\Omega\\) as an individual in this population. In this context, our experiment is to sample a person from \\(\\Omega\\) and ask them questions, take measurements, or follow them over time to ascertain disease occurrence. Events would be subpopulations of \\(\\Omega\\), such as \\(\\{\\omega \\in \\Omega: \\omega \\text{ lives in Ohio}\\}\\). This event occurs if the sampled individual \\(\\omega\\) lives in Ohio, and it does not occur if they live somewhere else.\n\n\n1.1.2 Set operations and logic\nThere are three basic set operations that take one or more given sets and define another set: complement, intersection, and union. Each operation has a simple interpretation in terms of logic.\n\nThe complement of a set \\(A\\) is \\[\\begin{equation}\nA^\\comp = \\{\\omega \\in \\Omega : \\omega \\not\\in A\\},\n\\end{equation}\\] which can be interpreted logically as not \\(A\\). If \\(A\\) is an event, then the event \\(A^\\comp\\) occurs if \\(\\omega \\not\\in A\\). For the same reason that “not not A” means “A”, we have \\((A^\\comp)^\\comp = A\\).\nThe intersection of two sets \\(A\\) and \\(B\\) is \\[\\begin{equation}\n  A \\cap B = \\{\\omega \\in \\Omega : \\omega \\in A \\text{ and } \\omega \\in B\\},\n\\end{equation}\\] which can be interpreted logically as \\(A\\) and \\(B\\). If \\(A\\) and \\(B\\) are events, then the event \\(A \\cap B\\) occurs if \\(\\omega \\in A\\) and \\(\\omega \\in B\\).\nThe union of two sets \\(A\\) and \\(B\\) is \\[\\begin{equation}\n  A \\cup B = \\{\\omega \\in \\Omega : \\omega \\in A \\text{ or } \\omega \\in B\\},\n\\end{equation}\\] which can be interpreted logically as \\(A\\) or \\(B\\) as long as we use “or” in an inclusive sense (i.e., and/or). If \\(A\\) and \\(B\\) are events, then the event \\(A \\cup B\\) occurs if \\(\\omega \\in A\\) or \\(\\omega \\in B\\).\n\nIf \\(A \\subseteq B\\), then \\(A \\cap B = A\\) and \\(A \\cup B = B\\). An important special case is that \\[\n  A \\cap A = A \\cup A = A.\n\\tag{1.1}\\] For the empty set \\(\\varnothing\\), we get \\(A \\cap \\varnothing = \\varnothing\\) and \\(A \\cup \\varnothing = A\\). For the sample space \\(\\Omega\\), we get \\(A \\cap \\Omega = A\\) and \\(A \\cup \\Omega = \\Omega\\).\nUnion and intersection are commutative operations like addition and multiplication, so the order of \\(A\\) and \\(B\\) does not matter: \\[\n  A \\cup B = B \\cup A\n\\] and \\[\n  A \\cap B = B \\cap A.\n\\] Events \\(A\\) and \\(B\\) are disjoint or mutually exclusive when \\(A \\cap B = \\varnothing\\). If \\(A\\) and \\(B\\) are disjoint, then at most of one of them can occur in a single experiment. Any set and its complement are disjoint, and the empty set \\(\\varnothing\\) is disjoint with itself and all other sets.\nIf \\(\\Omega\\) is a population, these set operations allow us to define subpopulations in terms of multiple traits. If the event \\(A = \\{\\omega \\in \\Omega: \\omega \\text{ lives in Ohio}\\}\\), then its complement \\(A^\\comp\\) contains all individuals in \\(\\Omega\\) who live outside Ohio. If the event \\(B = \\{\\omega \\in \\Omega: \\omega \\text{ is 42 years old}\\}\\), then the intersection \\(A \\cap B\\) contains everyone in \\(\\Omega\\) who is 42 years old and lives in Ohio. If \\(\\Omega\\) does not contain any such individual, then \\(A\\) and \\(B\\) are disjoint. The union \\(A \\cup B\\) contains everyone in \\(\\Omega\\) who lives in Ohio or is 42 years old. This could include both a 24-year-old who lives Ohio and a 42-year-old who lives Michigan.\n\n\n1.1.3 Venn diagrams\nA useful tool for understanding events and set operations is the Venn diagram.4 An example is shown below.\n\n\n1.1.4 Sequences of events*\nIntersections can be written for more than two events. The intersection of \\(A_1, A_2, \\ldots, A_n\\) is \\[\n  I_n = \\bigcap_{i = 1}^n A_i.\n\\tag{1.2}\\] Because set intersection is commutative and associative, any ordering of \\(A_1, \\ldots, A_n\\) produces the same intersection. The event \\(I_n\\) occurs if and only if all of the events \\(A_1, \\ldots, A_n\\) occur. Each new event makes the intersection smaller (i.e., never larger) in the sense that \\[\n  \\bigcap_{i = 1}^{n + 1} A_i \\subseteq I_n.\n\\] whenever \\(A_{n + 1}\\) is another event.\nSimilarly, unions can be written for more than two events. If \\(A_1, A_2, \\ldots, A_n\\) is a set of events, then their union is \\[\n  U_n = \\bigcup_{i = 1}^n A_i.\n\\tag{1.3}\\] Because set union is commutative and associative, any ordering of \\(A_1, \\ldots, A_n\\) produces the same union. The event \\(U_n\\) occurs if and only if at least one of the events \\(A_i\\) occurs. Each new event makes the union bigger (i.e., never smaller) in the sense that \\[\n  U_n \\subseteq \\bigcup_{i = 1}^{n + 1} A_i\n\\] whenever \\(A_{n + 1}\\) is another event.\nBoth unions and intersections can be defined for infinite sequences of events.5 To describe this, we let \\(n = \\infty\\) in the notation from Equation 1.2 or Equation 1.3. The union of any finite sequence of events can be turned into the union of an infinite sequence of events by adding an endless sequence of empty sets to the finite sequence. The new sequence is still a sequence of disjoint events, and each empty set \\(\\varnothing\\) leaves the union unchanged. If \\((A_1, A_2, \\ldots)\\) is an infinite sequence of events such that \\(A_i = \\varnothing\\) for all \\(i &gt; n\\), then \\[\n  \\bigcup_{i = 1}^\\infty A_i = \\bigcup_{i = 1}^n A_i.\n\\] This turns out to be useful when we try to give a mathematically rigorous definition of probability.\n\n\n1.1.5 Algebra of sets\\(^*\\)\nUnions, intersections, and complements can be combined in complex ways. Fortunately, there are a few basic principles that can be used to simplify these calculations. We have already seen that unions and intersections are commutative. Unions and intersections are also associative, so \\[\n  A \\cup (B \\cup C)\n  = (A \\cup B) \\cup C\n\\] and \\[\n  A \\cap (B \\cap C)\n  = (A \\cap B) \\cap C\n\\] for any sets \\(A\\), \\(B\\), and \\(C\\).\nDe Morgan’s laws describe how complements affect unions and intersections. If \\(A\\) and \\(B\\) are sets, then \\[\n  (A \\cap B)^\\comp\n  = A^\\comp \\cup B^\\comp\n\\tag{1.4}\\] because you are outside \\(A \\cap B\\) if and only f you are outside \\(A\\) or outside \\(B\\). Similarly, \\[\n  (A \\cup B)^\\comp\n  = A^\\comp \\cap B^\\comp.\n\\tag{1.5}\\] because you are outside \\(A \\cup B\\) if and only if you are outside \\(A\\) and outside \\(B\\). Note that each of these equations implies the other if we replace \\(A = (A^\\comp)^\\comp\\) with \\(A^\\comp\\) and replace \\(B = (B^\\comp)^\\comp\\) with \\(B^\\comp\\). They are two sides of the same coin, but it is helpful to remember them both.\nThe distributive properties describe how unions and intersections interact with each other. Recall that multiplication distributes over addition, so \\(a (b + c) = ab + ac\\). For any sets \\(A\\), \\(B\\), and \\(C\\), we have the following distributive properties:\n\nIntersections distribute over unions, so \\[\n  A \\cap (B \\cup C)\n  = (A \\cap B) \\cup (A \\cap C).\n\\]\nUnions distribute over intersections, so \\[\n  A \\cup (B \\cap C)\n  = (A \\cup B) \\cap (A \\cup C).\n\\]\n\nIntersections and unions also distribute over themselves. However, this is a consequence of commutativity, associativity, and Equation 1.1, not a separate property like the distributive rules above.",
    "crumbs": [
      "A. One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#probability",
    "href": "probability.html#probability",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.2 Probability",
    "text": "1.2 Probability\nA probability measure is a function that takes an event \\(A \\subseteq \\Omega\\) and returns a number \\(\\Pr(A) \\in [0, 1]\\) in any way that conforms to the following rules:\n\n\\(\\Pr(\\Omega) = 1\\).\n\\(\\Pr(A) \\in [0, 1]\\) for any event \\(A \\subseteq \\Omega\\).6\nThe addition rule: If \\((A_1, A_2, \\ldots)\\) is any sequence of disjoint events, then \\[\n  \\Pr\\Biggl( \\bigcup_{i = 1}^\\infty A_i\\Biggr)\n  = \\sum_{i = 1}^\\infty \\Pr(A_i).\n\\] The addition rule is stated in terms of an infinite sequence of disjoint events because this implies the addition rule for any finite sequence of disjoint events (see Section 1.1.4).\n\nIt is useful to think of probability as a generalization of our intuitions about area or volume. When there is no overlap in a set of two-dimensional shapes, we can get the total area they cover by adding up the areas of the individual shapes. Similarly, we can get the total volume taken up by a set of bowling balls by adding up their individual volumes.\nThere is a lot of debate about the meaning of probability, but its definition does not assume any particular interpretation. Probability calculations are based on the rules above no matter what we think it all means, and any interpretation that is consistent with these rules is valid.\n\n1.2.1 Probability calculations\nSeveral useful properties of probability follow immediately from the definition above. A short proof follows each result. To follow the proofs, it helps to draw Venn diagrams.\n\nTheorem 1.1 If \\(A\\) is an event, \\(\\Pr\\bigl(A^\\comp\\bigr) = 1 - \\Pr(A)\\).\n\n\nProof. Because \\(\\Omega = A \\cup A^\\comp\\) and \\(A\\) and \\(A^\\comp\\) are disjoint, we have \\[\n    \\Pr(A) + \\Pr\\bigl(A^\\comp\\bigr) = \\Pr(\\Omega) = 1\n  \\] by the addition rule. The result follows when we subtract \\(\\Pr(A)\\) from both sides.\n\n\nTheorem 1.2 If \\(A\\) and \\(B\\) are events such that \\(A \\subseteq B\\), then \\(\\Pr(A) = \\Pr(B) - \\Pr\\bigl(B \\cap A^\\comp\\bigr)\\). This implies that \\(\\Pr(A) \\leq \\Pr(B)\\).\n\n\nProof. Each element of \\(B\\) either is or is not in \\(A\\), so \\[\n    B = (B \\cap A) \\cup \\big(B \\cap A^\\comp\\big)\n    = A \\cup \\big(B \\cap A^\\comp\\big).\n  \\] where the second equality follows from the fact that \\(B \\cap A = A\\) because \\(A \\subseteq B\\). The two sets on the right-hand side are disjoint, so we have \\[\n    \\Pr(B) = \\Pr(A) + \\Pr\\bigl(B \\cap A^\\comp\\bigr)\n  \\] by the addition rule. The result follows if we subtract \\(\\Pr\\bigl(B \\cap A^\\comp\\bigr)\\) from both sides. This implies that \\(\\Pr(A) \\leq \\Pr(B)\\) because \\(\\Pr\\bigl(B \\cap A^\\comp\\bigr) \\geq 0\\).\n\n\nTheorem 1.3 If \\(A\\) and \\(B\\) are events, \\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\).\n\n\nProof. We can break \\(A \\cup B\\) into three disjoint sets: elements of \\(A\\) and not \\(B\\), elements of \\(B\\) and not \\(A\\), and elements of both \\(A\\) and \\(B\\). In set notation, this is \\[\n    A \\cup B = \\big(A \\cap B^\\comp\\big) \\cup \\big(B \\cap A^\\comp\\big) \\cup (A \\cap B).\n  \\] By the addition rule, \\[\n    \\Pr(A \\cup B) = \\Pr\\bigl(A \\cap B^\\comp\\bigr) + \\Pr\\bigl(B \\cap A^\\comp\\bigr) + \\Pr(A \\cap B).\n   \\tag{1.6}\\] By Theorem 1.2, we have \\[\n      \\Pr\\bigl(A \\cap B^\\comp\\bigr)\n      = \\Pr(A) - \\Pr(A \\cap B), \\\\\n  \\] because \\(A \\cap B \\subseteq A\\) and \\[\n      \\Pr\\bigl(B \\cap A^\\comp\\bigr)\n      = \\Pr(B) - \\Pr(A \\cap B).\n  \\] because \\(A \\cap B \\subseteq B\\). The result follows from substituting these back into Equation 1.6 and collecting terms involving \\(\\Pr(A \\cap B)\\). Intuitively, \\(\\Pr(A) + \\Pr(B)\\) includes the overlap \\(\\Pr(A \\cap B)\\) twice, so we have to subtract out one of them.",
    "crumbs": [
      "A. One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#random-variables",
    "href": "probability.html#random-variables",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.3 Random variables",
    "text": "1.3 Random variables\nThe outcomes of an experiment are not necessarily numbers. A random variable is a real-valued function defined on a sample space \\(\\Omega\\). In other words, a random variable \\(X\\) is a function that takes an argument \\(\\omega \\in \\Omega\\) as input and returns a value \\(X(\\omega) \\in \\mathbb{R}\\). Traditionally, random variables are written as capital letters and possible values are written as lower-case letters, so \\(\\Pr(X = x)\\) denotes the probability of the event \\[\n  \\{\\omega \\in \\Omega : X(\\omega) = x\\}.\n\\] For simplicity, random variables are usually written without the argument \\(\\omega\\).\nThe distinction between outcomes and random variables is useful because we can define multiple random variables on the same sample space. For example, the height, weight, and age of an individual \\(\\omega\\) sampled from a population \\(\\Omega\\) are different random variables defined on the same sample space.\n\n1.3.1 Indicator variables\nThe simplest random variables are indicator variables. For an event \\(A\\), the indicator variable \\[\n  \\indicator_A(\\omega)\n  = \\begin{cases}\n    1 & \\text{ if } \\omega \\in A, \\\\\n    0 & \\text{ if } \\omega \\not\\in A.\n  \\end{cases}\n\\] When sampling from a population, we could define indicator variables for membership in different subpopulations. Indicator variables are binary random variables, which take exactly two values. In practice, these values should be zero and one unless there is a specific reason to do otherwise.\nAll of the basic set operations above can be expressed in terms of indicator variables for sets.\n\nThe indicator function for the complement of \\(A\\) is \\[\n  \\indicator_{A^\\comp} = 1 - \\indicator_A.\n\\tag{1.7}\\]\nIf \\(B\\) is another event and \\(\\indicator_B\\) is its indicator variable, then the indicator variable for the intersection \\(A\\) and \\(B\\) is \\[\n  \\indicator_{A \\cap B}\n  = \\indicator_A \\indicator_B,\n\\tag{1.8}\\]\nThe indicator variable for the union \\(A \\cup B\\) is \\[\n  \\indicator_{A \\cup B}\n  = 1 - (1 - \\indicator_A) (1 - \\indicator_B)\n  = \\indicator_A + \\indicator_B - \\indicator_{A \\cap B}.\n\\tag{1.9}\\] This follows from Equation 1.5 because \\(A \\cup B = (A^\\comp \\cap B^\\comp)^\\comp\\).\n\n\n\n1.3.2 Probability distributions\nThe set of possible values of a random variable \\(X\\) is called the support of \\(X\\) and denoted \\(\\supp(X)\\).7 For example, the support of an indicator variable is \\(\\{0, 1\\}\\). In this section, we will focus on discrete random variables, which have a support on a finite or countably infinite set. There are two standard ways to describe the distribution of a discrete random variable:\n\nThe probability mass function (PMF) of a discrete random variable \\(X\\) is \\[\n  f(x) =\n  \\begin{cases}\n    \\Pr(X = x) &gt; 0  & \\text{ if } x \\in \\supp(X), \\\\\n    0               & \\text{ if } x \\not \\in \\supp(X).\n  \\end{cases}\n\\] Because \\(\\Pr(\\Omega) = 1\\), we always have \\[\n  \\sum_{x \\in \\supp(X)} f(x) = 1.\n\\]\nThe cumulative distribution function (CDF) of \\(X\\) is \\[\n  F(x)\n  = \\Pr(X \\leq x).\n\\] \\(F(x)\\) is monotonically increasing in \\(x\\), which means that \\(F(a) \\leq F(b)\\) whenever \\(a &lt; b\\). It has a jump upward of size \\(f(x)\\) at each \\(x \\in \\supp(X)\\), and its value at each such \\(x\\) is the value that it jumps to—not the value that it jumps up from. For sufficiently small \\(x\\), \\(F(x)\\) can be made arbitrarily close to zero. For sufficiently large \\(x\\), \\(F(x)\\) can be made arbitrarily close to one. More formally, we say that \\(\\lim_{x \\downarrow -\\infty} F(x) = 0\\) and \\(\\lim_{x \\uparrow \\infty} F(x) = 1\\).\n\nThe PMF and CDF provide equivalent descriptions of the distribution of \\(X\\) in the sense that either of these functions can be used to calculate the other. Given the PMF \\(f\\), the CDF is defined by \\[\n  F(x) = \\sum_{\\substack{v \\in \\supp(X): \\\\ v \\leq x}} f(v).\n\\] where the sum is taken over all \\(u \\in \\supp(X)\\) such that \\(u \\leq x\\). Given the CDF \\(F\\), the PMF is defined by \\[\n  f(x) = F(x) - \\max_{v \\leq x} F(v)\n\\] where the maximum is \\(F(v)\\) for the largest \\(v \\in \\supp(X)\\) such that \\(v &lt; x\\).\n\n\n1.3.3 Mean and variance\nThe mean or expected value of a random variable \\(X\\) is \\[\n  \\E(X)\n  = \\sum_{x \\in \\supp(X)} x \\Pr(X = x)\n  = \\sum_{x \\in \\supp(X)} x f(x),\n\\] where \\(f\\) is the PMF of \\(X\\). The mean is often written \\(\\mu\\), and it is often described as a measure of the “location” or “central tendency” of \\(X\\).\nIf \\(X\\) has \\(\\E(X) = \\mu\\), then \\((X - \\mu)^2\\) is another random variable. The variance of \\(X\\) is the expected value of \\((X - \\mu)^2\\): \\[\n  \\Var(X)\n  = \\E\\big[(X - \\mu)^2\\big]\n  = \\sum_{x \\in \\supp(X)} (x - \\mu)^2 f(x).\n\\] Because \\((x - \\mu)^2 \\geq 0\\) with equality if and only if \\(x = \\mu\\), we always have \\(\\Var(X) \\geq 0\\). We have \\(\\Var(X) = 0\\) if and only if \\(X = \\mu\\) with probability one. The variance is often written \\(\\sigma^2\\), and it is often described as a measure of the dispersion of \\(X\\) around the mean.\nThe square root of the variance is called the standard devation, which is often written \\(\\sigma\\). If a random variable \\(X\\) has units (e.g., length, weight, or time), the mean and the standard deviation have the same units as \\(X\\). For example, the mean and standard deviation of a length in meters both have units of meters, but the variance has units of \\(\\text{meters}^2\\).\n\n\n1.3.4 Bernoulli distribution\nThe distribution of an indicator variable is called the Bernoulli distribution.8 A random variable with the Bernoulli(\\(p\\)) distribution has the PMF \\[\n  f(x) =\n  \\begin{cases}\n    1 - p &\\text{if } x = 0\\\\\n    p     &\\text{if } x = 1.\n  \\end{cases}\n\\] If a random variable \\(X\\) has a Bernoulli(\\(p\\)) distribution, we write \\(X \\sim \\text{Bernoulli}(p)\\). The indicator variable for an event \\(A\\) has a Bernoulli distribution with \\(p = \\Pr(A)\\).\nIf \\(X \\sim \\Bernoulli(p)\\), then it has mean \\[\\begin{align}\n  \\E(X)\n  &= 0 \\times (1 - p) + 1 \\times p \\\\\n  &= p\n\\end{align}\\] and variance \\[\\begin{align}\n  \\Var(X)\n  &= (0 - p)^2(1 - p) + (1 - p)^2 p \\\\\n  &= p (1 - p).\n\\end{align}\\] The standard deviation is \\(\\sqrt{p (1 - p)}\\). This standard deviation is greater than zero unless \\(p = 0\\) or \\(p = 1\\). If \\(p = 0\\), then \\(X = 0\\) with probability one. If \\(p = 1\\), then \\(X = 1\\) with probability one.",
    "crumbs": [
      "A. One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#joint-and-marginal-distributions",
    "href": "probability.html#joint-and-marginal-distributions",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.4 Joint and marginal distributions",
    "text": "1.4 Joint and marginal distributions\nIf \\(X\\) and \\(Y\\) are random variables defined on the same probability space, then their joint probability mass function is \\[\n  f(x, y)\n  = \\Pr(X = x \\text{ and } Y = y)\n  = \\Pr\\big(\\{\\omega: X(\\omega) = x \\text{ and } Y(\\omega) = y\\}\\big).\n\\] The marginal probability mass functions are the PMFs of \\(X\\) or \\(Y\\) individually, which can be calculated from the joint PMF. The marginal PMF of \\(X\\) is \\[\n  f_X(x) = \\sum_{y \\in \\supp(Y)} f(x, y),\n\\] and the marginal PMF of \\(Y\\) is \\[\n  f_Y(y) = \\sum_{x \\in \\supp(X)} f(x, y).\n\\] These are called marginal distributions by analogy to the margins of a table.\nJoint distributions can be defined for more than two random variables. If \\(X_1, X_2, \\ldots, X_n\\) are random variables defined on the same sample space, then their joint PMF is \\[\n  f(x_1, x_2, \\ldots, x_n) = \\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n).\n\\] The marginal distribution of each \\(X_i\\) can be found by adding up the PMF over the support of all the other random variables. For example, \\[\n  f_{X_2}(x_2) = \\sum_{x_1 \\in \\supp(X_1)} \\sum_{x_3 \\in \\supp(X_3)} f(x_1, x_2, x_3).\n\\] when \\(n = 3\\). For larger \\(n\\), the formula gets uglier but the idea is the same.\nThe distinction between joint and marginal distributions is extremely important in epidemiology (and other applications of probability). The joint distribution can be used to find any of the the marginal distributions, but the marginal distributions cannot be used to find the joint distribution—unless we make additional assumptions.\n\n1.4.1 Linear combinations*\nIf \\(a\\) and \\(b\\) are constants, then \\(a X + b Y\\) is another random variable on \\(\\Omega\\). It is called a linear combination of \\(X\\) and \\(Y\\). Linear combinations can be defined for more than two random variables. If \\(X_1, \\ldots, X_n\\) are random variables defined on a sample space and \\(a_1, \\ldots, a_n\\) are constants, then \\[\n  \\sum_{i = 1}^n a_i X_i = a_1 X_1 + a_2 X_2 + \\cdots + a_n X_n\n\\] is a linear combination of \\(X_1, \\ldots, X_n\\). The constants can be any real numbers, including one and zero.\nSection 1.3.1 contains both examples and non-examples of linear combinations of random variables.\n\nThe indicator function for \\(A^\\comp\\) in Equation 1.7 is a linear combination of \\(\\indicator_A\\) and the random variable \\(\\indicator_\\Omega\\), which equals one for all \\(\\omega \\in \\Omega\\).\nThe indicator function for \\(A \\cup B\\) in Equation 1.9 is linear combination of the indicator variables \\(\\indicator_A\\), \\(\\indicator_B\\), and \\(\\indicator_{A \\cap B}\\).\nThe indicator function for \\(A \\cap B\\) in Equation 1.8 is not a linear combination of \\(\\indicator_A\\) and \\(\\indicator_B\\) because we have to multiply these two variables.\n\n\n\n1.4.2 Mean and covariance*\nIf \\(X\\) and \\(Y\\) are random variables defined on the same sample space and \\(a\\) and \\(b\\) are constants, the mean of the linear combination \\(a X + b Y\\) is \\[\n  \\E(a X + b Y) = a \\E(X) + b \\E(Y).\n\\] This is a direct consequence of the definition of expected value: \\[\n  \\begin{aligned}\n    \\E(a X + b Y)\n    &= \\sum_{x \\in \\supp(X)} \\sum_{y \\in \\supp(Y)} (a x + b y) f(x, y) \\\\\n    &= a \\sum_{x \\in \\supp(X)} \\bigg(x \\sum_{y \\in \\supp(Y)} f(x, y)\\bigg)\n      + b \\sum_{y \\in \\supp(Y)} \\bigg(y \\sum_{x \\in \\supp(X)} f(x, y)\\bigg) \\\\\n    &= a \\sum_{x \\in \\supp(X)} x f_X(x) + b \\sum_{y \\in \\supp(Y)} y f_Y(y).\n  \\end{aligned}\n\\] The algebra is not pretty, but the logic is straightforward. We split up the sum into parts depending only on \\(x\\) and only on \\(y\\) outside the joint PMF. In each part, we factor out a constant and find the marginal PMF. This same logic extends to a linear combination of any number of random variables.\nThe variance of \\(a X + b Y\\) is \\[\n  \\Var(a X + b Y) = a^2 \\Var(X) + b^2 \\Var(Y) + 2 a b \\Cov(X, Y)\n\\] where \\[\n  \\Cov(X, Y) = \\E\\bigl[\\big(X - \\E(X)\\big) \\big(Y - \\E(Y)\\big)\\bigr]\n\\] is called the covariance of \\(X\\) and \\(Y\\). Note that \\(\\Cov(X, Y) = \\Cov(Y, X)\\). Because \\(\\Var(X) = \\Cov(X, X)\\), variance is a special case of covariance.\nThe joint distribution of \\(X\\) and \\(Y\\) has a covariance matrix which is \\[\n  \\begin{bmatrix}\n    \\Var(X)     & \\Cov(X, Y) \\\\\n    \\Cov(X, Y)  & \\Var(Y)\n  \\end{bmatrix}\n\\] The variances are along the diagonal of the matrix, and the covariances appear off the diagonal. Because \\(\\Cov(X, Y) = \\Cov(Y, X)\\), covariance matrices are always symmetric (i.e., symmetric across the diagonal). Covariance matrices are an extremely useful tool for calculating the variances of linear combinations of random variables. For example: \\[\n  \\Var(a X + b Y)\n  = \\begin{pmatrix}\n      a & b\n    \\end{pmatrix}\n    \\begin{bmatrix}\n      \\Var(X)     & \\Cov(X, Y) \\\\\n      \\Cov(X, Y)  & \\Var(Y)\n    \\end{bmatrix}\n    \\begin{pmatrix}\n      a \\\\ b\n    \\end{pmatrix}\n\\] in matrix and vector notation from linear algebra. This logic extends to linear combinations of any number of random variables.\nThe covariance is the numerator of the Pearson correlation coefficient,9 which is \\[\n  \\rho_{XY} = \\rho_{YX}\n  = \\frac{\\Cov(X, Y)}{\\sqrt{\\Var(X) \\Var(Y)}}.\n\\] Because of the Cauchy-Schwarz inequality, it turns out that \\(\\rho_{XY} \\in [-1, 1]\\).\n\nWe get \\(\\rho_{XY} = -1\\) if and only if \\(Y = c X\\) for some negative constant \\(c\\).\nWe get \\(\\rho_{XY} = 1\\) if and only if \\(Y = c X\\) for some positive constant \\(c\\). For example, \\(\\rho_{XX} = 1\\) for any random variable \\(X\\).\nWe get \\(\\rho_{XY} = 0\\) if \\(X\\) and \\(Y\\) are independent in the sense that the value of one tells us nothing about the value of the other.10 However, it is possible to have \\(\\rho_{XY} = 0\\) when \\(X\\) and \\(Y\\) are not independent.\n\nIf we divide each entry \\(\\Cov(X, Y)\\) in a covariance matrix by \\(\\sqrt{\\Var(X) \\Var(Y)}\\), when we get a correlation matrix. Any correlation matrix is symmetric, and the entries along its diagonals are all ones.",
    "crumbs": [
      "A. One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#probability-and-disease-occurrence",
    "href": "probability.html#probability-and-disease-occurrence",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.5 Probability and disease occurrence",
    "text": "1.5 Probability and disease occurrence\nIn epidemiology, there are two fundamental measures of disease occurrence that are probabilities: prevalence and risk. In both cases, our experiment is to sample an individual \\(\\omega\\) from a population \\(\\Omega\\). The disease outcome is a binary random variable \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has the disease outcome}, \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] The set of individuals in \\(\\Omega\\) who have \\(D(\\omega) = 1\\) is an event in \\(\\Omega\\), and the prevalence or risk is its probability \\[\n  \\Pr(\\{\\omega \\in \\Omega: D(\\omega) = 1\\}).\n\\] The most important difference between prevalence and risk is the role of time in the definition of \\(D\\).\nThere is an important technicality to remember when we talk about disease onset and recovery. When a person has disease onset at time \\(\\tonset\\) and recovers at time \\(\\trec\\), they have disease for each \\(t \\in [\\tonset, \\trec)\\). We assume that \\(\\trec &gt; \\tonset\\) so this interval is nonempty. We let the onset and recovery times for person \\(i\\) be \\(\\tonset_i\\) and \\(\\trec_i\\), respectively. If a person has multiple episodes of the disease, each episode has its own \\(\\tonset\\) and \\(\\trec\\). For example, the \\(j^\\text{th}\\) episode in person \\(i\\) would have onset time \\(\\tonset_{ij}\\) and recovery time \\(\\trec_{ij}\\).\n\n1.5.1 Prevalence\nFor prevalence, the disease outcome is defined by choosing a time \\(t\\) and letting \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has disease at time } t, \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In other words, it is the proportion of the population \\(\\Omega\\) that disease at time \\(t\\). This includes individuals who have disease onset at time \\(\\tonset = t\\) but not individuals who recover from disease at time \\(\\trec = t\\). This is often called the point prevalence at time \\(t\\).\nAnother version of prevalence is period prevalence. For period prevalence, we choose a nonempty time interval \\((t_a, t_b]\\) and define \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has disease at any time } t \\in (t_a, t_b], \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In other words, it is the proportion of the population that has disease at any time in the interval \\((t_a, t_b]\\). This can include people who had disease onset at a time \\(\\tonset \\leq t_a\\) as long as \\(\\trec &gt; t_a\\). In particular, it includes all individuals who have disease onset at time \\(t_a\\), who must have \\(\\trec &gt; t_a\\).\n\n\n1.5.2 Risk (cumulative incidence)\nTo define risk or cumulative incidence, we first choose an nonempty time interval \\((t_a, t_b]\\) and let the population \\(\\Omega\\) consist only of individuals who do not have disease at time \\(t_a\\) and are at risk of disease onset after that. The disease outcome is defined as \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has } \\tonset \\in (t_a, t_b], \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In the population that is disease-free and at risk of disease at time \\(t_a\\), it is the proportion who have disease onset at \\(\\tonset \\leq t_b\\).\nPoint and interval prevalence are affected by both the frequency of disease onset and the duration of disease, the risk in any time interval depends only on the frequency of disease onset (see Figure~\\(\\ref{fig:prevdur}\\)). This makes risk a more useful measure for identifying the causes of disease, which is the primary goal of analytic epidemiology.\nAnother advantage of risk is that it can be used for outcomes that begin and end very quickly (e.g., traffic accidents or being hit by lightning) or outcomes that remove individuals from the population (e.g., emigration or death). Point prevalence would not be a useful measure of the impact of these outcomes on public health, and period prevalence would be nearly identical to the risk over the same period.\n\n\n\n\nLaplace, Pierre Simon. 1820. Théorie Analytique Des Probabilités. Vol. 7. Courcier.\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological Perspective.” In A History of Epidemiologic Methods and Concepts, edited by Alfredo Morabia, 3–125. Springer.",
    "crumbs": [
      "A. One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#footnotes",
    "href": "probability.html#footnotes",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "Pierre-Simone, marquis de Laplace (1749-1827) is often called the Newton of France. He proved that the solar system is stable, developed theories of ocean tides and gravitational potential, proved one of the first general versions of the central limit theorem, and pioneered the Bayesian interpretation of probability. For just six weeks in 1799, he was Minister of the Interior for France under Napoleon. His is one of the 72 names on the Eiffel Tower. ↩︎\n The natural numbers \\(\\mathbb{N} = \\{0, 1, 2, \\ldots\\}\\) are countably infinite, as are the integers \\(\\mathbb{Z}\\) and the rational numbers \\(\\mathbb{Q}\\). The real numbers \\(\\mathbb{R}\\) are uncountably infinite, as are the real numbers in any interval \\((a, b)\\) with \\(a &lt; b\\) and the irrational numbers. Uncountably infinite sets are infinitely larger than countably infinite sets. This distinction was discovered in the 1870s by the German mathematician Georg Cantor (1845–1918). It was considered shocking at the time, but it has become a cornerstone of modern mathematics.↩︎\n In experiments with uncountably infinite sample spaces, the probability of an event \\(A\\) cannot always be calculated by adding up the probabilities of \\(\\{\\omega\\}\\) for all \\(\\omega \\in A\\). For example: If we choose a number at uniformly at random in \\([0, 1]\\), the probability of getting any particular number \\(\\omega\\) is zero. The sum of the probabilities of all \\(\\{\\omega\\} \\subseteq A\\) is zero (if \\(A\\) is countable) or undefined (if \\(A\\) is uncountable). By maintaining a distinction between outcomes and events and by limiting probability calculations to countable (i.e., finite or countably infinite) sums, we end up with something coherent and useful.↩︎\n Named after John Venn (1834-1923), an English logician and philosopher who was one of the pioneers of the frequentist interpretation of probability. He was ordained as an Anglican priest in 1859 but resigned from the church in 1883. He was a prize-winning gardener of roses and white carrots and a prominent supporter of women’s right to vote. From 1903 until his death, he was President of Fellows in Gonville and Caius College at the University of Cambridge, where he is commemorated with a Venn diagram in a stained glass window.↩︎\n In probability, we only consider unions and intersections of finite or countably infinite sets of events. Although unions and intersections can be defined for uncountably infinite sets of events, it can be impossible to assign probabilities to the resulting sets (see the Banach-Tarski paradox). As an epidemiologist, this should not keep you up at night.↩︎\n Technically, we assign probabilities only to events in a set \\(\\mathcal{F}\\) of subsets of \\(\\Omega\\) that is required to contain \\(\\Omega\\) and to be closed under complements and countable unions. “Closed under complements” means that \\(A^\\comp \\in \\mathcal{F}\\) whenever \\(A \\in \\mathcal{F}\\). For example, \\(\\varnothing = \\Omega^\\comp\\) must be in \\(\\mathcal{F}\\) because \\(\\Omega \\in \\mathcal{F}\\). “Closed under countable unions” means that \\(\\bigcup_{i = 1}^\\infty A_i \\in \\mathcal{F}\\) whenever \\((A_1, A_2, \\ldots)\\) is a sequence of events in \\(\\mathcal{F}\\). The set \\(\\mathcal{F}\\) is called a \\(\\sigma\\)-algebra, and this restriction on the domain of probability helps avoid internal contradictions like the mind-blowing Banach-Tarski paradox.↩︎\n Technically, the support of \\(X\\) is the smallest closed set \\(S_X\\) such that \\(\\Pr(X \\in S_X) = 1\\). For a discrete random variable with support on a finite set, it is just the set of possible values. For a discrete random variable with support on a countably infinite set, it can include points whose probability mass is zero—a pathological case that we can safely ignore. For a continuous random variable, it can include values whose probability density is zero—a case that is not unusual or pathological.↩︎\n Named after Jacob Bernoulli (1655-1705), a Swiss mathematician who derived the first version of the law of large numbers and discovered the constant \\(e \\approx 2.718281828\\), which is the base for natural logarithms. He and his younger brother Johann Bernoulli (1667-1748) were some of the first mathematicians to try to understand and apply calculus, but their relationship eventually curdled into a jealous rivalry. A lunar impact crater called Bernoulli is named jointly after them.↩︎\n Named after Karl Pearson (1857-1936), an English mathematician who founded the modern discipline of mathematical statistics. In 1911, he started the world’s first university department of statistics at University College London. He was an outspoken socialist and supporter of women’s rights, but he was also a vocal proponent of social Darwinism and eugenics who opposed Jewish immigration into Britain.↩︎\n We will define independence of random variables more rigorously when we discuss conditional probabilities in Chapter 2.↩︎",
    "crumbs": [
      "A. One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "condprob.html#footnotes",
    "href": "condprob.html#footnotes",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "",
    "text": "Thomas Bayes (1701-1761) was an English Presbyterian minister from a family of Nonconformists (i.e., Protestants who did not observe the rules of the Church of England). He studied logic and theology at the University of Edinburgh and served as a minister in Tunbridge Wells near Kent, England. He was elected a Fellow of the Royal Society in 1742 for his defense of Newton’s calculus against a 1734 book called The Analyst: A Discourse Addressed to an Infidel Mathematician by Bishop George Berkeley (1685-1753). Late in life, Bayes became interested in probability and “inverse probability” (statistics). This essay was published posthumously. See https://en.wikipedia.org/wiki/Thomas_Bayes.↩︎",
    "crumbs": [
      "A. One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#footnotes",
    "href": "mlestimation.html#footnotes",
    "title": "3  Maximum Likelihood Estimation",
    "section": "",
    "text": "John Tukey (1915-2000) was an American mathematician and statistician who worked at Bell Labs and Princeton University. He developed the box plot, Tukey’s range test for multiple comparisons, and the fast Fourier transform. In 1947, he coined the term “bit” as shorthand for “binary digit”. See https://en.wikipedia.org/wiki/John_Tukey.↩︎",
    "crumbs": [
      "A. One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html#footnotes",
    "href": "bayes.html#footnotes",
    "title": "4  Bayesian Estimation",
    "section": "",
    "text": "Joseph Berkson (1899–1982) was an American physician and statistician at the Mayo Clinic in Rochester, Minnesota. He helped develop and popularize the use of logistic regression for binary outcomes, coining the term “logit” for the log odds in 1944. He also pioneered the study of selection bias, a special case of which is called “Berkson’s bias”. Later, he became a prominent opponent of the idea that smoking causes lung cancer. See https://en.wikipedia.org/wiki/Joseph_Berkson.↩︎",
    "crumbs": [
      "A. One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bayes, Thomas. 1763. “LII. An Essay\nTowards Solving a Problem in the Doctrine of Chances. By the Late\nRev. Mr. Bayes, FRS\nCommunicated by Mr. Price, in a Letter to\nJohn Canton, AMFRS.”\nPhilosophical Transactions of the Royal Society of London 53:\n370–418.\n\n\nBerkson, Joseph. 1942. “Tests of Significance Considered as\nEvidence.” Journal of the American Statistical\nAssociation 37 (219): 325–35.\n\n\nLaplace, Pierre Simon. 1820. Théorie Analytique Des\nProbabilités. Vol. 7. Courcier.\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological\nPerspective.” In A History of Epidemiologic Methods and\nConcepts, edited by Alfredo Morabia, 3–125. Springer.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera.\nSecond edition. John Churchill. https://wellcomecollection.org/works/uqa27qrt.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The\nAnnals of Mathematical Statistics 33 (1): 1–67.",
    "crumbs": [
      "References"
    ]
  }
]