[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analytical Epidemiology",
    "section": "",
    "text": "Preface\nOne day at lunch at the Harvard School of Public Health, I overheard Professor Murray Mittleman say: “I love epidemiology. It all fits together like a diamond.” As a second-year doctoral student in epidemiology, I was surprised to hear the subject described with such unstrained enthusiasm. It has taken years of study and experience for me to understand what he meant. On the way, I too have fallen in love.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "Analytical Epidemiology",
    "section": "Who this book is for",
    "text": "Who this book is for\nThis book is intended primarily for two audiences:\n\nEpidemiologists are often protected from the mathematical foundations of their field. The long-term price of this is “dogmatism, that is, a tendency to rigidly protect a partially understood theoretical heritage” (Morabia 2004). The mathematics needed for a deeper understanding of epidemiologic methods is within reach of anyone who has come far enough to need it. Whether you master this material or just learn to approach it with more patience than fear, you will be doing a service to epidemiology and to public health.\nBiostatisticians are familiar with probability and statistical inference, but applying statistics to solve scientific problems in public health requires skills different from those needed to prove that a method works under given assumptions. Epidemiology is a living example of the interplay between theory and applications in statistics, and epidemiologists have shown integrity, courage, and ingenuity in confronting causal questions with statistical tools.\n\nBeyond these audiences, I hope to explain the logic of epidemiology to any interested reader. It is possible that epidemiologic research has already helped save your life.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Analytical Epidemiology",
    "section": "How to use this book",
    "text": "How to use this book\nDifficult chapters, sections, subsections, and exercises are marked with an asterisk (*). These can be skipped without harming the logical flow of the book, but none of them is beyond the reach of a determined reader. The starring is recursive: Starred sections can be skipped within a starred chapter, starred subsections can be skipped within a starred section, and so on. Footnotes offer context or hint at more advanced material. All of them can be ignored if they do not seem useful or interesting.\nThis is a work in progress. You may find that some parts are unfinished or just bad. Please report errors (including typos) or submit suggestions (especially good examples) at:\nhttps://github.com/ekenah/analyticalepi/issues.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Analytical Epidemiology",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis book is written in LaTeX and Quarto with calculations and figures generated in R, Python, and Inkscape. I have also included many links to Wikipedia. These are free, open-source, and publicly available thanks to the work of many contributors.\nTony Barry, Devesh Kapur, Paul Farmer, and James H. Maguire guided me to a career in public health when I was an undergraduate. James Robins, Miguel A. Hernán, Marc Lipsitch, and Stephen P. Luby helped me become an epidemiologist, biostatistician, and epidemic modeler in graduate school. My career began under the mentorship of Ira M. Longini, Jr., and M. Elizabeth Halloran as a postdoctoral fellow at the Univerity of Washington and an assistant professor at the University of Florida. My colleagues Yang Yang, Grzegorz Rempała, Forrest Crawford, and Patrick Schnell have all provided useful comments. For their patience with early versions of this material, I am grateful to the students of STA 6177/PHC 6937 (Applied Survival Analysis) at the University of Florida from 2013 to 2016 and PUBHEPI 8430 (Epidemiology 4) at The Ohio State University from 2019 to the present.\nMy parents, Chris and Kate Kenah, courageously allowed me to travel to places they had never been to and do things I had been told to avoid. These experiences in the United States, India, South Africa, and especially Bangladesh opened my eyes to the terrible importance of clear thinking in public health. My wife, Asma Aktar, and our sons Rafi, Rayhan, and Rabi remind me every day how important it is to destroy everything that stifles humanity. To that end, I hope this book is useful.\nAny mistakes are my own, and God knows best (الله أعلم).\n\n\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological Perspective.” In A History of Epidemiologic Methods and Concepts, edited by Alfredo Morabia, 3–125. Springer.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera. Second edition. John Churchill. https://wellcomecollection.org/works/uqa27qrt.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "1.1 Sets, experiments, and events\nTo begin at the beginning, we will start with probability. Morabia (2004) accurately observed that “Epidemiology came late in human history because it had to wait for the emergence of probability.” This is probably the most difficult chapter of the book, but it will make all subsequent chapters easier. You can use it as a reference and come back to the difficult parts when you need them. Learning to think clearly about probability will give you a compass to find your way through difficult terrain in epidemiology.\nTo speak clearly about probabilities, we need some basic notation for sets. If \\(A\\) is a set that contains an element \\(a\\), we write \\[a \\in A.\\] If \\(A\\) and \\(B\\) are sets such that every element of \\(A\\) is also an element of \\(B\\), we write \\[A \\subseteq B.\\] to indicate that \\(A\\) is a subset of \\(B\\). Sets \\(A\\) and \\(B\\) are equal if and only if \\(A \\subseteq B\\) and \\(B \\subseteq A\\), which means they contain exactly the same elements. The empty set with no elements is denoted \\(\\varnothing\\). For any set \\(A\\), it is true that \\(A \\subseteq A\\) and \\(\\varnothing \\subseteq A\\).\nWe use \\(\\mathbb{R}\\) to denote the real numbers. Intervals are subsets of \\(\\mathbb{R}\\) that take one of the following forms: \\[\\begin{aligned}\n  (a, b) &= \\{x \\in \\mathbb{R}: a &lt; x &lt; b\\}, \\\\\n  (a, b] &= \\{x \\in \\mathbb{R}: a &lt; x \\leq b\\}, \\\\\n  [a, b) &= \\{x \\in \\mathbb{R}: a \\leq x &lt; b\\}, \\\\\n  [a, b] &= \\{x \\in \\mathbb{R}: a \\leq x \\leq b\\}. \\\\\n\\end{aligned}\\] An endpoint with a square bracket is included in the interval; an endpoint with a round bracket is not. We can have \\(a = -\\infty\\) or \\(b = \\infty\\) as long as we use a round bracket for the corresponding endpoint. For example, it is true that \\(\\mathbb{R} = (-\\infty, \\infty)\\). However, \\(\\mathbb{R} \\neq [-\\infty, \\infty]\\) because \\(\\pm \\infty\\) are not real numbers.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#sets-experiments-and-events",
    "href": "probability.html#sets-experiments-and-events",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "1.1.1 Experiments and events\nIn probability, an experiment is any process that will produce one outcome out of a set possible outcomes. The set of possible outcomes is called the sample space and is traditionally denoted \\(\\Omega\\). An experiment produces a single outcome \\(\\omega \\in \\Omega\\). For example, the sample space for a single coin flip is \\[\\Omega = \\{H, T\\},\\] where \\(\\omega = H\\) if we get heads and \\(\\omega = T\\) if we get tails.\nThe outcomes in the sample space must determine everything about the random outcome of the experiment. If we flip a coin twice, the sample space cannot be \\(\\{H, T\\}\\) because each \\(\\omega \\in \\Omega\\) must specify the outcome of both coin flips. Instead, \\[\\Omega = \\{HH, HT, TH, TT\\}\\] where \\(\\omega = XY\\) if we get \\(X\\) on the first flip and \\(Y\\) on the second. This helps us see, for example, that there are two ways to get one \\(H\\) and one \\(T\\) in two coin flips.\nThe purpose of probability is to summarize uncertainty about the outcomes of experiments. However, the outcomes themselves do not have probabilities. Probabilities are assigned to events, which are subsets of the sample space \\(\\Omega\\). If \\(A\\) is an event, then \\(A\\) occurs if and only if the outcome \\(\\omega\\) produced by our experiment is an element of \\(A\\) (i.e., if and only if \\(\\omega \\in A\\)). If we flip a coin twice, the event that we get two heads is \\(\\{HH\\}\\), the event that we get one head is \\(\\{HT, TH\\}\\), and the event that we get zero heads is \\(\\{TT\\}\\). By definition, the event \\(\\Omega\\) always occurs and the event \\(\\varnothing\\) never occurs.\nIn experiments with a finite or countably infinite sample space,2 the distinction between the outcome \\(\\omega\\) and the event \\(\\{\\omega\\}\\) can be safely ignored. In more complex experiments (e.g., taking a random sample from a standard normal distribution), this distinction is important.3 In all cases, experiments have outcomes and events have probabilities.\nIn epidemiology, it is often useful to think of the sample space \\(\\Omega\\) as being a population and each \\(\\omega \\in \\Omega\\) as an individual in this population. In this context, our experiment is to sample a person from \\(\\Omega\\) and ask them questions, take measurements, or follow them over time to ascertain disease occurrence. Events would be subpopulations of \\(\\Omega\\), such as \\(\\{\\omega \\in \\Omega: \\omega \\text{ lives in Ohio}\\}\\). This event occurs if the sampled individual \\(\\omega\\) lives in Ohio, and it does not occur if they live somewhere else.\n\n\n1.1.2 Set operations and logic\nThere are three basic set operations that take one or more sets and define another set: complement, intersection, and union. Each operation has a simple interpretation in terms of logic.\n\nThe complement of a set \\(A\\) is \\[A^\\comp = \\{\\omega \\in \\Omega : \\omega \\not\\in A\\},\\] which can be interpreted logically as not \\(A\\). If \\(A\\) is an event, then the event \\(A^\\comp\\) occurs if \\(\\omega \\not\\in A\\). For the same reason that “not not A” means “A”, we have \\((A^\\comp)^\\comp = A\\).\nThe intersection of two sets \\(A\\) and \\(B\\) is \\[A \\cap B = \\{\\omega \\in \\Omega : \\omega \\in A \\text{ and } \\omega \\in B\\},\\] which can be interpreted logically as \\(A\\) and \\(B\\). If \\(A\\) and \\(B\\) are events, then the event \\(A \\cap B\\) occurs if \\(\\omega \\in A\\) and \\(\\omega \\in B\\).\nThe union of two sets \\(A\\) and \\(B\\) is \\[A \\cup B = \\{\\omega \\in \\Omega : \\omega \\in A \\text{ or } \\omega \\in B\\},\\] which can be interpreted logically as \\(A\\) or \\(B\\) as long as we use an inclusive “or” (i.e., and/or). If \\(A\\) and \\(B\\) are events, then the event \\(A \\cup B\\) occurs if \\(\\omega \\in A\\) or \\(\\omega \\in B\\).\n\nIf \\(A \\subseteq B\\), then \\(A \\cap B = A\\) and \\(A \\cup B = B\\). An important special case is that \\[\n  A \\cap A = A \\cup A = A.\n\\tag{1.1}\\] For the empty set \\(\\varnothing\\), we get \\(A \\cap \\varnothing = \\varnothing\\) and \\(A \\cup \\varnothing = A\\). For the sample space \\(\\Omega\\), we get \\(A \\cap \\Omega = A\\) and \\(A \\cup \\Omega = \\Omega\\).\nUnion and intersection are commutative operations like addition and multiplication, so the order of \\(A\\) and \\(B\\) does not matter: \\[\n  A \\cup B = B \\cup A\n\\] and \\[\n  A \\cap B = B \\cap A.\n\\] Events \\(A\\) and \\(B\\) are disjoint or mutually exclusive when \\(A \\cap B = \\varnothing\\). If \\(A\\) and \\(B\\) are disjoint, then at most of one of them can occur in a single experiment. Any set and its complement are disjoint, and the empty set \\(\\varnothing\\) is disjoint with itself and all other sets.\nIf \\(\\Omega\\) is a population, these set operations allow us to define subpopulations in terms of multiple traits. If the event \\(A = \\{\\omega \\in \\Omega: \\omega \\text{ lives in Ohio}\\}\\), then its complement \\(A^\\comp\\) contains all individuals in \\(\\Omega\\) who live outside Ohio. If the event \\(B = \\{\\omega \\in \\Omega: \\omega \\text{ is 42 years old}\\}\\), then the intersection \\(A \\cap B\\) contains everyone in \\(\\Omega\\) who is 42 years old and lives in Ohio. If \\(\\Omega\\) does not contain any 42-year-old Ohio residents, then \\(A\\) and \\(B\\) are disjoint. The union \\(A \\cup B\\) contains everyone in \\(\\Omega\\) who lives in Ohio or is 42 years old. This could include both a 24-year-old who lives Ohio and a 42-year-old who lives Michigan.\n\n\n1.1.3 Venn diagrams\nA useful tool for understanding events and set operations is the Venn diagram.4 An example is shown in Figure 1.1. The rectangle represents \\(\\Omega\\), and the circles \\(A\\) and \\(B\\) represent events. \\(A^\\comp\\) is everything in \\(\\Omega\\) outside the circle \\(A\\), and \\(B^\\comp\\) is everything outside the circle \\(B\\). Their intersection \\(A \\cap B\\) is the area where the two circles overlap. Their union \\(A \\cup B\\) is everything contained in at least one of \\(A\\) or \\(B\\).\n\n\n\n\n\n\nFigure 1.1: Venn diagram showing events \\(A\\) and \\(B\\). The area contained in both events is their intersection \\(A \\cap B\\). The union \\(A \\cup B\\) is all area contained in at least one of \\(A\\) and \\(B\\), including \\(A \\cap B\\).\n\n\n\n\n\n1.1.4 Sequences of events*\nIntersections can be written for more than two events. The intersection of \\(A_1, A_2, \\ldots, A_n\\) is \\[\n  I_n = \\bigcap_{i = 1}^n A_i.\n\\tag{1.2}\\] Because set intersection is commutative and associative, any ordering of \\(A_1, \\ldots, A_n\\) produces the same intersection. The event \\(I_n\\) occurs if and only if all of the events \\(A_1, \\ldots, A_n\\) occur. Each new event makes the intersection smaller (i.e., never larger) in the sense that \\[\n  \\bigcap_{i = 1}^{n + 1} A_i \\subseteq I_n.\n\\] whenever \\(A_{n + 1}\\) is another event.\nSimilarly, unions can be written for more than two events. If \\(A_1, A_2, \\ldots, A_n\\) is a set of events, then their union is \\[\n  U_n = \\bigcup_{i = 1}^n A_i.\n\\tag{1.3}\\] Because set union is commutative and associative, any ordering of \\(A_1, \\ldots, A_n\\) produces the same union. The event \\(U_n\\) occurs if and only if at least one of the events \\(A_i\\) occurs. Each new event makes the union bigger (i.e., never smaller) in the sense that \\[\n  U_n \\subseteq \\bigcup_{i = 1}^{n + 1} A_i\n\\] whenever \\(A_{n + 1}\\) is another event.\nBoth unions and intersections can be defined for infinite sequences of events.5 To describe this, we let \\(n = \\infty\\) in the notation from Equation 1.2 or Equation 1.3. The union of any finite sequence of events can be turned into the union of an infinite sequence of events by adding an endless sequence of empty sets to the finite sequence. The new sequence is still a sequence of disjoint events, and each empty set \\(\\varnothing\\) leaves the union unchanged. If \\((A_1, A_2, \\ldots)\\) is an infinite sequence of events such that \\(A_i = \\varnothing\\) for all \\(i &gt; n\\), then \\[\n  \\bigcup_{i = 1}^\\infty A_i = \\bigcup_{i = 1}^n A_i.\n\\] This turns out to be useful when we try to give a mathematically rigorous definition of probability.\n\n\n1.1.5 Algebra of sets\\(^*\\)\nUnions, intersections, and complements can be combined in complex ways. Fortunately, there are a few basic principles that can be used to simplify these calculations. We have already seen that unions and intersections are commutative. Unions and intersections are also associative, so \\[\n  A \\cup (B \\cup C)\n  = (A \\cup B) \\cup C\n\\] and \\[\n  A \\cap (B \\cap C)\n  = (A \\cap B) \\cap C\n\\] for any sets \\(A\\), \\(B\\), and \\(C\\).\nDe Morgan’s laws describe how complements affect unions and intersections. If \\(A\\) and \\(B\\) are sets, then \\[\n  (A \\cap B)^\\comp\n  = A^\\comp \\cup B^\\comp\n\\tag{1.4}\\] because you are outside \\(A \\cap B\\) if and only f you are outside \\(A\\) or outside \\(B\\). Similarly, \\[\n  (A \\cup B)^\\comp\n  = A^\\comp \\cap B^\\comp.\n\\tag{1.5}\\] because you are outside \\(A \\cup B\\) if and only if you are outside \\(A\\) and outside \\(B\\). Note that each of these equations implies the other if we replace \\(A = (A^\\comp)^\\comp\\) with \\(A^\\comp\\) and replace \\(B = (B^\\comp)^\\comp\\) with \\(B^\\comp\\). They are two sides of the same coin, but it is helpful to remember them both.\nThe distributive properties describe how unions and intersections interact with each other. Recall that multiplication distributes over addition, so \\(a (b + c) = ab + ac\\). For any sets \\(A\\), \\(B\\), and \\(C\\), we have the following distributive properties:\n\nIntersections distribute over unions, so \\[\n  A \\cap (B \\cup C)\n  = (A \\cap B) \\cup (A \\cap C).\n\\]\nUnions distribute over intersections, so \\[\n  A \\cup (B \\cap C)\n  = (A \\cup B) \\cap (A \\cup C).\n\\]\n\nIntersections and unions also distribute over themselves. However, this is a consequence of commutativity, associativity, and Equation 1.1, not a separate property like the distributive rules above.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#probability",
    "href": "probability.html#probability",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.2 Probability",
    "text": "1.2 Probability\nA probability measure is a function that takes an event \\(A \\subseteq \\Omega\\) and returns a number \\(\\Pr(A) \\in [0, 1]\\) in any way that conforms to the following rules:\n\n\\(\\Pr(\\Omega) = 1\\).\n\\(\\Pr(A) \\in [0, 1]\\) for any event \\(A \\subseteq \\Omega\\).6\nThe addition rule: If \\((A_1, A_2, \\ldots)\\) is any sequence of disjoint events, then \\[\n  \\Pr\\Biggl( \\bigcup_{i = 1}^\\infty A_i\\Biggr)\n  = \\sum_{i = 1}^\\infty \\Pr(A_i).\n\\] The addition rule is stated in terms of an infinite sequence of disjoint events because this implies the addition rule for any finite sequence of disjoint events (see Section 1.1.4).\n\nIt is useful to think of probability as a generalization of our intuitions about area or volume. When there is no overlap in a set of two-dimensional shapes, we can get the total area they cover by adding up the areas of the individual shapes. Similarly, we can get the total volume taken up by a set of bowling balls by adding up their individual volumes.\nThere is a lot of debate about the meaning of probability, but its definition does not assume any particular interpretation. Probability calculations are based on the rules above no matter what we think it all means, and any interpretation consistent with these rules is valid.\n\n1.2.1 Probability calculations\nSeveral useful properties of probability follow immediately from the definition above. A short proof follows each result. To follow the proofs, it helps to draw Venn diagrams.\n\nTheorem 1.1 If \\(A\\) is an event, \\(\\Pr\\bigl(A^\\comp\\bigr) = 1 - \\Pr(A)\\).\n\n\nProof. Because \\(\\Omega = A \\cup A^\\comp\\) and \\(A\\) and \\(A^\\comp\\) are disjoint, we have \\[\n    \\Pr(A) + \\Pr\\bigl(A^\\comp\\bigr) = \\Pr(\\Omega) = 1\n  \\] by the addition rule. The result follows when we subtract \\(\\Pr(A)\\) from both sides.\n\n\nTheorem 1.2 If \\(A\\) and \\(B\\) are events such that \\(A \\subseteq B\\), then \\(\\Pr(A) = \\Pr(B) - \\Pr\\bigl(B \\cap A^\\comp\\bigr)\\). This implies that \\(\\Pr(A) \\leq \\Pr(B)\\).\n\n\nProof. Each element of \\(B\\) either is or is not in \\(A\\), so \\[\n    B = (B \\cap A) \\cup \\big(B \\cap A^\\comp\\big)\n    = A \\cup \\big(B \\cap A^\\comp\\big).\n  \\] where the second equality follows from the fact that \\(B \\cap A = A\\) because \\(A \\subseteq B\\). The two sets on the right-hand side are disjoint, so we have \\[\n    \\Pr(B) = \\Pr(A) + \\Pr\\bigl(B \\cap A^\\comp\\bigr)\n  \\] by the addition rule. The result follows if we subtract \\(\\Pr\\bigl(B \\cap A^\\comp\\bigr)\\) from both sides. This implies that \\(\\Pr(A) \\leq \\Pr(B)\\) because \\(\\Pr\\bigl(B \\cap A^\\comp\\bigr) \\geq 0\\).\n\n\nTheorem 1.3 If \\(A\\) and \\(B\\) are events, \\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\).\n\n\nProof. We can break \\(A \\cup B\\) into three disjoint sets: elements of \\(A\\) and not \\(B\\), elements of \\(B\\) and not \\(A\\), and elements of both \\(A\\) and \\(B\\). In set notation, this is \\[\n    A \\cup B = \\big(A \\cap B^\\comp\\big) \\cup \\big(B \\cap A^\\comp\\big) \\cup (A \\cap B).\n  \\] By the addition rule, \\[\n    \\Pr(A \\cup B) = \\Pr\\bigl(A \\cap B^\\comp\\bigr) + \\Pr\\bigl(B \\cap A^\\comp\\bigr) + \\Pr(A \\cap B).\n   \\tag{1.6}\\] By Theorem 1.2, we have \\[\n      \\Pr\\bigl(A \\cap B^\\comp\\bigr)\n      = \\Pr(A) - \\Pr(A \\cap B), \\\\\n  \\] because \\(A \\cap B \\subseteq A\\) and \\[\n      \\Pr\\bigl(B \\cap A^\\comp\\bigr)\n      = \\Pr(B) - \\Pr(A \\cap B).\n  \\] because \\(A \\cap B \\subseteq B\\). The result follows from substituting these back into Equation 1.6 and collecting terms involving \\(\\Pr(A \\cap B)\\). Intuitively, \\(\\Pr(A) + \\Pr(B)\\) includes the overlap \\(\\Pr(A \\cap B)\\) twice, so we have to subtract out one of them. This can be see clearly in Figure 1.1.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#random-variables",
    "href": "probability.html#random-variables",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.3 Random variables",
    "text": "1.3 Random variables\nThe outcomes of an experiment can be anything, not just numbers. A random variable is a real-valued function defined on a sample space \\(\\Omega\\). In other words, a random variable \\(X\\) is a function that takes an argument \\(\\omega \\in \\Omega\\) as input and returns a value \\(X(\\omega) \\in \\mathbb{R}\\). Traditionally, random variables are written as capital letters and possible values are written as lower-case letters, so \\(\\Pr(X = x)\\) denotes the probability of the event \\[\n  \\{\\omega \\in \\Omega : X(\\omega) = x\\}.\n\\] For simplicity, random variables are usually written without the argument \\(\\omega\\).\nThe distinction between outcomes and random variables is useful because we can define multiple random variables on the same sample space. For example, the height, weight, and age of an individual \\(\\omega\\) sampled from a population \\(\\Omega\\) are different random variables defined on the same sample space.\n\n1.3.1 Indicator variables\nThe simplest random variables are indicator variables. For an event \\(A\\), the indicator variable \\[\n  \\indicator_A(\\omega)\n  = \\begin{cases}\n    1 & \\text{ if } \\omega \\in A, \\\\\n    0 & \\text{ if } \\omega \\not\\in A.\n  \\end{cases}\n\\] Indicator variables are binary random variables, which take exactly two values. In practice, these values should be zero and one unless there is a specific reason to do otherwise. When sampling from a population, we can define indicator variables for membership in different subpopulations.\nAll of the basic set operations above can be expressed in terms of indicator variables for sets.\n\nThe indicator function for the complement of \\(A\\) is \\[\n  \\indicator_{A^\\comp} = 1 - \\indicator_A.\n\\tag{1.7}\\]\nIf \\(B\\) is another event and \\(\\indicator_B\\) is its indicator variable, then the indicator variable for the intersection \\(A\\) and \\(B\\) is the product of their indicator variables: \\[\n  \\indicator_{A \\cap B}\n  = \\indicator_A \\indicator_B.\n\\tag{1.8}\\]\nThe indicator variable for the union \\(A \\cup B\\) is \\[\n  \\indicator_{A \\cup B}\n  = 1 - (1 - \\indicator_A) (1 - \\indicator_B)\n  = \\indicator_A + \\indicator_B - \\indicator_{A \\cap B}.\n\\tag{1.9}\\] This follows from Equation 1.5 because \\(A \\cup B = (A^\\comp \\cap B^\\comp)^\\comp\\).\n\n\nR\n\n\n\n\nindicators.R\n\n## Indicator variables for events A and B, etc.\n\n# Setting the seed ensures that everyone gets the same random samples.\n# Functions are called using parentheses (round brackets).\n# The function rbinom() is a random sample from a binomial distribution.\nset.seed(42)\nn &lt;- 100\ndat &lt;- data.frame(A = rbinom(n, 1, 0.3))\ndat$B &lt;- rbinom(n, 1, 0.6)\n\n# inspecting a data frame\nnames(dat)  # variables in the data frame\nnrow(dat)   # number of rows (individuals)\nncol(dat)   # number of columns (variables)\ndim(dat)    # rows and columns in the data frame\nstr(dat)    # summary of the data frame structure (variables and types)\n\n# inspecting columns of a data frame (or vectors)\n# Our sample space or population consists of 100 individuals.\n# Square brackets are used for indices, which can be numbers or TRUE/FALSE.\ndat$A                 # indicator for A for all 100 individuals\ndat$A[10]             # indicator for A in individual 10\ndat$A[2:6]            # indicator variables for individuals 2 to 6\ndat$A[c(10, 20, 30)]  # A indicators for individuals 10, 20, and 30\nwhich(dat$A == 1)     # which individuals are in event A\nwhich(dat$A == 0)     # which individuals are not in event A\n\n# indicator variable for A complement\n# In R (and many other languages), \"!\" means \"not\".\n# The function as.integer() changes TRUE/FALSE to 1/0.\ndat$Acomp &lt;- as.integer(!dat$A)\n\n# indicator variable for A intersection B\n# In R (and many other languages), \"&\" means \"and\".\ndat$ABintersect &lt;- as.integer(dat$A & dat$B)\n\n# indicator variable for A union B\n# In R (and many other languages), \"|\" means \"or\".\ndat$ABunion &lt;- as.integer(dat$A | dat$B)\n\n# save the data frame as a CSV file\n# The file argument can be a path (e.g., \"./data/indicators.csv\" in Linux).\nwrite.csv(dat, file = \"indicators.csv\", row.names = FALSE)\n\n\n\n\n\n\n1.3.2 Probability distributions\nThe set of possible values of a random variable \\(X\\) is called the support of \\(X\\) and denoted \\(\\supp(X)\\).7 For example, the support of an indicator variable is \\(\\{0, 1\\}\\). In this section, we will focus on discrete random variables, which have a support on a finite or countably infinite set. There are two standard ways to describe the distribution of a discrete random variable:\n\nThe probability mass function (PMF) of a discrete random variable \\(X\\) is \\[\n  f(x) =\n  \\begin{cases}\n    \\Pr(X = x) &gt; 0  & \\text{ if } x \\in \\supp(X), \\\\\n    0               & \\text{ if } x \\not \\in \\supp(X).\n  \\end{cases}\n\\] Because \\(\\Pr(\\Omega) = 1\\), we always have \\[\n  \\sum_{x \\in \\supp(X)} f(x) = 1.\n\\]\nThe cumulative distribution function (CDF) of \\(X\\) is \\[\n  F(x)\n  = \\Pr(X \\leq x).\n\\] \\(F(x)\\) is monotonically increasing in \\(x\\), which means that \\(F(a) \\leq F(b)\\) whenever \\(a &lt; b\\). It has a jump upward of size \\(f(x)\\) at each \\(x \\in \\supp(X)\\), and its value at each such \\(x\\) is the value that it jumps to—not the value that it jumps up from. For sufficiently small \\(x\\), \\(F(x)\\) can be made arbitrarily close to zero. For sufficiently large \\(x\\), \\(F(x)\\) can be made arbitrarily close to one. More formally, we say that \\(\\lim_{x \\downarrow -\\infty} F(x) = 0\\) and \\(\\lim_{x \\uparrow \\infty} F(x) = 1\\).\n\nThe PMF and CDF provide equivalent descriptions of the distribution of \\(X\\) in the sense that either of these functions can be used to calculate the other. Given the PMF \\(f\\), the CDF is defined by \\[\n  F(x) = \\sum_{\\substack{v \\in \\supp(X): \\\\ v \\leq x}} f(v).\n\\] where the sum is taken over all \\(u \\in \\supp(X)\\) such that \\(u \\leq x\\). Given the CDF \\(F\\), the PMF is defined by \\[\n  f(x) = F(x) - \\max_{v \\leq x} F(v)\n\\] where the maximum is \\(F(v)\\) for the largest \\(v \\in \\supp(X)\\) such that \\(v &lt; x\\).\n\n\n1.3.3 Mean\nThe mean or expected value of a random variable \\(X\\) is \\[\n  \\E(X)\n  = \\sum_{x \\in \\supp(X)} x \\Pr(X = x)\n  = \\sum_{x \\in \\supp(X)} x f(x),\n\\] where \\(f\\) is the PMF of \\(X\\). The mean is often written \\(\\mu\\), and it is often described as a measure of the “location” or “central tendency” of \\(X\\).\nIndicators are an extremely useful for calculating probabilities using means. For any event \\(A\\), its probability is the mean of the indicator variable \\(\\indicator_A\\): \\[\n  \\Pr(A)\n  = 0 \\Pr(\\indicator_A = 0) + 1 \\Pr(\\indicator_A = 1)\n  = \\E(\\indicator_A).\n\\] This is a common way to calculate probabilities in data analyses.\n\nR\n\n\n\n\nprobabilities.R\n\n## Indicator variables and probability calculations\n\n# read in CSV file with indicator variables using the function read.csv()\n# The argument can be a path (e.g., \"./data/indicators.csv\" in Linux).\ndat &lt;- read.csv(\"indicators.csv\")\n\n# calculate probabilities from indicator variables using the function mean()\n# This will also work with TRUE/FALSE (i.e., logical) variables, which are\n# converted to TRUE = 1 and FALSE = 0 in calculations.\nprob_A &lt;- mean(dat$A)\nprob_B &lt;- mean(dat$B)\nprob_Acomp &lt;- mean(dat$Acomp)\nprob_ABintersect &lt;- mean(dat$ABintersect)\nprob_ABunion &lt;- mean(dat$ABunion)\n\n# Pr(A complement) = 1 - Pr(A)\nprob_Acomp\n1 - prob_A\n\n# Pr(A union B) = Pr(A) + Pr(B) - Pr(A intersect B)\nprob_ABunion\nprob_A + prob_B - prob_ABintersect\n\n# Beware of numerical error when comparing floating-point numbers!\n# This example is from The R Inferno by Patrick Burns.\n# https://www.burns-stat.com/pages/Tutor/R_inferno.pdf\n0.1 == 0.3 / 3\nsprintf(\"%.20f\", 0.1)\nsprintf(\"%.20f\", 0.3 / 3)\n\n# math can be more accurate than computers (which is not their fault)\nprob_ABunion == prob_A + prob_B - probABintersect\nsprintf(\"%.20f\", prob_ABunion)\nsprintf(\"%.20f\", prob_A + prob_B - prob_ABintersect)\n\n\n\n\n\n\n1.3.4 Variance\nIf \\(X\\) has \\(\\E(X) = \\mu\\), then \\((X - \\mu)^2\\) is another random variable. The variance of \\(X\\) is the expected value of \\((X - \\mu)^2\\): \\[\n  \\Var(X)\n  = \\E\\big[(X - \\mu)^2\\big]\n  = \\sum_{x \\in \\supp(X)} (x - \\mu)^2 f(x).\n\\] Because \\((x - \\mu)^2 \\geq 0\\) with equality if and only if \\(x = \\mu\\), we always have \\(\\Var(X) \\geq 0\\). We have \\(\\Var(X) = 0\\) if and only if \\(X = \\mu\\) with probability one. The variance is often written \\(\\sigma^2\\), and it is often described as a measure of the dispersion of \\(X\\) around the mean.\nThe square root of the variance is called the standard devation, which is often written \\(\\sigma\\). If a random variable \\(X\\) has units (e.g., length, weight, or time), the mean and the standard deviation have the same units as \\(X\\). For example, the mean and standard deviation of a length in meters both have units of \\(\\text{meters}\\) but the variance has units of \\(\\text{meters}^2\\).\n\n\n1.3.5 Bernoulli distribution\nThe distribution of an indicator variable is called the Bernoulli distribution.8 A random variable with the Bernoulli(\\(p\\)) distribution has the PMF \\[\n  f(x) =\n  \\begin{cases}\n    1 - p &\\text{if } x = 0\\\\\n    p     &\\text{if } x = 1.\n  \\end{cases}\n\\] Equivalently, it has the CDF \\[\n  F(x) =\n  \\begin{cases}\n    0     &\\text{if } x &lt; 0 \\\\\n    1 - p &\\text{if } x \\in [0, 1) \\\\\n    1     &\\text{if } x \\geq 1.\n  \\end{cases}\n\\] If a random variable \\(X\\) has a Bernoulli(\\(p\\)) distribution, we write \\(X \\sim \\text{Bernoulli}(p)\\). The indicator variable for an event \\(A\\) has a Bernoulli distribution with \\(p = \\Pr(A)\\).\nIf \\(X \\sim \\Bernoulli(p)\\), then it has mean \\[\n  \\E(X)\n  = 0 \\times (1 - p) + 1 \\times p \\\\\n  = p\n\\] and variance \\[\n  \\Var(X)\n  = (0 - p)^2(1 - p) + (1 - p)^2 p \\\\\n  = p (1 - p).\n\\] Its standard deviation is \\(\\sqrt{p (1 - p)}\\), which is greater than zero unless \\(p = 0\\) or \\(p = 1\\). If \\(p = 0\\), then \\(X = 0\\) with probability one. If \\(p = 1\\), then \\(X = 1\\) with probability one.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#joint-and-marginal-distributions",
    "href": "probability.html#joint-and-marginal-distributions",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.4 Joint and marginal distributions",
    "text": "1.4 Joint and marginal distributions\nIf \\(X\\) and \\(Y\\) are random variables defined on the same probability space, then their joint probability mass function is \\[\n  f(x, y)\n  = \\Pr(X = x \\text{ and } Y = y)\n  = \\Pr\\big(\\{\\omega: X(\\omega) = x \\text{ and } Y(\\omega) = y\\}\\big).\n\\] The marginal probability mass functions are the PMFs of \\(X\\) or \\(Y\\) individually, which can be calculated from the joint PMF. The marginal PMF of \\(X\\) is \\[\n  f_X(x) = \\sum_{y \\in \\supp(Y)} f(x, y),\n\\] and the marginal PMF of \\(Y\\) is \\[\n  f_Y(y) = \\sum_{x \\in \\supp(X)} f(x, y).\n\\] These are called marginal distributions by analogy to the margins of a table. The distinction between joint and marginal distributions is extremely important in epidemiology and other applications of probability.\nFor example, Table 1.1 shows the joint and marginal PMFs for two binary random variables \\(X\\) and \\(Y\\). By definition, \\[\n  f(0, 0) + f(0, 1) + f(1, 0) + f(1, 1) = 1.\n\\] In the table, it is clear that the joint distribution determines the marginal distributions. However, there are many different joint distributions that are consistent with the same marginal distributions. Thus, the marginal distributions do not determine the joint distribution.9\n\n\n\nTable 1.1: Joint and marginal PMFs for binary random variables \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Y = 0\\)\n\\(Y = 1\\)\n\\(X\\) margin\n\n\n\n\n\\(X = 0\\)\n\\(f(0, 0)\\)\n\\(f(0, 1)\\)\n\\(f_X(0) = f(0, 0) + f(0, 1)\\)\n\n\n\\(X = 1\\)\n\\(f(1, 0)\\)\n\\(f(1, 1)\\)\n\\(f_X(1) = f(1, 0) + f(1, 1)\\)\n\n\n\\(Y\\) margin\n\\(f_Y(0) = f(0, 0) + f(1, 0)\\)\n\\(f_Y(1) = f(0, 1) + f(1, 1)\\)\n1\n\n\n\n\n\n\n\nR\n\n\n\n\njointdist.R\n\n## Joint and marginal distributions of indicators for events A and B\n\n# read indicator variable data from the CSV file\ndat &lt;- read.csv(\"indicators.csv\")\nn &lt;- nrow(dat)\n\n# tables of counts\n# Putting \"&lt;name&gt; = \" before the vector creates a label.\ntable(A = dat$A)\ntable(B = dat$B)\n\n# joint table of counts\n# In table(), the first argument defines rows and the second defines columns.\n# The addmargins() functions adds the row, column, and overall sums.\ntable(A = dat$A, B = dat$B)\naddmargins(table(A = dat$A, B = dat$B))\n\n# tables of probabilities\n# Table margins match the distributions of A (rows) and B (columns).\ntable(Adist = dat$A) / n    # marginal distribution of A indicator\ntable(Bdist = dat$B) / n    # marginal distribution of B indicator\naddmargins(table(A = dat$A, B = dat$B)) / n   # joint distribution\n\n\n\n\nJoint distributions can be defined for more than two random variables. If \\(X_1, X_2, \\ldots, X_n\\) are random variables defined on the same sample space, then their joint PMF is \\[\n  f(x_1, x_2, \\ldots, x_n) = \\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n).\n\\] The marginal distribution of each \\(X_i\\) can be found by adding up the PMF over the support of all the other random variables. For example, \\[\n  f_{X_2}(x_2) = \\sum_{x_1 \\in \\supp(X_1)} \\sum_{x_3 \\in \\supp(X_3)} f(x_1, x_2, x_3).\n\\] when \\(n = 3\\). In this same case, we can talk about the joint distribution of any two variables marginalized over the third. For example, \\[\n  f_{X_2, X_3}(x_2, x_3) = \\sum_{x_1 \\in \\supp(X_1)} f(x_1, x_2, x_3).\n\\] For larger \\(n\\), the formulas gets uglier but the ideas are the same.\n\n1.4.1 Linear combinations*\nIf \\(a\\) and \\(b\\) are constants, then \\(a X + b Y\\) is another random variable on \\(\\Omega\\). It is called a linear combination of \\(X\\) and \\(Y\\). Linear combinations can be defined for more than two random variables. If \\(X_1, \\ldots, X_n\\) are random variables defined on a sample space and \\(a_1, \\ldots, a_n\\) are constants, then \\[\n  \\sum_{i = 1}^n a_i X_i = a_1 X_1 + a_2 X_2 + \\cdots + a_n X_n\n\\] is a linear combination of \\(X_1, \\ldots, X_n\\). The constants can be any real numbers, including one and zero.\nSection 1.3.1 contains both examples and non-examples of linear combinations of random variables.\n\nThe indicator function for \\(A^\\comp\\) in Equation 1.7 is a linear combination of \\(\\indicator_A\\) and the random variable \\(\\indicator_\\Omega\\), which equals one for all \\(\\omega \\in \\Omega\\).\nThe indicator function for \\(A \\cup B\\) in Equation 1.9 is linear combination of the indicator variables \\(\\indicator_A\\), \\(\\indicator_B\\), and \\(\\indicator_{A \\cap B}\\).\nThe indicator function for \\(A \\cap B\\) in Equation 1.8 is not a linear combination of \\(\\indicator_A\\) and \\(\\indicator_B\\) because we have to multiply these two variables.\n\nIf \\(X\\) and \\(Y\\) are random variables defined on the same sample space and \\(a\\) and \\(b\\) are constants, the mean of the linear combination \\(a X + b Y\\) is \\[\n  \\E(a X + b Y) = a \\E(X) + b \\E(Y).\n\\] This is a direct consequence of the definition of expected value: \\[\n  \\begin{aligned}\n    \\E(a X + b Y)\n    &= \\sum_{x \\in \\supp(X)} \\sum_{y \\in \\supp(Y)} (a x + b y) f(x, y) \\\\\n    &= a \\sum_{x \\in \\supp(X)} \\bigg(x \\sum_{y \\in \\supp(Y)} f(x, y)\\bigg)\n      + b \\sum_{y \\in \\supp(Y)} \\bigg(y \\sum_{x \\in \\supp(X)} f(x, y)\\bigg) \\\\\n    &= a \\sum_{x \\in \\supp(X)} x f_X(x) + b \\sum_{y \\in \\supp(Y)} y f_Y(y).\n  \\end{aligned}\n\\] The algebra is not pretty, but the logic is straightforward. We split up the sum into parts depending only on \\(x\\) and only on \\(y\\) outside the joint PMF. In each part, we factor out a constant and find the marginal PMF. This same logic extends to a linear combination of any number of random variables.\n\n\n1.4.2 Variance and covariance*\nThe variance of \\(a X + b Y\\) is \\[\n  \\Var(a X + b Y) = a^2 \\Var(X) + b^2 \\Var(Y) + 2 a b \\Cov(X, Y)\n\\] where \\[\n  \\Cov(X, Y) = \\E\\bigl[\\big(X - \\E(X)\\big) \\big(Y - \\E(Y)\\big)\\bigr]\n\\] is called the covariance of \\(X\\) and \\(Y\\). Note that \\(\\Cov(X, Y) = \\Cov(Y, X)\\). Because \\(\\Var(X) = \\Cov(X, X)\\), variance is a special case of covariance.\nThe joint distribution of \\(X\\) and \\(Y\\) has a covariance matrix which is \\[\n  \\begin{bmatrix}\n    \\Var(X)     & \\Cov(X, Y) \\\\\n    \\Cov(X, Y)  & \\Var(Y)\n  \\end{bmatrix}\n\\] The variances are along the diagonal of the matrix, and the covariances appear off the diagonal. Because \\(\\Cov(X, Y) = \\Cov(Y, X)\\), covariance matrices are always symmetric (i.e., symmetric across the diagonal). Covariance matrices are an extremely useful tool for calculating the variances of linear combinations of random variables. For example: \\[\n  \\Var(a X + b Y)\n  = \\begin{pmatrix}\n      a & b\n    \\end{pmatrix}\n    \\begin{bmatrix}\n      \\Var(X)     & \\Cov(X, Y) \\\\\n      \\Cov(X, Y)  & \\Var(Y)\n    \\end{bmatrix}\n    \\begin{pmatrix}\n      a \\\\ b\n    \\end{pmatrix}\n\\] in matrix and vector notation from linear algebra. This logic extends to linear combinations of any number of random variables.\nThe covariance is the numerator of the Pearson correlation coefficient,10 which is \\[\n  \\rho_{XY} = \\rho_{YX}\n  = \\frac{\\Cov(X, Y)}{\\sqrt{\\Var(X) \\Var(Y)}}.\n\\] Because of the Cauchy-Schwarz inequality, it turns out that \\(\\rho_{XY} \\in [-1, 1]\\).\n\nWe get \\(\\rho_{XY} = -1\\) if and only if \\(Y = c X\\) for some negative constant \\(c\\).\nWe get \\(\\rho_{XY} = 1\\) if and only if \\(Y = c X\\) for some positive constant \\(c\\). For example, \\(\\rho_{XX} = 1\\) for any random variable \\(X\\).\nWe get \\(\\rho_{XY} = 0\\) if \\(X\\) and \\(Y\\) are independent in the sense that the value of one tells us nothing about the value of the other.11 However, it is possible to have \\(\\rho_{XY} = 0\\) when \\(X\\) and \\(Y\\) are not independent.\n\nIf we divide each entry \\(\\Cov(X, Y)\\) in a covariance matrix by \\(\\sqrt{\\Var(X) \\Var(Y)}\\), when we get a correlation matrix. Any correlation matrix is symmetric, and the entries along its diagonals are all ones.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#probability-and-disease-occurrence",
    "href": "probability.html#probability-and-disease-occurrence",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.5 Probability and disease occurrence",
    "text": "1.5 Probability and disease occurrence\nIn epidemiology, there are two fundamental measures of disease occurrence that are probabilities: prevalence and risk. In both cases, our experiment is to sample an individual \\(\\omega\\) from a population \\(\\Omega\\). The disease outcome is a binary random variable \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has the disease outcome}, \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] The set of individuals in \\(\\Omega\\) who have \\(D(\\omega) = 1\\) is an event in \\(\\Omega\\), and our measure of disease occurrence is \\[\n  \\Pr(\\{\\omega \\in \\Omega: D(\\omega) = 1\\}).\n\\] The most important difference between prevalence and risk is the role of time in the definition of \\(D\\).\nThere is an important technical detail to remember when we talk about disease onset and recovery. When a person has disease onset at time \\(\\tonset\\) and recovers at time \\(\\trec\\), they have disease for each \\(t \\in [\\tonset, \\trec)\\). We assume that \\(\\trec &gt; \\tonset\\) so this interval is nonempty. We let the onset and recovery times for person \\(i\\) be \\(\\tonset_i\\) and \\(\\trec_i\\), respectively. If a person has multiple episodes of the disease, each episode has its own \\(\\tonset\\) and \\(\\trec\\). For example, the \\(j^\\text{th}\\) episode in person \\(i\\) would have onset time \\(\\tonset_{ij}\\) and recovery time \\(\\trec_{ij}\\).\nThe time scale used to define disease onset is flexible, and this flexibility is useful. The most obvious time scale is calendar time or absolute time. Another common time scale is age, which is an important determinant of the risk of many diseases. In some cases, time since an event is a useful time scale. The event that defines time scale could be a single event (e.g., exposure to contaminated food at a party) or an event that occurs at different times for different individuals (e.g., time since menopause). In general, it is wise to choose the time scale that corresponds to the most important time-varying determinant of disease onset. The chosen time scale is often called the analysis time scale.\n\n1.5.1 Prevalence\nFor prevalence, the disease outcome is defined by choosing a time \\(t\\) and letting \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has disease at time } t, \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In other words, it is the proportion of the population \\(\\Omega\\) that disease at time \\(t\\). This includes individuals who have disease onset at time \\(\\tonset = t\\) but not individuals who recover from disease at time \\(\\trec = t\\). This is often called the point prevalence at time \\(t\\).\nAnother version of prevalence is period prevalence. For period prevalence, we choose a nonempty time interval \\((t_a, t_b]\\) and define \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has disease at any time } t \\in (t_a, t_b], \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In other words, it is the proportion of the population that has disease at any time in the interval \\((t_a, t_b]\\). This includes prevalent cases at time \\(t_a\\) and cases with disease onset in \\((t_a, t_b]\\). The period prevalence in \\((t_a, t_b]\\) is the point prevalence at \\(t_a\\) plus the risk of disease onset in \\((t_a, t_b]\\), to which we now turn.\n\nR\n\n\n\n\nprevalence.R\n\n## Point and period prevalence\n\n# generate onset and recovery data for 100 individuals\n# Setting the seed ensures that everyone gets the same random numbers,\n# but it is strictly optional.\n# The function rexp() randomly samples from an exponential distribution.\nset.seed(42)\ncohort &lt;- data.frame(onset = rexp(100, rate = 0.4))\ncohort$duration &lt;- rexp(100, rate = 2)\ncohort$recovery &lt;- cohort$onset + cohort$duration\n\n# statistical summaries (mean, quartiles, range)\nsummary(cohort$onset)\nsummary(cohort$duration)\nsummary(cohort$recovery)\n\n# highest and lowest recovery times\n# The function sort() sorts the vector from lowest to highest.\n# head() returns the first 6 values of a vector; tails() returns the last 6.\nmin(cohort$onset)\nhead(sort(cohort$onset))    # lowest 6 values (first 6 in the sorted vector)\ntail(sort(cohort$onset))    # highest 6 values (last 6 in the sorted vector)\nmax(cohort$onset)\n\n# With a long vector, sorting repeatedly can be slow.\n# You can also control the number of elements returned by head() or tail().\nonset_ordered &lt;- sort(cohort$onset)\nhead(onset_ordered, n = 10)\ntail(onset_ordered, n = 10)\n\n# seeing rows and columns of the data frame\ncohort[1:10, c(\"onset\", \"duration\", \"recovery\")]\ncohort[c(10, 20, 50), c(\"onset\", \"recovery\")]\ncohort[which(cohort$recovery &lt; 1), c(\"onset\", \"recovery\")]\ncohort[, c(\"onset\", \"recovery\")]    # all rows\ncohort[c(2, 3, 5, 7, 11), ]         # all columns\n\n# point prevalence\nprev &lt;- function(t) {\n  # vector of TRUE/FALSE for prevalent cases at time t\n  prevalent &lt;- cohort$onset &lt;= t & cohort$recovery &gt; t\n  mean(prevalent)\n}\n\nprev(0)\nprev(1)\nprev(2)\nprev(6)\n\n# period prevalence\n# The parentheses around the logical tests are just for readability.\npdprev &lt;- function(ta, tb) {\n  # prevalent cases at t_a\n  prevalent_ta &lt;- (cohort$onset &lt;= ta & cohort$recovery &gt; ta)\n  # incident cases in (t_a, t_b]\n  incident_ab &lt;- (cohort$onset &gt; ta & cohort$onset &lt;= tb)\n  # mean indicator for prevalent at t_a or incident in (t_a, t_b]\n  mean(prevalent_ta | incident_ab)\n}\n\npdprev(0, 1)\npdprev(1, 2)\npdprev(0, 6)\n\n# save the data as a CSV file\nwrite.csv(cohort, \"cohort.csv\", row.names = FALSE)\n\n\n\n\n\n\n1.5.2 Risk (cumulative incidence) and the survival function\nTo define risk or cumulative incidence, we first choose an nonempty time interval \\((t_a, t_b]\\). The disease outcome is defined as \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has } \\tonset \\in (t_a, t_b], \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In the population that is disease-free and at risk of disease at time \\(t_a\\), it is the proportion who have disease onset at \\(\\tonset \\leq t_b\\). The risk is sometimes called the incidence proportion.\nThe risk depends on a specified interval \\((t_a, t_b]\\). We can always define our time scale so that \\(t_a = 0\\), so the risk in \\((t_a, t_b]\\) on the original time scale is the same as the risk in the interval \\((0, t_b - t_a]\\) on the analysis time scale. On the analysis time scale, the cumulative incidence function \\(F(t)\\) is the risk of disease in \\((0, t]\\) for any possible \\(t\\). The corresponding survival function is \\[\n  S(t)\n  = 1 - F(t),\n\\] which is the probability of no disease onset in \\((0, t]\\). In practice, it is often easier to calculate the survival function than to calculate the cumulative incidence function directly. There is only one way to survive disease-free through the interval \\((0, t]\\), but you can have disease onset at any time.\n\nR\n\n\n\n\nrisk.R\n\n## Risk, survival function, and cumulative incidence function\n\n# read data from CSV file\n# Change or remove \".R/\" in the path as needed to locate the cohort.csv file.\n# You can also re-generate the data as in prevalence.R using the same seed.\ncohort &lt;- read.csv(\"./R/cohort.csv\")\n\n# risk (cumpulative incidence)\nrisk &lt;- function(t) {\n  # vector of TRUE/FALSE for incident cases in (0, t]\n  incident &lt;- cohort$onset &lt;= t\n  mean(incident)\n}\n\nrisk(0)\nrisk(1)\nrisk(2)\nrisk(6)\n\n# cumulative incidence function\n# Vectorize() takes a function like risk() that takes a single number as input\n# and creates a function that can take a number or vector as input.\ncuminc &lt;- Vectorize(risk)\ncuminc(c(0, 1, 2, 6))\n\n# survival function\n# A simple function can be put on one line.\n# It takes the same input as cuminc(), so it can take a vector\nsurv &lt;- function(t) 1 - cuminc(t)\nsurv(c(0, 1, 2, 6))\n\n# plot the survival and cumulative incidence functions\nt &lt;- seq(0, 20, by = 0.1)\nplot(t, surv(t), type = \"l\",\n     xlab = \"Time\", ylab = \"Probability\")\nlines(t, cuminc(t), lty = \"dashed\")\ngrid()\nlegend(\"right\", bg = \"white\", lty = c(\"dashed\", \"solid\"),\n       legend = c(\"Cumulative incidence\", \"Survival\"))\n\n\n\n\nThe survival function has several important properties:\n\n\\(S(0) = 1\\) because \\((0, 0]\\) is an empty interval where no one can have disease onset.\nBecause \\(S(t)\\) is a probability, \\(S(t) \\in [0, 1]\\) for all \\(t\\).\n\\(S(t)\\) monotonically decreases (i.e., never increases) with increasing \\(t\\). If \\(t_a &lt; t_b\\), then the time interval \\((0, t_a]\\) is contained \\((0, t_b]\\). Everyone who survives disease-free through \\((0, t_b]\\) must have survived disease-free through \\((0, t_a]\\), but some people who survived through \\((0, t_a]\\) might not make it all the way through \\((0, t_b]\\). Thus, \\(S(t_a) \\geq S(t_b)\\) whenever \\(t_a &lt; t_b\\).\nIf the disease or event occurs eventually for all individuals in our population \\(\\Omega\\) (e.g., death), then \\(S(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\).\n\nEach of these probabilities follows directly from the definition of \\(S(t)\\). Similarly, the cumulative incidence function \\(F\\) has \\(F(0) = 0\\) and \\(F(t) \\in [0, 1]\\), and it is monotonically increasing (i.e., never decreasing) with increasing \\(t\\). If the disease or event occurs eventually in all individuals, then \\(F(t) \\rightarrow 1\\) as \\(t \\rightarrow \\infty\\). Figure 1.2 shows the survival and cumulative hazard curves for the data generated in the prevalence example above.\n\n\n\nCode\n\nsurv-fig.R\n\n## Plot of survival and cumulative incidence functions\n\n# read data from CSV file\n# Change or remove \".R/\" in the path as needed to locate the cohort.csv file.\n# You can also re-generate the data as in prevalence.R using the same seed.\ncohort &lt;- read.csv(\"./R/cohort.csv\")\n\n# risk (cumpulative incidence)\nrisk &lt;- function(t) {\n  # vector of TRUE/FALSE for incident cases in (0, t]\n  incident &lt;- cohort$onset &lt;= t\n  mean(incident)\n}\n\n# cumulative incidence function\ncuminc &lt;- Vectorize(risk)\n\n# survival function\nsurv &lt;- function(t) 1 - cuminc(t)\n\n# plot the survival and cumulative incidence functions\nt &lt;- seq(0, 20, by = 0.1)\nplot(t, surv(t), type = \"l\",\n     xlab = \"Time\", ylab = \"Probability\")\nlines(t, cuminc(t), lty = \"dashed\")\ngrid()\nlegend(\"right\", bg = \"white\", lty = c(\"dashed\", \"solid\"),\n       legend = c(\"Cumulative incidence\", \"Survival\"))\n\n\n\n\n\n\n\n\n\nFigure 1.2: Survival and cumulative incidence curves for the data from the prevalence example.\n\n\n\n\n\nHere, I will generally use the word “risk” to refer to the probability of disease onset in a specified interval. When there is possible confusion about the meaning of “risk”, I will use “cumulative incidence” instead. The terms “cumulative incidence function” and “survival function” are standard in survival analysis, which is the branch of statistics that studies times to events. The creative use of “risk” in public health and medicine should not make you shy away from using the word correctly.\n\n\n1.5.3 Prevalence and the duration of disease\nPoint and period prevalence are both affected by the duration of disease. Both measures will increase if the duration of disease increases. A simple illustration of this is given in Figure 1.3. For a fixed set of onset times, the point prevalence of disease at any time \\(t\\) either stays the same or increases when the duration of disease increases. The prevalence at time \\(t = 5\\) is \\(\\frac{2}{5} = 0.4\\) under the shorter duration of disease but \\(\\frac{3}{5} = 0.6\\) under the longer duration of disease. Period prevalence over any interval \\((t_a, t_b]\\) is affected by the duration of disease because it is the point prevalence at \\(t_a\\) (which is affected by disease duration) plus the risk of disease onset over \\((t_a, t_b]\\). In a given population, the relationship between prevalence, frequency of disease onset (incidence), and the duration of disease can be complex (Freeman and Hutchison 1980; Preston 1987; Keiding 1991; Alho 1992). The risk of disease in any given interval is not affected by the duration of disease.\n\n\n\nCode\n\nprevdur-fig.R\n\n## R code for prevalence and duration plot\nplot(0, 0, type = \"n\", xlim = c(0, 10), ylim = c(0, 5.5),\n     xlab = \"Time\", ylab = \"Individual\", yaxt = \"n\")\nAxis(side = 2, at = 1:5, labels = 1:5)\ngrid()\nstart &lt;- c(4, 1, 3, 2, 6)\nstop1 &lt;- c(7, 3, 6, 4, 7)\nstop2 &lt;- c(9, 4, 8, 7, 9)\narrows(x0 = start, y0 = 1:5, x1 = stop1, code = 3, length = 0.2, angle = 90)\narrows(x0 = stop1, y0 = 1:5, x1 = stop2, code = 2, length = 0.2, angle = 90,\n       col = \"darkgray\")\nabline(v = 5, lty = \"dashed\")\ntext(5.5, 0.5, label = \"t = 5\")\n\n\n\n\n\n\n\n\n\nFigure 1.3: Each black horizonal line shows the onset of disease and recovery from disease in a single individual. The gray lines show recoveries from disease if the disease duration increases.\n\n\n\n\n\n\n\n1.5.4 Descriptive and analytic epidemiology\nPrevalence is often a useful measure for descriptive epidemiology, which measures the distribution of disease over person, place, and time. Because prevalence depends on both incidence and duration of disease, a change in the prevalence of disease can generally be explained several different ways (MacMahon and Terry 1958; Dunn Jr 1962). For example, an increase in prevalence of human immunodeficiency virus (HIV) infection could be cause by an increase in the incidence of HIV infection (which is bad) or an increase in the life expectancy of HIV-infected people (which is good).\nRisk (cumulative incidence) is generally more useful than prevalence for analytic epidemiology, which attempts to identify the causes of a disease. Another advantage of risk is that it can be used for outcomes that begin and end very quickly (e.g., traffic accidents or being hit by lightning) and for outcomes that remove individuals from the population (e.g., emigration or death). Prevalence is not a useful measure of the public health impact of these events.\n\n\n\n\nAlho, Juha M. 1992. “On Prevalence, Incidence, and Duration in General Stable Populations.” Biometrics 48 (2): 587–92.\n\n\nDunn Jr, John E. 1962. “The Use of Incidence and Prevalence in the Study of Disease Development in a Population.” American Journal of Public Health 52 (7): 1107–18.\n\n\nFreeman, Jonathan, and George B Hutchison. 1980. “Prevalence, Incidence and Duration.” American Journal of Epidemiology 112 (5): 707–23.\n\n\nKeiding, Niels. 1991. “Age-Specific Incidence and Prevalence: A Statistical Perspective.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 154 (3): 371–96.\n\n\nLaplace, Pierre Simon. 1820. Théorie Analytique Des Probabilités. Vol. 7. Courcier.\n\n\nMacMahon, Brian, and William D Terry. 1958. “Application of Cohort Analysis to the Study of Time Trends in Neoplastic Disease.” Journal of Chronic Diseases 7 (1): 24–35.\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological Perspective.” In A History of Epidemiologic Methods and Concepts, edited by Alfredo Morabia, 3–125. Springer.\n\n\nPreston, Samuel H. 1987. “Relations Among Standard Epidemiologic Measures in a Population.” American Journal of Epidemiology 126 (2): 336–45.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#footnotes",
    "href": "probability.html#footnotes",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "Pierre-Simone, marquis de Laplace (1749-1827) is often called the Newton of France. He proved that the solar system is stable, developed theories of ocean tides and gravitational potential, proved one of the first general versions of the central limit theorem, and pioneered the Bayesian interpretation of probability. His is one of the 72 names on the Eiffel Tower. ↩︎\n The natural numbers \\(\\mathbb{N} = \\{0, 1, 2, \\ldots\\}\\) are countably infinite, as are the integers \\(\\mathbb{Z}\\) and the rational numbers \\(\\mathbb{Q}\\). The real numbers \\(\\mathbb{R}\\) are uncountably infinite, as are the real numbers in any nonempty interval \\((a, b)\\) and the irrational numbers. Uncountably infinite sets are infinitely larger than countably infinite sets. This distinction was discovered in the 1870s by the German mathematician Georg Cantor (1845–1918). It was considered shocking, but it has become a cornerstone of modern mathematics.↩︎\n In experiments with uncountably infinite sample spaces, the probability of an event \\(A\\) cannot always be calculated by adding up the probabilities of \\(\\{\\omega\\}\\) for all \\(\\omega \\in A\\). For example: If we choose a number at uniformly at random in \\([0, 1]\\), the probability of getting any particular number \\(\\omega\\) is zero. The sum of the probabilities of all \\(\\{\\omega\\} \\subseteq A\\) is zero (if \\(A\\) is countable) or undefined (if \\(A\\) is uncountable). By maintaining a distinction between outcomes and events and by limiting probability calculations to countable (i.e., finite or countably infinite) sums, we end up with something coherent and useful.↩︎\n Named after John Venn (1834-1923), an English logician and philosopher who was one of the pioneers of the frequentist interpretation of probability. He was ordained as an Anglican priest in 1859 but resigned from the church in 1883. He was a prize-winning gardener of roses and white carrots and a prominent supporter of women’s right to vote. From 1903 until his death, he was President of Fellows in Gonville and Caius College at the University of Cambridge, where he is commemorated with a Venn diagram in a stained glass window.↩︎\n In probability, we only consider unions and intersections of finite or countably infinite sets of events. Although unions and intersections can be defined for uncountably infinite sets of events, it can be impossible to assign probabilities to the resulting sets (see the Banach-Tarski paradox). As an epidemiologist, this should not keep you up at night.↩︎\n Technically, we assign probabilities only to events in a class \\(\\mathcal{F}\\) of subsets of \\(\\Omega\\) that is required to contain \\(\\Omega\\) and to be closed under complements and countable unions. “Closed under complements” means that \\(A^\\comp \\in \\mathcal{F}\\) whenever \\(A \\in \\mathcal{F}\\). For example, \\(\\varnothing = \\Omega^\\comp\\) must be in \\(\\mathcal{F}\\) because \\(\\Omega \\in \\mathcal{F}\\). “Closed under countable unions” means that \\(\\bigcup_{i = 1}^\\infty A_i \\in \\mathcal{F}\\) whenever \\((A_1, A_2, \\ldots)\\) is a sequence of events in \\(\\mathcal{F}\\). The class \\(\\mathcal{F}\\) is called a \\(\\sigma\\)-algebra or \\(\\sigma\\)-field, and this restriction on the domain of probability helps avoid internal contradictions like the Banach-Tarski paradox.↩︎\n Technically, the support of \\(X\\) is the smallest closed set \\(S_X\\) such that \\(\\Pr(X \\in S_X) = 1\\). For a discrete random variable with support on a finite set, it is just the set of possible values. For a discrete random variable with support on a countably infinite set, it can include points whose probability mass is zero—a pathological case that we can safely ignore. For a continuous random variable, it can include values whose probability density is zero—a case that is not unusual or pathological.↩︎\n Named after Jacob Bernoulli (1655-1705), a Swiss mathematician who derived the first version of the law of large numbers and discovered the constant \\(e \\approx 2.718281828\\), which is the base for natural logarithms. He and his younger brother Johann Bernoulli (1667-1748) were some of the first mathematicians to try to understand and apply calculus, but their relationship eventually curdled into a jealous rivalry. A lunar impact crater called Bernoulli is named jointly after them.↩︎\n This becomes a fundamental insight when we discuss hypothesis tests for independence as well as confounding and selection bias.↩︎\n Named after Karl Pearson (1857-1936), an English mathematician who founded the modern discipline of mathematical statistics. In 1911, he started the world’s first university department of statistics at University College London. He was an outspoken socialist and supporter of women’s rights, but he was also a vocal proponent of social Darwinism and eugenics who opposed Jewish immigration into Britain.↩︎\n We will define independence of random variables more rigorously when we discuss conditional probabilities in Chapter 2.↩︎",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "condprob.html",
    "href": "condprob.html",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "",
    "text": "2.1 Contingency tables\nSuppose we know that an event \\(A\\) occurred and want calculate the probability that \\(B\\) also occurred. The conditional probability of \\(B\\) given \\(A\\) is \\[\n    \\Pr(B \\given{} A) = \\frac{\\Pr(A \\cap B)}{\\Pr(A)}.\n\\tag{2.1}\\] Note that this is well-defined only if \\(\\Pr(A) &gt; 0\\). Conditional probabilities given \\(A\\) are just probabilities where the original sample space \\(\\Omega\\) has been replaced with an event \\(A \\subseteq \\Omega\\). Everything we have learned about probabilities applies to all of the conditional probabilities given the same event \\(A\\). Conditional probability is arguably the most important mathematical tool in epidemiology.\nIn statistics, a contingency table classifies individuals by two discrete variables, one that defines the rows and one that defines the columns. Each cell in the table contains the number of individuals who are in the intersection of the corresponding categories of the row and column variables. These numbers are called cell counts. The margins of the table contain row or column totals.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#contingency-tables",
    "href": "condprob.html#contingency-tables",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "",
    "text": "2.1.1 2x2 tables\nIn epidemiology, a 2x2 table is a contingency table based on a binary exposure variable and a binary disease outcome. We denote exposure by \\(X = 1\\) and no exposure by \\(X = 0\\), and we denote disease by \\(D = 1\\) and no disease by \\(D = 0\\). The precise definition of “disease” depends on context. In descriptive epidemiology, \\(D_i = 1\\) might mean that person \\(i\\) is a prevalent case of disease. In analytic epidemiology, \\(D_i = 1\\) might mean that person \\(i\\) had an onset of disease in an interval \\((t_\\text{start}, t_\\text{stop}]\\) on a relevant time scale. We put exposure in the rows and disease in the columns,2 and the exposure and disease categories are ordered so that individuals with \\(X = 1\\) and \\(D = 1\\) go in the top left corner. This is the most common arrangement in epidemiologic research, but it is not universal.\nTable 2.1 shows an example of a 2x2 table. There are \\(a\\) individuals with both exposure and disease, \\(b\\) individuals with exposure but not disease, \\(c\\) individuals with disease but no exposure, and \\(d\\) individuals with neither. In the rows, there are \\(r_1 = a + b\\) exposed individuals and \\(r_0 = c + d\\) unexposed individuals. In the columns, there are \\(k_1 = a + c\\) individuals who had a disease onset and \\(k_0 = b + d\\) individuals who did not. The row and column totals are called the margins of the table. The total number of individuals is \\(n = a + b + c + d\\).\n\n\n\nTable 2.1: 2x2 contingency table of exposure (\\(X\\)) and disease (\\(D\\)).\n\n\n\n\n\n\n\\(D = 1\\)\n\\(D = 0\\)\nTotal\n\n\n\n\n\\(X = 1\\)\n\\(a\\)\n\\(b\\)\n\\(r_1 = a + b\\)\n\n\n\\(X = 0\\)\n\\(c\\)\n\\(d\\)\n\\(r_0 = c + d\\)\n\n\nTotal\n\\(k_1 = a + c\\)\n\\(k_0 = b + d\\)\n\\(n = a + b + c + d\\)\n\n\n\n\n\n\n\n\n2.1.2 Conditional and marginal probabilities on a 2x2 table\nFor simplicity, we will assume that Table 2.1 represents the entire population \\(\\Omega\\). The conditional probability of disease given exposure is \\[\n    \\Pr(D = 1 \\given{} X = 1)\n    = \\frac{Pr(D = 1 \\vand X = 1)}{\\Pr(X = 1)}\n    = \\frac{a / n}{r_1 / n}\n    = \\frac{a}{r_1},\n\\] and the conditional probability of disease given no exposure is \\[\n  \\Pr(D = 1 \\given{} X = 0)\n  = \\frac{Pr(D = 1 \\vand X = 0)}{\\Pr(X = 0)}\n  = \\frac{c / n}{r_0 / n}\n  = \\frac{c}{r_0},\n\\] Similarly, the conditional probability of exposure given disease is \\[\n  \\Pr(X = 1 \\given{} D = 1)\n  = \\frac{\\Pr(X = 1 \\vand D = 1)}{\\Pr(D = 1)}\n  = \\frac{a / n}{k_1 / n}\n  = \\frac{a}{k_1},\n\\] and the conditional probabilty of exposure given no disease is \\[\n  \\Pr(X = 1 \\given{} D = 0)\n  = \\frac{\\Pr(X = 1 \\vand D = 0)}{\\Pr(D = 0)}\n  = \\frac{b / n}{k_0 / n}\n  = \\frac{b}{k_0}.\n\\] In all cases, the table total cancels out and we get a calculation in one row or one column.\nProbabilities in the original sample space \\(\\Omega\\) are called marginal probabilities because they are calculated using the margins of the contingency table. Marginal probabilities were used in the denominators of the conditional probabilities above. In Table 2.1, the marginal probability of exposure is \\[\n  \\Pr(X = 1) = \\frac{a + b}{a + b + c + d} = \\frac{r_1}{n},\n\\] and the marginal probability of disease is \\[\n  \\Pr(D = 1) = \\frac{a + c}{a + b + c + d} = \\frac{k_1}{n}.\n\\] Here, the marginal probabilities are probability masses from the marginal distribution of \\(X\\) (based on the row totals) or \\(D\\) (based on the column totals).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#multiplication-of-conditional-probabilities",
    "href": "condprob.html#multiplication-of-conditional-probabilities",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.2 Multiplication of conditional probabilities",
    "text": "2.2 Multiplication of conditional probabilities\nEquation 2.1 can be rearranged into \\[\n    \\Pr(A \\cap B) = \\Pr(B \\given{} A) \\Pr(A),\n\\tag{2.2}\\] exactly as described by Bayes at the beginning of this chapter (if we let \\(A\\) be the “1st event” and \\(B\\) be the “2d”). This depends only on the definition of conditional probability in Equation 2.1, not on any assumptions about the relationship between the events \\(A\\) and \\(B\\). This multiplication rule for conditional probabilities extends to any number of events. For three events \\(A\\), \\(B\\), and \\(C\\) such that \\(B \\cap C\\) and \\(C\\) have probabilities greater than zero, we have \\[\\begin{aligned}\n  \\Pr(A \\cap B \\cap C)\n  &= \\Pr(A \\given{} B \\cap C) \\Pr(B \\cap C) \\\\\n  &= \\Pr(A \\given{} B \\cap C) \\Pr(B \\given{} C) \\Pr(C).\n\\end{aligned}\\] To ensure that all of these conditional probabilities are well-defined, we need \\(B \\cap C\\) and \\(C\\) to have probabilities greater than zero. In practice, \\(\\Pr(A \\given{} B \\cap C)\\) is usually written \\(\\Pr(A \\given{} B, C)\\).\n\n2.2.1 Decision trees\nFigure 2.1 shows an example of a decision tree. The root of the tree is on the left and the leaves of the tree are on the right. Each node where two or more branches meet represents a decision. In the example, the root represents the decision \\(A\\) or \\(A^C\\) (i.e., not \\(A\\)). The two nodes connected to the root each represent the decision \\(B\\) or \\(B^C\\) (i.e., not \\(B\\)). Each branch of the tree is labeled with the conditional probability of the branch given the event that it branches out from. Because of the multiplication rule for conditional probabilities, the probability of each leaf is equal to the product of the probabilities along the branches connecting it to the root.\n\n\n\n\n\n\nFigure 2.1: A decision tree for events \\(A\\) and \\(B\\). The probability of each leaf is found by multiplying the probabilities along the branches leading from the root to the leaf.\n\n\n\n\n\n2.2.2 Independence of events\nThe events \\(A\\) and \\(B\\) are independent if \\[\n  \\Pr(A \\cap B) = \\Pr(A) \\Pr(B).\n\\tag{2.3}\\] When two events are independent, the occurrence (or not) of one event tells us nothing about whether the other event occurred: If \\(\\Pr(A) &gt; 0\\), equation Equation 2.3 is equivalent to \\(\\Pr(B \\given{} A) = \\Pr(B)\\). If \\(\\Pr(B) &gt; 0\\), it is equivalent to \\(\\Pr(A \\given{} B) = \\Pr(A)\\). If \\(A\\) and \\(B\\) are not independent, the occurrence of \\(A\\) contains information about the occurrence of \\(B\\) and vice versa.\nIndependence of events \\(A\\) and \\(B\\) implies that the events \\(A\\) and \\(B^\\comp\\) are also independent: \\[\n  \\begin{aligned}\n    \\Pr\\bigl(A \\cap B^\\comp\\bigr)\n    &= \\Pr(A) - \\Pr(A \\cap B) \\\\\n    &= \\Pr(A) - \\Pr(A) \\Pr(B) \\\\\n    &= \\Pr(A) \\big(1 - \\Pr(B)\\big) \\\\\n    &= \\Pr(A) \\Pr\\bigl(B^\\comp\\bigr).\n  \\end{aligned}\n\\] A similar argument shows that \\(A^\\comp\\) and \\(B\\) are independent. Because \\(A^\\comp \\cap B^\\comp = (A \\cup B)^\\comp\\) by DeMorgan’s laws (see Section 1.1.5), \\[\n  \\begin{aligned}\n    \\Pr\\bigl(A^\\comp \\cap B^\\comp\\bigr)\n    &= 1 - \\Pr(A \\cup B) \\\\\n    &= 1 - \\Pr(A) - \\Pr(B) - \\Pr(A \\cap B) \\\\\\\n    &= 1 - \\Pr(A) - \\Pr(B) - \\Pr(A) \\Pr(B) \\\\\n    &= \\big(1 - \\Pr(A)\\big) \\big(1 - \\Pr(B)\\big) \\\\\n    &= \\Pr\\bigl(A^\\comp\\bigr) \\Pr\\bigl(B^\\comp\\bigr).\n  \\end{aligned}\n\\] Therefore, independence of two events implies independence between any combination of themselves or their complements.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#sensitivity-and-specificity",
    "href": "condprob.html#sensitivity-and-specificity",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.3 Sensitivity and specificity",
    "text": "2.3 Sensitivity and specificity\nIn the epidemiology of screening and diagnostic tests, several of the most important concepts are conditional probabilities. If we classify disease status into diseased (\\(D^+\\)) and nondiseased (\\(D^-\\)) and the test result into positive (\\(T^+\\)) and negative (\\(T^-\\)), we have the four possible combinations Table 2.2.\n\n\n\nTable 2.2: Disease status (\\(D^+\\)/\\(D^-\\)) and test result (\\(T^+\\)/\\(T^-\\)).\n\n\n\n\n\n\n\\(T^+\\)\n\\(T^-\\)\n\n\n\n\n\\(D^+\\)\nTrue positive\nFalse negative\n\n\n\\(D^-\\)\nFalse positive\nTrue negative\n\n\n\n\n\n\nThe sensitivity of a test is the conditional probability that the test is positive given that the individual tested has the disease: \\[\n  \\text{sensitivity} = \\Pr(T^+ \\given{} D^+).\n\\] The specificity of a test is the conditional probability that the test is negative given that the individual tested does not have the disease: \\[\n  \\text{specificity} = \\Pr(T^- \\given{} D^-).\n\\] In both cases, we are conditioning on the disease status of the individual being tested. These concepts were introduced by Yerushalmy (1947) in a comparison of different types of chest X-rays for tuberculosis case detection.\n\nR\n\n\n\n\nsensspec.R\n\n## Sensitivity and specificity\n\n# generate diagnostic testing data\nset.seed(42)\nn &lt;- 500\ndtdat &lt;- data.frame(disease = rbinom(n, 1, 0.5))\ndtdat$testpos &lt;- ifelse(dtdat$disease,\n                        rbinom(n, 1, 0.85), rbinom(n, 1, 0.05))\n\n# prevalence\nmean(dtdat$disease)\n# Pr(T+)\nmean(dtdat$testpos)\n\n# sensitivity\nmean(dtdat$testpos[dtdat$disease == TRUE])\nsum(dtdat$disease & dtdat$testpos) / sum(dtdat$disease)\n\n# specificity\n1 - mean(dtdat$testpos[dtdat$disease == FALSE])\nmean(!dtdat$testpos[dtdat$disease == FALSE])\n\n\n\n\nMaximizing either sensitivity or specificity alone does not necessarily lead to good screening or diagnostic test: A test where everyone tests positive has perfect sensitivity but zero specificity, and a test where everyone tests negative has perfect specificity but zero sensitivity. There is almost always a tradeoff where higher sensitivity leads to lower specificity and vice versa.\n\n2.3.1 Example: Diabetes testing\nRemein and Wilkerson (1961) describe an early study of diabetes screening conducted by the United States Public Health Service in Boston City Hospital between 1954 and 1957. They recruited early-morning patients who were not febrile or acutely ill. Those willing to participate gave urine and blood samples. Next, they were given a meal meant to approximate an average breakfast or light lunch (a sandwich, 5 grams of butter, 60 grams of cheese, and three filled cookies). After the meal, they gave further urine and blood samples at one, two, and three hours after eating. The samples were analyzed using four different blood tests and six different urine tests. Participants returned for a follow-up visit between 3 and 21 days after the screening tests, where a definitive diagnosis of diabetes was made using an oral glucose tolerance test and a physical examination according to criteria established by a group of experts.\nA total of 595 participants completed both visits. Table 2.3 is a reconstruction of the data for the Somogyi-Nelson blood test based on the 580 participants (70 with diabetes and 510 without) who took the test at all four time points. In the table, a positive test is defined as a blood glucose concentration above 130 mg/dL (milligrams per deciliter).\n\n\n\nTable 2.3: Sensitivity and specificity of the Somogyi-Nelson blood glucose test for diabetes where \\(T^+\\) corresponds to a concentration above 130 mg/dL.\n\n\n\n\n\n\n\\(T^+\\)\n\\(T^-\\)\nSensitivity and specificity\n\n\n\n\nBefore meal\n\n\n\\(D^+\\)\n31\n39\n\\(\\text{sens} = 31 / 70 \\approx 0.443\\)\n\n\n\\(D^-\\)\n5\n505\n\\(\\text{spec} = 505 / 510 \\approx 0.990\\)\n\n\nOne hour after meal\n\n\n\\(D^+\\)\n55\n15\n\\(\\text{sens} = 55 / 70 \\approx 0.786\\)\n\n\n\\(D^-\\)\n48\n462\n\\(\\text{spec} = 462 / 510 \\approx 0.906\\)\n\n\nTwo hours after meal\n\n\n\\(D^+\\)\n45\n25\n\\(\\text{sens} = 45 / 70 \\approx 0.643\\)\n\n\n\\(D^-\\)\n16\n494\n\\(\\text{spec} = 494 / 510 \\approx 0.969\\)\n\n\nThree hours after meal\n\n\n\\(D^+\\)\n34\n36\n\\(\\text{sens} = 34 / 70 \\approx 0.486\\)\n\n\n\\(D^-\\)\n1\n509\n\\(\\text{spec} = 509 / 510 \\approx 0.998\\)\n\n\n\n\n\n\n\nR\n\n\n\n\nRWtable.R\n\n## Table 2 from Remein and Wilkerson (Journal of Chronic Disease, 1961)\n\n# function to generate numbers based on sensitivity and specificity\nRWtable &lt;- function(sens, spec, n1=70, n0=510) {\n  # arguments:  sensitivity, specificity,\n  #             n1 is number of diabetics, n0 is number of nondiabetics\n  tp &lt;- round(sens * n1)\n  fp &lt;- round((1 - spec) * n0)\n  tn &lt;- round(spec * n0)\n  fn &lt;- round((1 - sens) * n1)\n  return(c(truepos = tp, falsepos = fp, trueneg = tn, falseneg = fn))\n}\n\nRWtable(0.443, 0.990)   # before meal\nRWtable(0.786, 0.906)   # one hour after\nRWtable(0.643, 0.969)   # two hours after\nRWtable(0.486, 0.998)   # three hours after\n\n\n\n\n\n\n2.3.2 Receiver operating characteristic (ROC) curves*\nThe tradeoff between sensitivity and sensitivity in choosing a cutoff to distinguish positive and negative tests can be seen using a receiver operating characteristic (ROC) curve (Lusted 1971a, 1971b; Swets 1988; Zweig and Campbell 1993). These curves were originally used in World War II to analyze the performance of radar systems locating ships and airplanes. They were applied to diagnostic tests in the late 1950s in the first attempt to automate the classification of Pap smears to detect cervical cancer (Bostrom, Sawyer, and Tolles 1959; Lusted 1984; Bengtsson and Malm 2014).\nThe horizontal axis of an ROC curve plots \\[\n  1 - \\text{specificity} = \\Pr(T^+ \\given{} D^-),\n\\] and its vertical axis plots sensitivity, which is \\(\\Pr(T^+ \\given{} D^+)\\). In general, the best test is the one whose ROC curve stays closest to the top left corner, which represents a test with perfect sensitivity and specificity.\nFigure 2.2 shows four ROC curves based on data from Remein and Wilkerson (1961): one for the Somogyi-Nelson blood test before the meal and one each for the tests one, two, and three hours after the meal. For all four tests, the curves are based on the combinations of sensitivity and specificity for glucose concentration cutoffs from 70 mg/dL to 200 mg/dL. In these tests, using a higher glucose concentration cutoff to define a positive test leads to lower sensitivity and higher specificity.\n\n\n\nCode\n\nROCcurve.R\n\n# data from Table 2 in Remein and Wilkerson (Journal of Chronic Disease, 1961)\nSNdat &lt;- data.frame(cutoff = seq(70, 200, by = 10))\nSNdat$sens_pre &lt;- c(95.7, 91.4, 82.9, 65.7, 54.3, 50.0, 44.3, 37.1, 30.0,\n                    25.7, 25.7, 22.9, 21.4, 17.1) / 100\nSNdat$spec_pre &lt;- c(11.0, 36.3, 65.7, 84.7, 92.7, 96.7, 99.0, 99.6, 99.8,\n                    99.8, 99.8, 99.8, 100.0, 100.0) / 100\nSNdat$sens_1hr &lt;- c(100.0, 97.1, 97.1, 95.7, 92.9, 88.6, 78.6, 68.6, 57.1,\n                    52.9, 47.1, 40.0, 34.3, 28.6) / 100\nSNdat$spec_1hr &lt;- c(8.2, 22.4, 39.0, 57.3, 70.6, 83.3, 90.6, 95.1, 97.8,\n                    99.4, 99.6, 99.8, 100.0, 100.0) / 100\nSNdat$sens_2hr &lt;- c(98.6, 97.1, 94.3, 88.6, 85.7, 71.4, 64.3, 57.1, 50.0,\n                    47.1, 42.9, 38.6, 34.3, 27.1) / 100\nSNdat$spec_2hr &lt;- c(8.8, 25.5, 47.6, 69.8, 84.1, 92.5, 96.9, 99.4, 99.6,\n                    99.8, 100.0, 100.0, 100.0, 100.0) / 100\nSNdat$sens_3hr &lt;- c(94.3, 91.4, 82.9, 70.0, 60.0, 51.4, 48.6, 41.4, 32.9,\n                    28.6, 28.6, 28.6, 24.3, 20.0) / 100\nSNdat$spec_3hr &lt;- c(8.6, 34.7, 67.5, 86.5, 95.3, 98.2, 99.8,\n                    rep(100.0, 7)) / 100\n# write.csv(SNdat, \"SNdat.csv\", row.names = FALSE)\n\n# ROC curves with labels\nplot(1 - SNdat$spec_pre, SNdat$sens_pre, type = \"n\",\n     xlim = c(0, 1), ylim = c(0, 1),\n     xlab = \"1 - Specificity = Pr(T+ | D-)\",\n     ylab = \"Sensitivity = Pr(T+ | D+)\")\ngrid()\nlines(1 - SNdat$spec_pre, SNdat$sens_pre, col = \"darkgray\")\nlines(1 - SNdat$spec_1hr, SNdat$sens_1hr, lty = \"solid\")\nlines(1 - SNdat$spec_2hr, SNdat$sens_2hr, lty = \"dashed\")\nlines(1 - SNdat$spec_3hr, SNdat$sens_3hr, lty = \"dotted\")\npoints(1 - SNdat[SNdat$cutoff == 130, c(3, 5, 7, 9)],\n       SNdat[SNdat$cutoff == 130, c(2, 4, 6, 8)])\npoints(1 - SNdat$spec_pre[seq(2, 12, by = 2)],\n       SNdat$sens_pre[seq(2, 12, by = 2)], pch = 8)\ntext(1 - SNdat$spec_pre[seq(2, 12, by = 2)] + c(0, .09, .09, .1, .1, .1),\n     SNdat$sens_pre[seq(2, 12, by = 2)] + c(-.05, -.02, -.02, 0, 0, -.01),\n     labels = c(\"80 mg/dL\", \"100 mg/dL\", \"120 mg/dL\", \"140 mg/dL\",\n                \"160 mg/dL\", \"180 mg/dL\"))\nabline(0, 1, lty = \"dotted\", col = \"darkgray\")\ntext(.51, .49, adj = c(.5, 1), srt = 42,\n     label = \"Useless tests (T and D independent)\")\npoints(c(0, 0, 1), c(0, 1, 1), pch = 3)\ntext(.01, .99, adj = c(0, 1), label = \"Perfect test\")\ntext(.01, .01, adj = c(0, 0), label = \"Everyone tests negative\")\ntext(.99, .99, adj = c(1, 0), srt = 90, label = \"Everyone tests positive\")\nlegend(\"bottomright\", bg = \"white\",\n       lty = c(\"solid\", \"solid\", \"dashed\", \"dotted\", NA),\n       col = c(\"darkgray\", rep(\"black\", 4)), pch = c(rep(NA, 4), 1),\n       legend = c(\"Before meal  (AUC = 0.825)\",\n                  \"1 hour after   (AUC = 0.923)\",\n                  \"2 hours after (AUC = 0.904)\",\n                  \"3 hours after (AUC = 0.839)\", \"130 mg/dL cutoff\"))\n\n\n\n\n\n\n\n\n\nFigure 2.2: ROC curves for Somogyi-Nelson blood tests conducted before the mean and at 1-3 hours after the meal. Cutoff values for the before-meal ROC curve are labeled, and the points corresponding to the 130 mg/dL cutoff along each curve are circled.\n\n\n\n\n\nROC curves are sometimes compared using the area under the curve (AUC), with greater area under the curve corresponding roughly to a better test (Bamber 1975; Hanley and McNeil 1982). For a test that is positive when a clinical measurement is above a given cutoff, the AUC is the probability that a person with disease has a higher value than a person without disease.3 In this example, it is the probability that a true diabetic has a higher blood glucose concentration than a true nondiabetic at the time blood glucose concentration is measured. The AUCs in Figure 2.2 show clearly that the tests one and two hours after the meal, which have curves above and to the left of the other two curves, are more accurate than the tests before and three hours after the meal. This is biologically plausible: Before the meal, there is no glucose load. Three hours after the meal, the glucose from the meal has largely been processed.\n\nR\n\n\n\n\nauc.R\n\n## areas under the ROC curves\n\n# load Somogyi-Nelson test data generated for Figure 2.2 (if needed)\n# The argument can contain a path before the file name.\nSNdat &lt;- read.csv(\"SNdat.csv\")\n\nauc &lt;- function(x, y) {\n  # x is an increasing list of specificities\n  # y is a decreasing list of sensitivities\n  roc &lt;- approxfun(c(1, 1 - x, 0), c(1, y, 0), ties = \"max\")\n  area &lt;- integrate(function(x) roc(x), 0, 1)\n  return(area)\n}\nauc(SNdat$spec_pre, SNdat$sens_pre)\nauc(SNdat$spec_1hr, SNdat$sens_1hr)\nauc(SNdat$spec_2hr, SNdat$sens_2hr)\nauc(SNdat$spec_3hr, SNdat$sens_3hr)\n\n\n\n\nThe test one hour after the meal with a 130 mg/dL cutoff has a good combination of sensitivity and specificity. It is near the top left corner, where perfect tests live. If a diagnostic test was completely useless, the test results (\\(T^+\\) or \\(T^-\\)) would be independent of disease status (\\(D^+\\) or \\(D^-\\)). In that case, \\[\n  \\Pr(T^+ \\given{} D^+) = \\Pr(T^+ \\given{} D^-) = \\Pr(T^+).\n\\] Thus, the ROC curve for a useless test follows the diagonal line from the lower left corner \\((0, 0)\\) to the upper right corner \\((1, 1)\\), and it has an AUC of 0.5. Tests below the diagonal on an ROC curve are worse than useless: the definitions of positive and negative should be reversed.\nThe sensitivity and specificity of a test tell us how accurate it is with a given definition of positive and negative. The ROC curve shows us how this accuracy depends on the cutoff between positive and negative tests, and the area under the curve shows us how well the underlying clinical measurement (e.g., blood glucose concentration) can distinguish between people with and without disease. However, the best cutoff for a test depends on its purpose, the population to be tested, and the benefit of identifying a true positive or negative versus the harm of a false positive or negative (Blumberg 1957; Kessel 1962).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#law-of-total-probability",
    "href": "condprob.html#law-of-total-probability",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.4 Law of total probability",
    "text": "2.4 Law of total probability\nSuppose \\(A_1, \\ldots, A_n\\) are disjoint events such that their union is \\(\\Omega\\). This is called a partition of \\(\\Omega\\). An important special case is when we partition \\(\\Omega\\) into \\(A\\) and \\(A^\\comp\\).\nLet \\(B\\) be another event. Every \\(\\omega \\in B\\) is in exactly one of the \\(A_i\\). For each \\(i\\), \\(B \\cap A_i\\) is the part of \\(B\\) that is contained in \\(A_i\\). The event \\(B\\) is the union of these subsets: \\[\n  B = \\bigcup_{i = 1}^n (B \\cap A_i).\n\\] Because \\(A_i\\) are disjoint, so are the subsets \\(B \\cap A_i\\). By the addition rule for probabilities of disjoint sets, we have \\[\n  \\Pr(B) = \\sum_{i = 1}^n \\Pr(B \\cap A_i)\n\\] which is the sum of the \\(\\Pr(B \\cap A_i)\\).4 Using the multiplication rule for conditional probabilities in Equation 2.2 on each \\(\\Pr(B \\cap A_i)\\), we get \\[\n  \\Pr(B) = \\sum_{i = 1}^n \\Pr(B \\given{} A_i) \\Pr(A_i).\n\\] This is called the law of total probability.\n\n2.4.1 Example: probability of a positive or negative test\nWe can use the law of total probability to calculate the probability of a positive or negative test based on the sensitivity and specificity of the test and the prevalence of disease. Because all individuals either do or do not have the disease,5 we have \\[\n  T^+ = (T^+ \\cap D^+) \\cup (T^+ \\cap D^-).\n\\] These two groups are mutually exclusive, so \\[\n  \\Pr(T^+) = \\Pr(T^+ \\cap D^+) + \\Pr(T^+ \\cap D^-).\n\\] We can calculate each probability on the right-hand side using the multiplication rule in Equation 2.2: \\[\n  \\begin{aligned}\n    \\Pr(T^+ \\cap D^+)\n    &= \\Pr(T^+ \\given{} D^+) \\Pr(D^+)\n      = \\text{sensitivity} \\times \\text{prevalence},\\\\\n    \\Pr(T^+ \\cap D^-)\n    &= \\Pr(T^+ \\given{} D^-) \\Pr(D^-)\n      = (1 - \\text{specificity}) \\times (1 - \\text{prevalence}).\n  \\end{aligned}\n\\] Putting everything together, we get \\[\n  \\begin{aligned}\n    \\Pr(T^+)\n    &= \\Pr(T^+ \\given{} D^+) \\Pr(D^+) + \\Pr(T^+ \\given{} D^-) \\Pr(D^-) \\\\\n    &= \\text{sensitivity} \\times \\text{prevalence}\n      + (1 - \\text{specificity}) \\times (1 - \\text{prevalence}).\n  \\end{aligned}\n\\tag{2.4}\\] A similar chain of reasoning shows that \\[\n  \\Pr(T^-) = (1 - \\text{sensitivity}) \\times \\text{prevalence} + \\text{specificity} \\times (1 - \\text{prevalence}),\n\\] which equals \\(1 - \\Pr(T^+)\\).\nFigure 2.3 shows how the probability of a positive test depends on the prevalence of disease using the example of the Somogyi-Nelson test one hour after the meal in Table 2.3. With a cutoff of 130 mg/dL, the test has a sensitivity of 0.786 and a specificity of 0.906. At low prevalences, the test overestimates the prevalence of diabetes due to imperfect specificity. A high prevalences, it underestimates the prevalence of diabetes due to imperfect sensitivity. The errors cancel out somewhere near a prevalence of 30%.\n\n\n\nCode\n\ntestpos.R\n\n## probability of testing positive as a function of prevalence\n\n# function to generate testing data\ntdat &lt;- function(prev, sens=0.786, spec=0.906) {\n  # defaults are sensitivity and sensitivity one hour after the meal\n  truepos &lt;- sens * prev\n  falsepos &lt;- (1 - spec) * (1 - prev)\n  trueneg &lt;- spec * (1 - prev)\n  falseneg &lt;- (1 - spec) * prev\n  pos &lt;- truepos + falsepos\n  neg &lt;- 1 - pos\n  ppv &lt;- truepos / pos\n  npv &lt;- trueneg / neg\n  return(data.frame(prev = prev, sens = sens, spec = spec,\n                    truepos = truepos, falsepos = falsepos,\n                    trueneg = trueneg, falseneg = falseneg,\n                    pos = pos, neg = neg, ppv = ppv, npv = npv))\n}\ntdat_1hr &lt;- tdat(seq(0, 1, by = .01))\nwrite.csv(tdat_1hr, \"R/tdat_1hr.csv\", row.names = FALSE)\n\n# plot\nplot(tdat_1hr$prev, tdat_1hr$pos, type = \"n\", xlim = c(0, 1), ylim = c(0, 1),\n     xlab = \"Prevalence of disease = Pr(D+)\",\n     ylab = \"Probability of positive test = Pr(T+)\")\npolygon(c(tdat_1hr$prev, 1, 0), c(tdat_1hr$pos, 0, 0),\n        border = NA, col = \"gray\")\npolygon(c(tdat_1hr$prev, 1, 0), c(tdat_1hr$falsepos, 0, 0),\n        border = NA, col = \"darkgray\")\ngrid()\nlines(tdat_1hr$prev, tdat_1hr$falsepos, col = \"gray\")\nlines(tdat_1hr$prev, tdat_1hr$pos)\nabline(0, 1, lty = \"dotted\")\ntext(0.1, 0.02, adj = c(0, 0), label = \"False positives\")\ntext(0.2, 0.1, adj = c(0, 0), label = \"True positives\")\n\n\n\n\n\n\n\n\n\nFigure 2.3: The probability of a positive Somogyi-Nelson diabetes test one hour after the meal as a function of the hypothetical prevalence of diabetes. The black dotted line shows the true prevalence of diabetes.\n\n\n\n\n\n\n\n2.4.2 Standardization\nIn epidemiology, it is often useful to think of our sample space \\(\\Omega\\) as a population and the outcomes \\(\\omega \\in \\Omega\\) as individuals. The sets \\(A_1, \\ldots, A_n\\) into which we partition the sample space are disjoint subpopulations (e.g., age groups). Let \\(\\Pr(D \\given{} A_i)\\) be the prevalence of disease in subpopulation \\(A_i\\) at a given time point. Then the overall prevalence of disease is \\[\n  \\Pr(D) = \\sum_{i = 1}^n \\Pr(D \\given{} A_i) \\Pr(A_i).\n\\tag{2.5}\\] This application of the law of total probability is called standardization. By changing the \\(\\Pr(A_i)\\), we can use the subpopulation prevalences to calculate the prevalence of disease in a population with any desired composition of subpopulations. Equation 2.5 can also be used to calculate population-level risk from the subpopulation-specific risks in any given time interval. In the form of standardization, the law of total probability is one of the most important tools in epidemiology.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#bayes-rule",
    "href": "condprob.html#bayes-rule",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.5 Bayes’ rule",
    "text": "2.5 Bayes’ rule\nBayes’ rule (Bayes 1763) relates the two conditional probabilities \\(\\Pr(A \\given{} B)\\) and \\(\\Pr(B \\given{} A)\\): \\[\n  \\Pr(A \\given{} B) = \\frac{\\Pr(B \\cap A)}{\\Pr(B)} = \\frac{\\Pr(B \\given{} A) \\Pr(A)}{\\Pr(B)}.\n\\tag{2.6}\\] Often, the law of total probability is used to calculate \\(\\Pr(B)\\) in the denominator. For example: \\[\n  \\Pr(A \\given{} B) = \\frac{\\Pr(B \\given{} A) \\Pr(A)}{\\Pr(B \\given{} A) \\Pr(A) + \\Pr(B \\given{} A^\\comp) \\Pr(A^\\comp)}.\n\\] Bayes’ rule is an incredibly useful application of conditional probabilities, and it forms the theoretical foundation for Bayesian statistical inference.\n\n2.5.1 Positive and negative predictive values\nSensitivity and specificity tell us how disease status predicts the result of a test, but they do not tell us how to interpret a test result. If you test positive, it is important to know the conditional probability that you truly have disease given that you tested positive. This is called the positive predictive value (PPV): \\[\n  \\text{PPV} = \\Pr(D^+ \\given{} T^+).\n\\] If you test negative, it is important to know the conditional probability that you are truly disease-free given that you tested negative. This is called the negative predictive value (NPV): \\[\n  \\text{NPV} = \\Pr(D^- \\given{} T^-).\n\\] These terms were introduced by Vecchio (1966). Table 2.4 shows the PPV and NPV for the Somogyi-Nelson diabetes tests from Table 2.3.\n\n\n\nTable 2.4: PPV and NPV of the Somogyi-Nelson blood glucose test for diabetes where \\(T^+\\) corresponds to a concentration above 130 mg/dL.\n\n\n\n\n\n\n\\(T^+\\)\n\\(T^-\\)\nPPV and NPV\n\n\n\n\nBefore meal\n\n\n\\(D^+\\)\n31\n39\n\\(\\text{PPV} = 31 / 36 \\approx 0.861\\)\n\n\n\\(D^-\\)\n5\n505\n\\(\\text{NPV} = 505 / 544 \\approx 0.928\\)\n\n\nTotal\n36\n544\n\n\n\nOne hour after meal\n\n\n\\(D^+\\)\n55\n15\n\\(\\text{PPV} = 55 / 103 \\approx 0.534\\)\n\n\n\\(D^-\\)\n48\n462\n\\(\\text{NPV} = 462 / 477 \\approx 0.969\\)\n\n\nTotal\n103\n477\n\n\n\nTwo hours after meal\n\n\n\\(D^+\\)\n45\n25\n\\(\\text{PPV} = 45 / 61 \\approx 0.738\\)\n\n\n\\(D^-\\)\n16\n494\n\\(\\text{NPV} = 494 / 519 \\approx 0.952\\)\n\n\nTotal\n61\n519\n\n\n\nThree hours after meal\n\n\n\\(D^+\\)\n34\n36\n\\(\\text{PPV} = 34 / 35 \\approx 0.971\\)\n\n\n\\(D^-\\)\n1\n509\n\\(\\text{NPV} = 509 / 545 \\approx 0.934\\)\n\n\nTotal\n35\n545\n\n\n\n\n\n\n\nVecchio (1966) showed that the PPV and NPV depend on the prevalence of disease as well as the sensitivity and specificity of the test. To calculate the PPV and NPV, we use Bayes’ rule to switch the conditional probabilities from \\(\\Pr(T \\given{} D)\\) to \\(\\Pr(D \\given{} T)\\). From the definition of PPV and Bayes’ rule, we get \\[\n  \\Pr(D^+ \\given{} T^+)\n  = \\frac{\\Pr(T^+ \\cap D^+)}{\\Pr(T^+)}\n  = \\frac{\\Pr(T^+ \\given{} D^+) \\Pr(D^+)}{\\Pr(T^+)}.\n\\] The sensitivity of the test and the prevalence of disease are in the numerator, and \\(\\Pr(T+)\\) is in Equation 2.4. Putting this all together, we get \\[\n  \\text{PPV}\n  = \\frac{\\text{sensitivity} \\times \\text{prevalence}}{\\text{sensitivity} \\times \\text{prevalence}\n    + (1 - \\text{specificity}) \\times (1 - \\text{prevalence})}.\n\\] The numerator is the probability of a true positive test, and the denominator is the probability of a (true or false) positive test. By a similar argument, \\[\n  \\text{NPV} = \\frac{\\text{specificity} \\times (1 - \\text{prevalence})}{\\text{specificity} \\times (1 - \\text{prevalence})\n    + (1 - \\text{sensitivity}) \\times \\text{prevalence}}.\n  \\label{eq:npv}\n\\] The numerator is the probability of a true negative test, and the denominator is the probability of a (true or false) negative test.\nFigure 2.4 shows how the positive and negative predictive values of a test depend on the prevalence of disease for the Somogyi-Nelson test before the meal and one hour after the meal in Remein and Wilkerson (1961). With a cutoff of 130 mg/dL, the sensitivity and specificity are \\(0.443\\) and \\(0.990\\) before the meal and \\(0.786\\) and \\(0.906\\) one hour after the meal. As prevalence increases, PPV increases and NPV decreases. A perfect test would have PPV and NPV equal to one at all prevalences.\n\n\n\nCode\n\npredval.R\n\n## Predictive values as a function of prevalence\n\n# uses tdat_1hr data and tdat() function from Figure 2.3 (testpos.R)\n# tdat_1hr &lt;- read.csv(\"tdat_1hr.csv\")\n# generate data using the sensitivity and specificity of the pre-meal test\ntdat_pre &lt;- tdat(seq(0, 1, by = .01), sens = 0.443, spec = 0.990)\n\n# plot of PPV and NPV as a function of diabetes prevalence\nplot(tdat_1hr$prev, tdat_1hr$ppv, type = \"n\", xlim = c(0, 1), ylim = c(0, 1),\n     xlab = \"Prevalence of disease = Pr(D+)\",\n     ylab = \"Predictive value = Pr(D | T)\")\ngrid()\nlines(tdat_1hr$prev, tdat_1hr$ppv)\nlines(tdat_1hr$prev, tdat_1hr$npv, lty = \"dashed\")\nlines(tdat_pre$prev, tdat_pre$ppv, col = \"darkgray\")\nlines(tdat_pre$prev, tdat_pre$npv, lty = \"dashed\", col = \"darkgray\")\nlegend(\"bottom\", lty = c(\"solid\", \"dashed\", \"solid\", \"dashed\"),\n       col = c(\"darkgray\", \"darkgray\", \"black\", \"black\"),\n       bg = \"white\", inset = 0.05,\n       legend = c(\"PPV before meal\", \"NPV before meal\",\n                  \"PPV 1 hour after\", \"NPV 1 hour after\"))\n\n\n\n\n\n\n\n\n\nFigure 2.4: Positive and negative predictive values of the Somogyi-Nelson diabetes test before the meal (gray) and one hour after the meal (black) as a function of diabetes prevalence.\n\n\n\n\n\n\n\n2.5.2 Likelihood ratios*\nFor a probability \\(p\\), the odds is \\[\n  \\theta = \\frac{p}{1 - p}.\n\\] While a probability lives in \\([0, 1]\\), the odds can go from zero (for \\(p = 0\\)) to infinity (as \\(p\\) approaches one). There is a one-to-one relationship between probabilities and odds, so we can calculate the probability of an event if we know the odds. If the odds is \\(\\theta\\), the corresponding probability is \\[\n  p = \\frac{\\theta}{1 + \\theta}.\n\\] Odds and odds ratios have an important role in epidemiology and statistical inference. In a Bayesian statistical framework, odds ratios give us a simple way to update our knowledge about the probability of an event given new information.\nSuppose we know the prevalence of a disease in a population \\(\\Omega\\). We randomly sample an individual \\(\\omega \\in \\Omega\\) and give them a diagnostic test. If we randomly sample an individual \\(\\omega\\) from a population \\(\\Omega\\), the odds that \\(\\omega\\) has disease is \\[\n  \\frac{\\Pr(D^+)}{1 - \\Pr(D^+)}\n  = \\frac{\\Pr(D^+)}{\\Pr(D^-)}.\n\\] where \\(\\Pr(D^+)\\) is the prevalence of disease. This is called the prior odds of disease. If \\(\\omega\\) tests positive for the disease, the conditional odds that they have disease is \\[\n  \\frac{PPV}{1 - PPV}\n  = \\frac{\\Pr(D^+ \\given{} T^+)}{\\Pr(D^- \\given{} T^+)}\n  = \\frac{\\Pr(D^+ \\cap T^+)}{\\Pr(D^- \\cap T^+)},\n\\] where we have cancelled out \\(\\Pr(T^+)\\) from the numerator and the denominator in the last expression. This is called the posterior odds of disease. The second expression above shows that the probability corresponding to the posterior odds is the PPV.\nUsing the multiplication rule for conditional probabilities, we get \\[\n  \\frac{\\Pr(D^+ \\cap T^+)}{\\Pr(D^- \\cap T^+)}\n  = \\frac{\\Pr(T^+ \\given D^+) \\Pr(D^+)}{\\Pr(T^+ \\given D^-) \\Pr(D^-)}\n  = \\frac{\\text{sensitivity}}{1 - \\text{specificity}}\n    \\times \\frac{\\Pr(D^+)}{\\Pr(D^-)}.\n\\] The term \\(\\text{sensitivity} / (1 - \\text{specificity})\\) is called the likelihood ratio. If our individual \\(\\omega\\) tests positive for disease, \\[\n  \\text{posterior odds of } D^+\n  = \\text{likelihood ratio} \\times \\text{prior odds of } D^+.\n\\] The likelihood ratio is a measure of how much we learn from a positive test result, and it does not depend on the prevalence of disease [Lusted (1971b); Swets (1973); Fagan (1975); Albert (1982); Zweig and Campbell (1993)}. Because an ROC curve plots sensitivity on the vertical axis and \\(1 - \\text{specificity}\\) on the horizontal axis, the likelihood ratio for a given test is the slope of the line from the point \\((0, 0)\\) to the point representing the test.\nTable 2.5 shows the prior odds, likelihood ratio, posterior odds, and PPV for the Somogyi-Nelson blood glucose tests for diabetes from 580 participants (70 with diabetes and 510 without) in Remein and Wilkerson (1961). Note that the tests with the highest likelihood ratios come from the glucose measurements that had the lowest AUCs in Figure 2.2. These tests have high likelihood ratios despite their low sensitivity because they have specificities near one. The test with the best combination of sensitivity and specificity in Table 2.3 has the lowest likelhood ratio. Like other summaries of diagnostic test performance, the likelihood ratio by itself does not determine the best test for a given purpose.\n\n\n\nTable 2.5: Prior odds, likelihood ratios, posterior odds, and PPV for the Somogyi-Nelson blood glucose test for diabetes where \\(T^+\\) corresponds to a concentration above 130 mg/dL.\n\n\n\n\n\nTest\nPrior odds\nLikelihood ratio\nPosterior odds\nPPV\n\n\n\n\nBefore meal\n\\(70 / 510 \\approx 0.137\\)\n45.171\n\\(31 / 5 = 6.200\\)\n\\(31 / 36 \\approx 0.861\\)\n\n\n1 hour after\n\\(70 / 510 \\approx 0.137\\)\n8.348\n\\(55 / 48 \\approx 1.146\\)\n\\(55 / 103 \\approx 0.534\\)\n\n\n2 hours after\n\\(70 / 510 \\approx 0.137\\)\n20.491\n\\(45 / 16 \\approx 2.813\\)\n\\(45 / 61 \\approx 0.738\\)\n\n\n3 hours after\n\\(70 / 510 \\approx 0.137\\)\n247.714\n\\(34 / 1 = 34.000\\)\n\\(34 / 35 \\approx 0.971\\)\n\n\n\n\n\n\n\n\n\n\nAlbert, Adelin. 1982. “On the Use and Computation of Likelihood Ratios in Clinical Chemistry.” Clinical Chemistry 28 (5): 1113–19.\n\n\nBamber, Donald. 1975. “The Area Above the Ordinal Dominance Graph and the Area Below the Receiver Operating Characteristic Graph.” Journal of Mathematical Psychology 12 (4): 387–415.\n\n\nBayes, Thomas. 1763. “LII. An Essay Towards Solving a Problem in the Doctrine of Chances. By the Late Rev. Mr. Bayes, FRS Communicated by Mr. Price, in a Letter to John Canton, AMFRS.” Philosophical Transactions of the Royal Society of London 53: 370–418.\n\n\nBengtsson, Ewert, and Patrik Malm. 2014. “Screening for Cervical Cancer Using Automated Analysis of PAP-Smears.” Computational and Mathematical Methods in Medicine 2014: 842037.\n\n\nBlumberg, Mark S. 1957. “Evaluating Health Screening Procedures.” Operations Research 5 (3): 351–60.\n\n\nBostrom, RC, HS Sawyer, and WE Tolles. 1959. “Instrumentation for Automatically Prescreening Cytological Smears.” Proceedings of the IRE 47 (11): 1895–1900.\n\n\nFagan, Terrence J. 1975. “Nomogram for Bayes’s Theorem.” New England Journal of Medicine 293 (5): 257.\n\n\nHanley, James A, and Barbara J McNeil. 1982. “The Meaning and Use of the Area Under a Receiver Operating Characteristic (ROC) Curve.” Radiology 143 (1): 29–36.\n\n\nKessel, Elton. 1962. “Diabetes Detection: An Improved Approach.” Journal of Chronic Diseases 15 (12): 1109–21.\n\n\nLusted, Lee B. 1971a. “Decision-Making Studies in Patient Management.” New England Journal of Medicine 284 (8): 416–24.\n\n\n———. 1971b. “Signal Detectability and Medical Decision-Making.” Science 171 (3977): 1217–19.\n\n\n———. 1984. “ROC Recollected.” Medical Decision Making 4: 131–35.\n\n\nRemein, Quentin R, and Hugh LC Wilkerson. 1961. “The Efficiency of Screening Tests for Diabetes.” Journal of Chronic Diseases 13 (1): 6–21.\n\n\nRothman, Kenneth J. 1981. “Induction and Latent Periods.” American Journal of Epidemiology 114 (2): 253–59.\n\n\nSwets, John A. 1973. “The Relative Operating Characteristic in Psychology: A Technique for Isolating Effects of Response Bias Finds Wide Use in the Study of Perception and Cognition.” Science 182 (4116): 990–1000.\n\n\n———. 1988. “Measuring the Accuracy of Diagnostic Systems.” Science 240 (4857): 1285–93.\n\n\nVecchio, Thomas J. 1966. “Predictive Value of a Single Diagnostic Test in Unselected Populations.” New England Journal of Medicine 274 (21): 1171–73.\n\n\nYerushalmy, Jacob. 1947. “Statistical Problems in Assessing Methods of Medical Diagnosis, with Special Reference to X-Ray Techniques.” Public Health Reports (1896-1970) 62 (40): 1432–49.\n\n\nZweig, Mark H, and Gregory Campbell. 1993. “Receiver-Operating Characteristic (ROC) Plots: A Fundamental Evaluation Tool in Clinical Medicine.” Clinical Chemistry 39 (4): 561–77.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#footnotes",
    "href": "condprob.html#footnotes",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "",
    "text": "Thomas Bayes (1701-1761) was an English Presbyterian minister from a family of Nonconformists (i.e., Protestants who did not observe the rules of the Church of England). He studied logic and theology at the University of Edinburgh and served as a minister in Tunbridge Wells near Kent, England. He was elected a Fellow of the Royal Society in 1742 for his defense of Newton’s calculus against a 1734 book called The Analyst: A Discourse Addressed to an Infidel Mathematician by Bishop George Berkeley (1685-1753). Late in life, Bayes became interested in probability and “inverse probability” (statistics). This essay was published posthumously, and it has had a profound effect on modern statistics.↩︎\n This is partly to respect the linear algebra convention that rows come before columns in matrix indices, so \\(M_{ij}\\) is the entry in row \\(i\\) and column \\(j\\) of the matrix \\(M\\). In analytic epidemiology, exposure must occur before any disease that it causes, so we let the exposure define the rows.↩︎\n For a test that is positive when a clinical measurement is below a given cutoff, it is the probability that a person with disease has a lower value than a person without disease. Bamber (1975) showed that the AUC is closely related to the Wilcoxon rank sum statistic for the null hypothesis that the diseased and nondiseased have the same distribution for the measurement on which the test is based.↩︎\n The symbol \\(\\Sigma\\), which is an upper-case Greek letter \\(\\sigma\\) (sigma), stands for a sum. For products, we use \\(\\Pi\\), which is an upper-case Greek letter \\(\\pi\\) (pi).↩︎\n Many diseases are complex processes (Rothman 1981), making any binary classification of disease status somewhat arbitrary. Here, we assume that we have an operational definition of disease status that allows a reasonable binary classification.↩︎",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#footnotes",
    "href": "mlestimation.html#footnotes",
    "title": "3  Maximum Likelihood Estimation",
    "section": "",
    "text": "John Tukey (1915-2000) was an American mathematician and statistician who worked at Bell Labs and Princeton University. He developed the box plot, Tukey’s range test for multiple comparisons, and the fast Fourier transform. In 1947, he coined the term “bit” as shorthand for “binary digit”. See https://en.wikipedia.org/wiki/John_Tukey.↩︎",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html#footnotes",
    "href": "bayes.html#footnotes",
    "title": "4  Bayesian Estimation",
    "section": "",
    "text": "Joseph Berkson (1899–1982) was an American physician and statistician at the Mayo Clinic in Rochester, Minnesota. He helped develop and popularize the use of logistic regression for binary outcomes, coining the term “logit” for the log odds in 1944. He also pioneered the study of selection bias, a special case of which is called “Berkson’s bias”. Later, he became a prominent opponent of the idea that smoking causes lung cancer. See https://en.wikipedia.org/wiki/Joseph_Berkson.↩︎",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Albert, Adelin. 1982. “On the Use and Computation of Likelihood\nRatios in Clinical Chemistry.” Clinical Chemistry 28\n(5): 1113–19.\n\n\nAlho, Juha M. 1992. “On Prevalence, Incidence, and Duration in\nGeneral Stable Populations.” Biometrics 48 (2): 587–92.\n\n\nBamber, Donald. 1975. “The Area Above the Ordinal Dominance Graph\nand the Area Below the Receiver Operating Characteristic Graph.”\nJournal of Mathematical Psychology 12 (4): 387–415.\n\n\nBayes, Thomas. 1763. “LII. An Essay\nTowards Solving a Problem in the Doctrine of Chances. By the Late\nRev. Mr. Bayes, FRS\nCommunicated by Mr. Price, in a Letter to\nJohn Canton, AMFRS.”\nPhilosophical Transactions of the Royal Society of London 53:\n370–418.\n\n\nBengtsson, Ewert, and Patrik Malm. 2014. “Screening for Cervical\nCancer Using Automated Analysis of PAP-Smears.”\nComputational and Mathematical Methods in Medicine 2014:\n842037.\n\n\nBerkson, Joseph. 1942. “Tests of Significance Considered as\nEvidence.” Journal of the American Statistical\nAssociation 37 (219): 325–35.\n\n\nBlumberg, Mark S. 1957. “Evaluating Health Screening\nProcedures.” Operations Research 5 (3): 351–60.\n\n\nBostrom, RC, HS Sawyer, and WE Tolles. 1959. “Instrumentation for\nAutomatically Prescreening Cytological Smears.” Proceedings\nof the IRE 47 (11): 1895–1900.\n\n\nDunn Jr, John E. 1962. “The Use of Incidence and Prevalence in the\nStudy of Disease Development in a Population.” American\nJournal of Public Health 52 (7): 1107–18.\n\n\nFagan, Terrence J. 1975. “Nomogram for Bayes’s\nTheorem.” New England Journal of Medicine 293 (5): 257.\n\n\nFreeman, Jonathan, and George B Hutchison. 1980. “Prevalence,\nIncidence and Duration.” American Journal of\nEpidemiology 112 (5): 707–23.\n\n\nHanley, James A, and Barbara J McNeil. 1982. “The Meaning and Use\nof the Area Under a Receiver Operating Characteristic (ROC)\nCurve.” Radiology 143 (1): 29–36.\n\n\nKeiding, Niels. 1991. “Age-Specific Incidence and Prevalence: A\nStatistical Perspective.” Journal of the Royal Statistical\nSociety: Series A (Statistics in Society) 154 (3): 371–96.\n\n\nKessel, Elton. 1962. “Diabetes Detection: An Improved\nApproach.” Journal of Chronic Diseases 15 (12): 1109–21.\n\n\nLaplace, Pierre Simon. 1820. Théorie Analytique Des\nProbabilités. Vol. 7. Courcier.\n\n\nLusted, Lee B. 1971a. “Decision-Making Studies in Patient\nManagement.” New England Journal of Medicine 284 (8):\n416–24.\n\n\n———. 1971b. “Signal Detectability and Medical\nDecision-Making.” Science 171 (3977): 1217–19.\n\n\n———. 1984. “ROC Recollected.” Medical\nDecision Making 4: 131–35.\n\n\nMacMahon, Brian, and William D Terry. 1958. “Application of Cohort\nAnalysis to the Study of Time Trends in Neoplastic Disease.”\nJournal of Chronic Diseases 7 (1): 24–35.\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological\nPerspective.” In A History of Epidemiologic Methods and\nConcepts, edited by Alfredo Morabia, 3–125. Springer.\n\n\nPreston, Samuel H. 1987. “Relations Among Standard Epidemiologic\nMeasures in a Population.” American Journal of\nEpidemiology 126 (2): 336–45.\n\n\nRemein, Quentin R, and Hugh LC Wilkerson. 1961. “The Efficiency of\nScreening Tests for Diabetes.” Journal of Chronic\nDiseases 13 (1): 6–21.\n\n\nRothman, Kenneth J. 1981. “Induction and Latent Periods.”\nAmerican Journal of Epidemiology 114 (2): 253–59.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera.\nSecond edition. John Churchill. https://wellcomecollection.org/works/uqa27qrt.\n\n\nSwets, John A. 1973. “The Relative Operating Characteristic in\nPsychology: A Technique for Isolating Effects of Response Bias Finds\nWide Use in the Study of Perception and Cognition.”\nScience 182 (4116): 990–1000.\n\n\n———. 1988. “Measuring the Accuracy of Diagnostic Systems.”\nScience 240 (4857): 1285–93.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The\nAnnals of Mathematical Statistics 33 (1): 1–67.\n\n\nVecchio, Thomas J. 1966. “Predictive Value of a Single Diagnostic\nTest in Unselected Populations.” New England Journal of\nMedicine 274 (21): 1171–73.\n\n\nYerushalmy, Jacob. 1947. “Statistical Problems in Assessing\nMethods of Medical Diagnosis, with Special Reference to\nX-Ray Techniques.” Public Health Reports\n(1896-1970) 62 (40): 1432–49.\n\n\nZweig, Mark H, and Gregory Campbell. 1993. “Receiver-Operating\nCharacteristic (ROC) Plots: A Fundamental Evaluation Tool\nin Clinical Medicine.” Clinical Chemistry 39 (4):\n561–77.",
    "crumbs": [
      "References"
    ]
  }
]