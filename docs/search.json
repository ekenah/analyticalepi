[
  {
    "objectID": "mlestimation.html",
    "href": "mlestimation.html",
    "title": "3  Maximum Likelihood Estimation",
    "section": "",
    "text": "3.1 Binomial likelihood\nIn probability, we are told the rules of the game and then we predict what it will look like. In statistics, we watch the game and try to figure out the rules. Roughly speaking, statistics (game to rules) is the reverse of probability (rules to game). When done well, statistics helps us learn from observations while accounting honestly for uncertainty. An outstanding early example statistics applied to public health is the work of Florence Nightingale (1820-1910), who collected data and developed statistical graphics to demonstrate the need for public health reforms in the British Army in the 1850s (Cohen 1984; Winkelstein Jr 2009).2\nHere, we will use estimation of a probability as an example of maximum likelihood estimation, which is used for parameter estimation throughout frequentist statistics. It gives us a way to find point estimates of parameters that are optimal in large samples in a sense that we will explain below. It is also the foundation for hypothesis tests and confidence intervals, which give us an accurate way to account for uncertainty in statistical inference.\nIn Section 3.1.1, we used the prevalence \\(p\\) in our population to figure out the distribution of the number \\(X\\) of diseased individuals in a sample of size \\(n\\). This is probability. The corresponding statistical problem would be to estimate the prevalence \\(p\\) after seeing \\(X = x\\) infected individuals in a sample of size \\(n\\).\nWhen our experiment is to sample multiple individuals from a population, the analogy between the outcomes \\(\\omega \\in \\Omega\\) and the individuals in the population breaks down. Recall that when we flip a coin twice, each \\(\\omega \\in \\Omega\\) must specify the outcomes of both flips. When the experiment is to sample \\(n\\) individuals from a population, the entire sample is a single outcome \\(\\omega\\) and \\(\\Omega\\) contains all possible samples of \\(n\\) individuals from the population. If the population size is \\(N\\), then the number of possible samples of size \\(n\\) is given by the binomial coefficient \\[\n  \\binom{N}{n} = \\frac{N!}{n! (N - n)!},\n\\] where \\(k!\\) denotes \\(k\\) factorial. Factorials are defined by \\(0! = 1\\) and \\(k! = k \\cdot (k - 1)!\\) for any integer \\(k &gt; 0\\). For example, \\(1! = 1\\), \\(2! = 2\\), \\(3! = 6\\), \\(4! = 24\\), \\(5! = 120\\), and so on. For \\(k &gt; 0\\), \\(k!\\) is the product of all positive integers up to and including \\(k\\), which grows extremely fast as \\(k\\) increases.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#binomial-likelihood",
    "href": "mlestimation.html#binomial-likelihood",
    "title": "3  Maximum Likelihood Estimation",
    "section": "",
    "text": "3.1.1 Binomial distribution\nSuppose we sample \\(n\\) individuals from a population \\(\\Omega\\) and test them for disease. For simplicity, we assume that the diagnostic test has perfect sensitivity and specificity. Let \\(Y_i\\) denote whether person \\(i\\) in the sample has disease, and let \\(X\\) be the total number who have disease. Then \\[\n  X = \\sum_{i = 1}^n Y_i,\n\\] so it is a linear combination of the \\(Y_i\\). Each \\(Y_i\\) is a Bernoulli(\\(p\\)) random variable, where \\(p\\) is the prevalence of disease in the population. When \\(N\\) is much larger than \\(n\\) (for which we write \\(N \\gg n\\)), the test results for each person in the sample are approximately independent.\nThe distribution of a sum of \\(n\\) independent Bernoulli(\\(p\\)) random variables is called a binomial(n, p) distribution.3 The probability \\(Y_1 = 1\\) is \\(p\\), and the probability that \\(Y_1 = 0\\) is \\((1 - p)\\), so we can handle both cases by writing \\[\n  \\Pr(Y_1 = y_1) = p^{y_1} (1 - p)^{1 - y_1}.\n\\] When the \\(Y_i\\) are independent, each \\(Y_i\\) has a Bernoulli(p) distribution (see Section 1.3.5) and \\[\n  \\Pr(Y_1 = y_1, Y_2 = y_2, \\ldots, Y_n = y_n)\n  = \\prod_{i = 1}^n \\Pr(Y_i = y_i)\n  = \\prod_{i = 1}^n p^{y_i} (1 - p)^{1 - y_i}\n\\] by the multiplication rule for independent events. Substituting \\(x = \\sum_{i = 1}^n y_i\\), we get \\[\n  \\Pr(Y_1 = y_1, Y_2 = y_2, \\ldots, Y_n = y_n)\n  = p^x (1 - p)^{n - x}.\n\\] The value of \\(x\\) depends only on the sum of the \\(y_i\\), and there are \\(\\binom{n}{x}\\) different ways to get \\(x\\) cases of disease out of \\(n\\) sampled individuals. By the addition rule for disjoint events, we get \\[\n  \\Pr(X = x) = \\binom{n}{x} p^x (1 - p)^x.\n\\tag{3.1}\\] This is the probability mass function (PMF) of the binomial distribution. The set of possible values of a binomial(n, p) random variable \\(X\\) is \\(\\supp(X) = \\{0, 1, \\ldots, n\\}\\).\nSection 1.3.5 showed that a Bernoulli(\\(p\\)) random variable has expected value \\(p\\) and variance \\(p(1 - p)\\). Because a binomial(\\(n\\), \\(p\\)) random variable is the sum of \\(n\\) independent Bernoulli(\\(p\\)) random variables, its expected value is \\[\n  \\E(X) = n p.\n\\] by the rule for expectations of linear combinations in Equation 1.11. Its variance is \\[\n  \\Var(X) = n p (1 - p)\n\\] by the rule for variances of linear combinations in Equation 1.12. The covariances are all zero because the \\(Y_i\\) are independent.\n\nR\n\n\n\n\nbinomdist.R\n\n## binomial distribution\n\n# binomial PMF\n# The second and third arguments are n (\"size\") and p (\"prob\").\ndbinom(2, 10, 0.4)\ndbinom(0:10, 10, 0.4)\nsum(dbinom(0:10, 10, 0.4))\n\n# binomial CDF\npbinom(0:10, 10, 0.4)\ncumsum(dbinom(0:10, 10, 0.4))\n\n# binomial quantiles\nqbinom(c(0.25, 0.5, 0.75, 1), 10, 0.4)\n\n# random samples\nrbinom(20, 10, 0.4)\nx &lt;- rbinom(1000, 10, 0.4)\nmean(x)\nvar(x)\n\n\n\n\n\n\n3.1.2 Likelihood and log likelihood\nIn probability, we know the prevalence of disease \\(p\\) and we deduce the distribution of the number of diseased individuals \\(X\\) in a sample of size \\(n\\). In statistics, we observe \\(X = x\\) and use this to estimate \\(p\\). To do this, we rewrite the binomial PMF Equation 3.1 as a function of \\(p\\) instead of \\(x\\): \\[\n  L(p) = \\binom{n}{x} p^x (1 - p)^{n - x}.\n\\tag{3.2}\\] This is the binomial likelihood function. The right-hand sides of Equation 3.1 and Equation 3.2 are identical, and they produce exactly the same value given the same \\(x\\) and \\(p\\). However, the two equations define different functions. In binomial PMF in Equation 3.1, the prevalence \\(p\\) is fixed and the number of diseased individuals \\(x\\) is the argument of the function. In the binomial likelihood function in Equation 3.2, the number of diseased individuals \\(x\\) is fixed and the prevalence \\(p\\) is the argument of the function. The PMF belongs to probability, and the likelihood belongs to statistics.\nThe log likelihood is the natural logarithm (i.e., the logarithm to base \\(e = 2.718281828\\ldots\\))4 of the likelihood function. For binomial log likelihood is \\[\n  \\ell(p) = \\ln \\binom{n}{x} + x \\ln p + (n - x) \\ln (1 - p).\n\\] Because the logarithm turns products into sums, it is generally much easier to handle the log likelihood than the likelihood itself. The term \\(\\ln \\binom{n}{x}\\) does not depend on \\(p\\), so it can be ignored. Intuitively, this tells us that the total number \\(x = y_1 + y_2 + \\cdots + y_n\\) of individuals with disease in our sample contains the same information about the prevalence of disease as the sequence \\(y_1, y_2, \\ldots, y_n\\) of disease indicators.\nFor any given \\(p\\), we can think of \\(\\ell(p)\\) as a random variable whose value is determined by our sample of size \\(n\\). Let \\(\\ptrue\\) be the true prevalence of disease. By Gibb’s inequality,5 \\[\n  \\E[\\ell(\\ptrue)] &gt; \\E[\\ell(p)]\n\\] for all \\(p \\neq \\ptrue\\). This inequality is about the expected value of the log likelihood over all possible samples of size \\(n\\). For any given sample, it is possible that \\(\\ell(\\ptrue)\\) is not the maximum of the log likelihood. However, this inequality is an important part of the justification for estimating \\(p\\) by maximizing the log likelihood (Boos and Stefanski 2013). Because function \\(v \\mapsto \\ln(v)\\) is strictly increasing in \\(v\\), the likelihood \\(L(p)\\) and the log likelihood \\(\\ell(p)\\) are maximized at exactly the same value of \\(p\\).\n\n\n3.1.3 Score function\nTo find the maximum of the log likelihood, we find the value of \\(p\\) where its slope is zero. This is the mathematical version of the insight that the ground at the top of a hill is level. The score function is the first derivative of the log likelihood \\[\n  U(p)\n  = \\frac{\\text{d}}{\\text{d} p} \\ell(p)\n  = \\frac{x}{p} - \\frac{n - x}{1 - p},\n\\] which is the slope of \\(\\ell(p)\\) at \\(p\\). To find where the slope equals zero, we solve the score equation \\[\n  U(\\hat{p})\n  = \\frac{x}{\\hat{p}} - \\frac{n - x}{1 - \\hat{p}}\n  = 0\n\\tag{3.3}\\] where \\(\\hat{p}\\) denotes the maximum likelihood estimate (MLE) of \\(\\ptrue\\). When the dust settles, we get \\[\n  \\hat{p} = \\frac{x}{n}\n\\] so our MLE of the prevalence is just the proportion of our sample who has disease.\nTo confirm that this is a maximum instead of a minimum, we need to look at the second derivative of \\(\\ell\\). When we walk across the top of a hill, we go from walking uphill to walking downhill so the slope is decreasing. If \\(\\ell(p)\\) is maximized at \\(\\hat{p}\\), then the slope of the slope (i.e., the second derivative) should be negative. The second derivative of \\(\\ell(p)\\) at \\(\\hat{p}\\) is \\[\n  \\frac{\\text{d}}{\\text{d} p} U(p) = \\frac{\\text{d}^2}{\\text{d} p^2} \\ell(p)\n  = -\\frac{x}{p^2} - \\frac{n - x}{(1 - p)^2}.\n\\tag{3.4}\\] This is negative for any \\(p \\in (0, 1)\\). Thus, the log likelihood is maximized at \\(\\hat{p}\\) if \\(x &gt; 0\\) and \\(x &lt; n\\).\nWhen \\(x = 0\\) or \\(x = n\\), the log likelihood \\(\\ell(p)\\) has no maximum at any \\(p \\in (0, 1)\\). Instead, the maximum occurs at one of the boundaries of the set of possible \\(p\\). When \\(x = 0\\), our MLE of \\(\\ptrue\\) is \\(\\hat{p} = 0\\). When \\(x = n\\), our maximum likelihood estimate is \\(\\hat{p} = 1\\).\n\n\n3.1.4 Expected and observed information*\nFor any given \\(p\\), we can think of the score \\(U(p)\\) as a random variable that has an expected value and a variance. If \\(\\ptrue = p\\), the expected value of the score is always zero: \\[\n  \\E_p[U(p)]\n  = \\E_p\\bigg[\\frac{X}{p} - \\frac{n - X}{1 - p}\\bigg]\n  = \\frac{\\E_p(X)}{p} - \\frac{\\E_p(n - X)}{1 - p}\n  = \\frac{n p}{p} - \\frac{n (1 - p)}{1 - p}\n  = 0\n\\] where we use the subscript \\(p\\) to indicate that the expected value is calculated assuming that \\(\\ptrue = p\\). Because \\(\\E_p[U(p)] = 0\\), the corresponding variance of the score is \\[\n  \\mathcal{I}(p) = \\Var_p[U(p)] = \\E_p[U(p)^2],\n\\] by Equation 1.10. This is called the expected Fisher information or expected information.6 It can be used to calculate confidence limits for \\(\\ptrue\\).\nUnder regularity conditions that are met when \\(\\ptrue \\in (0, 1)\\), the Fisher information \\(\\mathcal{I}(p)\\) can be calculated using the second derivative of the log likelihood \\(\\ell(p)\\) from Equation 3.4.7 Specifically, \\(\\mathcal{I}(p)\\) is the expected value of the negative second derivative of \\(\\ell(p)\\): \\[\n  \\mathcal{I}(p)\n  = \\E_p\\Bigg[-\\frac{\\text{d}^2}{\\text{d} p^2} \\ell(p)\\Bigg]\n  = \\E_p\\bigg[\\frac{X}{p^2} + \\frac{n - X}{(1 - p)^2}\\bigg],\n\\tag{3.5}\\] where the subscript \\(p\\) indicates that the expected value is calculated assuming that \\(\\ptrue = p\\). Using Equation 1.11 and the binomial(\\(n\\), \\(p\\)) distribution for \\(X\\), this simplifies to \\[\n  \\mathcal{I}(p)\n  = \\frac{\\E(X)}{p^2} + \\frac{\\E(n - X)}{(1 - p)^2}\n  = \\frac{n}{p} + \\frac{n}{1 - p}\n  = \\frac{n}{p (1 - p)}.\n\\] Because \\(\\ptrue\\) is unknown, the expected information is often evaluated at \\(\\hat{p}\\). In some models, the expected information can be difficult to calculate.\nThe negative second derivative of \\(\\ell(p)\\) inside the expectation in Equation 3.5 evaluated is called the observed Fisher information or observed information \\[\nI(p) = -\\frac{\\text{d}^2}{\\text{d} p^2} \\ell(p)\n= \\frac{x}{p^2} + \\frac{n - x}{(1 - p)^2}\n\\tag{3.6}\\] For the binomial distribution \\(I(\\hat{p}) = \\mathcal{I}(\\hat{p})\\) but this equality does not hold at other values of \\(p\\). The observed information is an unbiased estimator of the expected information, and it can always be calculated from the data. It often produces more accurate variance estimates than the expected information (Efron and Hinkley 1978; Kenward and Molenberghs 1998; Reid 2003). However, it is generally safe to use whichever is most convenient (Boos and Stefanski 2013).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#large-sample-theory",
    "href": "mlestimation.html#large-sample-theory",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.2 Large-sample theory",
    "text": "3.2 Large-sample theory\nThe log likelihood, the score function, and the Fisher and observed information give us all of the pieces we need to calculate point and interval estimates of \\(\\ptrue\\). To put them together, we use two fundamental results from probability theory about the behavior of sample means. The law of large numbers justifies point estimates and the central limit theorem justifies hypothesis tests and interval estimates, which can be obtained in three standard ways.\n\n3.2.1 Sample mean (average)\nIf \\(Y_1, Y_2, \\ldots, Y_n\\) are random variables, then the sample mean or average is \\[\n  \\hat{\\mu}_n = \\frac{1}{n} \\sum_{i = 1}^n Y_i.\n\\] This sample mean can be thought of as a random variable whose value is determined when we observe \\(Y_1 = y_1, Y_2 = y_2, \\ldots, Y_n = y_n\\). If each \\(Y_i\\) has \\(\\E(Y_i) = \\mu\\), then \\[\n  \\E[\\hat{\\mu}_n]\n  = \\frac{1}{n} \\sum_{i = 1}^n \\E[Y_i]\n  = \\frac{1}{n} n \\mu\n  = \\mu\n\\tag{3.7}\\] by Equation 1.11. Thus, the sample mean \\(\\hat{\\mu}_n\\) is an unbiased estimate of \\(\\mu\\) for any sample size \\(n\\). When the \\(Y_i\\) are indicator variables, \\(\\hat{\\mu}_n\\) is just the proportion of the sample with \\(Y_i = 1\\).\n\n\n3.2.2 Law of large numbers and consistency\nIf the \\(Y_i\\) are independent and each has \\(\\Var(Y_i) = \\sigma^2\\), then \\[\n  \\Var(\\hat{\\mu}_n)\n  = \\frac{1}{n^2} \\sum_{i = 1}^n \\Var(Y_i)\n  = \\frac{1}{n^2} n \\sigma^2\n  = \\frac{\\sigma^2}{n}\n\\tag{3.8}\\] by Equation 1.12. Thus, the variance of \\(\\hat{\\mu}_n\\) decreases as the sample size \\(n\\) increases. The standard deviation of \\(\\hat{\\mu}_n\\) is proportional to \\(1 / \\sqrt{n}\\). As \\(n \\rightarrow \\infty\\), we should have \\(\\hat{\\mu}_n \\rightarrow \\mu\\). This is called the law of large numbers, and it holds even when \\(\\sigma^2 = \\infty\\).\n\nTheorem 3.1 (Law of Large Numbers) If \\(Y_1, Y_2, \\ldots\\) is an infinite sequence of independent and identically-distributed (IID) random variables with mean \\(\\mu &lt; \\infty\\) and variance \\(\\sigma^2 \\leq \\infty\\), then\n\\[\n    \\hat{\\mu}_n \\rightarrow \\mu\n\\] as \\(n \\rightarrow \\infty\\).8\n\nOur maximum likelihood estimate \\(\\hat{p}_n\\) is a sample mean: \\[\n  \\hat{p}_n = \\frac{X}{n} = \\frac{1}{n} \\sum_{i = 1}^n Y_i.\n\\] where each \\(Y_i \\sim \\text{Bernoulli}(\\ptrue)\\) and the \\(Y_i\\) are independent. Therefore, the LLN implies that \\[\n  \\hat{p}_n \\rightarrow \\ptrue\n\\] as \\(n \\rightarrow \\infty\\). This convergence is shown in Figure 3.1. An estimate that converges to its true value as \\(n \\rightarrow \\infty\\) is called consistent. Intuitively, this means that \\(\\hat{p}_n\\) is guaranteed to be close to \\(\\ptrue\\) in a large sample. However, the LLN does not specify how close or how large a sample we need.\n\n\n\nCode\n\nlln.R\n\n## Law of large numbers\n\nn &lt;- 1000\nx &lt;- seq(n)\nplot(x, cumsum(rbinom(n, 1, .5)) / x, type = \"n\", ylim = c(0, 1),\n     xlab = \"Number of samples\", ylab = \"Sample mean\")\ngrid()\nlines(x, cumsum(rbinom(n, 1, .5)) / x, lty = \"solid\")\nlines(x, cumsum(rbinom(n, 1, .5)) / x, lty = \"dashed\")\nlines(x, cumsum(rbinom(n, 1, .5)) / x, lty = \"dotted\")\nabline(h = .5)\n\n\n\n\n\n\n\n\n\nFigure 3.1: The LLN at work. Each line traces the sample means calculated from a sequence of random samples \\(x_1, x_2, x_3, \\ldots\\) from a Bernoulli(0.5) distribution. For each sequence, the y-coordinate above \\(n\\) is the sample mean from the first \\(n\\) random samples in the sequence. The true mean of 0.5 is marked by a solid horizontal line.\n\n\n\n\n\n\n\n\n3.2.3 Central limit theorem and the normal distribution\nWhen both the mean and variance of the \\(Y_i\\) are finite, the central limit theorem (CLT) allows us to say something about how far away our sample mean \\(\\hat{\\mu}_n\\) is from the true value \\(\\mu\\). It is the most important result in all of probability and statistics.\n\nTheorem 3.2 (Central Limit Theorem) If \\(Y_1, Y_2, \\ldots\\) is an infinite sequence of IID random variables with finite mean \\(\\mu\\) and variance \\(\\sigma^2 &lt; \\infty\\), then \\[\n  Z_n\n  = \\frac{\\hat{\\mu}_n - \\E(\\hat{\\mu_n})}{\\sqrt{\\Var(\\hat{\\mu}_n)}}\n  = \\frac{\\sqrt{n} (\\hat{\\mu}_n - \\mu)}{\\sqrt{\\sigma^2}}\n\\] has a distribution that converges to a normal distribution or Gaussian distribution with mean zero and variance one as \\(n \\rightarrow \\infty\\).9 Because of this, we say that \\(\\hat{\\mu}_n\\) is asymptotically normal.\n\nThe normal distribution is a distribution for a continuous random variable, which can take any value on an interval or even on all of \\(\\mathbb{R}\\). Instead of a PMF, a continuous random variable \\(Z\\) has a probability density function (PDF). If \\(Z\\) is a continuous random variable with PDF \\(f(z)\\) and \\([a, b]\\) is an interval, then \\[\n  \\Pr\\bigl(Z \\in [a, b]\\bigr) = \\int_a^b f(z) \\,\\text{d} z.\n\\] The integral on the right-hand side represents the area under \\(f(z)\\) over the interval \\([a, b]\\). The cumulative distribution function of \\(Z\\) is \\[\n  F(z) = \\int_{-\\infty}^z f(u) \\,\\text{d} u,\n\\] where the integral on the right-hand side represents the area under \\(f(z)\\) over the interval \\((-\\infty, u]\\). For the same reason that the values of the PMF for any discrete random variable add up to one, we have \\[\n\\Pr(Z \\in \\mathbb{R})\n= \\int_{-\\infty}^\\infty f(z) \\,\\text{d} z\n= 1\n\\] for any continuous random variable \\(Z\\). Like the PMF and CDF of a discrete random variable, the PDF and CDF of a continuous random variable contain the same information about the distribution of \\(Z\\).\nThe PDF of the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) is \\[\n  f(z, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(z - \\mu)^2}{2 \\sigma^2}}.\n\\] The standard normal distribution has \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\). It is such an important distribution that its PDF and CDF have special notation. The standard normal PDF is \\[\n  \\phi(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{z^2}{2}},\n\\] and its CDF is \\(\\Phi(z)\\). These functions and the relationship between them are illustrated in Figure 3.2. A normal distribution is denoted \\(N(\\mu, \\sigma^2)\\), so the standard normal distribution is written \\(N(0, 1)\\).\n\n\n\nCode\n\nnormplots.R\n\n## Normal distribution PDF and CDF\n\n# set grid of plots\npar(mfrow = c(2, 1), mar = c(2, 5, 2, 2) + 0.1)\n\n# define variables\nx &lt;- seq(-3.5, 3.5, by = 0.01)\na &lt;- 0\nb &lt;- 2\n\n# plot of PDF\nplot(x, dnorm(x), type = \"n\",\n    ylab = expression(paste(\"PDF \", phi1(z))))\ngrid()\nlines(x, dnorm(x))\npolygon(x = c(b, a, seq(a, b, by = 0.01)),\n        y = c(0, 0, dnorm(seq(a, b, by = 0.01))),\n        lty = \"dashed\", col = \"darkgray\")\ntext(0.4, 0.18, labels = \"Area = Pr(0 &lt; Z &lt; 2)\", srt = 90)\n\n# plot of CDF\nplot(x, pnorm(x), type = \"n\",\n     ylab = expression(paste(\"CDF \", Phi(z))))\ngrid()\nlines(x, pnorm(x))\nsegments(c(-4, -4), pnorm(c(a, b)), c(a, b), pnorm(c(a, b)),\n         lty = \"dashed\")\nsegments(c(a, b), c(-1, -1), c(a, b), pnorm(c(a, b)), lty = \"dashed\")\narrows(-3, pnorm(a), -3, pnorm(b), code = 3, length = 0.1)\ntext(-1.7, sum(pnorm(c(a, b))) / 2, labels = \"Change = Pr(0 &lt; Z &lt; 2)\")\n\n\n\n\n\n\n\n\n\nFigure 3.2: The PDF (top) and CDF (bottom) of a standard normal random variable \\(Z\\). If \\(X \\sim N(0, 1)\\), then \\(\\Pr(0 &lt; X &lt; 2)\\) equals the shaded area under the PDF as well as the change in the CDF from \\(0\\) to \\(2\\). This same relationship between the CDF and the PDF holds for all continuous random variables and any interval \\((a, b)\\).\n\n\n\n\n\n\nR\n\n\n\n\nnormdist.R\n\n## normal (Gaussian) distribution\n\n# normal PDF\n# Second and third arguments are mean and SD (not variance).\n# The defaults are mean = 0 and SD = 1.\ndnorm(2, 1.2, 5)\n\n# normal CDF (using default mean and variance)\npnorm(1.96)\npnorm(1.96) - pnorm(-1.96)\n\n# normal quantiles\nqnorm(0.975)\npnorm(qnorm(0.975))\n\n# random samples (using named arguments)\nrnorm(25, mean = 2.3, sd = 3)\n\n\n\n\nFor our estimated probability \\(\\hat{p}_n\\) is a sample mean of IID \\(Y_i\\) with \\(\\E(Y_i) = \\ptrue\\) and \\(\\Var(Y_i) = \\ptrue (1 - \\ptrue)\\). When \\(n\\) is large, \\[\n  Z_n\n  = \\frac{\\sqrt{n} (\\hat{p}_n - \\ptrue)}{\\sqrt{\\ptrue (1 - \\ptrue)}}\n  = \\frac{\\hat{p}_n - \\ptrue}{\\sqrt{\\mathcal{I}(\\ptrue)^{-1}}}\n\\tag{3.9}\\] has a distribution that is close to a standard normal distribution. Figure 3.3 shows this convergence is shown for sample means where \\(Y_i \\sim \\Bernoulli(0.1)\\). The CLT does not guarantee that the distribution of \\(Z_n\\) is approximately normal in any given sample. It only guarantees that the normal approximation holds eventually as \\(n\\) increases. When the \\(Y_i \\sim \\Bernoulli(p)\\), the normal approximation is typically good when \\(n p (1 - p) &gt; 5\\).\n\n\n\nCode\n\nclt.R\n\n## Central limit theorem\n\n# probability mass function for sample mean\ndbline &lt;- function(n, p=.5, ...) {\n  x &lt;- (seq(-.5, n + .5) / n - p) * sqrt(n / (p * (1 - p)))\n  y &lt;- c(0, dbinom(0:n, n, p), 0) * sqrt(p * (1 - p) * n)\n  lines(stepfun(x, y), pch = NA, ...)\n}\n\n# define grid of plots\npar(mfrow = c(2, 2))\nx &lt;- seq(-4, 4, by = .01)\n\n# n = 20\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 20\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(20, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n# n = 50\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 50\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(50, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n# n = 100\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 100\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(100, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n# n = 250\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 250\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(250, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n\n\n\n\n\n\n\n\nFigure 3.3: The CLT at work. The dashed lines show the PMF of the distribution of the average from a sample of size \\(n\\) from a Bernoulli(0.1) distribution. The solid line is the standard normal PDF.\n\n\n\n\n\n\n\n3.2.4 Efficiency of maximum likelihood estimators*\nWe have used the LLN and the CLT to show that \\(\\hat{p}_n\\) is consistent and asymptotically normal, which are both wonderful properties for an estimator to have. However, they do not prove that \\(\\hat{p}_n\\) is the best estimator of \\(\\ptrue\\) in any particular sense. In Equation 3.9, the variance of \\(\\hat{p}_n\\) was \\[\n  \\mathcal{I}(\\ptrue)^{-1} = \\frac{\\ptrue (1 - \\ptrue)}{n},\n\\] which is the inverse of the Fisher information. It turns out that no other unbiased estimator of \\(\\ptrue\\) can have lower variance, so \\(\\hat{p}_n\\) is the minimum-variance unbiased estimator of \\(\\ptrue\\).\nSuppose \\(\\theta\\) is a parameter for a family of PMFs or PDFs \\(f(y, \\theta)\\) such that the true PMF or PDF is \\(f(y, \\theta_\\true)\\). When we observe \\(Y_1 = y_1, Y_2 = y_2, \\ldots Y_n = y_n\\), the likelihood is \\[\n  L(\\theta) = \\prod_{i = 1} f(y_i, \\theta),\n\\] and the log likelihood is \\[\n  \\ell(\\theta) = \\ln L(\\theta) = \\sum_{i = 1}^n \\ln f(y_i, \\theta).\n\\] The score function is \\[\n  U(\\theta) = \\frac{\\text{d}}{\\text{d} \\theta} \\ell(\\theta),\n\\] and the MLE is the solution of the score equation \\(U(\\hat{\\theta}) = 0\\). The Fisher information is \\[\n  \\mathcal{I}(\\theta) = \\E_\\theta\\bigg[\\frac{\\text{d}^2}{\\text{d} \\theta^2} \\ell(\\theta)\\bigg],\n\\] and \\(\\Var(\\hat{\\theta}) = \\mathcal{I}(\\theta)^{-1}\\). If \\(\\bar{\\theta}\\) is any unbiased estimator of the true value \\(\\theta_\\text{true}\\), then \\[\n  \\Var(\\bar{\\theta}) \\geq \\mathcal{I}(\\theta_\\text{true})^{-1}.\n\\] This result is called the Cramér-Rao lower bound (Rao 1945; Cramér 1946),10 No unbiased estimator of \\(\\theta_\\true\\) can have smaller variance than the MLE \\(\\hat{\\theta}\\). Maximum likelihood estimates are consistent, asymptotically normal, and asymptotically efficient when the likelihood is correct (Boos and Stefanski 2013).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#hypothesis-testing",
    "href": "mlestimation.html#hypothesis-testing",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.3 Hypothesis testing",
    "text": "3.3 Hypothesis testing\nIn a hypothesis test, we specify a null hypothesis and then decide whether to reject it based on the value of a test statistic. A null hypothesis often takes the form \\[\n  H_0: \\theta_\\text{true} = \\theta_0.\n\\tag{3.10}\\] We reject \\(H_0\\) if the test statistic appears inconsistent with its distribution under \\(H_0\\). Otherwise, we fail to reject \\(H_0\\). It is traditional to avoid saying that \\(H_0\\) was accepted.\n\n3.3.1 Hypothesis tests and diagnostic tests\nIf we think of \\(H_0\\) as not having the disease and rejecting \\(H_0\\) as testing positive for the disease, a hypothesis test is analogous to a diagnostic test. Table 3.1 shows the possible outcomes of a hypothesis test, and its margins show the correspondence to diagnostic testing (Diamond and Forrester 1983). A false positive occurs when we reject \\(H_0\\) when it is true, which is called a type I error. A false negative occurs when we fail to reject \\(H_0\\) when it is false, which is called type II error.\n\n\n\nTable 3.1: Truth of \\(H_0\\) and hypothesis test results.\n\n\n\n\n\n\nReject \\(H_0\\) (\\(T^+\\))\nFail to reject \\(H_0\\) (\\(T^-\\))\n\n\n\n\n\\(H_0\\) false (\\(D^+\\))\nTrue positive\nFalse negative = type II error\n\n\n\\(H_0\\) true (\\(D^-\\))\nFalse positive = type I error\nTrue negative\n\n\n\n\n\n\nA hypothesis test has analogs of sensitivity and specificity. The equivalent of specificity is \\(1 - \\alpha\\) where \\[\n  \\alpha\n  = \\Pr(\\text{reject } H_0 \\given{} H_0 \\text{ true})\n\\] is the probability of a type I error. This is also called the significance level of the test. The equivalent of sensitivity is the power of the test, which is \\(1 - \\beta\\) where \\[\n  \\beta\n  = \\Pr(\\text{fail to reject } H_0 \\given{} H_0 \\text{ false})\n\\] is the probability of a type II error.\nA hypothesis test also has analogs of positive and negative predictive values (PPV and NPV). Just like the PPV and NPV of a dignostic test depend on the prevalence of disease, the PPV and NPV of a hypothesis test depend on the prior probability that \\(H_0\\) is true, which is the probability that \\(H_0\\) is true based on what we know before we see the test result. For a hypothesis test, the PPV is \\[\n  \\Pr(H_0 \\text{ false} \\given{} H_0 \\text{ rejected})\n  = \\frac{(1 - \\beta) \\Pr(H_0 \\text{ false})}\n  {(1 - \\beta) \\Pr(H_0 \\text{ false}) + \\alpha \\Pr(H_0 \\text{ true})}\n\\tag{3.11}\\] by Bayes’ rule. Similarly, the NPV of the hypothesis test is \\[\n  \\Pr(H_0 \\text{ true} \\given H_0 \\text{ not rejected})\n  = \\frac{(1 - \\alpha) \\Pr(H_0 \\text{ true})}\n  {(1 - \\alpha) \\Pr(H_0 \\text{ true}) + \\beta \\Pr(H_0 \\text{ false})}.\n  \\label{eq:hynpv}\n\\tag{3.12}\\] The conditional probability that \\(H_0\\) is true given the result of the hypothesis test is called the posterior probability of \\(H_0\\).\n\n\n3.3.2 Wald, score, and likelihood ratio tests\nIn a maximum likelihood framework, there are three classical tests for a null hypothesis of the form \\[\n  H_0: \\ptrue = p_0.\n\\] These tests are asymptotically equivalent, which means that they produce similar results in large samples. The best way to visualize the different tests is to look at a graph of the log likelihood function. ?fig-mle shows the log likelihood function for a binary outcome with \\(x = 60\\) events out of \\(n = 100\\) trials and a null hypothesis \\(H_0: p_\\true = 0.5\\). All three tests generalize to null hypotheses involving multiple parameters (Boos and Stefanski 2013).\nThe Wald test (Wald 1943) of \\(H_0\\) looks at the distance between the MLE \\(\\hat{p}\\) and the hypothesized value \\(p_0\\)(Wald 1943), rejecting \\(H_0\\) when this distance is sufficiently large.11 An example is shown in Figure 3.4. The Wald test statistic is \\[\n  W = \\frac{(\\hat{p} - p_0)^2}{I(\\hat{p})}\n  = \\frac{n (\\hat{p} - p_0)^2}{\\hat{p} (1 - \\hat{p})}\n  \\approxsim \\chi^2_1\n\\tag{3.13}\\] under \\(H_0\\), where \\(I(\\hat{p})\\) is the observed information from Equation 3.6. The \\(\\chi^2_1\\) distribution is the distribution of \\(Z^2\\) if \\(Z \\sim N(0, 1)\\).\nThe score test looks at the slope of the log likelihood at \\(p_0\\), rejecting \\(H_0\\) if this slope is sufficiently far from zero (Rao 1948; Aitchison and Silvey 1958). An example is shown in Figure 3.4. It score test statistic is \\[\n  S = \\frac{U(p_0)^2}{\\mathcal{I}(p_0)}\n  = \\frac{n (\\hat{p} - p_0)^2}{p_0 (1 - p_0)}\n  \\approxsim \\chi^2_1\n\\tag{3.14}\\] under \\(H_0\\), where \\(\\mathcal{I}(p_0)\\) is the expected information from Equation 3.5. The numerator of the score statistic is the same as for the Wald statistic in Equation 3.13, but the denominator uses the expected information at \\(p_0\\) instead of the observed information at \\(\\hat{p}\\). In score tests, it generally better to use the expected information than the observed information (Freedman 2007). The most important advantage of the score test is that it only needs the hypothesized null value \\(p_0\\), so it can be done without finding the maximum likelihood estimate \\(\\hat{p}\\).\nThe likelihood ratio test looks at the vertical distance between \\(\\ell(\\hat{p})\\) (which is the maximum) and \\(\\ell(p_0)\\), rejecting \\(H_0\\) if this distance is sufficiently large Wilks (1938).12 An example is shown in Figure 3.4. The likelihood ratio test statistic is \\[\n  L = 2\\big(\\ell(\\hat{p}) - \\ell(p_0)\\big)\n  \\approxsim \\chi^2_1\n\\tag{3.15}\\] under \\(H_0\\). The Neyman-Pearson lemma (Neyman and Pearson 1933) shows that the likelihood ratio test is the most powerful of all hypothesis test for comparing two hypotheses \\(H_0: \\ptrue = p_0\\) and \\(H_1: \\ptrue = p_1\\) at a fixed significance level.\n\n\n\nCode\n\nhtests.R\n\n## Hypothesis tests based on the log likelihood\n\n# binomial log likelihood, score, and information functions\nbin_loglik &lt;- function(p, k=60, n=100) {\n  k * log(p) + (n - k) * log(1 - p)\n}\nbin_score &lt;- function(p, k=60, n=100) {\n  k / p - (n - k) / (1 - p)\n}\nbin_information &lt;- function(p, k=60, n=100) {\n  k / p^2 + (n - k) / (1 - p)^2\n}\n\n# plot showing Wald, score, and likelihood ratio tests\np &lt;- seq(0.4, 0.8, length.out = 200)\nplot(p, bin_loglik(p), type = \"n\",\n     xlim = c(0.40, 0.70), ylim = c(-72, -66),\n     main = \"Tests of the null hypothesis p = 0.5\",\n     xlab = \"p\", ylab = \"ln L(p)\")\ngrid()\nlines(p, bin_loglik(p))\nabline(v = c(0.5, 0.6), lty = \"dotted\")\nabline(h = c(bin_loglik(0.5), bin_loglik(0.6)), lty = \"dashed\")\nabline(a = bin_loglik(0.5) - bin_score(0.5) * 0.5, b = bin_score(0.5),\n       col = \"darkgray\")\ntext(c(0.5, 0.6), c(-67.05, -67),\n     labels = c(expression(p[0]), expression(hat(p))))\ntext(0.55, -70.7, labels = \"Wald test\")\narrows(0.5, -70.5, 0.6, code = 3, length = 0.1)\narrows(0.475, bin_loglik(0.5), y1 = bin_loglik(0.6),\n       code = 3, length = 0.1)\ntext(0.45, -68.3, labels = \"LRT\")\n\n# The slope is the tangent of the angle to the x-axis.\n# We also must account for the different scales on the x- and y-axes.\n# 0.3 / 6 is xdist / ydist (see xlim and ylim above)\nscore_angle &lt;- atan(bin_score(0.5) * 0.3 / 6)\nangles &lt;- seq(0, score_angle, by = 0.01)\nscore_x &lt;- 0.5 + 0.04 * cos(angles)\nscore_y &lt;- bin_loglik(0.5) + 0.04 * (6 / 0.3) * sin(angles)\nlines(score_x, score_y)\ntext(0.56, -68.8, \"Score test\")\narrows(score_x[2], score_y[2], score_x[1], score_y[1], length = 0.1)\narrows(rev(score_x)[2], rev(score_y)[2], rev(score_x)[1], rev(score_y)[1],\n       length = 0.1)\n\n\n\n\n\n\n\n\n\nFigure 3.4: Binomial log likelihood function for \\(x = 60\\) and \\(n = 100\\). The null value of \\(p\\) is \\(p_0 = 0.5\\) and the maximum likelihood estimate is \\(\\hat{p} = 0.6\\).\n\n\n\n\n\n\n\n3.3.3 Critical values and p-values\nThe Neyman-Pearson approach to hypothesis testing fixes the significance level \\(\\alpha\\) before calculating the test statistic and deciding whether to reject \\(H_0\\).13 The decision to reject the null hypothesis depends on the value of the test statistic, which is compared to a critical value calculated based on the distribution of the test statistic under \\(H_0\\). If \\(Z \\sim N(0, 1)\\) under \\(H_0\\) \\[\n  \\Pr\\big(|Z| \\geq z_{1 - \\frac{\\alpha}{2}} \\given{} H_0 \\text{ true}\\big)\n  = 1 - \\alpha.\n\\] Because \\(Z^2 \\sim \\chi^2_1\\) when \\(Z \\sim N(0, 1)\\), this is equivalent to \\[\n  \\Pr\\big(Z^2 \\geq z_{1 - \\frac{\\alpha}{2}}^2 \\given{} H_0 \\text{ true}\\big)\n  = 1 - \\alpha.\n\\] In the Wald, score, and likelihood ratio tests above, \\(H_0\\) is rejected if the test statistic is larger than the critical value \\(z_{1 - \\frac{\\alpha}{2}}^2\\). For \\(\\alpha = 0.05\\), we have \\(z_{0.975} \\approx 1.96\\) so critical value for the \\(\\chi^2_1\\) distribution is \\(1.96^2 \\approx 3.84\\). The test statistic and critical value in a hypothesis test are analogous to the clinical measurement and cutoff in a diagnostic test.\nInstead of making a binary decision, it is more informative to calculate a measure of the evidence against \\(H_0\\). The p-value for a given test statistic is the lowest value of \\(\\alpha\\) at which the test would still fail to reject \\(H_0\\). A hypothesis test with significance level \\(\\alpha\\) rejects \\(H_0\\) if the p-value is \\(\\leq \\alpha\\). For the Wald, score, or likelihood ratio tests above, \\[\n  \\text{p-value} = 1 - F_{\\chi^2_1}(\\text{test statistic})\n\\] where \\(F_{\\chi^2_1}\\) is the CDF of the \\(\\chi^2_1\\) distribution If we think of the test statistic as the clinical measurement underlying a diagnostic test, the p-value equals \\(1 - \\text{spec}_\\text{max}\\) where \\(\\text{spec}_\\text{max}\\) is the highest specificity under which we would still get a positive test (i.e., reject \\(H_0\\)).",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#confidence-intervals",
    "href": "mlestimation.html#confidence-intervals",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.4 Confidence intervals",
    "text": "3.4 Confidence intervals\nA p-value is more informative than a binary decision whether to reject \\(H_0\\), but it is still more useful to know what values of \\(p\\) are plausibly consistent with the data we observed (Rothman 1978). The \\(1 - \\alpha\\) confidence interval for \\(\\ptrue\\) is the set of all possible null values \\(p_0\\) such that we would fail to reject \\(H_0: \\ptrue = p_0\\) in a hypothesis test with significance level \\(\\alpha\\). The endpoints of the confidence interval are called are called confidence limits. Just as different clinical measurements lead to different diagnostic tests, different hypothesis tests lead to different confidence intervals.\nIf we calculate a confidence interval many times with independent data sets, the \\(1 - \\alpha\\) confidence interval should contain \\(\\ptrue\\) with probability \\(1 - \\alpha\\). The actual probability that the confidence interval contains \\(\\ptrue\\) is called the coverage probability. A good confidence interval should have a coverage probability close to \\(1 - \\alpha\\) while being as narrow as possible. The Wald, score, and likelihood ratio tests from Section 3.3.2 are large-sample tests because they rely on consistency and asymptotic normality of the maximum likelihood estimate \\(\\hat{p}\\). All three tests can be inverted to produce confidence intervals that perform well in large samples. In smaller samples, the score and likelihood ratio confidence intervals often have better coverage probability and width than the Wald confidence interval (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001).\n\n3.4.1 Wald confidence intervals and the delta method\nThe Wald confidence limits come from solving the equation \\[\n  \\frac{(\\hat{p} - p)^2}{\\hat{p} (1 - \\hat{p}) / n}\n  = z_{1 - \\frac{\\alpha}{2}}^2.\n\\tag{3.16}\\] for \\(p\\), which gives us \\[\n  \\hat{p} \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p} (1 - \\hat{p})}{n}}.\n\\tag{3.17}\\] The coverage probabilities of Wald confidence intervals can be much lower than \\(1 - \\alpha\\), especially when \\(p_\\true\\) is close to zero or one (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001).\nAnother problem with the Wald confidence interval for \\(\\ptrue\\) is that it can have bounds outside \\([0, 1]\\). One way to avoid this is to calculate confidence limits for a transformation of \\(\\hat{p}\\) using the delta method. A good transformation \\(g(p)\\) should have continuous first derivatives and be strictly increasing or decreasing, so each value of \\(g(p)\\) corresponds to a single value of \\(p\\) (i.e., \\(g\\) is one-to-one). The delta method derives the approximate normal distribution \\(g(\\hat{p})\\) using the approximation \\[\n  g(\\hat{p}) \\approx g(\\ptrue) + g'(\\ptrue) (\\hat{p} - \\ptrue).\n\\] where \\(g'(\\ptrue)\\) is the slope of \\(g\\) at \\(\\ptrue\\). An example of this approximation is shown in Figure 3.5. The key insight is that \\[\n  \\Var[g(\\hat{p})] \\approx g'(\\ptrue)^2 \\Var(\\hat{p}),\n\\] which is a generalization of the fact that \\(\\Var(c \\hat{p}) = c^2 \\Var(\\hat{p})\\) for any constant \\(c\\). If \\(\\hat{p}\\) has an approximate \\(N\\bigl(\\ptrue, \\Var(\\hat{p})\\bigr)\\) distribution in large samples, then \\[\n  g(\\hat{p}) \\approxsim N\\bigl(g(\\ptrue), g'(\\ptrue)^2 \\Var(\\hat{p})\\bigr).\n\\] in large samples. Because our estimator \\(\\hat{p}\\) is consistent, we can replace the unknown \\(\\ptrue\\) with \\(\\hat{p}\\). Because \\(g\\) is one-to-one, we can calculate confidence limits for \\(\\ptrue\\) using the confidence limits for \\(g(\\ptrue)\\).\n\n\n\nCode\n\ndelta.R\n\n## Approximation used by the delta method\n\np &lt;- seq(0.02, 0.98, by = 0.01)\nlogit &lt;- function(p) log(p) - log(1 - p)\n\n# plot\nplot(p, logit(p), type = \"n\",\n     xlab = \"p\", ylab = \"logit(p)\")\ngrid()\nlines(p, logit(p))\npoints(0.6, logit(0.6))\nabline(logit(0.6) - 2.5, 1 / 0.24, lty = \"dashed\")\ntext(0.6, -1,\n     labels = expression(paste(\"logit(p) - \", logit(0.6) %~~% logit,\n                               \"'(0.6) (p - 0.6)\")))\n\n\n\n\n\n\n\n\n\nFigure 3.5: The approximation used by the delta method using the logistic transformation for a binomial confidence interval near \\(\\hat{p} = 0.6\\). The black curve is \\(\\logit(p)\\), and the dashed line shows the tangent line at \\(p = 0.6\\).\n\n\n\n\n\n\nA widely used transformation for probabilities is the logit transformation \\[\n  \\logit(p) = \\ln\\Bigl(\\frac{p}{1 - p}\\Bigr).\n\\] The odds corresponding to the probability \\(p\\) is \\(\\frac{p}{1 - p}\\), so the logit is the natural logarithm of the odds. The logit transformation maps the interval \\((0, 1)\\) onto all of $:\n\nAs \\(p \\rightarrow 0\\), the odds \\(p / (1 - p) \\rightarrow 0\\) and \\(\\logit(p) \\rightarrow -\\infty\\).\nWhen \\(p = 1 / 2\\), the odds \\(p / (1 - p) = 1\\) and \\(\\logit(p) = 0\\).\nAs \\(p \\rightarrow 1\\), the odds \\(p / (1 - p) \\rightarrow \\infty\\) and \\(\\logit(p) \\rightarrow \\infty\\).\n\nTo use the delta method, we need to calculate the derivative of \\(\\logit(p)\\). By the chain rule, \\[\n  \\logit'(p)\n  = \\frac{1 - p}{p} \\frac{1}{(1 - p)^2}\n  = \\frac{1}{p (1 - p)},\n\\] which is continuous and strictly positive for all \\(p \\in (0, 1)\\). By the delta method, the variance of \\(\\logit(\\hat{p})\\) is approximately \\[\n  \\logit'(\\ptrue)^2 \\frac{\\ptrue (1 - \\ptrue)}{n}\n  = \\frac{1}{\\ptrue^2 (1 - \\ptrue)^2} \\frac{\\ptrue (1 - \\ptrue)}{n}\n  = \\frac{1}{n \\ptrue (1 - \\ptrue)}.\n\\] When we replace the unknown \\(\\ptrue\\) with our MLE \\(\\hat{p}\\), we get the following confidence limits for \\(\\logit(\\ptrue)\\): \\[\n  \\logit(\\hat{p}) \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\frac{1}{n \\hat{p} (1 - \\hat{p})}}.\n\\] To get confidence limits for \\(\\ptrue\\), we use the inverse function for the logit, which is \\[\n  \\expit(v) = \\frac{e^v}{1 + e^v} = \\frac{1}{1 + e^{-v}}.\n\\] This is called the logistic function. If the confidence limits for \\(\\logit(\\ptrue)\\) are \\(a\\) and \\(b\\), then the confidence limits for \\(\\ptrue\\) are \\(\\expit(a)\\) and \\(\\expit(b)\\). These are guaranteed to be in \\((0, 1)\\) because \\(\\expit(v) \\in (0, 1)\\) for any \\(v \\in \\mathbb{R}\\). The logit-transformed confidence interval can have narrower width and a coverage probability closer to \\(1 - \\alpha\\) than the untransformed Wald confidence interval (Agresti 2013).\n\n\n3.4.2 Score (Wilson) confidence intervals\nThe score or Wilson confidence limits come from solving the equation \\[\n  \\frac{(\\hat{p} - p)^2}{p (1 - p) / n}\n  = z_{1 - \\frac{\\alpha}{2}}^2.\n  \\label{eq:eqscore}\n\\tag{3.18}\\] for \\(p\\) (Wilson 1927). This differs from Equation 3.16 for the Wald confidence interval because it uses \\(p\\) instead of \\(\\hat{p}\\) in the denominator. It is a quadratic equation in \\(p\\), so it has two solutions. The center of the resulting confidence interval is \\[\n  \\tilde{p}\n  = \\hat{p} \\Bigg(\\frac{n}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg)\n    + \\frac{1}{2} \\Bigg(\\frac{z_{1 - \\frac{\\alpha}{2}}^2}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg)\n  = \\frac{x + \\frac{1}{2} z_{1 - \\frac{\\alpha}{2}}^2}{n + z_{1 - \\frac{\\alpha}{2}}^2},\n\\tag{3.19}\\] where \\(x\\) is the number of diseased individuals in our sample. This is a weighted average of \\(\\hat{p}\\) and \\(\\sfrac{1}{2}\\) with weights proportional to \\(n\\) and \\(z_{1 - \\frac{\\alpha}{2}}^2\\), respectively. The resulting confidence interval is \\[\n  \\tilde{p} \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\tilde{V}}\n\\] where \\[\n  \\tilde{V}\n  = \\frac{\\hat{p} (1 - \\hat{p})}{n + z_{1 - \\frac{\\alpha}{2}}^2} \\Bigg(\\frac{n}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg)\n    + \\frac{\\big(\\frac{1}{2}\\big)^2}{n + z_{1 - \\frac{\\alpha}{2}}^2} \\Bigg(\\frac{z_{1 - \\frac{\\alpha}{2}}^2}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg).\n\\] This variance is a weighted average of the variances of sample proportions equal to \\(\\hat{p}\\) and \\(1 / 2\\) with the same weights as in \\(\\tilde{p}\\) and with \\(n + z_{1 - \\frac{\\alpha}{2}}^2\\) instead of \\(n\\) in the denominator. Wilson confidence intervals are narrower than the corresponding Wald intervals, and they have coverage probabilities much closer to \\(1 - \\alpha\\) (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001).\nThe Agresti-Coull confidence interval is a simplification of the Wilson confidence interval that replaces \\(\\hat{p}\\) with \\(\\tilde{p}\\) in the Wald confidence interval to get the confidence limits \\[\n  \\tilde{p} \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\frac{\\tilde{p} (1 - \\tilde{p})}{n}}.\n\\] Because \\(z_{0.975} \\approx 1.96\\), we have \\(\\tilde{p} \\approx \\frac{k + 2}{n + 4}\\) for a 95% confidence interval. In this case, the Agresti-Coull interval is often implemented as follows: ``Add two successes and two failures and then use the Wald formula’’ (Agresti and Coull 1998). This interval is only slightly wider than the score confidence interval, and the two intervals are nearly identical for \\(n &gt; 40\\) (Brown, Cai, and DasGupta 2001).\nThe likelihood ratio test can also be inverted to get confidence intervals, but these can only be calculated numerically. For the binomial model, the likelihood ratio and score confidence intervals are nearly identical (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001). The score intervals are more common in practice because they are easier to calculate.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#small-sample-estimation",
    "href": "mlestimation.html#small-sample-estimation",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.5 Small-sample estimation*",
    "text": "3.5 Small-sample estimation*\nMaximum likelihood estimates are consistent, asymptotically normal, and asymptotically efficient. However, they are not guaranteed to perform well in any finite sample. For a sample of \\(n\\) independent Bernoulli(\\(p\\)) random variables, the sum has a binomial(\\(n, p\\)) distribution and this can be used to find the finite-sample distribution of the sample mean. This distribution can be used directly to calculate point estimates, p-values, and confidence limits.\nConfidence limits calculated using the finite-sample distribution of a test statistic under \\(H_0\\) are called exact confidence limits. They can often be constructed to have a coverage probability of at least \\(1 - \\alpha\\). However, their coverage probabilities are often higher than \\(1 - \\alpha\\), and they can be much wider than approximate \\(1 - \\alpha\\) confidence intervals for the same parameter (Agresti and Coull 1998).\nIf the finite-sample distribution of the test statistic is not known exactly, it is possible to calculate point estimates, p-values, or confidence limits using simulations. This is the basic idea behind the bootstrap (Efron and Tibshirani 1994) and Monte Carlo methods (Robert and Casella 2004).\n\n3.5.1 Median unbiased estimate\nThe median unbiased estimate of \\(\\ptrue\\) is the value of \\(p\\) that makes \\[\n  \\Pr\\nolimits_p(X &lt; x) = \\Pr\\nolimits_p(X &gt; x)\n\\] where we use the subscript \\(p\\) to indicate that these probabilities are calculated assuming \\(\\ptrue = p\\). If \\(p_\\text{med}\\) is the median unbiased estimate, then \\[\n  \\sum_{k = 0}^{x - 1} \\binom{n}{k} p_\\text{med}^k \\big(1 - p_\\text{med}\\big)^{n - k}\n  + \\frac{1}{2} \\binom{n}{x} p_\\text{med}^k \\big(1 - p_\\text{med}\\big)^{n - x}\n  = \\frac{1}{2},\n\\] and \\[\n  \\frac{1}{2} \\binom{n}{x} p_\\text{med}^x \\big(1 - p_\\text{med}\\big)^{n - x}\n  + \\sum_{k = x + 1}^n \\binom{n}{k} p_\\text{med}^k \\big(1 - p_\\text{med}\\big)^{n - k}\n  = \\frac{1}{2}.\n\\] The median of the distribution of \\(p_\\text{med}\\) is always \\(\\ptrue\\) (Birnbaum 1964), which is a slightly different notion of unbiasedness than the unbiasedness of \\(\\hat{p}\\) where \\(\\E(\\hat{p}) = \\ptrue\\).\n\n\n3.5.2 Exact (Clopper-Pearson) and mid-p confidence intervals\nThe exact or Clopper-Pearson confidence limits for \\(\\ptrue\\) use the finite-sample distribution of the sample mean \\(\\hat{p}\\) Clopper and Pearson (1934). When \\(x &gt; 0\\), the lower \\(1 - \\alpha\\) confidence limit is the solution to \\[\n  \\sum_{k = x}^n \\binom{n}{k} p_\\text{lower}^k (1 - p_\\text{lower})^{n - k}\n  = \\frac{\\alpha}{2},\n\\tag{3.20}\\] so the upper tail of the binomial(\\(n\\), \\(p_\\text{lower}\\)) distribution has probability \\(\\alpha / 2\\). When \\(x = 0\\), we set \\(p_\\text{lower} = 0\\). When \\(x &lt; n\\), the upper confidence limit is the solution to \\[\n  \\sum_{k = 0}^x \\binom{n}{k} p_\\text{upper}^k (1 - p_\\text{upper})^{n - k}\n  = \\frac{\\alpha}{2},\n\\tag{3.21}\\] so the lower tail of the binomial(\\(n\\), \\(p_\\text{upper}\\)) distribution has probability \\(\\alpha / 2\\). When \\(x = n\\), we set \\(p_\\text{upper} = 1\\). This interval is guaranteed to have a coverage probability of at least \\(1 - \\alpha\\), but the price for this is that it is always wider than the Wald and Wilson confidence intervals (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001). In general, the score or likelihood ratio confidence intervals have better combinations of coverage probability and width.\nTo make exact confidence limits less conservative, we can include only \\(\\frac{1}{2} \\Pr(X = x)\\) instead of \\(\\Pr(X = x)\\) in the calculation of the tail probabilities in Equation 3.21 and Equation 3.20. The resulting confidence intervals are called mid-p exact confidence intervals (Lancaster 1961, berry1995mid). The lower \\(1 - \\alpha\\) mid-p exact confidence limit is the solution to \\[\n  \\frac{1}{2} \\binom{n}{x} p_\\text{lower}^x (1 - p_\\text{lower})^{n - x}\n    + \\sum_{k = x + 1}^n \\binom{n}{k} p_\\text{lower}^k (1 - p_\\text{lower})^{n - k}\n  = \\frac{\\alpha}{2}.\n\\] and the upper limit is the solution to \\[\n  \\sum_{k = 0}^{x - 1} \\binom{n}{k} p_\\text{upper}^k (1 - p_\\text{upper})^{n - k}\n    + \\frac{1}{2} \\binom{n}{x} p_\\text{upper}^x (1 - p_\\text{upper})^{n - x}\n  = \\frac{\\alpha}{2}.\n\\] The mid-p exact confidence limits are have good combinations of coverage probability and width as well as good perfomance in small samples (Brown, Cai, and DasGupta 2001).\n\n\n\n\nAgresti, Alan. 2013. Categorical Data Analysis. Third. Vol. 792. John Wiley & Sons.\n\n\nAgresti, Alan, and Brent A Coull. 1998. “Approximate Is Better Than ‘Exact’ for Interval Estimation of Binomial Proportions.” The American Statistician 52 (2): 119–26.\n\n\nAitchison, John, and SD Silvey. 1958. “Maximum-Likelihood Estimation of Parameters Subject to Restraints.” The Annals of Mathematical Statistics 29: 813–28.\n\n\nBirnbaum, Allan. 1964. “Median-Unbiased Estimators.” Bulletin of Mathematical Statistics 11: 25–34.\n\n\nBoos, Dennis D, and Leonard A Stefanski. 2013. Essential Statistical Inference. Springer.\n\n\nBrown, Lawrence D, T Tony Cai, and Anirban DasGupta. 2001. “Interval Estimation for a Binomial Proportion.” Statistical Science 16 (2): 101–17.\n\n\nClopper, Charles J, and Egon S Pearson. 1934. “The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial.” Biometrika 26 (4): 404–13.\n\n\nCohen, I Bernard. 1984. “Florence Nightingale.” Scientific American 250 (3): 128–37.\n\n\nCramér, Harald. 1946. Mathematical Methods of Statistics. Princeton University Press.\n\n\nDiamond, George A, and James S Forrester. 1983. “Clinical Trials and Statistical Verdicts: Probable Grounds for Appeal.” Annals of Internal Medicine 98 (3): 385–94.\n\n\nEfron, Bradley, and David V Hinkley. 1978. “Assessing the Accuracy of the Maximum Likelihood Estimator: Observed Versus Expected Fisher Information.” Biometrika 65 (3): 457–83.\n\n\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. Chapman & Hall/CRC.\n\n\nFreedman, David A. 2007. “How Can the Score Test Be Inconsistent?” The American Statistician 61 (4): 291–95.\n\n\nKenward, Michael G, and Geert Molenberghs. 1998. “Likelihood Based Frequentist Inference When Data Are Missing at Random.” Statistical Science 13 (3): 236–47.\n\n\nLancaster, H Oliver. 1961. “Significance Tests in Discrete Distributions.” Journal of the American Statistical Association 56 (294): 223–34.\n\n\nNeyman, Jerzy, and Egon Sharpe Pearson. 1933. “On the Problem of the Most Efficient Tests of Statistical Hypotheses.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 231 (694-706): 289–337.\n\n\nRao, C Radhakrishna. 1945. “Information and Accuracy Attainable in the Estimation of Statistical Parameters.” Bulletin of the Calcutta Mathematical Society 37 (3): 81–91.\n\n\n———. 1948. “Large Sample Tests of Statistical Hypotheses Concerning Several Parameters with Applications to Problems of Estimation.” In Mathematical Proceedings of the Cambridge Philosophical Society, 44:50–57. Cambridge University Press.\n\n\nReid, Nancy. 2003. “Asymptotics and the Theory of Inference.” The Annals of Statistics 31 (6): 1695–2095.\n\n\nRobert, Christian P, and George Casella. 2004. Monte Carlo Statistical Methods. Second edition. Springer.\n\n\nRothman, Kenneth J. 1978. “A Show of Confidence.” New England Journal of Medicine 299 (24): 1362–63.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\nWald, Abraham. 1943. “Tests of Statistical Hypotheses Concerning Several Parameters When the Number of Observations Is Large.” Transactions of the American Mathematical Society 54 (3): 426–82.\n\n\nWilks, Samuel S. 1938. “The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses.” The Annals of Mathematical Statistics 9 (1): 60–62.\n\n\nWilson, Edwin B. 1927. “Probable Inference, the Law of Succession, and Statistical Inference.” Journal of the American Statistical Association 22 (158): 209–12.\n\n\nWinkelstein Jr, Warren. 2009. “Florence Nightingale: Founder of Modern Nursing and Hospital Epidemiology.” Epidemiology 20 (2): 311.",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#footnotes",
    "href": "mlestimation.html#footnotes",
    "title": "3  Maximum Likelihood Estimation",
    "section": "",
    "text": "John Tukey (1915-2000) was an American mathematician and statistician who worked at Bell Labs and Princeton University. He developed the box plot, Tukey’s range test for multiple comparisons, and the fast Fourier transform. In 1947, he coined the term “bit” as shorthand for “binary digit”.↩︎\n She was elected a member of the Royal Statistical Society in 1859, where she was the first woman. In 1860, she founded the world’s first modern nursing school at St. Thomas Hospital in London.↩︎\n For finite \\(N\\), \\(X\\) actually has a hypergeometric distribution because the test results are not exactly independent. If the first person in our sample has disease, the probability that the next person we sample has disease is slightly less than \\(p\\). If the first person in our sample does not have disease, the probability that the next person we sample has disease is slightly greater than \\(p\\). When \\(N \\gg n\\), this hypergeometric distribution is approximately binomial(\\(n\\), \\(p\\)).↩︎\n Euler’s number \\(e\\) is named after Leonhard Euler (1707–1783), a Swiss mathematician who introduced the notation \\(f(x)\\) for mathematical functions and the letter \\(i\\) to denote the imaginary unit \\(\\sqrt{-1}\\). He spent most of his life in Berlin and St. Petersburg, and he is widely considered the greatest mathematician of the 18th century. The number \\(e\\) was first discovered in 1683 by Jacob Bernoulli (the namesake of the Bernoulli distribution) when studying compound interest, where \\(e = \\lim_{n \\rightarrow \\infty} (1 + 1 / n)^n\\). In 1748, Euler proved that \\(e = \\frac{1}{0!} + \\frac{1}{1!} + \\frac{1}{2!} + \\frac{1}{3!} + \\cdots\\).↩︎\n This is named for Josiah Willard Gibbs (1839–1903), an American scientist who earned the first American doctorate in engineering in 1863 and went on to work on statistical mechanics, thermodynamics, optics, and vector calculus as a professor of physics at Yale. Albert Einstein called him the greatest mind in American history.↩︎\n Named after Ronald Fisher (1890–1962), who established the foundations of maximum likelihood inference between 1912 and 1922. He was the most important statistician of the 20th century, and he was one of the founders of population genetics. He had poor eyesight for his entire life, which led him to develop a formidable sense of geometry in his head. However, he was also a leading eugenicist and one of the most vocal opponents of the hypothesis that smoking causes lung cancer.↩︎\n For estimating a parameter \\(\\theta\\), the conditions are these: (1) The set of possible values of the observed data \\(X\\) does not depend on \\(\\theta\\). (2) Each \\(\\theta\\) produces a different distribution of \\(X\\). (3) The true value of \\(\\theta\\) is in the interior of the set of possible values. (4) The log likelihood \\(\\ell(\\theta)\\) has continuous first and second derivatives with respect to \\(\\theta\\) in a neighborhood of \\(\\theta_\\true\\). These conditions are met by the binomial likelihood when \\(\\ptrue \\in (0, 1)\\).↩︎\n For simplicity, we are being vague about what we mean by \\(\\hat{\\mu}_n \\rightarrow \\mu\\). Probability has several different notions of convergence/. The weak LLN guarantees convergence in probability, which means that \\(\\lim_{n \\rightarrow \\infty} \\Pr\\big(|\\hat{\\mu}_n - \\mu| &gt; \\varepsilon\\big) = 0\\) for any \\(\\varepsilon &gt; 0\\). The strong LLN guarantees convergence almost surely, which means that \\(\\Pr\\big(\\lim_{n \\rightarrow \\infty} \\hat{\\mu}_n = \\mu\\big) = 1\\).↩︎\n Named after Carl Friedrich Gauss (1777-1855), a German mathematician who is widely considered one of the greatest mathematicians of all time. He discovered the normal distribution in 1809, but the CLT itself was first proved by Laplace in 1810 (see Chapter 1).↩︎\n Named after Swedish statistician Harald Cramér (1893–1985), who was a professor at Stockholm University, and Indian-American statistician Calyampudi Radhakrishna (C. R.) Rao (1920–2023), who was a professor at the Indian Statistical Institute, the University of Cambridge, the University of Pittsburgh, and Pennsylvania State University.↩︎\n Named after Abraham Wald (1902–1950), a Jewish Hungarian mathematician who was invited to move from Vienna to the United States in 1938 after Nazi Germany annexed Austria. He worked at the Statistical Research Group at Columbia University during World War II. In 1950, he and his wife were killed in a plane crash in India, where he was visiting the Indian Statistical Institute.↩︎\n Samuel S. Wilks (1906–1964) was an American mathematician and statistician who grew up on a farm in Texas, got a Ph.D. at the University of Iowa, and went on to be a professor at Princeton University.↩︎\n This approach to hypothesis testing was pioneered in the 1929s by Jerzy Neyman (1894–1981), a Polish mathematician and statistician who founded the first department of statistics in the United States at the University of California, Berkeley in 1938, and Egon Pearson (1895–1980), a British statistician who was a professor at University College London like his father Karl Pearson. ↩︎",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html#footnotes",
    "href": "bayes.html#footnotes",
    "title": "4  Bayesian Estimation",
    "section": "",
    "text": "Joseph Berkson (1899–1982) was an American physician and statistician at the Mayo Clinic in Rochester, Minnesota. He helped develop and popularize the use of logistic regression for binary outcomes, coining the term “logit” for the log odds in 1944. He also pioneered the study of selection bias, a special case of which is called “Berkson’s bias”. Later, he became a prominent opponent of the idea that smoking causes lung cancer. See https://en.wikipedia.org/wiki/Joseph_Berkson.↩︎",
    "crumbs": [
      "One-Sample Inference for Risks and Rates",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  }
]