[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analytical Epidemiology",
    "section": "",
    "text": "Preface\nOne day at lunch at the Harvard School of Public Health, I overheard Professor Murray Mittleman say: “I love epidemiology. It all fits together like a diamond.” As a second-year doctoral student in epidemiology, I was surprised to hear the subject described with such unstrained enthusiasm. It has taken years of study and experience for me to understand what he meant. On the way, I too have fallen in love.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "Analytical Epidemiology",
    "section": "Who this book is for",
    "text": "Who this book is for\nThis book is intended primarily for two audiences:\n\nEpidemiologists are often protected from the mathematical foundations of their field. The long-term price of this is “dogmatism, that is, a tendency to rigidly protect a partially understood theoretical heritage” (Morabia 2004). The mathematics needed for a deeper understanding of epidemiologic methods is within reach of anyone who has come far enough to need it. Whether you master this material or just learn to approach it with more patience than fear, you will be doing a service to epidemiology and to public health.\nBiostatisticians are familiar with probability and statistical inference, but applying statistics to solve scientific problems in public health requires skills different from those needed to prove that a method works under given assumptions. Epidemiology is a living example of the interplay between theory and applications in statistics, and epidemiologists have shown integrity, courage, and ingenuity in confronting causal questions with statistical tools.\n\nBeyond these audiences, I hope to explain the logic of epidemiology to any interested reader. It is possible that epidemiologic research has already helped save your life.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Analytical Epidemiology",
    "section": "How to use this book",
    "text": "How to use this book\nDifficult chapters, sections, subsections, and exercises are marked with an asterisk (*). These can be skipped without harming the logical flow of the book, but none of them is beyond the reach of a determined reader. The starring is recursive: Starred sections can be skipped within a starred chapter, starred subsections can be skipped within a starred section, and so on. Footnotes offer context or hint at more advanced material, and they can be ignored if they do not seem useful or interesting.\nThis is a work in progress. You may find that some parts are unfinished or just bad. Please report errors (including typos) or submit suggestions (especially good examples) at:\nhttps://github.com/ekenah/analyticalepi/issues.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Analytical Epidemiology",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis book is written in LaTeX and Quarto with calculations and figures generated in R, Python, and Inkscape. I have also included many links to Wikipedia. These are free, open-source, and publicly available thanks to the work of many contributors. I am also grateful to Google Scholar, PubMed, and the libraries at Ohio State, the University of Florida, the University of Washington, and Harvard University for giving me access to the scientific literature in epidemiology, statistics, public health, and medicine.\nTony Barry, Devesh Kapur, Paul Farmer, and James H. Maguire guided me to a career in public health when I was an undergraduate. James Robins, Miguel A. Hernán, Marc Lipsitch, and Stephen P. Luby helped me become an epidemiologist, biostatistician, and epidemic modeler in graduate school. My career began under the mentorship of Ira M. Longini, Jr., and M. Elizabeth Halloran as a postdoctoral fellow at the Univerity of Washington and an assistant professor at the University of Florida. My colleagues Yang Yang, Grzegorz Rempała, Forrest Crawford, and Patrick Schnell have all provided useful comments. For their patience with early versions of this material, I am grateful to the students of STA 6177/PHC 6937 (Applied Survival Analysis) at the University of Florida from 2013 to 2016 and PUBHEPI 8430 (Epidemiology 4) at The Ohio State University from 2019 to the present.\nMy parents, Chris and Kate Kenah, courageously allowed me to travel to places they had never been to and do things I had been told to avoid. These experiences in the United States, India, South Africa, and especially Bangladesh opened my eyes to the terrible importance of clear thinking in public health. My wife, Asma Aktar, and our sons Rafi, Rayhan, and Rabi remind me every day how important it is to destroy everything that stifles humanity. To that end, I hope this book is useful.\nAny mistakes are my own, and God knows best (الله أعلم).\n\n\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological Perspective.” In A History of Epidemiologic Methods and Concepts, edited by Alfredo Morabia, 3–125. Springer.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera. Second edition. John Churchill. https://wellcomecollection.org/works/uqa27qrt.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "1.1 Sets, experiments, and events\nTo begin at the beginning, we will start with probability. Morabia (2004) accurately observed that “Epidemiology came late in human history because it had to wait for the emergence of probability.” This is probably the most difficult chapter of the book, but it will make all subsequent chapters easier. You can use it as a reference and come back to the difficult parts when you need them. Learning to think clearly about probability will give you a compass to find your way through difficult terrain in epidemiology.\nTo speak clearly about probabilities, we need some basic notation for sets. If \\(A\\) is a set that contains an element \\(a\\), we write \\[a \\in A.\\] If \\(A\\) and \\(B\\) are sets such that every element of \\(A\\) is also an element of \\(B\\), we write \\[A \\subseteq B.\\] to indicate that \\(A\\) is a subset of \\(B\\). Sets \\(A\\) and \\(B\\) are equal if and only if \\(A \\subseteq B\\) and \\(B \\subseteq A\\), which means they contain exactly the same elements. The empty set with no elements is denoted \\(\\varnothing\\). For any set \\(A\\), it is true that \\(A \\subseteq A\\) and \\(\\varnothing \\subseteq A\\).\nWe use \\(\\mathbb{R}\\) to denote the real numbers. Intervals are subsets of \\(\\mathbb{R}\\) that take one of the following forms: \\[\\begin{aligned}\n  (a, b) &= \\{x \\in \\mathbb{R}: a &lt; x &lt; b\\}, \\\\\n  (a, b] &= \\{x \\in \\mathbb{R}: a &lt; x \\leq b\\}, \\\\\n  [a, b) &= \\{x \\in \\mathbb{R}: a \\leq x &lt; b\\}, \\\\\n  [a, b] &= \\{x \\in \\mathbb{R}: a \\leq x \\leq b\\}. \\\\\n\\end{aligned}\\] An endpoint with a square bracket is included in the interval; an endpoint with a round bracket is not. We can have \\(a = -\\infty\\) or \\(b = \\infty\\) as long as we use a round bracket for the corresponding endpoint. For example, it is true that \\(\\mathbb{R} = (-\\infty, \\infty)\\). However, \\(\\mathbb{R} \\neq [-\\infty, \\infty]\\) because \\(\\pm \\infty\\) are not real numbers.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#sets-experiments-and-events",
    "href": "probability.html#sets-experiments-and-events",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "1.1.1 Experiments and events\nIn probability, an experiment is any process that will produce one outcome out of a set possible outcomes. The set of possible outcomes is called the sample space and is traditionally denoted \\(\\Omega\\). An experiment produces a single outcome \\(\\omega \\in \\Omega\\). For example, the sample space for a single coin flip is \\[\\Omega = \\{H, T\\},\\] where \\(\\omega = H\\) if we get heads and \\(\\omega = T\\) if we get tails.\nThe outcomes in the sample space must determine everything about the random outcome of the experiment. If we flip a coin twice, the sample space cannot be \\(\\{H, T\\}\\) because each \\(\\omega \\in \\Omega\\) must specify the outcome of both coin flips. Instead, \\[\\Omega = \\{HH, HT, TH, TT\\}\\] where \\(\\omega = XY\\) if we get \\(X\\) on the first flip and \\(Y\\) on the second. This helps us see, for example, that there are two ways to get one \\(H\\) and one \\(T\\) in two coin flips.\nThe purpose of probability is to summarize uncertainty about the outcomes of experiments. However, the outcomes themselves do not have probabilities. Probabilities are assigned to events, which are subsets of the sample space \\(\\Omega\\). If \\(A\\) is an event, then \\(A\\) occurs if and only if the outcome \\(\\omega\\) produced by our experiment is an element of \\(A\\) (i.e., if and only if \\(\\omega \\in A\\)). If we flip a coin twice, the event that we get two heads is \\(\\{HH\\}\\), the event that we get one head is \\(\\{HT, TH\\}\\), and the event that we get zero heads is \\(\\{TT\\}\\). By definition, the event \\(\\Omega\\) always occurs and the event \\(\\varnothing\\) never occurs.\nIn experiments with a finite or countably infinite sample space,2 the distinction between the outcome \\(\\omega\\) and the event \\(\\{\\omega\\}\\) can be safely ignored. In more complex experiments (e.g., taking a random sample from a standard normal distribution), this distinction is important.3 In all cases, experiments have outcomes and events have probabilities.\nIn epidemiology, it is often useful to think of the sample space \\(\\Omega\\) as being a population and each \\(\\omega \\in \\Omega\\) as an individual in this population. In this context, our experiment is to sample a person from \\(\\Omega\\) and ask them questions, take measurements, or follow them over time to ascertain disease occurrence. Events would be subpopulations of \\(\\Omega\\), such as \\(\\{\\omega \\in \\Omega: \\omega \\text{ lives in Ohio}\\}\\). This event occurs if the sampled individual \\(\\omega\\) lives in Ohio, and it does not occur if they live somewhere else.\n\n\n1.1.2 Set operations and logic\nThere are three basic set operations that take one or more sets and define another set: complement, intersection, and union. Each operation has a simple interpretation in terms of logic.\n\nThe complement of a set \\(A\\) is \\[A^\\comp = \\{\\omega \\in \\Omega : \\omega \\not\\in A\\},\\] which can be interpreted logically as not \\(A\\). If \\(A\\) is an event, then the event \\(A^\\comp\\) occurs if \\(\\omega \\not\\in A\\). For the same reason that “not not A” means “A”, we have \\((A^\\comp)^\\comp = A\\).\nThe intersection of two sets \\(A\\) and \\(B\\) is \\[A \\cap B = \\{\\omega \\in \\Omega : \\omega \\in A \\text{ and } \\omega \\in B\\},\\] which can be interpreted logically as \\(A\\) and \\(B\\). If \\(A\\) and \\(B\\) are events, then the event \\(A \\cap B\\) occurs if \\(\\omega \\in A\\) and \\(\\omega \\in B\\).\nThe union of two sets \\(A\\) and \\(B\\) is \\[A \\cup B = \\{\\omega \\in \\Omega : \\omega \\in A \\text{ or } \\omega \\in B\\},\\] which can be interpreted logically as \\(A\\) or \\(B\\) as long as we use an inclusive “or” (i.e., and/or). If \\(A\\) and \\(B\\) are events, then the event \\(A \\cup B\\) occurs if \\(\\omega \\in A\\) or \\(\\omega \\in B\\).\n\nIf \\(A \\subseteq B\\), then \\(A \\cap B = A\\) and \\(A \\cup B = B\\). An important special case is that \\[\n  A \\cap A = A \\cup A = A.\n\\tag{1.1}\\] For the empty set \\(\\varnothing\\), we get \\(A \\cap \\varnothing = \\varnothing\\) and \\(A \\cup \\varnothing = A\\). For the sample space \\(\\Omega\\), we get \\(A \\cap \\Omega = A\\) and \\(A \\cup \\Omega = \\Omega\\).\nUnion and intersection are commutative operations like addition and multiplication, so the order of \\(A\\) and \\(B\\) does not matter: \\[\n  A \\cup B = B \\cup A\n\\] and \\[\n  A \\cap B = B \\cap A.\n\\] Events \\(A\\) and \\(B\\) are disjoint or mutually exclusive when \\(A \\cap B = \\varnothing\\). If \\(A\\) and \\(B\\) are disjoint, then at most of one of them can occur in a single experiment. Any set and its complement are disjoint, and the empty set \\(\\varnothing\\) is disjoint with itself and all other sets.\nIf \\(\\Omega\\) is a population, these set operations allow us to define subpopulations in terms of multiple traits. If the event \\(A = \\{\\omega \\in \\Omega: \\omega \\text{ lives in Ohio}\\}\\), then its complement \\(A^\\comp\\) contains all individuals in \\(\\Omega\\) who live outside Ohio. If the event \\(B = \\{\\omega \\in \\Omega: \\omega \\text{ is 42 years old}\\}\\), then the intersection \\(A \\cap B\\) contains everyone in \\(\\Omega\\) who is 42 years old and lives in Ohio. If \\(\\Omega\\) does not contain any 42-year-old Ohio residents, then \\(A\\) and \\(B\\) are disjoint. The union \\(A \\cup B\\) contains everyone in \\(\\Omega\\) who lives in Ohio or is 42 years old. This could include both a 24-year-old who lives Ohio and a 42-year-old who lives Michigan.\n\n\n1.1.3 Venn diagrams\nA useful tool for understanding events and set operations is the Venn diagram.4 An example is shown in Figure 1.1. The rectangle represents \\(\\Omega\\), and the circles \\(A\\) and \\(B\\) represent events. \\(A^\\comp\\) is everything in \\(\\Omega\\) outside the circle \\(A\\), and \\(B^\\comp\\) is everything outside the circle \\(B\\). Their intersection \\(A \\cap B\\) is the area where the two circles overlap. Their union \\(A \\cup B\\) is everything contained in at least one of \\(A\\) or \\(B\\).\n\n\n\n\n\n\nFigure 1.1: Venn diagram showing events \\(A\\) and \\(B\\). The area contained in both events is their intersection \\(A \\cap B\\). The union \\(A \\cup B\\) is all area contained in at least one of \\(A\\) and \\(B\\), including \\(A \\cap B\\).\n\n\n\n\n\n1.1.4 Sequences of events*\nIntersections can be written for more than two events. The intersection of \\(A_1, A_2, \\ldots, A_n\\) is \\[\n  I_n = \\bigcap_{i = 1}^n A_i.\n\\tag{1.2}\\] Because set intersection is commutative and associative, any ordering of \\(A_1, \\ldots, A_n\\) produces the same intersection. The event \\(I_n\\) occurs if and only if all of the events \\(A_1, \\ldots, A_n\\) occur. Each new event makes the intersection smaller (i.e., never larger) in the sense that \\[\n  \\bigcap_{i = 1}^{n + 1} A_i \\subseteq I_n.\n\\] whenever \\(A_{n + 1}\\) is another event.\nSimilarly, unions can be written for more than two events. If \\(A_1, A_2, \\ldots, A_n\\) is a set of events, then their union is \\[\n  U_n = \\bigcup_{i = 1}^n A_i.\n\\tag{1.3}\\] Because set union is commutative and associative, any ordering of \\(A_1, \\ldots, A_n\\) produces the same union. The event \\(U_n\\) occurs if and only if at least one of the events \\(A_i\\) occurs. Each new event makes the union bigger (i.e., never smaller) in the sense that \\[\n  U_n \\subseteq \\bigcup_{i = 1}^{n + 1} A_i\n\\] whenever \\(A_{n + 1}\\) is another event.\nBoth unions and intersections can be defined for infinite sequences of events.5 To describe this, we let \\(n = \\infty\\) in the notation from Equation 1.2 or Equation 1.3. The union of any finite sequence of events can be turned into the union of an infinite sequence of events by adding an endless sequence of empty sets to the finite sequence. The new sequence is still a sequence of disjoint events, and each empty set \\(\\varnothing\\) leaves the union unchanged. If \\((A_1, A_2, \\ldots)\\) is an infinite sequence of events such that \\(A_i = \\varnothing\\) for all \\(i &gt; n\\), then \\[\n  \\bigcup_{i = 1}^\\infty A_i = \\bigcup_{i = 1}^n A_i.\n\\] This turns out to be useful when we try to give a mathematically rigorous definition of probability.\n\n\n1.1.5 Algebra of sets\\(^*\\)\nUnions, intersections, and complements can be combined in complex ways. Fortunately, there are a few basic principles that can be used to simplify these calculations. We have already seen that unions and intersections are commutative. Unions and intersections are also associative, so \\[\n  A \\cup (B \\cup C)\n  = (A \\cup B) \\cup C\n\\] and \\[\n  A \\cap (B \\cap C)\n  = (A \\cap B) \\cap C\n\\] for any sets \\(A\\), \\(B\\), and \\(C\\).\nDe Morgan’s laws describe how complements affect unions and intersections. If \\(A\\) and \\(B\\) are sets, then \\[\n  (A \\cap B)^\\comp\n  = A^\\comp \\cup B^\\comp\n\\tag{1.4}\\] because you are outside \\(A \\cap B\\) if and only f you are outside \\(A\\) or outside \\(B\\). Similarly, \\[\n  (A \\cup B)^\\comp\n  = A^\\comp \\cap B^\\comp.\n\\tag{1.5}\\] because you are outside \\(A \\cup B\\) if and only if you are outside \\(A\\) and outside \\(B\\). Note that each of these equations implies the other if we replace \\(A = (A^\\comp)^\\comp\\) with \\(A^\\comp\\) and replace \\(B = (B^\\comp)^\\comp\\) with \\(B^\\comp\\). They are two sides of the same coin, but it is helpful to remember them both.\nThe distributive properties describe how unions and intersections interact with each other. Recall that multiplication distributes over addition, so \\(a (b + c) = ab + ac\\). For any sets \\(A\\), \\(B\\), and \\(C\\), we have the following distributive properties:\n\nIntersections distribute over unions, so \\[\n  A \\cap (B \\cup C)\n  = (A \\cap B) \\cup (A \\cap C).\n\\]\nUnions distribute over intersections, so \\[\n  A \\cup (B \\cap C)\n  = (A \\cup B) \\cap (A \\cup C).\n\\]\n\nIntersections and unions also distribute over themselves. However, this is a consequence of commutativity, associativity, and Equation 1.1, not a separate property like the distributive rules above.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#probability",
    "href": "probability.html#probability",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.2 Probability",
    "text": "1.2 Probability\nA probability measure is a function that takes an event \\(A \\subseteq \\Omega\\) and returns a number \\(\\Pr(A) \\in [0, 1]\\) in any way that conforms to the following rules:\n\n\\(\\Pr(\\Omega) = 1\\).\n\\(\\Pr(A) \\in [0, 1]\\) for any event \\(A \\subseteq \\Omega\\).6\nThe addition rule: If \\((A_1, A_2, \\ldots)\\) is any sequence of disjoint events, then \\[\n  \\Pr\\Biggl( \\bigcup_{i = 1}^\\infty A_i\\Biggr)\n  = \\sum_{i = 1}^\\infty \\Pr(A_i).\n\\] The addition rule is stated in terms of an infinite sequence of disjoint events because this implies the addition rule for any finite sequence of disjoint events (see Section 1.1.4).\n\nIt is useful to think of probability as a generalization of our intuitions about area or volume. When there is no overlap in a set of two-dimensional shapes, we can get the total area they cover by adding up the areas of the individual shapes. Similarly, we can get the total volume taken up by a set of bowling balls by adding up their individual volumes.\nThere is a lot of debate about the meaning of probability, but its definition does not assume any particular interpretation. Probability calculations are based on the rules above no matter what we think it all means, and any interpretation consistent with these rules is valid.\n\n1.2.1 Probability calculations\nSeveral useful properties of probability follow immediately from the definition above. A short proof follows each result. To follow the proofs, it helps to draw Venn diagrams.\n\nTheorem 1.1 If \\(A\\) is an event, \\(\\Pr\\bigl(A^\\comp\\bigr) = 1 - \\Pr(A)\\).\n\n\nProof. Because \\(\\Omega = A \\cup A^\\comp\\) and \\(A\\) and \\(A^\\comp\\) are disjoint, we have \\[\n    \\Pr(A) + \\Pr\\bigl(A^\\comp\\bigr) = \\Pr(\\Omega) = 1\n  \\] by the addition rule. The result follows when we subtract \\(\\Pr(A)\\) from both sides.\n\n\nTheorem 1.2 If \\(A\\) and \\(B\\) are events such that \\(A \\subseteq B\\), then \\(\\Pr(A) = \\Pr(B) - \\Pr\\bigl(B \\cap A^\\comp\\bigr)\\). This implies that \\(\\Pr(A) \\leq \\Pr(B)\\).\n\n\nProof. Each element of \\(B\\) either is or is not in \\(A\\), so \\[\n    B = (B \\cap A) \\cup \\big(B \\cap A^\\comp\\big)\n    = A \\cup \\big(B \\cap A^\\comp\\big).\n  \\] where the second equality follows from the fact that \\(B \\cap A = A\\) because \\(A \\subseteq B\\). The two sets on the right-hand side are disjoint, so we have \\[\n    \\Pr(B) = \\Pr(A) + \\Pr\\bigl(B \\cap A^\\comp\\bigr)\n  \\] by the addition rule. The result follows if we subtract \\(\\Pr\\bigl(B \\cap A^\\comp\\bigr)\\) from both sides. This implies that \\(\\Pr(A) \\leq \\Pr(B)\\) because \\(\\Pr\\bigl(B \\cap A^\\comp\\bigr) \\geq 0\\).\n\n\nTheorem 1.3 If \\(A\\) and \\(B\\) are events, \\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\).\n\n\nProof. We can break \\(A \\cup B\\) into three disjoint sets: elements of \\(A\\) and not \\(B\\), elements of \\(B\\) and not \\(A\\), and elements of both \\(A\\) and \\(B\\). In set notation, this is \\[\n    A \\cup B = \\big(A \\cap B^\\comp\\big) \\cup \\big(B \\cap A^\\comp\\big) \\cup (A \\cap B).\n  \\] By the addition rule, \\[\n    \\Pr(A \\cup B) = \\Pr\\bigl(A \\cap B^\\comp\\bigr) + \\Pr\\bigl(B \\cap A^\\comp\\bigr) + \\Pr(A \\cap B).\n   \\tag{1.6}\\] By Theorem 1.2, we have \\[\n      \\Pr\\bigl(A \\cap B^\\comp\\bigr)\n      = \\Pr(A) - \\Pr(A \\cap B), \\\\\n  \\] because \\(A \\cap B \\subseteq A\\) and \\[\n      \\Pr\\bigl(B \\cap A^\\comp\\bigr)\n      = \\Pr(B) - \\Pr(A \\cap B).\n  \\] because \\(A \\cap B \\subseteq B\\). The result follows from substituting these back into Equation 1.6 and collecting terms involving \\(\\Pr(A \\cap B)\\). Intuitively, \\(\\Pr(A) + \\Pr(B)\\) includes the overlap \\(\\Pr(A \\cap B)\\) twice, so we have to subtract out one of them. This can be see clearly in Figure 1.1.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#random-variables",
    "href": "probability.html#random-variables",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.3 Random variables",
    "text": "1.3 Random variables\nThe outcomes of an experiment can be anything, not just numbers. A random variable is a real-valued function defined on a sample space \\(\\Omega\\). In other words, a random variable \\(X\\) is a function that takes an argument \\(\\omega \\in \\Omega\\) as input and returns a value \\(X(\\omega) \\in \\mathbb{R}\\). Traditionally, random variables are written as capital letters and possible values are written as lower-case letters, so \\(\\Pr(X = x)\\) denotes the probability of the event \\[\n  \\{\\omega \\in \\Omega : X(\\omega) = x\\}.\n\\] For simplicity, random variables are usually written without the argument \\(\\omega\\).\nThe distinction between outcomes and random variables is useful because we can define multiple random variables on the same sample space. For example, the height, weight, and age of an individual \\(\\omega\\) sampled from a population \\(\\Omega\\) are different random variables defined on the same sample space.\n\n1.3.1 Indicator variables\nThe simplest random variables are indicator variables. For an event \\(A\\), the indicator variable \\[\n  \\indicator_A(\\omega)\n  = \\begin{cases}\n    1 & \\text{ if } \\omega \\in A, \\\\\n    0 & \\text{ if } \\omega \\not\\in A.\n  \\end{cases}\n\\] Indicator variables are binary random variables, which take exactly two values. In practice, these values should be zero and one unless there is a specific reason to do otherwise. When sampling from a population, we can define indicator variables for membership in different subpopulations.\nAll of the basic set operations above can be expressed in terms of indicator variables for sets.\n\nThe indicator function for the complement of \\(A\\) is \\[\n  \\indicator_{A^\\comp} = 1 - \\indicator_A.\n\\tag{1.7}\\]\nIf \\(B\\) is another event and \\(\\indicator_B\\) is its indicator variable, then the indicator variable for the intersection \\(A\\) and \\(B\\) is the product of their indicator variables: \\[\n  \\indicator_{A \\cap B}\n  = \\indicator_A \\indicator_B.\n\\tag{1.8}\\]\nThe indicator variable for the union \\(A \\cup B\\) is \\[\n  \\indicator_{A \\cup B}\n  = 1 - (1 - \\indicator_A) (1 - \\indicator_B)\n  = \\indicator_A + \\indicator_B - \\indicator_{A \\cap B}.\n\\tag{1.9}\\] This follows from Equation 1.5 because \\(A \\cup B = (A^\\comp \\cap B^\\comp)^\\comp\\).\n\n\nR\n\n\n\n\nindicators.R\n\n## Indicator variables for events A and B, etc.\n\n# Setting the seed ensures that everyone gets the same random samples.\n# Functions are called using parentheses (round brackets).\n# The function rbinom() is a random sample from a binomial distribution.\nset.seed(42)\nn &lt;- 100\ndat &lt;- data.frame(A = rbinom(n, 1, 0.3))\ndat$B &lt;- rbinom(n, 1, 0.6)\n\n# inspecting a data frame\nnames(dat)  # variables in the data frame\nnrow(dat)   # number of rows (individuals)\nncol(dat)   # number of columns (variables)\ndim(dat)    # rows and columns in the data frame\nstr(dat)    # summary of the data frame structure (variables and types)\n\n# inspecting columns of a data frame (or vectors)\n# Our sample space or population consists of 100 individuals.\n# Square brackets are used for indices, which can be numbers or TRUE/FALSE.\ndat$A                 # indicator for A for all 100 individuals\ndat$A[10]             # indicator for A in individual 10\ndat$A[2:6]            # indicator variables for individuals 2 to 6\ndat$A[c(10, 20, 30)]  # A indicators for individuals 10, 20, and 30\nwhich(dat$A == 1)     # which individuals are in event A\nwhich(dat$A == 0)     # which individuals are not in event A\n\n# indicator variable for A complement\n# In R (and many other languages), \"!\" means \"not\".\n# The function as.integer() changes TRUE/FALSE to 1/0.\ndat$Acomp &lt;- as.integer(!dat$A)\n\n# indicator variable for A intersection B\n# In R (and many other languages), \"&\" means \"and\".\ndat$ABintersect &lt;- as.integer(dat$A & dat$B)\n\n# indicator variable for A union B\n# In R (and many other languages), \"|\" means \"or\".\ndat$ABunion &lt;- as.integer(dat$A | dat$B)\n\n# save the data frame as a CSV file\n# The file argument can be a path (e.g., \"./data/indicators.csv\" in Linux).\nwrite.csv(dat, file = \"indicators.csv\", row.names = FALSE)\n\n\n\n\n\n\n1.3.2 Probability distributions\nThe set of possible values of a random variable \\(X\\) is called the support of \\(X\\) and denoted \\(\\supp(X)\\).7 For example, the support of an indicator variable is \\(\\{0, 1\\}\\). In this section, we will focus on discrete random variables, which have a support on a finite or countably infinite set. There are two standard ways to describe the distribution of a discrete random variable:\n\nThe probability mass function (PMF) of a discrete random variable \\(X\\) is \\[\n  f(x) =\n  \\begin{cases}\n    \\Pr(X = x) &gt; 0  & \\text{ if } x \\in \\supp(X), \\\\\n    0               & \\text{ if } x \\not \\in \\supp(X).\n  \\end{cases}\n\\] Because \\(\\Pr(\\Omega) = 1\\), we always have \\[\n  \\sum_{x \\in \\supp(X)} f(x) = 1.\n\\]\nThe cumulative distribution function (CDF) of \\(X\\) is \\[\n  F(x)\n  = \\Pr(X \\leq x).\n\\] \\(F(x)\\) is monotonically increasing in \\(x\\), which means that \\(F(a) \\leq F(b)\\) whenever \\(a &lt; b\\). It has a jump upward of size \\(f(x)\\) at each \\(x \\in \\supp(X)\\), and its value at each such \\(x\\) is the value that it jumps to—not the value that it jumps up from. For sufficiently small \\(x\\), \\(F(x)\\) can be made arbitrarily close to zero. For sufficiently large \\(x\\), \\(F(x)\\) can be made arbitrarily close to one. More formally, we say that \\(\\lim_{x \\downarrow -\\infty} F(x) = 0\\) and \\(\\lim_{x \\uparrow \\infty} F(x) = 1\\).\n\nThe PMF and CDF provide equivalent descriptions of the distribution of \\(X\\) in the sense that either of these functions can be used to calculate the other. Given the PMF \\(f\\), the CDF is defined by \\[\n  F(x) = \\sum_{\\substack{v \\in \\supp(X): \\\\ v \\leq x}} f(v).\n\\] where the sum is taken over all \\(u \\in \\supp(X)\\) such that \\(u \\leq x\\). Given the CDF \\(F\\), the PMF is defined by \\[\n  f(x) = F(x) - \\max_{v \\leq x} F(v)\n\\] where the maximum is \\(F(v)\\) for the largest \\(v \\in \\supp(X)\\) such that \\(v &lt; x\\).\n\n\n1.3.3 Mean\nThe mean or expected value of a random variable \\(X\\) is \\[\n  \\E(X)\n  = \\sum_{x \\in \\supp(X)} x \\Pr(X = x)\n  = \\sum_{x \\in \\supp(X)} x f(x),\n\\] where \\(f\\) is the PMF of \\(X\\). The mean is often written \\(\\mu\\), and it is often described as a measure of the “location” or “central tendency” of \\(X\\).\nIndicators are an extremely useful for calculating probabilities using means. For any event \\(A\\), its probability is the mean of the indicator variable \\(\\indicator_A\\): \\[\n  \\Pr(A)\n  = 0 \\Pr(\\indicator_A = 0) + 1 \\Pr(\\indicator_A = 1)\n  = \\E(\\indicator_A).\n\\] This is a common way to calculate probabilities in data analyses.\n\nR\n\n\n\n\nprobabilities.R\n\n## Indicator variables and probability calculations\n\n# read in CSV file with indicator variables using the function read.csv()\n# The argument can be a path (e.g., \"./data/indicators.csv\" in Linux).\ndat &lt;- read.csv(\"indicators.csv\")\n\n# calculate probabilities from indicator variables using the function mean()\n# This will also work with TRUE/FALSE (i.e., logical) variables, which are\n# converted to TRUE = 1 and FALSE = 0 in calculations.\nprob_A &lt;- mean(dat$A)\nprob_B &lt;- mean(dat$B)\nprob_Acomp &lt;- mean(dat$Acomp)\nprob_ABintersect &lt;- mean(dat$ABintersect)\nprob_ABunion &lt;- mean(dat$ABunion)\n\n# Pr(A complement) = 1 - Pr(A)\nprob_Acomp\n1 - prob_A\n\n# Pr(A union B) = Pr(A) + Pr(B) - Pr(A intersect B)\nprob_ABunion\nprob_A + prob_B - prob_ABintersect\n\n# Beware of numerical error when comparing floating-point numbers!\n# This example is from The R Inferno by Patrick Burns.\n# https://www.burns-stat.com/pages/Tutor/R_inferno.pdf\n0.1 == 0.3 / 3\nsprintf(\"%.20f\", 0.1)\nsprintf(\"%.20f\", 0.3 / 3)\n\n# math can be more accurate than computers (which is not their fault)\nprob_ABunion == prob_A + prob_B - probABintersect\nsprintf(\"%.20f\", prob_ABunion)\nsprintf(\"%.20f\", prob_A + prob_B - prob_ABintersect)\n\n\n\n\n\n\n1.3.4 Variance\nIf \\(X\\) has \\(\\E(X) = \\mu\\), then \\((X - \\mu)^2\\) is another random variable. The variance of \\(X\\) is the expected value of \\((X - \\mu)^2\\): \\[\n  \\Var(X)\n  = \\E\\big[(X - \\mu)^2\\big]\n  = \\sum_{x \\in \\supp(X)} (x - \\mu)^2 f(x).\n\\] {eq-Var} Because \\((x - \\mu)^2 \\geq 0\\) with equality if and only if \\(x = \\mu\\), we always have \\(\\Var(X) \\geq 0\\). We have \\(\\Var(X) = 0\\) if and only if \\(X = \\mu\\) with probability one. An equivalent expression for the variance that is often easier to use is: \\[\n  \\Var(X) = \\E(X^2) - \\mu^2\n\\tag{1.10}\\] where \\(\\E(X^2)\\) is the expected value of the random variable \\(X^2\\). The variance is often written \\(\\sigma^2\\), and it is often described as a measure of the dispersion of \\(X\\) around the mean.\nThe square root of the variance is called the standard devation, which is often written \\(\\sigma\\). If a random variable \\(X\\) has units (e.g., length, weight, or time), the mean and the standard deviation have the same units as \\(X\\). For example, the mean and standard deviation of a length in meters both have units of \\(\\text{meters}\\) but the variance has units of \\(\\text{meters}^2\\).\n\n\n1.3.5 Bernoulli distribution\nThe distribution of an indicator variable is called the Bernoulli distribution.8 A random variable with the Bernoulli(\\(p\\)) distribution has the PMF \\[\n  f(x)\n  = p^x (1 - p)^{1 - x}\n  = \\begin{cases}\n    1 - p &\\text{if } x = 0\\\\\n    p     &\\text{if } x = 1.\n  \\end{cases}\n\\] Equivalently, it has the CDF \\[\n  F(x)\n  = \\begin{cases}\n    0     &\\text{if } x &lt; 0 \\\\\n    1 - p &\\text{if } x \\in [0, 1) \\\\\n    1     &\\text{if } x \\geq 1.\n  \\end{cases}\n\\] If a random variable \\(X\\) has a Bernoulli(\\(p\\)) distribution, we write \\(X \\sim \\text{Bernoulli}(p)\\). The indicator variable for an event \\(A\\) has a Bernoulli distribution with \\(p = \\Pr(A)\\).\nIf \\(X \\sim \\Bernoulli(p)\\), then it has mean \\[\n  \\E(X)\n  = 0 \\times (1 - p) + 1 \\times p \\\\\n  = p\n\\] and variance \\[\n  \\Var(X)\n  = (0 - p)^2(1 - p) + (1 - p)^2 p \\\\\n  = p (1 - p).\n\\] Its standard deviation is \\(\\sqrt{p (1 - p)}\\), which is greater than zero unless \\(p = 0\\) or \\(p = 1\\). If \\(p = 0\\), then \\(X = 0\\) with probability one. If \\(p = 1\\), then \\(X = 1\\) with probability one.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#joint-and-marginal-distributions",
    "href": "probability.html#joint-and-marginal-distributions",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.4 Joint and marginal distributions",
    "text": "1.4 Joint and marginal distributions\nIf \\(X\\) and \\(Y\\) are random variables defined on the same probability space, then their joint probability mass function is \\[\n  f(x, y)\n  = \\Pr(X = x \\text{ and } Y = y)\n  = \\Pr\\big(\\{\\omega: X(\\omega) = x \\text{ and } Y(\\omega) = y\\}\\big).\n\\] The marginal probability mass functions are the PMFs of \\(X\\) or \\(Y\\) individually, which can be calculated from the joint PMF. The marginal PMF of \\(X\\) is \\[\n  f_X(x) = \\sum_{y \\in \\supp(Y)} f(x, y),\n\\] and the marginal PMF of \\(Y\\) is \\[\n  f_Y(y) = \\sum_{x \\in \\supp(X)} f(x, y).\n\\] These are called marginal distributions by analogy to the margins of a table. The distinction between joint and marginal distributions is extremely important in epidemiology and other applications of probability.\nFor example, Table 1.1 shows the joint and marginal PMFs for two binary random variables \\(X\\) and \\(Y\\). By definition, \\[\n  f(0, 0) + f(0, 1) + f(1, 0) + f(1, 1) = 1.\n\\] In the table, it is clear that the joint distribution determines the marginal distributions. However, there are many different joint distributions that are consistent with the same marginal distributions. Thus, the marginal distributions do not determine the joint distribution.9\n\n\n\nTable 1.1: Joint and marginal PMFs for binary random variables \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Y = 0\\)\n\\(Y = 1\\)\n\\(X\\) margin\n\n\n\n\n\\(X = 0\\)\n\\(f(0, 0)\\)\n\\(f(0, 1)\\)\n\\(f_X(0) = f(0, 0) + f(0, 1)\\)\n\n\n\\(X = 1\\)\n\\(f(1, 0)\\)\n\\(f(1, 1)\\)\n\\(f_X(1) = f(1, 0) + f(1, 1)\\)\n\n\n\\(Y\\) margin\n\\(f_Y(0) = f(0, 0) + f(1, 0)\\)\n\\(f_Y(1) = f(0, 1) + f(1, 1)\\)\n1\n\n\n\n\n\n\n\nR\n\n\n\n\njointdist.R\n\n## Joint and marginal distributions of indicators for events A and B\n\n# read indicator variable data from the CSV file\ndat &lt;- read.csv(\"indicators.csv\")\nn &lt;- nrow(dat)\n\n# tables of counts\n# Putting \"&lt;name&gt; = \" before the vector creates a label.\ntable(A = dat$A)\ntable(B = dat$B)\n\n# joint table of counts\n# In table(), the first argument defines rows and the second defines columns.\n# The addmargins() functions adds the row, column, and overall sums.\ntable(A = dat$A, B = dat$B)\naddmargins(table(A = dat$A, B = dat$B))\n\n# tables of probabilities\n# Table margins match the distributions of A (rows) and B (columns).\ntable(Adist = dat$A) / n    # marginal distribution of A indicator\ntable(Bdist = dat$B) / n    # marginal distribution of B indicator\naddmargins(table(A = dat$A, B = dat$B)) / n   # joint distribution\n\n\n\n\nJoint distributions can be defined for more than two random variables. If \\(X_1, X_2, \\ldots, X_n\\) are random variables defined on the same sample space, then their joint PMF is \\[\n  f(x_1, x_2, \\ldots, x_n) = \\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n).\n\\] The marginal distribution of each \\(X_i\\) can be found by adding up the PMF over the support of all the other random variables. For example, \\[\n  f_{X_2}(x_2) = \\sum_{x_1 \\in \\supp(X_1)} \\sum_{x_3 \\in \\supp(X_3)} f(x_1, x_2, x_3).\n\\] when \\(n = 3\\). In this same case, we can talk about the joint distribution of any two variables marginalized over the third. For example, \\[\n  f_{X_2, X_3}(x_2, x_3) = \\sum_{x_1 \\in \\supp(X_1)} f(x_1, x_2, x_3).\n\\] For larger \\(n\\), the formulas gets uglier but the ideas are the same.\n\n1.4.1 Linear combinations*\nIf \\(a\\) and \\(b\\) are constants, then \\(a X + b Y\\) is another random variable on \\(\\Omega\\). It is called a linear combination of \\(X\\) and \\(Y\\). Linear combinations can be defined for more than two random variables. If \\(X_1, \\ldots, X_n\\) are random variables defined on a sample space and \\(a_1, \\ldots, a_n\\) are constants, then \\[\n  \\sum_{i = 1}^n a_i X_i = a_1 X_1 + a_2 X_2 + \\cdots + a_n X_n\n\\] is a linear combination of \\(X_1, \\ldots, X_n\\). The constants can be any real numbers, including one and zero.\nSection 1.3.1 contains both examples and non-examples of linear combinations of random variables.\n\nThe indicator function for \\(A^\\comp\\) in Equation 1.7 is a linear combination of \\(\\indicator_A\\) and the random variable \\(\\indicator_\\Omega\\), which equals one for all \\(\\omega \\in \\Omega\\).\nThe indicator function for \\(A \\cup B\\) in Equation 1.9 is linear combination of the indicator variables \\(\\indicator_A\\), \\(\\indicator_B\\), and \\(\\indicator_{A \\cap B}\\).\nThe indicator function for \\(A \\cap B\\) in Equation 1.8 is not a linear combination of \\(\\indicator_A\\) and \\(\\indicator_B\\) because we have to multiply these two variables.\n\nIf \\(X\\) and \\(Y\\) are random variables defined on the same sample space and \\(a\\) and \\(b\\) are constants, the mean of the linear combination \\(a X + b Y\\) is \\[\n  \\E(a X + b Y) = a \\E(X) + b \\E(Y).\n\\tag{1.11}\\] This is a direct consequence of the definition of expected value: \\[\n  \\begin{aligned}\n    \\E(a X + b Y)\n    &= \\sum_{x \\in \\supp(X)} \\sum_{y \\in \\supp(Y)} (a x + b y) f(x, y) \\\\\n    &= a \\sum_{x \\in \\supp(X)} \\bigg(x \\sum_{y \\in \\supp(Y)} f(x, y)\\bigg)\n      + b \\sum_{y \\in \\supp(Y)} \\bigg(y \\sum_{x \\in \\supp(X)} f(x, y)\\bigg) \\\\\n    &= a \\sum_{x \\in \\supp(X)} x f_X(x) + b \\sum_{y \\in \\supp(Y)} y f_Y(y).\n  \\end{aligned}\n\\] The algebra is not pretty, but the logic is straightforward. We split up the sum into parts depending only on \\(x\\) and only on \\(y\\) outside the joint PMF. In each part, we factor out a constant and find the marginal PMF. This same logic extends to a linear combination of any number of random variables.\n\n\n1.4.2 Variance and covariance*\nThe variance of \\(a X + b Y\\) is \\[\n  \\Var(a X + b Y) = a^2 \\Var(X) + b^2 \\Var(Y) + 2 a b \\Cov(X, Y)\n\\tag{1.12}\\] where \\[\n  \\Cov(X, Y) = \\E\\bigl[\\big(X - \\E(X)\\big) \\big(Y - \\E(Y)\\big)\\bigr]\n\\] is called the covariance of \\(X\\) and \\(Y\\). Note that \\(\\Cov(X, Y) = \\Cov(Y, X)\\). Because \\(\\Var(X) = \\Cov(X, X)\\), variance is a special case of covariance. When \\(X\\) and \\(Y\\) are independent in the sense that the value of one tells us nothing about the value of the other, then \\(\\Cov(X, Y) = 0\\) and \\(\\Var(a X + b Y) = a^2 \\Var(X) + b^2 Var(Y)\\).10\nThe joint distribution of \\(X\\) and \\(Y\\) has a covariance matrix which is \\[\n  \\begin{bmatrix}\n    \\Var(X)     & \\Cov(X, Y) \\\\\n    \\Cov(X, Y)  & \\Var(Y)\n  \\end{bmatrix}\n\\] The variances are along the diagonal of the matrix, and the covariances appear off the diagonal. Because \\(\\Cov(X, Y) = \\Cov(Y, X)\\), covariance matrices are always symmetric (i.e., symmetric across the diagonal). Covariance matrices are an extremely useful tool for calculating the variances of linear combinations of random variables. For example: \\[\n  \\Var(a X + b Y)\n  = \\begin{pmatrix}\n      a & b\n    \\end{pmatrix}\n    \\begin{bmatrix}\n      \\Var(X)     & \\Cov(X, Y) \\\\\n      \\Cov(X, Y)  & \\Var(Y)\n    \\end{bmatrix}\n    \\begin{pmatrix}\n      a \\\\ b\n    \\end{pmatrix}\n\\] in matrix and vector notation from linear algebra. This logic extends to linear combinations of any number of random variables.\nThe covariance is the numerator of the Pearson correlation coefficient,11 which is \\[\n  \\rho_{XY} = \\rho_{YX}\n  = \\frac{\\Cov(X, Y)}{\\sqrt{\\Var(X) \\Var(Y)}}.\n\\] Because of the Cauchy-Schwarz inequality, it turns out that \\(\\rho_{XY} \\in [-1, 1]\\).\n\nWe get \\(\\rho_{XY} = -1\\) if and only if \\(Y = c X\\) for some negative constant \\(c\\).\nWe get \\(\\rho_{XY} = 1\\) if and only if \\(Y = c X\\) for some positive constant \\(c\\). For example, \\(\\rho_{XX} = 1\\) for any random variable \\(X\\).\nWe get \\(\\rho_{XY} = 0\\) if (but not only if) \\(X\\) and \\(Y\\) are independent. However, it is possible to have \\(\\rho_{XY} = 0\\) when \\(X\\) and \\(Y\\) are not independent.\n\nIf we divide each entry \\(\\Cov(X, Y)\\) in a covariance matrix by \\(\\sqrt{\\Var(X) \\Var(Y)}\\), when we get a correlation matrix. Any correlation matrix is symmetric, and the entries along its diagonals are all ones.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#probability-and-disease-occurrence",
    "href": "probability.html#probability-and-disease-occurrence",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "1.5 Probability and disease occurrence",
    "text": "1.5 Probability and disease occurrence\nIn epidemiology, there are two fundamental measures of disease occurrence that are probabilities: prevalence and risk. In both cases, our experiment is to sample an individual \\(\\omega\\) from a population \\(\\Omega\\). The disease outcome is a binary random variable \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has the disease outcome}, \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] The set of individuals in \\(\\Omega\\) who have \\(D(\\omega) = 1\\) is an event in \\(\\Omega\\), and our measure of disease occurrence is \\[\n  \\Pr(\\{\\omega \\in \\Omega: D(\\omega) = 1\\}).\n\\] The most important difference between prevalence and risk is the role of time in the definition of \\(D\\).\nThere is an important technical detail to remember when we talk about disease onset and recovery. When a person has disease onset at time \\(\\tonset\\) and recovers at time \\(\\trec\\), they have disease for each \\(t \\in [\\tonset, \\trec)\\). We assume that \\(\\trec &gt; \\tonset\\) so this interval is nonempty. We let the onset and recovery times for person \\(i\\) be \\(\\tonset_i\\) and \\(\\trec_i\\), respectively. If a person has multiple episodes of the disease, each episode has its own \\(\\tonset\\) and \\(\\trec\\). For example, the \\(j^\\text{th}\\) episode in person \\(i\\) would have onset time \\(\\tonset_{ij}\\) and recovery time \\(\\trec_{ij}\\).\nThe time scale used to define disease onset is flexible, and this flexibility is useful. The most obvious time scale is calendar time or absolute time. Another common time scale is age, which is an important determinant of the risk of many diseases. In some cases, time since an event is a useful time scale. The event that defines time scale could be a single event (e.g., exposure to contaminated food at a party) or an event that occurs at different times for different individuals (e.g., time since menopause). In general, it is wise to choose the time scale that corresponds to the most important time-varying determinant of disease onset. The chosen time scale is often called the analysis time scale.\n\n1.5.1 Prevalence\nFor prevalence, the disease outcome is defined by choosing a time \\(t\\) and letting \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has disease at time } t, \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In other words, it is the proportion of the population \\(\\Omega\\) that disease at time \\(t\\). This includes individuals who have disease onset at time \\(\\tonset = t\\) but not individuals who recover from disease at time \\(\\trec = t\\). This is often called the point prevalence at time \\(t\\).\nAnother version of prevalence is period prevalence. For period prevalence, we choose a nonempty time interval \\((t_a, t_b]\\) and define \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has disease at any time } t \\in (t_a, t_b], \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In other words, it is the proportion of the population that has disease at any time in the interval \\((t_a, t_b]\\). This includes prevalent cases at time \\(t_a\\) and cases with disease onset in \\((t_a, t_b]\\). The period prevalence in \\((t_a, t_b]\\) is the point prevalence at \\(t_a\\) plus the risk of disease onset in \\((t_a, t_b]\\), to which we now turn.\n\nR\n\n\n\n\nprevalence.R\n\n## Point and period prevalence\n\n# generate onset and recovery data for 100 individuals\n# Setting the seed ensures that everyone gets the same random numbers,\n# but it is strictly optional.\n# The function rexp() randomly samples from an exponential distribution.\nset.seed(42)\ncohort &lt;- data.frame(onset = rexp(100, rate = 0.4))\ncohort$duration &lt;- rexp(100, rate = 2)\ncohort$recovery &lt;- cohort$onset + cohort$duration\n\n# statistical summaries (mean, quartiles, range)\nsummary(cohort$onset)\nsummary(cohort$duration)\nsummary(cohort$recovery)\n\n# highest and lowest recovery times\n# The function sort() sorts the vector from lowest to highest.\n# head() returns the first 6 values of a vector; tails() returns the last 6.\nmin(cohort$onset)\nhead(sort(cohort$onset))    # lowest 6 values (first 6 in the sorted vector)\ntail(sort(cohort$onset))    # highest 6 values (last 6 in the sorted vector)\nmax(cohort$onset)\n\n# With a long vector, sorting repeatedly can be slow.\n# You can also control the number of elements returned by head() or tail().\nonset_ordered &lt;- sort(cohort$onset)\nhead(onset_ordered, n = 10)\ntail(onset_ordered, n = 10)\n\n# seeing rows and columns of the data frame\ncohort[1:10, c(\"onset\", \"duration\", \"recovery\")]\ncohort[c(10, 20, 50), c(\"onset\", \"recovery\")]\ncohort[which(cohort$recovery &lt; 1), c(\"onset\", \"recovery\")]\ncohort[, c(\"onset\", \"recovery\")]    # all rows\ncohort[c(2, 3, 5, 7, 11), ]         # all columns\n\n# point prevalence\nprev &lt;- function(t) {\n  # vector of TRUE/FALSE for prevalent cases at time t\n  prevalent &lt;- cohort$onset &lt;= t & cohort$recovery &gt; t\n  mean(prevalent)\n}\n\nprev(0)\nprev(1)\nprev(2)\nprev(6)\n\n# period prevalence\n# The parentheses around the logical tests are just for readability.\npdprev &lt;- function(ta, tb) {\n  # prevalent cases at t_a\n  prevalent_ta &lt;- (cohort$onset &lt;= ta & cohort$recovery &gt; ta)\n  # incident cases in (t_a, t_b]\n  incident_ab &lt;- (cohort$onset &gt; ta & cohort$onset &lt;= tb)\n  # mean indicator for prevalent at t_a or incident in (t_a, t_b]\n  mean(prevalent_ta | incident_ab)\n}\n\npdprev(0, 1)\npdprev(1, 2)\npdprev(0, 6)\n\n# save the data as a CSV file\nwrite.csv(cohort, \"cohort.csv\", row.names = FALSE)\n\n\n\n\n\n\n1.5.2 Risk (cumulative incidence) and the survival function\nTo define risk or cumulative incidence, we first choose an nonempty time interval \\((t_a, t_b]\\). The disease outcome is defined as \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has } \\tonset \\in (t_a, t_b], \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] In the population that is disease-free and at risk of disease at time \\(t_a\\), it is the proportion who have disease onset at \\(\\tonset \\leq t_b\\). The risk is sometimes called the incidence proportion.\nThe risk depends on a specified interval \\((t_a, t_b]\\). We can always define our time scale so that \\(t_a = 0\\), so the risk in \\((t_a, t_b]\\) on the original time scale is the same as the risk in the interval \\((0, t_b - t_a]\\) on the analysis time scale. On the analysis time scale, the cumulative incidence function \\(F(t)\\) is the risk of disease in \\((0, t]\\) for any possible \\(t\\). The corresponding survival function is \\[\n  S(t)\n  = 1 - F(t),\n\\] which is the probability of no disease onset in \\((0, t]\\). In practice, it is often easier to calculate the survival function than to calculate the cumulative incidence function directly. There is only one way to survive disease-free through the interval \\((0, t]\\), but you can have disease onset at any time.\n\nR\n\n\n\n\nrisk.R\n\n## Risk, survival function, and cumulative incidence function\n\n# read data from CSV file\n# Change or remove \".R/\" in the path as needed to locate the cohort.csv file.\n# You can also re-generate the data as in prevalence.R using the same seed.\ncohort &lt;- read.csv(\"./R/cohort.csv\")\n\n# risk (cumpulative incidence)\nrisk &lt;- function(t) {\n  # vector of TRUE/FALSE for incident cases in (0, t]\n  incident &lt;- cohort$onset &lt;= t\n  mean(incident)\n}\n\nrisk(0)\nrisk(1)\nrisk(2)\nrisk(6)\n\n# cumulative incidence function\n# Vectorize() takes a function like risk() that takes a single number as input\n# and creates a function that can take a number or vector as input.\ncuminc &lt;- Vectorize(risk)\ncuminc(c(0, 1, 2, 6))\n\n# survival function\n# A simple function can be put on one line.\n# It takes the same input as cuminc(), so it can take a vector\nsurv &lt;- function(t) 1 - cuminc(t)\nsurv(c(0, 1, 2, 6))\n\n# plot the survival and cumulative incidence functions\nt &lt;- seq(0, 20, by = 0.1)\nplot(t, surv(t), type = \"l\",\n     xlab = \"Time\", ylab = \"Probability\")\nlines(t, cuminc(t), lty = \"dashed\")\ngrid()\nlegend(\"right\", bg = \"white\", lty = c(\"dashed\", \"solid\"),\n       legend = c(\"Cumulative incidence\", \"Survival\"))\n\n\n\n\nThe survival function has several important properties:\n\n\\(S(0) = 1\\) because \\((0, 0]\\) is an empty interval where no one can have disease onset.\nBecause \\(S(t)\\) is a probability, \\(S(t) \\in [0, 1]\\) for all \\(t\\).\n\\(S(t)\\) monotonically decreases (i.e., never increases) with increasing \\(t\\). If \\(t_a &lt; t_b\\), then the time interval \\((0, t_a]\\) is contained \\((0, t_b]\\). Everyone who survives disease-free through \\((0, t_b]\\) must have survived disease-free through \\((0, t_a]\\), but some people who survived through \\((0, t_a]\\) might not make it all the way through \\((0, t_b]\\). Thus, \\(S(t_a) \\geq S(t_b)\\) whenever \\(t_a &lt; t_b\\).\nIf the disease or event occurs eventually for all individuals in our population \\(\\Omega\\) (e.g., death), then \\(S(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\).\n\nEach of these probabilities follows directly from the definition of \\(S(t)\\). Similarly, the cumulative incidence function \\(F\\) has \\(F(0) = 0\\) and \\(F(t) \\in [0, 1]\\), and it is monotonically increasing (i.e., never decreasing) with increasing \\(t\\). If the disease or event occurs eventually in all individuals, then \\(F(t) \\rightarrow 1\\) as \\(t \\rightarrow \\infty\\). Figure 1.2 shows the survival and cumulative hazard curves for the data generated in the prevalence example above.\n\n\n\nCode\n\nsurv-fig.R\n\n## Plot of survival and cumulative incidence functions\n\n# read data from CSV file\n# Change or remove \".R/\" in the path as needed to locate the cohort.csv file.\n# You can also re-generate the data as in prevalence.R using the same seed.\ncohort &lt;- read.csv(\"./R/cohort.csv\")\n\n# risk (cumpulative incidence)\nrisk &lt;- function(t) {\n  # vector of TRUE/FALSE for incident cases in (0, t]\n  incident &lt;- cohort$onset &lt;= t\n  mean(incident)\n}\n\n# cumulative incidence function\ncuminc &lt;- Vectorize(risk)\n\n# survival function\nsurv &lt;- function(t) 1 - cuminc(t)\n\n# plot the survival and cumulative incidence functions\nt &lt;- seq(0, 20, by = 0.1)\nplot(t, surv(t), type = \"l\",\n     xlab = \"Time\", ylab = \"Probability\")\nlines(t, cuminc(t), lty = \"dashed\")\ngrid()\nlegend(\"right\", bg = \"white\", lty = c(\"dashed\", \"solid\"),\n       legend = c(\"Cumulative incidence\", \"Survival\"))\n\n\n\n\n\n\n\n\n\nFigure 1.2: Survival and cumulative incidence curves for the data from the prevalence example.\n\n\n\n\n\nHere, I will generally use the word “risk” to refer to the probability of disease onset in a specified interval. When there is possible confusion about the meaning of “risk”, I will use “cumulative incidence” instead. The terms “cumulative incidence function” and “survival function” are standard in survival analysis, which is the branch of statistics that studies times to events. The creative use of “risk” in public health and medicine should not make you shy away from using the word correctly.\n\n\n1.5.3 Prevalence and the duration of disease\nPoint and period prevalence are both affected by the duration of disease. Both measures will increase if the duration of disease increases. A simple illustration of this is given in Figure 1.3. For a fixed set of onset times, the point prevalence of disease at any time \\(t\\) either stays the same or increases when the duration of disease increases. The prevalence at time \\(t = 5\\) is \\(\\frac{2}{5} = 0.4\\) under the shorter duration of disease but \\(\\frac{3}{5} = 0.6\\) under the longer duration of disease. Period prevalence over any interval \\((t_a, t_b]\\) is affected by the duration of disease because it is the point prevalence at \\(t_a\\) (which is affected by disease duration) plus the risk of disease onset over \\((t_a, t_b]\\). In a given population, the relationship between prevalence, frequency of disease onset (incidence), and the duration of disease can be complex (Freeman and Hutchison 1980; Preston 1987; Keiding 1991; Alho 1992). The risk of disease in any given interval is not affected by the duration of disease.\n\n\n\nCode\n\nprevdur-fig.R\n\n## R code for prevalence and duration plot\nplot(0, 0, type = \"n\", xlim = c(0, 10), ylim = c(0, 5.5),\n     xlab = \"Time\", ylab = \"Individual\", yaxt = \"n\")\nAxis(side = 2, at = 1:5, labels = 1:5)\ngrid()\nstart &lt;- c(4, 1, 3, 2, 6)\nstop1 &lt;- c(7, 3, 6, 4, 7)\nstop2 &lt;- c(9, 4, 8, 7, 9)\narrows(x0 = start, y0 = 1:5, x1 = stop1, code = 3, length = 0.2, angle = 90)\narrows(x0 = stop1, y0 = 1:5, x1 = stop2, code = 2, length = 0.2, angle = 90,\n       col = \"darkgray\")\nabline(v = 5, lty = \"dashed\")\ntext(5.5, 0.5, label = \"t = 5\")\n\n\n\n\n\n\n\n\n\nFigure 1.3: Each black horizonal line shows the onset of disease and recovery from disease in a single individual. The gray lines show recoveries from disease if the disease duration increases.\n\n\n\n\n\n\n\n1.5.4 Descriptive and analytic epidemiology\nPrevalence is often a useful measure for descriptive epidemiology, which measures the distribution of disease over person, place, and time. Because prevalence depends on both incidence and duration of disease, a change in the prevalence of disease can generally be explained several different ways (MacMahon and Terry 1958; Dunn Jr 1962). For example, an increase in prevalence of human immunodeficiency virus (HIV) infection could be cause by an increase in the incidence of HIV infection (which is bad) or an increase in the life expectancy of HIV-infected people (which is good).\nRisk (cumulative incidence) is generally more useful than prevalence for analytic epidemiology, which attempts to identify the causes of a disease. Another advantage of risk is that it can be used for outcomes that begin and end very quickly (e.g., traffic accidents or being hit by lightning) and for outcomes that remove individuals from the population (e.g., emigration or death). Prevalence is not a useful measure of the public health impact of these events.\n\n\n\n\nAlho, Juha M. 1992. “On Prevalence, Incidence, and Duration in General Stable Populations.” Biometrics 48 (2): 587–92.\n\n\nDunn Jr, John E. 1962. “The Use of Incidence and Prevalence in the Study of Disease Development in a Population.” American Journal of Public Health 52 (7): 1107–18.\n\n\nFreeman, Jonathan, and George B Hutchison. 1980. “Prevalence, Incidence and Duration.” American Journal of Epidemiology 112 (5): 707–23.\n\n\nKeiding, Niels. 1991. “Age-Specific Incidence and Prevalence: A Statistical Perspective.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 154 (3): 371–96.\n\n\nLaplace, Pierre Simon. 1820. Théorie Analytique Des Probabilités. Vol. 7. Courcier.\n\n\nMacMahon, Brian, and William D Terry. 1958. “Application of Cohort Analysis to the Study of Time Trends in Neoplastic Disease.” Journal of Chronic Diseases 7 (1): 24–35.\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological Perspective.” In A History of Epidemiologic Methods and Concepts, edited by Alfredo Morabia, 3–125. Springer.\n\n\nPreston, Samuel H. 1987. “Relations Among Standard Epidemiologic Measures in a Population.” American Journal of Epidemiology 126 (2): 336–45.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "probability.html#footnotes",
    "href": "probability.html#footnotes",
    "title": "1  Probability, Random Variables, and Disease Occurrence",
    "section": "",
    "text": "Pierre-Simone, marquis de Laplace (1749-1827) is often called the Newton of France. He proved that the solar system is stable, developed theories of ocean tides and gravitational potential, proved one of the first general versions of the central limit theorem, and pioneered the Bayesian interpretation of probability. His is one of the 72 names on the Eiffel Tower. ↩︎\n The natural numbers \\(\\mathbb{N} = \\{0, 1, 2, \\ldots\\}\\) are countably infinite, as are the integers \\(\\mathbb{Z}\\) and the rational numbers \\(\\mathbb{Q}\\). The real numbers \\(\\mathbb{R}\\) are uncountably infinite, as are the real numbers in any nonempty interval \\((a, b)\\) and the irrational numbers. Uncountably infinite sets are infinitely larger than countably infinite sets. This distinction was discovered in the 1870s by the German mathematician Georg Cantor (1845–1918). It was considered shocking, but it has become a cornerstone of modern mathematics.↩︎\n In experiments with uncountably infinite sample spaces, the probability of an event \\(A\\) cannot always be calculated by adding up the probabilities of \\(\\{\\omega\\}\\) for all \\(\\omega \\in A\\). For example: If we choose a number at uniformly at random in \\([0, 1]\\), the probability of getting any particular number \\(\\omega\\) is zero. The sum of the probabilities of all \\(\\{\\omega\\} \\subseteq A\\) is zero (if \\(A\\) is countable) or undefined (if \\(A\\) is uncountable). By maintaining a distinction between outcomes and events and by limiting probability calculations to countable (i.e., finite or countably infinite) sums, we end up with something coherent and useful.↩︎\n Named after John Venn (1834-1923), an English logician and philosopher who was one of the pioneers of the frequentist interpretation of probability. He was ordained as an Anglican priest in 1859 but resigned from the church in 1883. He was a prize-winning gardener of roses and white carrots and a prominent supporter of women’s right to vote. From 1903 until his death, he was President of Fellows in Gonville and Caius College at the University of Cambridge, where he is commemorated with a Venn diagram in a stained glass window.↩︎\n In probability, we only consider unions and intersections of finite or countably infinite sets of events. Although unions and intersections can be defined for uncountably infinite sets of events, it can be impossible to assign probabilities to the resulting sets (see the Banach-Tarski paradox). As an epidemiologist, this should not keep you up at night.↩︎\n Technically, we assign probabilities only to events in a class \\(\\mathcal{F}\\) of subsets of \\(\\Omega\\) that is required to contain \\(\\Omega\\) and to be closed under complements and countable unions. “Closed under complements” means that \\(A^\\comp \\in \\mathcal{F}\\) whenever \\(A \\in \\mathcal{F}\\). For example, \\(\\varnothing = \\Omega^\\comp\\) must be in \\(\\mathcal{F}\\) because \\(\\Omega \\in \\mathcal{F}\\). “Closed under countable unions” means that \\(\\bigcup_{i = 1}^\\infty A_i \\in \\mathcal{F}\\) whenever \\((A_1, A_2, \\ldots)\\) is a sequence of events in \\(\\mathcal{F}\\). The class \\(\\mathcal{F}\\) is called a \\(\\sigma\\)-algebra or \\(\\sigma\\)-field, and this restriction on the domain of probability helps avoid internal contradictions like the Banach-Tarski paradox.↩︎\n Technically, the support of \\(X\\) is the smallest closed set \\(S_X\\) such that \\(\\Pr(X \\in S_X) = 1\\). For a discrete random variable with support on a finite set, it is just the set of possible values. For a discrete random variable with support on a countably infinite set, it can include points whose probability mass is zero—a pathological case that we can safely ignore. For a continuous random variable, it can include values whose probability density is zero—a case that is not unusual or pathological.↩︎\n Named after Jacob Bernoulli (1655-1705), a Swiss mathematician who derived the first version of the law of large numbers and discovered the constant \\(e \\approx 2.718281828\\), which is the base for natural logarithms. He and his younger brother Johann Bernoulli (1667-1748) were some of the first mathematicians to try to understand and apply calculus, but their relationship eventually curdled into a jealous rivalry. A lunar impact crater called Bernoulli is named jointly after them.↩︎\n This becomes a fundamental insight when we discuss hypothesis tests for independence as well as confounding and selection bias.↩︎\n Discrete random variables \\(X\\) and \\(Y\\) are independent if \\(\\Pr(X = x \\vand Y = y) = \\Pr(X = x) \\Pr(Y = y)\\) for any possible values \\(x \\in \\supp(X)\\) and \\(y \\in \\supp(Y)\\). We will discuss independence more rigorously when we discuss conditional probabilities in Chapter 2.↩︎\n Named after Karl Pearson (1857-1936), an English mathematician who founded the modern discipline of mathematical statistics. In 1911, he started the world’s first university department of statistics at University College London. He was an outspoken socialist and supporter of women’s rights, but he was also a vocal proponent of social Darwinism and eugenics who opposed Jewish immigration into Britain.↩︎",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, Random Variables, and Disease Occurrence</span>"
    ]
  },
  {
    "objectID": "condprob.html",
    "href": "condprob.html",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "",
    "text": "2.1 Contingency tables\nSuppose we know that an event \\(A\\) occurred and want calculate the probability that \\(B\\) also occurred. The conditional probability of \\(B\\) given \\(A\\) is \\[\n    \\Pr(B \\given{} A) = \\frac{\\Pr(A \\cap B)}{\\Pr(A)}.\n\\tag{2.1}\\] Note that this is well-defined only if \\(\\Pr(A) &gt; 0\\). Conditional probabilities given \\(A\\) are just probabilities where the original sample space \\(\\Omega\\) has been replaced with an event \\(A \\subseteq \\Omega\\). Everything we have learned about probabilities applies to all of the conditional probabilities given the same event \\(A\\). Conditional probability is arguably the most important mathematical tool in epidemiology.\nIn statistics, a contingency table classifies individuals by two discrete variables, one that defines the rows and one that defines the columns. Each cell in the table contains the number of individuals who are in the intersection of the corresponding categories of the row and column variables. These numbers are called cell counts. The margins of the table contain row or column totals.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#contingency-tables",
    "href": "condprob.html#contingency-tables",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "",
    "text": "2.1.1 2x2 tables\nIn epidemiology, a 2x2 table is a contingency table based on a binary exposure variable and a binary disease outcome. We denote exposure by \\(X = 1\\) and no exposure by \\(X = 0\\), and we denote disease by \\(D = 1\\) and no disease by \\(D = 0\\). The precise definition of “disease” depends on context. In descriptive epidemiology, \\(D_i = 1\\) might mean that person \\(i\\) is a prevalent case of disease. In analytic epidemiology, \\(D_i = 1\\) might mean that person \\(i\\) had an onset of disease in an interval \\((t_\\text{start}, t_\\text{stop}]\\) on a relevant time scale. We put exposure in the rows and disease in the columns,2 and the exposure and disease categories are ordered so that individuals with \\(X = 1\\) and \\(D = 1\\) go in the top left corner. This is the most common arrangement in epidemiologic research, but it is not universal.\nTable 2.1 shows an example of a 2x2 table. There are \\(a\\) individuals with both exposure and disease, \\(b\\) individuals with exposure but not disease, \\(c\\) individuals with disease but no exposure, and \\(d\\) individuals with neither. In the rows, there are \\(r_1 = a + b\\) exposed individuals and \\(r_0 = c + d\\) unexposed individuals. In the columns, there are \\(k_1 = a + c\\) individuals who had a disease onset and \\(k_0 = b + d\\) individuals who did not. The row and column totals are called the margins of the table. The total number of individuals is \\(n = a + b + c + d\\).\n\n\n\nTable 2.1: 2x2 table of exposure (\\(X\\)) and disease (\\(D\\)).\n\n\n\n\n\n\n\\(D = 1\\)\n\\(D = 0\\)\nTotal\n\n\n\n\n\\(X = 1\\)\n\\(a\\)\n\\(b\\)\n\\(r_1 = a + b\\)\n\n\n\\(X = 0\\)\n\\(c\\)\n\\(d\\)\n\\(r_0 = c + d\\)\n\n\nTotal\n\\(k_1 = a + c\\)\n\\(k_0 = b + d\\)\n\\(n = a + b + c + d\\)\n\n\n\n\n\n\n\n\n2.1.2 Joint and marginal probabilities\nHere, we assume that Table 2.1 represents our entire population \\(\\Omega\\) and our experiment is to randomly sample an individual \\(\\omega \\in \\Omega\\) and measure their exposure status \\(X(\\omega)\\) and their disease status \\(D(\\omega)\\). Probabilities involving both \\(X\\) and \\(D\\) are called joint probabilities, and they can be calculated using the cell counts. In Table 2.1, the four joint probabilities are \\[\n  \\begin{aligned}\n    \\Pr(X = 1 \\text{ and } D = 1) &= a / n, \\\\\n    \\Pr(X = 1 \\text{ and } D = 0) &= b / n, \\\\\n    \\Pr(X = 0 \\text{ and } D = 1) &= c / n, \\\\\n    \\Pr(X = 0 \\text{ and } D = 0) &= d / n.\n  \\end{aligned}\n\\] Together, these probabilities defined the joint distribution of the random variables \\(X\\) and \\(D\\) via their joint probability mass function (PMF).\nProbabilities involving \\(X\\) or \\(D\\) alone are called marginal probabilities because they are calculated using the margins of the table. In Table 2.1, the marginal probabilities for exposure \\(X\\) are \\[\n  \\begin{aligned}\n    \\Pr(X = 1) &= r_1 / n, \\\\\n    \\Pr(X = 0) &= r_0 / n.\n  \\end{aligned}\n\\] Together, these define the marginal distribution of \\(X\\), which is Bernoulli(\\(r_1 / n\\)). The marginal probabilities for disease \\(D\\) are \\[\n  \\begin{aligned}\n  \\Pr(D = 1) &= k_1 / n, \\\\\n  \\Pr(D = 0) &= k_0 / n.\n  \\end{aligned}\n\\] Together, these define the marginal distribution of \\(D\\), which is Bernoulli(\\(k_1 / n\\)).\n\n\n2.1.3 Conditional probabilities\nJoint and marginal probabilities can be used to calculate conditional probabilities, which have a joint probability in the numerator and a marginal probability in the denominator. As before, we assume that Table 2.1 represents our entire population \\(\\Omega\\) and our experiment is to randomly sample an individual \\(\\omega \\in \\Omega\\) and measure \\(X(\\omega)\\) and \\(D(\\omega)\\). In Table 2.1, the conditional probability of disease given exposure is \\[\n    \\Pr(D = 1 \\given{} X = 1)\n    = \\frac{Pr(D = 1 \\vand X = 1)}{\\Pr(X = 1)}\n    = \\frac{a / n}{r_1 / n}\n    = \\frac{a}{r_1},\n\\] and the conditional probability of disease given no exposure is \\[\n  \\Pr(D = 1 \\given{} X = 0)\n  = \\frac{Pr(D = 1 \\vand X = 0)}{\\Pr(X = 0)}\n  = \\frac{c / n}{r_0 / n}\n  = \\frac{c}{r_0},\n\\] Similarly, the conditional probability of exposure given disease is \\[\n  \\Pr(X = 1 \\given{} D = 1)\n  = \\frac{\\Pr(X = 1 \\vand D = 1)}{\\Pr(D = 1)}\n  = \\frac{a / n}{k_1 / n}\n  = \\frac{a}{k_1},\n\\] and the conditional probabilty of exposure given no disease is \\[\n  \\Pr(X = 1 \\given{} D = 0)\n  = \\frac{\\Pr(X = 1 \\vand D = 0)}{\\Pr(D = 0)}\n  = \\frac{b / n}{k_0 / n}\n  = \\frac{b}{k_0}.\n\\] In all cases, the table total cancels out and we get a calculation in one row (for conditional probabilities given \\(X\\)) or one column (for conditional probabilities given \\(D\\)).",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#multiplication-of-conditional-probabilities",
    "href": "condprob.html#multiplication-of-conditional-probabilities",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.2 Multiplication of conditional probabilities",
    "text": "2.2 Multiplication of conditional probabilities\nEquation 2.1 can be rearranged into \\[\n    \\Pr(A \\cap B) = \\Pr(B \\given{} A) \\Pr(A),\n\\tag{2.2}\\] exactly as described by Bayes at the beginning of this chapter (if we let \\(A\\) be the “1st event” and \\(B\\) be the “2d”). This depends only on the definition of conditional probability in Equation 2.1, not on any assumptions about the relationship between the events \\(A\\) and \\(B\\). This multiplication rule for conditional probabilities extends to any number of events. For three events \\(A\\), \\(B\\), and \\(C\\) such that \\(B \\cap C\\) and \\(C\\) have probabilities greater than zero, we have \\[\\begin{aligned}\n  \\Pr(A \\cap B \\cap C)\n  &= \\Pr(A \\given{} B \\cap C) \\Pr(B \\cap C) \\\\\n  &= \\Pr(A \\given{} B \\cap C) \\Pr(B \\given{} C) \\Pr(C).\n\\end{aligned}\\] To ensure that all of these conditional probabilities are well-defined, we need \\(B \\cap C\\) and \\(C\\) to have probabilities greater than zero. In practice, \\(\\Pr(A \\given{} B \\cap C)\\) is usually written \\(\\Pr(A \\given{} B, C)\\).\n\n2.2.1 Decision trees\nFigure 2.1 shows an example of a decision tree. The root of the tree is on the left and the leaves of the tree are on the right. Each node where two or more branches meet represents a decision. In the example, the root represents the decision \\(A\\) or \\(A^C\\) (i.e., not \\(A\\)). The two nodes connected to the root each represent the decision \\(B\\) or \\(B^C\\) (i.e., not \\(B\\)). Each branch of the tree is labeled with the conditional probability of the branch given the event that it branches out from. Because of the multiplication rule for conditional probabilities, the probability of each leaf is equal to the product of the probabilities along the branches connecting it to the root.\n\n\n\n\n\n\nFigure 2.1: A decision tree for events \\(A\\) and \\(B\\). The probability of each leaf is found by multiplying the probabilities along the branches leading from the leaf back to the root.\n\n\n\n\n\n2.2.2 Independence of events\nThe events \\(A\\) and \\(B\\) are independent if \\[\n  \\Pr(A \\cap B) = \\Pr(A) \\Pr(B).\n\\tag{2.3}\\] When two events are independent, the occurrence (or not) of one event tells us nothing about whether the other event occurred: If \\(\\Pr(A) &gt; 0\\), equation Equation 2.3 is equivalent to \\(\\Pr(B \\given{} A) = \\Pr(B)\\). If \\(\\Pr(B) &gt; 0\\), it is equivalent to \\(\\Pr(A \\given{} B) = \\Pr(A)\\). If \\(A\\) and \\(B\\) are not independent, the occurrence of \\(A\\) contains information about the occurrence of \\(B\\) and vice versa.\nIndependence of events \\(A\\) and \\(B\\) implies that the events \\(A\\) and \\(B^\\comp\\) are also independent: \\[\n  \\begin{aligned}\n    \\Pr\\bigl(A \\cap B^\\comp\\bigr)\n    &= \\Pr(A) - \\Pr(A \\cap B) \\\\\n    &= \\Pr(A) - \\Pr(A) \\Pr(B) \\\\\n    &= \\Pr(A) \\big(1 - \\Pr(B)\\big) \\\\\n    &= \\Pr(A) \\Pr\\bigl(B^\\comp\\bigr).\n  \\end{aligned}\n\\] A similar argument shows that \\(A^\\comp\\) and \\(B\\) are independent. Because \\(A^\\comp \\cap B^\\comp = (A \\cup B)^\\comp\\) by DeMorgan’s laws (see Section 1.1.5), \\[\n  \\begin{aligned}\n    \\Pr\\bigl(A^\\comp \\cap B^\\comp\\bigr)\n    &= 1 - \\Pr(A \\cup B) \\\\\n    &= 1 - \\Pr(A) - \\Pr(B) - \\Pr(A \\cap B) \\\\\\\n    &= 1 - \\Pr(A) - \\Pr(B) - \\Pr(A) \\Pr(B) \\\\\n    &= \\big(1 - \\Pr(A)\\big) \\big(1 - \\Pr(B)\\big) \\\\\n    &= \\Pr\\bigl(A^\\comp\\bigr) \\Pr\\bigl(B^\\comp\\bigr).\n  \\end{aligned}\n\\] Therefore, independence of two events implies independence between any combination of themselves or their complements.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#sensitivity-and-specificity",
    "href": "condprob.html#sensitivity-and-specificity",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.3 Sensitivity and specificity",
    "text": "2.3 Sensitivity and specificity\nIn the epidemiology of screening and diagnostic tests, several of the most important concepts are conditional probabilities. If we classify disease status into diseased (\\(D^+\\)) and nondiseased (\\(D^-\\)) and the test result into positive (\\(T^+\\)) and negative (\\(T^-\\)), we have the four possible combinations Table 2.2.\n\n\n\nTable 2.2: Disease status (\\(D^+\\)/\\(D^-\\)) and test result (\\(T^+\\)/\\(T^-\\)).\n\n\n\n\n\n\n\\(T^+\\)\n\\(T^-\\)\n\n\n\n\n\\(D^+\\)\nTrue positive\nFalse negative\n\n\n\\(D^-\\)\nFalse positive\nTrue negative\n\n\n\n\n\n\nThe sensitivity of a test is the conditional probability that the test is positive given that the individual tested has the disease: \\[\n  \\sens = \\Pr(T^+ \\given{} D^+).\n\\] The specificity of a test is the conditional probability that the test is negative given that the individual tested does not have the disease: \\[\n  \\spec = \\Pr(T^- \\given{} D^-).\n\\] In both cases, we are conditioning on the disease status of the individual being tested. These concepts were introduced by Yerushalmy (1947) in a comparison of different types of chest X-rays for tuberculosis case detection.\n\nR\n\n\n\n\nsensspec.R\n\n## Sensitivity and specificity\n\n# generate diagnostic testing data\nset.seed(42)\nn &lt;- 500\ndtdat &lt;- data.frame(disease = rbinom(n, 1, 0.5))\ndtdat$testpos &lt;- ifelse(dtdat$disease,\n                        rbinom(n, 1, 0.85), rbinom(n, 1, 0.05))\n\n# prevalence\nmean(dtdat$disease)\n# Pr(T+)\nmean(dtdat$testpos)\n\n# sensitivity\nmean(dtdat$testpos[dtdat$disease == TRUE])\nsum(dtdat$disease & dtdat$testpos) / sum(dtdat$disease)\n\n# specificity\n1 - mean(dtdat$testpos[dtdat$disease == FALSE])\nmean(!dtdat$testpos[dtdat$disease == FALSE])\n\n\n\n\nMaximizing either sensitivity or specificity alone does not necessarily lead to good screening or diagnostic test: A test where everyone tests positive has perfect sensitivity but zero specificity, and a test where everyone tests negative has perfect specificity but zero sensitivity. There is almost always a tradeoff where higher sensitivity leads to lower specificity and vice versa.\n\n2.3.1 Example: Diabetes testing\nRemein and Wilkerson (1961) describe an early study of diabetes screening conducted by the United States Public Health Service in Boston City Hospital between 1954 and 1957. They recruited early-morning patients who were not febrile or acutely ill. Those willing to participate gave urine and blood samples. Next, they were given a meal meant to approximate an average breakfast or light lunch (a sandwich, 5 grams of butter, 60 grams of cheese, and three filled cookies). After the meal, they gave further urine and blood samples at one, two, and three hours after eating. The samples were analyzed using four different blood tests and six different urine tests. Participants returned for a follow-up visit between 3 and 21 days after the screening tests, where a definitive diagnosis of diabetes was made using an oral glucose tolerance test and a physical examination according to criteria established by a group of experts.\nA total of 595 participants completed both visits. Table 2.3 is a reconstruction of the data for the Somogyi-Nelson blood test based on the 580 participants (70 with diabetes and 510 without) who took the test at all four time points. In the table, a positive test is defined as a blood glucose concentration above 130 mg/dL (milligrams per deciliter).\n\n\n\nTable 2.3: Sensitivity and specificity of the Somogyi-Nelson blood glucose test for diabetes where \\(T^+\\) corresponds to a concentration above 130 mg/dL.\n\n\n\n\n\n\n\\(T^+\\)\n\\(T^-\\)\nSensitivity and specificity\n\n\n\n\nBefore meal\n\n\n\\(D^+\\)\n31\n39\n\\(\\sens = 31 / 70 \\approx 0.443\\)\n\n\n\\(D^-\\)\n5\n505\n\\(\\spec = 505 / 510 \\approx 0.990\\)\n\n\nOne hour after meal\n\n\n\\(D^+\\)\n55\n15\n\\(\\sens = 55 / 70 \\approx 0.786\\)\n\n\n\\(D^-\\)\n48\n462\n\\(\\spec = 462 / 510 \\approx 0.906\\)\n\n\nTwo hours after meal\n\n\n\\(D^+\\)\n45\n25\n\\(\\sens = 45 / 70 \\approx 0.643\\)\n\n\n\\(D^-\\)\n16\n494\n\\(\\spec = 494 / 510 \\approx 0.969\\)\n\n\nThree hours after meal\n\n\n\\(D^+\\)\n34\n36\n\\(\\sens = 34 / 70 \\approx 0.486\\)\n\n\n\\(D^-\\)\n1\n509\n\\(\\spec = 509 / 510 \\approx 0.998\\)\n\n\n\n\n\n\n\nR\n\n\n\n\nRWtable.R\n\n## Table 2 from Remein and Wilkerson (Journal of Chronic Disease, 1961)\n\n# function to generate numbers based on sensitivity and specificity\nRWtable &lt;- function(sens, spec, n1=70, n0=510) {\n  # arguments:  sensitivity, specificity,\n  #             n1 is number of diabetics, n0 is number of nondiabetics\n  tp &lt;- round(sens * n1)\n  fp &lt;- round((1 - spec) * n0)\n  tn &lt;- round(spec * n0)\n  fn &lt;- round((1 - sens) * n1)\n  return(c(truepos = tp, falsepos = fp, trueneg = tn, falseneg = fn))\n}\n\nRWtable(0.443, 0.990)   # before meal\nRWtable(0.786, 0.906)   # one hour after\nRWtable(0.643, 0.969)   # two hours after\nRWtable(0.486, 0.998)   # three hours after\n\n\n\n\n\n\n2.3.2 Receiver operating characteristic (ROC) curves*\nThe tradeoff between sensitivity and sensitivity in choosing a clinical measurement cutoff to distinguish positive and negative tests can be seen using a receiver operating characteristic (ROC) curve (Lusted 1971a, 1971b; Swets 1988; Zweig and Campbell 1993). These curves were originally used in World War II to analyze the performance of radar systems locating ships and airplanes. They were applied to diagnostic tests in the late 1950s in the first attempt to automate the classification of Pap smears to detect cervical cancer (Bostrom, Sawyer, and Tolles 1959; Lusted 1984; Bengtsson and Malm 2014).\nEach combination of a clinical measurement and a cutoff between positive and negative tests defines a diagnostic or screening test that has a sensitivity \\(\\sens \\in [0, 1]\\) and a specificity \\(\\spec \\in [0, 1]\\). The horizontal axis of an ROC curve plots \\[\n  1 - \\spec = \\Pr(T^+ \\given{} D^-),\n\\] and its vertical axis plots \\(\\sens = \\Pr(T^+ \\given{} D^+)\\). The test corresponds to a point \\((1 - \\spec, \\sens)\\) in the unit square \\([0, 1] \\times [0, 1]\\). The best tests correspond to points close to the top left corner \\((0, 1)\\), which represents a test with perfect specificity (so \\(1 - \\spec = 0\\)) and perfect sensitivity.\nFor a sequence of cutoffs, a given clinical measurement produces a curve connecting the points produced by the tests based on it. Figure 2.2 shows four ROC curves based on data from Remein and Wilkerson (1961): one for the Somogyi-Nelson blood glucose measurement before the meal and one each for the measurements one, two, and three hours after the meal. For all four measurements, the curves are based on the combinations of sensitivity and specificity for glucose concentration cutoffs from 70 mg/dL to 200 mg/dL. In these tests, using a higher glucose concentration cutoff to define a positive test leads to lower sensitivity and higher specificity.\n\n\n\nCode\n\nROCcurve.R\n\n# data from Table 2 in Remein and Wilkerson (Journal of Chronic Disease, 1961)\nSNdat &lt;- data.frame(cutoff = seq(70, 200, by = 10))\nSNdat$sens_pre &lt;- c(95.7, 91.4, 82.9, 65.7, 54.3, 50.0, 44.3, 37.1, 30.0,\n                    25.7, 25.7, 22.9, 21.4, 17.1) / 100\nSNdat$spec_pre &lt;- c(11.0, 36.3, 65.7, 84.7, 92.7, 96.7, 99.0, 99.6, 99.8,\n                    99.8, 99.8, 99.8, 100.0, 100.0) / 100\nSNdat$sens_1hr &lt;- c(100.0, 97.1, 97.1, 95.7, 92.9, 88.6, 78.6, 68.6, 57.1,\n                    52.9, 47.1, 40.0, 34.3, 28.6) / 100\nSNdat$spec_1hr &lt;- c(8.2, 22.4, 39.0, 57.3, 70.6, 83.3, 90.6, 95.1, 97.8,\n                    99.4, 99.6, 99.8, 100.0, 100.0) / 100\nSNdat$sens_2hr &lt;- c(98.6, 97.1, 94.3, 88.6, 85.7, 71.4, 64.3, 57.1, 50.0,\n                    47.1, 42.9, 38.6, 34.3, 27.1) / 100\nSNdat$spec_2hr &lt;- c(8.8, 25.5, 47.6, 69.8, 84.1, 92.5, 96.9, 99.4, 99.6,\n                    99.8, 100.0, 100.0, 100.0, 100.0) / 100\nSNdat$sens_3hr &lt;- c(94.3, 91.4, 82.9, 70.0, 60.0, 51.4, 48.6, 41.4, 32.9,\n                    28.6, 28.6, 28.6, 24.3, 20.0) / 100\nSNdat$spec_3hr &lt;- c(8.6, 34.7, 67.5, 86.5, 95.3, 98.2, 99.8,\n                    rep(100.0, 7)) / 100\n# write.csv(SNdat, \"SNdat.csv\", row.names = FALSE)\n\n# ROC curves with labels\nplot(1 - SNdat$spec_pre, SNdat$sens_pre, type = \"n\",\n     xlim = c(0, 1), ylim = c(0, 1),\n     xlab = \"1 - Specificity = Pr(T+ | D-)\",\n     ylab = \"Sensitivity = Pr(T+ | D+)\")\ngrid()\nlines(1 - SNdat$spec_pre, SNdat$sens_pre, col = \"darkgray\")\nlines(1 - SNdat$spec_1hr, SNdat$sens_1hr, lty = \"solid\")\nlines(1 - SNdat$spec_2hr, SNdat$sens_2hr, lty = \"dashed\")\nlines(1 - SNdat$spec_3hr, SNdat$sens_3hr, lty = \"dotted\")\npoints(1 - SNdat[SNdat$cutoff == 130, c(3, 5, 7, 9)],\n       SNdat[SNdat$cutoff == 130, c(2, 4, 6, 8)])\npoints(1 - SNdat$spec_pre[seq(2, 12, by = 2)],\n       SNdat$sens_pre[seq(2, 12, by = 2)], pch = 8)\ntext(1 - SNdat$spec_pre[seq(2, 12, by = 2)] + c(0, .09, .09, .1, .1, .1),\n     SNdat$sens_pre[seq(2, 12, by = 2)] + c(-.05, -.02, -.02, 0, 0, -.01),\n     labels = c(\"80 mg/dL\", \"100 mg/dL\", \"120 mg/dL\", \"140 mg/dL\",\n                \"160 mg/dL\", \"180 mg/dL\"))\nabline(0, 1, lty = \"dotted\", col = \"darkgray\")\ntext(.51, .49, adj = c(.5, 1), srt = 42,\n     label = \"Useless tests (T and D independent)\")\npoints(c(0, 0, 1), c(0, 1, 1), pch = 3)\ntext(.01, .99, adj = c(0, 1), label = \"Perfect test\")\ntext(.01, .01, adj = c(0, 0), label = \"Everyone tests negative\")\ntext(.99, .99, adj = c(1, 0), srt = 90, label = \"Everyone tests positive\")\nlegend(\"bottomright\", bg = \"white\",\n       lty = c(\"solid\", \"solid\", \"dashed\", \"dotted\", NA),\n       col = c(\"darkgray\", rep(\"black\", 4)), pch = c(rep(NA, 4), 1),\n       legend = c(\"Before meal  (AUC = 0.825)\",\n                  \"1 hour after   (AUC = 0.923)\",\n                  \"2 hours after (AUC = 0.904)\",\n                  \"3 hours after (AUC = 0.839)\", \"130 mg/dL cutoff\"))\n\n\n\n\n\n\n\n\n\nFigure 2.2: ROC curves for Somogyi-Nelson blood tests conducted before the meal and at 1-3 hours after the meal. Cutoff values for the before-meal curve are labeled, and the points corresponding to the 130 mg/dL cutoff along the curve for each blood glucose measurement are circled.\n\n\n\n\n\nROC curves for different clinical measurements can be compared using the area under the curve (AUC), which is the area between the x-axis \\([0, 1]\\) and the ROC curve. Greater AUC corresponds to a measurement that is better able to distinguish between disease and no disease (Bamber 1975; Hanley and McNeil 1982). For a test that is positive when a clinical measurement is above a given cutoff, the AUC is the probability that a person with disease has a higher value than a person without disease.3 In this example, it is the probability that a true diabetic has a higher blood glucose concentration than a true nondiabetic at the time blood glucose concentration is measured. A measurement that was always higher (or always lower) for individuals with disease than individuals without disease would have \\(\\text{AUC} = 1\\). The AUCs in Figure 2.2 show clearly that the tests one and two hours after the meal, which have curves above and to the left of the other two curves, better distinguish between diabetics and nondiabetics than the tests before and three hours after the meal. This is biologically plausible: Before the meal, there is no glucose load. Three hours after the meal, the glucose from the meal has largely been absorbed.\n\nR\n\n\n\n\nauc.R\n\n## areas under the ROC curves\n\n# load Somogyi-Nelson test data generated for Figure 2.2 (if needed)\n# The argument can contain a path before the file name.\nSNdat &lt;- read.csv(\"SNdat.csv\")\n\nauc &lt;- function(x, y) {\n  # x is an increasing list of specificities\n  # y is a decreasing list of sensitivities\n  roc &lt;- approxfun(c(1, 1 - x, 0), c(1, y, 0), ties = \"max\")\n  area &lt;- integrate(function(x) roc(x), 0, 1)\n  return(area)\n}\nauc(SNdat$spec_pre, SNdat$sens_pre)\nauc(SNdat$spec_1hr, SNdat$sens_1hr)\nauc(SNdat$spec_2hr, SNdat$sens_2hr)\nauc(SNdat$spec_3hr, SNdat$sens_3hr)\n\n\n\n\nThe test one hour after the meal with a 130 mg/dL cutoff has a good combination of sensitivity and specificity. It is near the top left corner, where perfect tests live. If a diagnostic test was completely useless, the test results (\\(T^+\\) or \\(T^-\\)) would be independent of disease status (\\(D^+\\) or \\(D^-\\)). In that case, \\[\n  \\Pr(T^+ \\given{} D^+) = \\Pr(T^+ \\given{} D^-) = \\Pr(T^+).\n\\] Thus, the ROC curve for a useless test follows the diagonal line from the lower left corner \\((0, 0)\\) to the upper right corner \\((1, 1)\\), and it has an AUC of 0.5. Tests below the diagonal on an ROC curve are worse than useless: the definitions of positive and negative should be reversed.\nThe sensitivity and specificity of a test tell us how accurate it is with a given definition of positive and negative. The ROC curve shows us how this accuracy depends on the cutoff between positive and negative tests, and the area under the curve shows us how well the underlying clinical measurement (e.g., blood glucose concentration) can distinguish between people with and without disease. However, the best cutoff for a test depends on its purpose, the population to be tested, and the benefit of identifying a true positive or negative versus the harm of a false positive or negative (Blumberg 1957; Kessel 1962).",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#law-of-total-probability",
    "href": "condprob.html#law-of-total-probability",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.4 Law of total probability",
    "text": "2.4 Law of total probability\nSuppose \\(A_1, \\ldots, A_n\\) are disjoint events such that their union is \\(\\Omega\\). This is called a partition of \\(\\Omega\\). An important special case is when we partition \\(\\Omega\\) into \\(A\\) and \\(A^\\comp\\).\nLet \\(B\\) be another event. Every \\(\\omega \\in B\\) is in exactly one of the \\(A_i\\). For each \\(i\\), \\(B \\cap A_i\\) is the part of \\(B\\) that is contained in \\(A_i\\). The event \\(B\\) is the union of these subsets: \\[\n  B = \\bigcup_{i = 1}^n (B \\cap A_i).\n\\] Because \\(A_i\\) are disjoint, so are the subsets \\(B \\cap A_i\\). By the addition rule for probabilities of disjoint sets, we have \\[\n  \\Pr(B) = \\sum_{i = 1}^n \\Pr(B \\cap A_i)\n\\] which is the sum of the \\(\\Pr(B \\cap A_i)\\).4 Using the multiplication rule for conditional probabilities in Equation 2.2 on each \\(\\Pr(B \\cap A_i)\\), we get \\[\n  \\Pr(B) = \\sum_{i = 1}^n \\Pr(B \\given{} A_i) \\Pr(A_i).\n\\] This is called the law of total probability.\n\n2.4.1 Example: probability of a positive or negative test\nWe can use the law of total probability to calculate the probability of a positive or negative test based on the sensitivity and specificity of the test and the prevalence of disease. Because all individuals either do or do not have the disease,5 we have \\[\n  T^+ = (T^+ \\cap D^+) \\cup (T^+ \\cap D^-).\n\\] These two groups are mutually exclusive, so \\[\n  \\Pr(T^+) = \\Pr(T^+ \\cap D^+) + \\Pr(T^+ \\cap D^-).\n\\] We can calculate each probability on the right-hand side using the multiplication rule in Equation 2.2: \\[\n  \\begin{aligned}\n    \\Pr(T^+ \\cap D^+)\n    &= \\Pr(T^+ \\given{} D^+) \\Pr(D^+)\n      = \\text{sensitivity} \\times \\text{prevalence},\\\\\n    \\Pr(T^+ \\cap D^-)\n    &= \\Pr(T^+ \\given{} D^-) \\Pr(D^-)\n      = (1 - \\text{specificity}) \\times (1 - \\text{prevalence}).\n  \\end{aligned}\n\\] Putting everything together, we get \\[\n  \\begin{aligned}\n    \\Pr(T^+)\n    &= \\Pr(T^+ \\given{} D^+) \\Pr(D^+) + \\Pr(T^+ \\given{} D^-) \\Pr(D^-) \\\\\n    &= \\text{sensitivity} \\times \\text{prevalence}\n      + (1 - \\text{specificity}) \\times (1 - \\text{prevalence}).\n  \\end{aligned}\n\\tag{2.4}\\] A similar chain of reasoning shows that \\[\n  \\Pr(T^-) = (1 - \\text{sensitivity}) \\times \\text{prevalence} + \\text{specificity} \\times (1 - \\text{prevalence}),\n\\] which equals \\(1 - \\Pr(T^+)\\).\nFigure 2.3 shows how the probability of a positive test depends on the prevalence of disease using the example of the Somogyi-Nelson test one hour after the meal in Table 2.3. With a cutoff of 130 mg/dL, the test has a sensitivity of 0.786 and a specificity of 0.906. At low prevalences, the test overestimates the prevalence of diabetes due to imperfect specificity. A high prevalences, it underestimates the prevalence of diabetes due to imperfect sensitivity. The errors cancel out somewhere near a prevalence of 30%.\n\n\n\nCode\n\ntestpos.R\n\n## probability of testing positive as a function of prevalence\n\n# function to generate testing data\ntdat &lt;- function(prev, sens=0.786, spec=0.906) {\n  # defaults are sensitivity and sensitivity one hour after the meal\n  truepos &lt;- sens * prev\n  falsepos &lt;- (1 - spec) * (1 - prev)\n  trueneg &lt;- spec * (1 - prev)\n  falseneg &lt;- (1 - spec) * prev\n  pos &lt;- truepos + falsepos\n  neg &lt;- 1 - pos\n  ppv &lt;- truepos / pos\n  npv &lt;- trueneg / neg\n  return(data.frame(prev = prev, sens = sens, spec = spec,\n                    truepos = truepos, falsepos = falsepos,\n                    trueneg = trueneg, falseneg = falseneg,\n                    pos = pos, neg = neg, ppv = ppv, npv = npv))\n}\ntdat_1hr &lt;- tdat(seq(0, 1, by = .01))\nwrite.csv(tdat_1hr, \"R/tdat_1hr.csv\", row.names = FALSE)\n\n# plot\nplot(tdat_1hr$prev, tdat_1hr$pos, type = \"n\", xlim = c(0, 1), ylim = c(0, 1),\n     xlab = \"Prevalence of disease = Pr(D+)\",\n     ylab = \"Probability of positive test = Pr(T+)\")\npolygon(c(tdat_1hr$prev, 1, 0), c(tdat_1hr$pos, 0, 0),\n        border = NA, col = \"gray\")\npolygon(c(tdat_1hr$prev, 1, 0), c(tdat_1hr$falsepos, 0, 0),\n        border = NA, col = \"darkgray\")\ngrid()\nlines(tdat_1hr$prev, tdat_1hr$falsepos, col = \"gray\")\nlines(tdat_1hr$prev, tdat_1hr$pos)\nabline(0, 1, lty = \"dotted\")\ntext(0.1, 0.02, adj = c(0, 0), label = \"False positives\")\ntext(0.2, 0.1, adj = c(0, 0), label = \"True positives\")\n\n\n\n\n\n\n\n\n\nFigure 2.3: The probability of a positive Somogyi-Nelson diabetes test one hour after the meal as a function of the hypothetical prevalence of diabetes. The black dotted line shows the true prevalence of diabetes.\n\n\n\n\n\n\n\n2.4.2 Standardization\nIn epidemiology, it is often useful to think of our sample space \\(\\Omega\\) as a population and the outcomes \\(\\omega \\in \\Omega\\) as individuals. The sets \\(A_1, \\ldots, A_n\\) into which we partition the sample space are disjoint subpopulations (e.g., age groups). Let \\(\\Pr(D \\given{} A_i)\\) be the prevalence of disease in subpopulation \\(A_i\\) at a given time point. Then the overall prevalence of disease is \\[\n  \\Pr(D) = \\sum_{i = 1}^n \\Pr(D \\given{} A_i) \\Pr(A_i).\n\\tag{2.5}\\] This application of the law of total probability is called standardization. By changing the \\(\\Pr(A_i)\\), we can use the subpopulation prevalences to calculate the prevalence of disease in a population with any desired composition of subpopulations. Equation 2.5 can also be used to calculate population-level risk from the subpopulation-specific risks in any given time interval. In the form of standardization, the law of total probability is one of the most important tools in epidemiology.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#bayes-rule",
    "href": "condprob.html#bayes-rule",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "2.5 Bayes’ rule",
    "text": "2.5 Bayes’ rule\nBayes’ rule (Bayes 1763) relates the conditional probabilities \\(\\Pr(A \\given{} B)\\) and \\(\\Pr(B \\given{} A)\\): \\[\n  \\Pr(A \\given{} B) = \\frac{\\Pr(B \\cap A)}{\\Pr(B)} = \\frac{\\Pr(B \\given{} A) \\Pr(A)}{\\Pr(B)}.\n\\tag{2.6}\\] In the denominator, the law of total probability is often used to calculate \\(\\Pr(B)\\) via partitioning \\(\\Omega\\) into \\(A\\) and \\(A^\\comp\\). This gives us \\[\n  \\Pr(A \\given{} B) = \\frac{\\Pr(B \\given{} A) \\Pr(A)}{\\Pr(B \\given{} A) \\Pr(A) + \\Pr(B \\given{} A^\\comp) \\Pr(A^\\comp)}.\n\\] Bayes’ rule is an incredibly useful application of conditional probabilities, and it forms the theoretical foundation for Bayesian statistical inference.\n\n2.5.1 Positive and negative predictive values\nSensitivity and specificity tell us how disease status predicts the result of a test, but they do not tell us how to interpret a test result. If you test positive, it is important to know the conditional probability that you truly have disease given that you tested positive. This is called the positive predictive value (PPV): \\[\n  \\text{PPV} = \\Pr(D^+ \\given{} T^+).\n\\] If you test negative, it is important to know the conditional probability that you are truly disease-free given that you tested negative. This is called the negative predictive value (NPV): \\[\n  \\text{NPV} = \\Pr(D^- \\given{} T^-).\n\\] These terms were introduced by Vecchio (1966). Table 2.4 shows the PPV and NPV for the Somogyi-Nelson diabetes tests from Table 2.3.\n\n\n\nTable 2.4: PPV and NPV of the Somogyi-Nelson blood glucose test for diabetes where \\(T^+\\) corresponds to a concentration above 130 mg/dL.\n\n\n\n\n\n\n\\(T^+\\)\n\\(T^-\\)\nPPV and NPV\n\n\n\n\nBefore meal\n\n\n\\(D^+\\)\n31\n39\n\\(\\text{PPV} = 31 / 36 \\approx 0.861\\)\n\n\n\\(D^-\\)\n5\n505\n\\(\\text{NPV} = 505 / 544 \\approx 0.928\\)\n\n\nTotal\n36\n544\n\n\n\nOne hour after meal\n\n\n\\(D^+\\)\n55\n15\n\\(\\text{PPV} = 55 / 103 \\approx 0.534\\)\n\n\n\\(D^-\\)\n48\n462\n\\(\\text{NPV} = 462 / 477 \\approx 0.969\\)\n\n\nTotal\n103\n477\n\n\n\nTwo hours after meal\n\n\n\\(D^+\\)\n45\n25\n\\(\\text{PPV} = 45 / 61 \\approx 0.738\\)\n\n\n\\(D^-\\)\n16\n494\n\\(\\text{NPV} = 494 / 519 \\approx 0.952\\)\n\n\nTotal\n61\n519\n\n\n\nThree hours after meal\n\n\n\\(D^+\\)\n34\n36\n\\(\\text{PPV} = 34 / 35 \\approx 0.971\\)\n\n\n\\(D^-\\)\n1\n509\n\\(\\text{NPV} = 509 / 545 \\approx 0.934\\)\n\n\nTotal\n35\n545\n\n\n\n\n\n\n\nVecchio (1966) showed that the PPV and NPV depend on the prevalence of disease as well as the sensitivity and specificity of the test. To calculate the PPV and NPV, we use Bayes’ rule to switch the conditional probabilities from \\(\\Pr(T \\given{} D)\\) to \\(\\Pr(D \\given{} T)\\). From the definition of PPV and Bayes’ rule, we get \\[\n  \\Pr(D^+ \\given{} T^+)\n  = \\frac{\\Pr(T^+ \\cap D^+)}{\\Pr(T^+)}\n  = \\frac{\\Pr(T^+ \\given{} D^+) \\Pr(D^+)}{\\Pr(T^+)}.\n\\] The sensitivity of the test and the prevalence of disease are in the numerator, and \\(\\Pr(T+)\\) is in Equation 2.4. Putting this all together, we get \\[\n  \\text{PPV}\n  = \\frac{\\text{sensitivity} \\times \\text{prevalence}}{\\text{sensitivity} \\times \\text{prevalence}\n    + (1 - \\text{specificity}) \\times (1 - \\text{prevalence})}.\n\\] The numerator is the probability of a true positive test, and the denominator is the probability of a (true or false) positive test. By a similar argument, \\[\n  \\text{NPV} = \\frac{\\text{specificity} \\times (1 - \\text{prevalence})}{\\text{specificity} \\times (1 - \\text{prevalence})\n    + (1 - \\text{sensitivity}) \\times \\text{prevalence}}.\n  \\label{eq:npv}\n\\] The numerator is the probability of a true negative test, and the denominator is the probability of a (true or false) negative test.\nFigure 2.4 shows how the positive and negative predictive values of a test depend on the prevalence of disease for the Somogyi-Nelson test before the meal and one hour after the meal in Remein and Wilkerson (1961). With a cutoff of 130 mg/dL, the sensitivity and specificity are \\(0.443\\) and \\(0.990\\) before the meal and \\(0.786\\) and \\(0.906\\) one hour after the meal. If prevalence equals zero, the PPV is zero and the NPV equals one because no one has disease. As prevalence increases, PPV increases and NPV decreases. If the prevalence equals one, the PPV is one and the NPV is zero because everyone has disease. A perfect test would have PPV and NPV equal to one at all prevalences.\n\n\n\nCode\n\npredval.R\n\n## Predictive values as a function of prevalence\n\n# uses tdat_1hr data and tdat() function from Figure 2.3 (testpos.R)\n# tdat_1hr &lt;- read.csv(\"tdat_1hr.csv\")\n# generate data using the sensitivity and specificity of the pre-meal test\ntdat_pre &lt;- tdat(seq(0, 1, by = .01), sens = 0.443, spec = 0.990)\n\n# plot of PPV and NPV as a function of diabetes prevalence\nplot(tdat_1hr$prev, tdat_1hr$ppv, type = \"n\", xlim = c(0, 1), ylim = c(0, 1),\n     xlab = \"Prevalence of disease = Pr(D+)\",\n     ylab = \"Predictive value = Pr(D | T)\")\ngrid()\nlines(tdat_1hr$prev, tdat_1hr$ppv)\nlines(tdat_1hr$prev, tdat_1hr$npv, lty = \"dashed\")\nlines(tdat_pre$prev, tdat_pre$ppv, col = \"darkgray\")\nlines(tdat_pre$prev, tdat_pre$npv, lty = \"dashed\", col = \"darkgray\")\nlegend(\"bottom\", lty = c(\"solid\", \"dashed\", \"solid\", \"dashed\"),\n       col = c(\"darkgray\", \"darkgray\", \"black\", \"black\"),\n       bg = \"white\", inset = 0.05,\n       legend = c(\"PPV before meal\", \"NPV before meal\",\n                  \"PPV 1 hour after\", \"NPV 1 hour after\"))\n\n\n\n\n\n\n\n\n\nFigure 2.4: Positive and negative predictive values of the Somogyi-Nelson diabetes test before the meal (gray) and one hour after the meal (black) as a function of diabetes prevalence.\n\n\n\n\n\n\n\n2.5.2 Likelihood ratios*\nFor a probability \\(p\\), the odds is \\[\n  \\theta = \\frac{p}{1 - p}.\n\\] While a probability lives in \\([0, 1]\\), the odds can go from zero (for \\(p = 0\\)) to infinity (as \\(p\\) approaches one). There is a one-to-one relationship between probabilities and odds, so we can calculate the probability of an event if we know the odds. If the odds is \\(\\theta\\), the corresponding probability is \\[\n  p = \\frac{\\theta}{1 + \\theta}.\n\\] Odds and odds ratios have an important role in epidemiology and statistical inference. In a Bayesian statistical framework, odds ratios give us a simple way to update our knowledge about the probability of an event given new information.\nSuppose we know the prevalence of a disease in a population \\(\\Omega\\). We randomly sample an individual \\(\\omega \\in \\Omega\\) and give them a diagnostic test. If we randomly sample an individual \\(\\omega\\) from a population \\(\\Omega\\), the odds that \\(\\omega\\) has disease is \\[\n  \\frac{\\Pr(D^+)}{1 - \\Pr(D^+)}\n  = \\frac{\\Pr(D^+)}{\\Pr(D^-)}.\n\\] where \\(\\Pr(D^+)\\) is the prevalence of disease. This is called the prior odds of disease. If \\(\\omega\\) tests positive for the disease, the conditional odds that they have disease is \\[\n  \\frac{PPV}{1 - PPV}\n  = \\frac{\\Pr(D^+ \\given{} T^+)}{\\Pr(D^- \\given{} T^+)}\n  = \\frac{\\Pr(D^+ \\cap T^+)}{\\Pr(D^- \\cap T^+)},\n\\] where we have cancelled out \\(\\Pr(T^+)\\) from the numerator and the denominator in the last expression. This is called the posterior odds of disease. The second expression above shows that the probability corresponding to the posterior odds is the PPV.\nUsing the multiplication rule for conditional probabilities, we get \\[\n  \\frac{\\Pr(D^+ \\cap T^+)}{\\Pr(D^- \\cap T^+)}\n  = \\frac{\\Pr(T^+ \\given D^+) \\Pr(D^+)}{\\Pr(T^+ \\given D^-) \\Pr(D^-)}\n  = \\frac{\\text{sensitivity}}{1 - \\text{specificity}}\n    \\times \\frac{\\Pr(D^+)}{\\Pr(D^-)}.\n\\] The term \\(\\text{sensitivity} / (1 - \\text{specificity})\\) is called the likelihood ratio. If our individual \\(\\omega\\) tests positive for disease, \\[\n  \\text{posterior odds of } D^+\n  = \\text{likelihood ratio} \\times \\text{prior odds of } D^+.\n\\] The likelihood ratio is a measure of how much we learn from a positive test result, and it does not depend on the prevalence of disease [Lusted (1971b); Swets (1973); Fagan (1975); Albert (1982); Zweig and Campbell (1993)}. Because an ROC curve plots sensitivity on the vertical axis and \\(1 - \\text{specificity}\\) on the horizontal axis, the likelihood ratio for a given test is the slope of the line from the point \\((0, 0)\\) to the point representing the test.\nTable 2.5 shows the prior odds, likelihood ratio, posterior odds, and PPV for the Somogyi-Nelson blood glucose tests for diabetes from 580 participants (70 with diabetes and 510 without) in Remein and Wilkerson (1961). Note that the tests with the highest likelihood ratios come from the glucose measurements that had the lowest AUCs in Figure 2.2. These tests have high likelihood ratios despite their low sensitivity because they have specificities near one. The test with the best combination of sensitivity and specificity in Table 2.3 has the lowest likelhood ratio. Like other summaries of diagnostic test performance, the likelihood ratio by itself does not determine the best test for a given purpose.\n\n\n\nTable 2.5: Prior odds, likelihood ratios, posterior odds, and PPV for the Somogyi-Nelson blood glucose test for diabetes where \\(T^+\\) corresponds to a concentration above 130 mg/dL.\n\n\n\n\n\nTest\nPrior odds\nLikelihood ratio\nPosterior odds\nPPV\n\n\n\n\nBefore meal\n\\(70 / 510 \\approx 0.137\\)\n45.171\n\\(31 / 5 = 6.200\\)\n\\(31 / 36 \\approx 0.861\\)\n\n\n1 hour after\n\\(70 / 510 \\approx 0.137\\)\n8.348\n\\(55 / 48 \\approx 1.146\\)\n\\(55 / 103 \\approx 0.534\\)\n\n\n2 hours after\n\\(70 / 510 \\approx 0.137\\)\n20.491\n\\(45 / 16 \\approx 2.813\\)\n\\(45 / 61 \\approx 0.738\\)\n\n\n3 hours after\n\\(70 / 510 \\approx 0.137\\)\n247.714\n\\(34 / 1 = 34.000\\)\n\\(34 / 35 \\approx 0.971\\)\n\n\n\n\n\n\n\n\n\n\nAlbert, Adelin. 1982. “On the Use and Computation of Likelihood Ratios in Clinical Chemistry.” Clinical Chemistry 28 (5): 1113–19.\n\n\nBamber, Donald. 1975. “The Area Above the Ordinal Dominance Graph and the Area Below the Receiver Operating Characteristic Graph.” Journal of Mathematical Psychology 12 (4): 387–415.\n\n\nBayes, Thomas. 1763. “LII. An Essay Towards Solving a Problem in the Doctrine of Chances. By the Late Rev. Mr. Bayes, FRS Communicated by Mr. Price, in a Letter to John Canton, AMFRS.” Philosophical Transactions of the Royal Society of London 53: 370–418.\n\n\nBengtsson, Ewert, and Patrik Malm. 2014. “Screening for Cervical Cancer Using Automated Analysis of PAP-Smears.” Computational and Mathematical Methods in Medicine 2014: 842037.\n\n\nBlumberg, Mark S. 1957. “Evaluating Health Screening Procedures.” Operations Research 5 (3): 351–60.\n\n\nBostrom, RC, HS Sawyer, and WE Tolles. 1959. “Instrumentation for Automatically Prescreening Cytological Smears.” Proceedings of the IRE 47 (11): 1895–1900.\n\n\nFagan, Terrence J. 1975. “Nomogram for Bayes’s Theorem.” New England Journal of Medicine 293 (5): 257.\n\n\nHanley, James A, and Barbara J McNeil. 1982. “The Meaning and Use of the Area Under a Receiver Operating Characteristic (ROC) Curve.” Radiology 143 (1): 29–36.\n\n\nKessel, Elton. 1962. “Diabetes Detection: An Improved Approach.” Journal of Chronic Diseases 15 (12): 1109–21.\n\n\nLusted, Lee B. 1971a. “Decision-Making Studies in Patient Management.” New England Journal of Medicine 284 (8): 416–24.\n\n\n———. 1971b. “Signal Detectability and Medical Decision-Making.” Science 171 (3977): 1217–19.\n\n\n———. 1984. “ROC Recollected.” Medical Decision Making 4: 131–35.\n\n\nRemein, Quentin R, and Hugh LC Wilkerson. 1961. “The Efficiency of Screening Tests for Diabetes.” Journal of Chronic Diseases 13 (1): 6–21.\n\n\nRothman, Kenneth J. 1981. “Induction and Latent Periods.” American Journal of Epidemiology 114 (2): 253–59.\n\n\nSwets, John A. 1973. “The Relative Operating Characteristic in Psychology: A Technique for Isolating Effects of Response Bias Finds Wide Use in the Study of Perception and Cognition.” Science 182 (4116): 990–1000.\n\n\n———. 1988. “Measuring the Accuracy of Diagnostic Systems.” Science 240 (4857): 1285–93.\n\n\nVecchio, Thomas J. 1966. “Predictive Value of a Single Diagnostic Test in Unselected Populations.” New England Journal of Medicine 274 (21): 1171–73.\n\n\nYerushalmy, Jacob. 1947. “Statistical Problems in Assessing Methods of Medical Diagnosis, with Special Reference to X-Ray Techniques.” Public Health Reports (1896-1970) 62 (40): 1432–49.\n\n\nZweig, Mark H, and Gregory Campbell. 1993. “Receiver-Operating Characteristic (ROC) Plots: A Fundamental Evaluation Tool in Clinical Medicine.” Clinical Chemistry 39 (4): 561–77.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "condprob.html#footnotes",
    "href": "condprob.html#footnotes",
    "title": "2  Conditional Probability and Diagnostic Tests",
    "section": "",
    "text": "Thomas Bayes (1701-1761) was an English Presbyterian minister from a family of Nonconformists (i.e., Protestants who did not observe the rules of the Church of England). He studied logic and theology at the University of Edinburgh and served as a minister in Tunbridge Wells near Kent, England. He was elected a Fellow of the Royal Society in 1742 for his defense of Newton’s calculus against a 1734 book called The Analyst: A Discourse Addressed to an Infidel Mathematician by Bishop George Berkeley (1685-1753). Late in life, Bayes became interested in probability and “inverse probability” (statistics). This essay was published posthumously, and it has had a profound effect on modern statistics.↩︎\n This is partly to respect the linear algebra convention that rows come before columns in matrix indices, so \\(M_{ij}\\) is the entry in row \\(i\\) and column \\(j\\) of the matrix \\(M\\). In analytic epidemiology, exposure must occur before any disease that it causes, so we let the exposure define the rows.↩︎\n For a test that is positive when a clinical measurement is below a given cutoff, it is the probability that a person with disease has a lower value than a person without disease. Bamber (1975) showed that the AUC is closely related to the Wilcoxon rank sum statistic for the null hypothesis that the diseased and nondiseased have the same distribution for the measurement on which the test is based.↩︎\n The symbol \\(\\Sigma\\), which is an upper-case Greek letter \\(\\sigma\\) (sigma), stands for a sum. For products, we use \\(\\Pi\\), which is an upper-case Greek letter \\(\\pi\\) (pi).↩︎\n Many diseases are complex processes (Rothman 1981), making any binary classification of disease status somewhat arbitrary. Here, we assume that we have an operational definition of disease status that allows a reasonable binary classification.↩︎",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Probability and Diagnostic Tests</span>"
    ]
  },
  {
    "objectID": "mlestimation.html",
    "href": "mlestimation.html",
    "title": "3  Maximum Likelihood Estimation",
    "section": "",
    "text": "3.1 Binomial likelihood\nIn probability, we are told the rules of the game and then we predict what it will look like. In statistics, we watch the game and try to figure out the rules. Roughly speaking, statistics (game to rules) is the reverse of probability (rules to game). When done well, statistics helps us learn from observations while accounting honestly for uncertainty. An outstanding early example of statistics applied to public health is the work of Florence Nightingale (1820-1910), who collected data and developed statistical graphics to demonstrate the need for public health reforms in the British Army in the 1850s (Cohen 1984; Winkelstein Jr 2009).2\nHere, we will use estimation of a probability as an example of maximum likelihood estimation, which is used for parameter estimation throughout frequentist statistical inference, where the probability of an event \\(A\\) is interpreted as the limiting value of the proportion of \\(n\\) repetitions of an experiment in which \\(A\\) occurs as \\(n \\rightarrow \\infty\\). Maximum likelihood estimation gives us a way to find point estimates of parameters that are optimal in large samples. It is also the foundation for three types of hypothesis tests and confidence intervals that are widely used to quantify uncertainty in frequentist inference.\nIn Section 3.1.1, we used the prevalence \\(p\\) in our population to figure out the distribution of the number \\(X\\) of diseased individuals in a sample of size \\(n\\). This is probability. The corresponding statistical problem would be to estimate the prevalence \\(p\\) after seeing \\(X = x\\) infected individuals in a sample of size \\(n\\).\nWhen our experiment is to sample multiple individuals from a population, the analogy between the outcomes \\(\\omega \\in \\Omega\\) and the individuals in the population breaks down. Recall that when we flip a coin twice, each \\(\\omega \\in \\Omega\\) must specify the outcomes of both flips. When the experiment is to sample \\(n\\) individuals from a population, the entire sample is a single outcome \\(\\omega\\) and \\(\\Omega\\) contains all possible samples of \\(n\\) individuals from the population. If the population size is \\(N\\), then the number of possible samples of size \\(n\\) is given by the binomial coefficient \\[\n  \\binom{N}{n} = \\frac{N!}{n! (N - n)!},\n\\] where \\(k!\\) denotes \\(k\\) factorial. Factorials are defined by \\(0! = 1\\) and \\(k! = k \\cdot (k - 1)!\\) for any integer \\(k &gt; 0\\). For example, \\(1! = 1\\), \\(2! = 2\\), \\(3! = 6\\), \\(4! = 24\\), \\(5! = 120\\), and so on. For \\(k &gt; 0\\), \\(k!\\) is the product of all positive integers up to and including \\(k\\), which grows extremely fast as \\(k\\) increases.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#binomial-likelihood",
    "href": "mlestimation.html#binomial-likelihood",
    "title": "3  Maximum Likelihood Estimation",
    "section": "",
    "text": "3.1.1 Binomial distribution\nSuppose we sample \\(n\\) individuals from a population \\(\\Omega\\) and test them for disease. For simplicity, we assume that the diagnostic test has perfect sensitivity and specificity. Let \\(Y_i\\) denote whether person \\(i\\) in the sample has disease, and let \\(X\\) be the total number who have disease. Then \\[\n  X = \\sum_{i = 1}^n Y_i,\n\\] so it is a linear combination of the \\(Y_i\\). Each \\(Y_i\\) is a Bernoulli(\\(p\\)) random variable, where \\(p\\) is the prevalence of disease in the population. When \\(N\\) is much larger than \\(n\\) (for which we write \\(N \\gg n\\)), the test results for each person in the sample are approximately independent.\nThe distribution of a sum of \\(n\\) independent Bernoulli(\\(p\\)) random variables is called a binomial(n, p) distribution.3 The probability \\(Y_1 = 1\\) is \\(p\\), and the probability that \\(Y_1 = 0\\) is \\((1 - p)\\), so we can handle both cases by writing \\[\n  \\Pr(Y_1 = y_1) = p^{y_1} (1 - p)^{1 - y_1}.\n\\] When the \\(Y_i\\) are independent, each \\(Y_i\\) has a Bernoulli(p) distribution (see Section 1.3.5) and \\[\n  \\Pr(Y_1 = y_1, Y_2 = y_2, \\ldots, Y_n = y_n)\n  = \\prod_{i = 1}^n \\Pr(Y_i = y_i)\n  = \\prod_{i = 1}^n p^{y_i} (1 - p)^{1 - y_i}\n\\] by the multiplication rule for independent events. Substituting \\(x = \\sum_{i = 1}^n y_i\\), we get \\[\n  \\Pr(Y_1 = y_1, Y_2 = y_2, \\ldots, Y_n = y_n)\n  = p^x (1 - p)^{n - x}.\n\\] The value of \\(x\\) depends only on the sum of the \\(y_i\\), and there are \\(\\binom{n}{x}\\) different ways to get \\(x\\) cases of disease out of \\(n\\) sampled individuals. By the addition rule for disjoint events, we get \\[\n  \\Pr(X = x) = \\binom{n}{x} p^x (1 - p)^x.\n\\tag{3.1}\\] This is the probability mass function (PMF) of the binomial distribution. The set of possible values of a binomial(n, p) random variable \\(X\\) is \\(\\supp(X) = \\{0, 1, \\ldots, n\\}\\).\nSection 1.3.5 showed that a Bernoulli(\\(p\\)) random variable has expected value \\(p\\) and variance \\(p(1 - p)\\). Because a binomial(\\(n\\), \\(p\\)) random variable is the sum of \\(n\\) independent Bernoulli(\\(p\\)) random variables, its expected value is \\[\n  \\E(X) = n p.\n\\] by the rule for expectations of linear combinations in Equation 1.11. Its variance is \\[\n  \\Var(X) = n p (1 - p)\n\\] by the rule for variances of linear combinations in Equation 1.12. The covariances are all zero because the \\(Y_i\\) are independent.\n\nR\n\n\n\n\nbinomdist.R\n\n## binomial distribution\n\n# binomial PMF\n# The second and third arguments are n (\"size\") and p (\"prob\").\ndbinom(2, 10, 0.4)\ndbinom(0:10, 10, 0.4)\nsum(dbinom(0:10, 10, 0.4))\n\n# binomial CDF\npbinom(0:10, 10, 0.4)\ncumsum(dbinom(0:10, 10, 0.4))\n\n# binomial quantiles\nqbinom(c(0.25, 0.5, 0.75, 1), 10, 0.4)\n\n# random samples\nrbinom(20, 10, 0.4)\nx &lt;- rbinom(1000, 10, 0.4)\nmean(x)\nvar(x)\n\n\n\n\n\n\n3.1.2 Likelihood and log likelihood\nIn probability, we know the prevalence of disease \\(p\\) and we deduce the distribution of the number of diseased individuals \\(X\\) in a sample of size \\(n\\). In statistics, we observe \\(X = x\\) and use this to estimate \\(p\\). To do this, we rewrite the binomial PMF Equation 3.1 as a function of \\(p\\) instead of \\(x\\): \\[\n  L(p) = \\binom{n}{x} p^x (1 - p)^{n - x}.\n\\tag{3.2}\\] This is the binomial likelihood function. The right-hand sides of Equation 3.1 and Equation 3.2 are identical, and they produce exactly the same value given the same \\(x\\) and \\(p\\). However, the two equations define different functions. In binomial PMF in Equation 3.1, the prevalence \\(p\\) is fixed and the number of diseased individuals \\(x\\) is the argument of the function. In the binomial likelihood function in Equation 3.2, the number of diseased individuals \\(x\\) is fixed and the prevalence \\(p\\) is the argument of the function. The PMF belongs to probability, and the likelihood belongs to statistics.\nThe log likelihood is the natural logarithm (i.e., the logarithm to base \\(e = 2.718281828\\ldots\\))4 of the likelihood function. For binomial log likelihood is \\[\n  \\ell(p) = \\ln \\binom{n}{x} + x \\ln p + (n - x) \\ln (1 - p).\n\\] Because the logarithm turns products into sums, it is generally much easier to handle the log likelihood than the likelihood itself. The term \\(\\ln \\binom{n}{x}\\) does not depend on \\(p\\), so it can be ignored. Intuitively, this tells us that the total number \\(x = y_1 + y_2 + \\cdots + y_n\\) of individuals with disease in our sample contains the same information about the prevalence of disease as the sequence \\(y_1, y_2, \\ldots, y_n\\) of disease indicators.\nFor any given \\(p\\), we can think of \\(\\ell(p)\\) as a random variable whose value is determined by our sample of size \\(n\\). Let \\(\\ptrue\\) be the true prevalence of disease. By Gibb’s inequality,5 \\[\n  \\E[\\ell(\\ptrue)] &gt; \\E[\\ell(p)]\n\\] for all \\(p \\neq \\ptrue\\). This inequality is about the expected value of the log likelihood over all possible samples of size \\(n\\). For any given sample, it is possible that \\(\\ell(\\ptrue)\\) is not the maximum of the log likelihood. However, this inequality is an important part of the justification for estimating \\(p\\) by maximizing the log likelihood (Boos and Stefanski 2013). Because function \\(v \\mapsto \\ln(v)\\) is strictly increasing in \\(v\\), the likelihood \\(L(p)\\) and the log likelihood \\(\\ell(p)\\) are maximized at exactly the same value of \\(p\\).\n\n\n3.1.3 Score function\nTo find the maximum of the log likelihood, we find the value of \\(p\\) where its slope is zero. This is the mathematical version of the insight that the ground at the top of a hill is level. The score function is the first derivative of the log likelihood \\[\n  U(p)\n  = \\frac{\\text{d}}{\\text{d} p} \\ell(p)\n  = \\frac{x}{p} - \\frac{n - x}{1 - p},\n\\] which is the slope of \\(\\ell(p)\\) at \\(p\\). To find where the slope equals zero, we solve the score equation \\[\n  U(\\hat{p})\n  = \\frac{x}{\\hat{p}} - \\frac{n - x}{1 - \\hat{p}}\n  = 0\n\\tag{3.3}\\] where \\(\\hat{p}\\) denotes the maximum likelihood estimate (MLE) of \\(\\ptrue\\). When the dust settles, we get \\[\n  \\hat{p} = \\frac{x}{n}\n\\] so our MLE of the prevalence is just the proportion of our sample who has disease.\nTo confirm that this is a maximum instead of a minimum, we need to look at the second derivative of \\(\\ell\\). When we walk across the top of a hill, we go from walking uphill to walking downhill so the slope is decreasing. If \\(\\ell(p)\\) is maximized at \\(\\hat{p}\\), then the slope of the slope (i.e., the second derivative) should be negative. The second derivative of \\(\\ell(p)\\) at \\(\\hat{p}\\) is \\[\n  \\frac{\\text{d}}{\\text{d} p} U(p) = \\frac{\\text{d}^2}{\\text{d} p^2} \\ell(p)\n  = -\\frac{x}{p^2} - \\frac{n - x}{(1 - p)^2}.\n\\tag{3.4}\\] This is negative for any \\(p \\in (0, 1)\\). Thus, the log likelihood is maximized at \\(\\hat{p}\\) if \\(x &gt; 0\\) and \\(x &lt; n\\).\nWhen \\(x = 0\\) or \\(x = n\\), the log likelihood \\(\\ell(p)\\) has no maximum at any \\(p \\in (0, 1)\\). Instead, the maximum occurs at one of the boundaries of the set of possible \\(p\\). When \\(x = 0\\), our MLE of \\(\\ptrue\\) is \\(\\hat{p} = 0\\). When \\(x = n\\), our maximum likelihood estimate is \\(\\hat{p} = 1\\).\n\n\n3.1.4 Expected and observed information*\nFor any given \\(p\\), we can think of the score \\(U(p)\\) as a random variable that has an expected value and a variance. If \\(\\ptrue = p\\), the expected value of the score is always zero: \\[\n  \\E_p[U(p)]\n  = \\E_p\\bigg[\\frac{X}{p} - \\frac{n - X}{1 - p}\\bigg]\n  = \\frac{\\E_p(X)}{p} - \\frac{\\E_p(n - X)}{1 - p}\n  = \\frac{n p}{p} - \\frac{n (1 - p)}{1 - p}\n  = 0\n\\] where we use the subscript \\(p\\) to indicate that the expected value is calculated assuming that \\(\\ptrue = p\\). Because \\(\\E_p[U(p)] = 0\\), the corresponding variance of the score is \\[\n  \\mathcal{I}(p) = \\Var_p[U(p)] = \\E_p[U(p)^2],\n\\] by Equation 1.10. This is called the expected Fisher information or expected information.6 It can be used to calculate confidence limits for \\(\\ptrue\\).\nUnder regularity conditions that are met when \\(\\ptrue \\in (0, 1)\\), the Fisher information \\(\\mathcal{I}(p)\\) can be calculated using the second derivative of the log likelihood \\(\\ell(p)\\) from Equation 3.4.7 Specifically, \\(\\mathcal{I}(p)\\) is the expected value of the negative second derivative of \\(\\ell(p)\\): \\[\n  \\mathcal{I}(p)\n  = \\E_p\\Bigg[-\\frac{\\text{d}^2}{\\text{d} p^2} \\ell(p)\\Bigg]\n  = \\E_p\\bigg[\\frac{X}{p^2} + \\frac{n - X}{(1 - p)^2}\\bigg],\n\\tag{3.5}\\] where the subscript \\(p\\) indicates that the expected value is calculated assuming that \\(\\ptrue = p\\). Using Equation 1.11 and the binomial(\\(n\\), \\(p\\)) distribution for \\(X\\), this simplifies to \\[\n  \\mathcal{I}(p)\n  = \\frac{\\E(X)}{p^2} + \\frac{\\E(n - X)}{(1 - p)^2}\n  = \\frac{n}{p} + \\frac{n}{1 - p}\n  = \\frac{n}{p (1 - p)}.\n\\] Because \\(\\ptrue\\) is unknown, the expected information is often evaluated at \\(\\hat{p}\\). In some models, the expected information can be difficult to calculate.\nThe negative second derivative of \\(\\ell(p)\\) inside the expectation in Equation 3.5 evaluated is the observed Fisher information or observed information \\[\nI(p) = -\\frac{\\text{d}^2}{\\text{d} p^2} \\ell(p)\n= \\frac{x}{p^2} + \\frac{n - x}{(1 - p)^2}.\n\\tag{3.6}\\] For the binomial distribution \\(I(\\hat{p}) = \\mathcal{I}(\\hat{p})\\) but this equality does not hold at other values of \\(p\\). The observed information is an unbiased estimator of the expected information, and it can always be calculated from the data. It often produces more accurate variance estimates than the expected information (Efron and Hinkley 1978; Kenward and Molenberghs 1998; Reid 2003). However, it is generally safe to use whichever is most convenient (Boos and Stefanski 2013).",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#large-sample-theory",
    "href": "mlestimation.html#large-sample-theory",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.2 Large-sample theory",
    "text": "3.2 Large-sample theory\nThe log likelihood, the score function, and the Fisher and observed information give us all of the pieces we need to calculate point and interval estimates of \\(\\ptrue\\). To put them together, we use two fundamental results from probability theory about the behavior of sample means. The law of large numbers justifies point estimates and the central limit theorem justifies hypothesis tests and interval estimates, which can be obtained in three standard ways.\n\n3.2.1 Sample mean (average)\nIf \\(Y_1, Y_2, \\ldots, Y_n\\) are random variables, then the sample mean or average is \\[\n  \\hat{\\mu}_n = \\frac{1}{n} \\sum_{i = 1}^n Y_i.\n\\] This sample mean can be thought of as a random variable whose value is determined when we observe \\(Y_1 = y_1, Y_2 = y_2, \\ldots, Y_n = y_n\\). If each \\(Y_i\\) has \\(\\E(Y_i) = \\mu\\), then \\[\n  \\E[\\hat{\\mu}_n]\n  = \\frac{1}{n} \\sum_{i = 1}^n \\E[Y_i]\n  = \\frac{1}{n} n \\mu\n  = \\mu\n\\tag{3.7}\\] by Equation 1.11. Thus, the sample mean \\(\\hat{\\mu}_n\\) is an unbiased estimate of \\(\\mu\\) for any sample size \\(n\\). When the \\(Y_i\\) are indicator variables, \\(\\hat{\\mu}_n\\) is just the proportion of the sample with \\(Y_i = 1\\).\n\n\n3.2.2 Law of large numbers and consistency\nIf the \\(Y_i\\) are independent and each has \\(\\Var(Y_i) = \\sigma^2\\), then \\[\n  \\Var(\\hat{\\mu}_n)\n  = \\frac{1}{n^2} \\sum_{i = 1}^n \\Var(Y_i)\n  = \\frac{1}{n^2} n \\sigma^2\n  = \\frac{\\sigma^2}{n}\n\\tag{3.8}\\] by Equation 1.12. Thus, the variance of \\(\\hat{\\mu}_n\\) decreases as the sample size \\(n\\) increases. The standard deviation of \\(\\hat{\\mu}_n\\) is proportional to \\(1 / \\sqrt{n}\\). As \\(n \\rightarrow \\infty\\), we should have \\(\\hat{\\mu}_n \\rightarrow \\mu\\). This is called the law of large numbers, and it holds even when \\(\\sigma^2 = \\infty\\).\n\nTheorem 3.1 (Law of Large Numbers) If \\(Y_1, Y_2, \\ldots\\) is an infinite sequence of independent and identically-distributed (IID) random variables with mean \\(\\mu &lt; \\infty\\) and variance \\(\\sigma^2 \\leq \\infty\\), then\n\\[\n    \\hat{\\mu}_n \\rightarrow \\mu\n\\] as \\(n \\rightarrow \\infty\\).8\n\nOur maximum likelihood estimate \\(\\hat{p}_n\\) is a sample mean: \\[\n  \\hat{p}_n = \\frac{X}{n} = \\frac{1}{n} \\sum_{i = 1}^n Y_i.\n\\] where each \\(Y_i \\sim \\text{Bernoulli}(\\ptrue)\\) and the \\(Y_i\\) are independent. Therefore, the LLN implies that \\[\n  \\hat{p}_n \\rightarrow \\ptrue\n\\] as \\(n \\rightarrow \\infty\\). This convergence is shown in Figure 3.1. An estimate that converges to its true value as \\(n \\rightarrow \\infty\\) is called consistent. Intuitively, this means that \\(\\hat{p}_n\\) is guaranteed to be close to \\(\\ptrue\\) in a large sample. However, the LLN does not specify how close or how large a sample we need.\n\n\n\nCode\n\nlln.R\n\n## Law of large numbers\n\nn &lt;- 1000\nx &lt;- seq(n)\nplot(x, cumsum(rbinom(n, 1, .5)) / x, type = \"n\", ylim = c(0, 1),\n     xlab = \"Number of samples\", ylab = \"Sample mean\")\ngrid()\nlines(x, cumsum(rbinom(n, 1, .5)) / x, lty = \"solid\")\nlines(x, cumsum(rbinom(n, 1, .5)) / x, lty = \"dashed\")\nlines(x, cumsum(rbinom(n, 1, .5)) / x, lty = \"dotted\")\nabline(h = .5)\n\n\n\n\n\n\n\n\n\nFigure 3.1: The LLN at work. Each line traces the sample means calculated from a sequence of random samples \\(x_1, x_2, x_3, \\ldots\\) from a Bernoulli(0.5) distribution. For each sequence, the y-coordinate above \\(n\\) is the sample mean from the first \\(n\\) random samples in the sequence. The true mean of 0.5 is marked by a solid horizontal line.\n\n\n\n\n\n\n\n\n3.2.3 Central limit theorem and the normal distribution\nWhen both the mean and variance of the \\(Y_i\\) are finite, the central limit theorem (CLT) allows us to say something about how far away our sample mean \\(\\hat{\\mu}_n\\) is from the true value \\(\\mu\\). It is the most important result in all of probability and statistics.\n\nTheorem 3.2 (Central Limit Theorem) If \\(Y_1, Y_2, \\ldots\\) is an infinite sequence of IID random variables with finite mean \\(\\mu\\) and variance \\(\\sigma^2 &lt; \\infty\\), then \\[\n  Z_n\n  = \\frac{\\hat{\\mu}_n - \\E(\\hat{\\mu_n})}{\\sqrt{\\Var(\\hat{\\mu}_n)}}\n  = \\frac{\\sqrt{n} (\\hat{\\mu}_n - \\mu)}{\\sqrt{\\sigma^2}}\n\\] has a distribution that converges to a normal distribution or Gaussian distribution with mean zero and variance one as \\(n \\rightarrow \\infty\\).9 Because of this, we say that \\(\\hat{\\mu}_n\\) is asymptotically normal.\n\nThe normal distribution is a distribution for a continuous random variable, which can take any value on an interval or even on all of \\(\\mathbb{R}\\). Instead of a PMF, a continuous random variable \\(Z\\) has a probability density function (PDF). If \\(Z\\) is a continuous random variable with PDF \\(f(z)\\) and \\([a, b]\\) is an interval, then \\[\n  \\Pr\\bigl(Z \\in [a, b]\\bigr) = \\int_a^b f(z) \\,\\text{d} z.\n\\] The integral on the right-hand side represents the area under \\(f(z)\\) over the interval \\([a, b]\\). The cumulative distribution function of \\(Z\\) is \\[\n  F(z) = \\int_{-\\infty}^z f(u) \\,\\text{d} u,\n\\] where the integral on the right-hand side represents the area under \\(f(z)\\) over the interval \\((-\\infty, u]\\). For the same reason that the values of the PMF for any discrete random variable add up to one, we have \\[\n\\Pr(Z \\in \\mathbb{R})\n= \\int_{-\\infty}^\\infty f(z) \\,\\text{d} z\n= 1\n\\] for any continuous random variable \\(Z\\). Like the PMF and CDF of a discrete random variable, the PDF and CDF of a continuous random variable contain the same information about the distribution of \\(Z\\).\nThe PDF of the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) is \\[\n  f(z, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(z - \\mu)^2}{2 \\sigma^2}}.\n\\] The standard normal distribution has \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\). It is such an important distribution that its PDF and CDF have special notation. The standard normal PDF is \\[\n  \\phi(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{z^2}{2}},\n\\] and its CDF is \\(\\Phi(z)\\). These functions and the relationship between them are illustrated in Figure 3.2. A normal distribution is denoted \\(N(\\mu, \\sigma^2)\\), so the standard normal distribution is written \\(N(0, 1)\\).\n\n\n\nCode\n\nnormplots.R\n\n## Normal distribution PDF and CDF\n\n# set grid of plots\npar(mfrow = c(2, 1), mar = c(2, 5, 2, 2) + 0.1)\n\n# define variables\nx &lt;- seq(-3.5, 3.5, by = 0.01)\na &lt;- 0\nb &lt;- 2\n\n# plot of PDF\nplot(x, dnorm(x), type = \"n\",\n    ylab = expression(paste(\"PDF \", phi1(z))))\ngrid()\nlines(x, dnorm(x))\npolygon(x = c(b, a, seq(a, b, by = 0.01)),\n        y = c(0, 0, dnorm(seq(a, b, by = 0.01))),\n        lty = \"dashed\", col = \"darkgray\")\ntext(0.4, 0.18, labels = \"Area = Pr(0 &lt; Z &lt; 2)\", srt = 90)\n\n# plot of CDF\nplot(x, pnorm(x), type = \"n\",\n     ylab = expression(paste(\"CDF \", Phi(z))))\ngrid()\nlines(x, pnorm(x))\nsegments(c(-4, -4), pnorm(c(a, b)), c(a, b), pnorm(c(a, b)),\n         lty = \"dashed\")\nsegments(c(a, b), c(-1, -1), c(a, b), pnorm(c(a, b)), lty = \"dashed\")\narrows(-3, pnorm(a), -3, pnorm(b), code = 3, length = 0.1)\ntext(-1.7, sum(pnorm(c(a, b))) / 2, labels = \"Change = Pr(0 &lt; Z &lt; 2)\")\n\n\n\n\n\n\n\n\n\nFigure 3.2: The PDF (top) and CDF (bottom) of a standard normal random variable \\(Z\\). If \\(X \\sim N(0, 1)\\), then \\(\\Pr(0 &lt; X &lt; 2)\\) equals the shaded area under the PDF as well as the change in the CDF from \\(0\\) to \\(2\\). This same relationship between the CDF and the PDF holds for all continuous random variables and any interval \\((a, b)\\).\n\n\n\n\n\n\nR\n\n\n\n\nnormdist.R\n\n## normal (Gaussian) distribution\n\n# normal PDF\n# Second and third arguments are mean and SD (not variance).\n# The defaults are mean = 0 and SD = 1.\ndnorm(2, 1.2, 5)\n\n# normal CDF (using default mean and variance)\npnorm(1.96)\npnorm(1.96) - pnorm(-1.96)\n\n# normal quantiles\nqnorm(0.975)\npnorm(qnorm(0.975))\n\n# random samples (using named arguments)\nrnorm(25, mean = 2.3, sd = 3)\n\n\n\n\nFor our estimated probability \\(\\hat{p}_n\\) is a sample mean of IID \\(Y_i\\) with \\(\\E(Y_i) = \\ptrue\\) and \\(\\Var(Y_i) = \\ptrue (1 - \\ptrue)\\). When \\(n\\) is large, \\[\n  Z_n\n  = \\frac{\\sqrt{n} (\\hat{p}_n - \\ptrue)}{\\sqrt{\\ptrue (1 - \\ptrue)}}\n  = \\frac{\\hat{p}_n - \\ptrue}{\\sqrt{\\mathcal{I}(\\ptrue)^{-1}}}\n\\tag{3.9}\\] has a distribution that is close to a standard normal distribution. Figure 3.3 shows this convergence is shown for sample means where \\(Y_i \\sim \\Bernoulli(0.1)\\). The CLT does not guarantee that the distribution of \\(Z_n\\) is approximately normal in any given sample. It only guarantees that the normal approximation holds eventually as \\(n\\) increases. When the \\(Y_i \\sim \\Bernoulli(p)\\), the normal approximation is typically good when \\(n p (1 - p) &gt; 5\\).\n\n\n\nCode\n\nclt.R\n\n## Central limit theorem\n\n# probability mass function for sample mean\ndbline &lt;- function(n, p=.5, ...) {\n  x &lt;- (seq(-.5, n + .5) / n - p) * sqrt(n / (p * (1 - p)))\n  y &lt;- c(0, dbinom(0:n, n, p), 0) * sqrt(p * (1 - p) * n)\n  lines(stepfun(x, y), pch = NA, ...)\n}\n\n# define grid of plots\npar(mfrow = c(2, 2))\nx &lt;- seq(-4, 4, by = .01)\n\n# n = 20\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 20\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(20, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n# n = 50\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 50\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(50, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n# n = 100\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 100\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(100, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n# n = 250\nplot(x, dnorm(x), type = \"n\", ylim = c(0, .5),\n     main = \"n = 250\", xlab = \"Z score\", ylab = \"Probability density\")\ngrid()\ndbline(250, p = .1, lty = \"dashed\")\nlines(x, dnorm(x), col = \"darkgray\")\n\n\n\n\n\n\n\n\n\nFigure 3.3: The CLT at work. The dashed lines show the PMF of the distribution of the average from a sample of size \\(n\\) from a Bernoulli(0.1) distribution. The solid line is the standard normal PDF.\n\n\n\n\n\n\n\n3.2.4 Efficiency of maximum likelihood estimators*\nWe have used the LLN and the CLT to show that \\(\\hat{p}_n\\) is consistent and asymptotically normal, which are both wonderful properties for an estimator to have. However, they do not prove that \\(\\hat{p}_n\\) is the best estimator of \\(\\ptrue\\) in any particular sense. In Equation 3.9, the variance of \\(\\hat{p}_n\\) was \\[\n  \\mathcal{I}(\\ptrue)^{-1} = \\frac{\\ptrue (1 - \\ptrue)}{n},\n\\] which is the inverse of the Fisher information. It turns out that no other unbiased estimator of \\(\\ptrue\\) can have lower variance, so \\(\\hat{p}_n\\) is the minimum-variance unbiased estimator of \\(\\ptrue\\).\nSuppose \\(\\theta\\) is a parameter for a family of PMFs or PDFs \\(f(y, \\theta)\\) such that the true PMF or PDF is \\(f(y, \\theta_\\true)\\). When we observe \\(Y_1 = y_1, Y_2 = y_2, \\ldots Y_n = y_n\\), the likelihood is \\[\n  L(\\theta) = \\prod_{i = 1} f(y_i, \\theta),\n\\] and the log likelihood is \\[\n  \\ell(\\theta) = \\ln L(\\theta) = \\sum_{i = 1}^n \\ln f(y_i, \\theta).\n\\] The score function is \\[\n  U(\\theta) = \\frac{\\text{d}}{\\text{d} \\theta} \\ell(\\theta),\n\\] and the MLE is the solution of the score equation \\(U(\\hat{\\theta}) = 0\\). The Fisher information is \\[\n  \\mathcal{I}(\\theta) = \\E_\\theta\\bigg[\\frac{\\text{d}^2}{\\text{d} \\theta^2} \\ell(\\theta)\\bigg],\n\\] and \\(\\Var(\\hat{\\theta}) = \\mathcal{I}(\\theta)^{-1}\\). If \\(\\bar{\\theta}\\) is any unbiased estimator of the true value \\(\\theta_\\text{true}\\), then \\[\n  \\Var(\\bar{\\theta}) \\geq \\mathcal{I}(\\theta_\\text{true})^{-1}.\n\\] This result is called the Cramér-Rao lower bound (Rao 1945; Cramér 1946),10 No unbiased estimator of \\(\\theta_\\true\\) can have smaller variance than the MLE \\(\\hat{\\theta}\\). Maximum likelihood estimates are consistent, asymptotically normal, and asymptotically efficient when the likelihood is correct (Boos and Stefanski 2013).",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#hypothesis-testing",
    "href": "mlestimation.html#hypothesis-testing",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.3 Hypothesis testing",
    "text": "3.3 Hypothesis testing\nIn a hypothesis test, we specify a null hypothesis and then decide whether to reject it based on the value of a test statistic. A null hypothesis often takes the form \\[\n  H_0: \\theta_\\text{true} = \\theta_0.\n\\tag{3.10}\\] We reject \\(H_0\\) if the test statistic appears inconsistent with its distribution under \\(H_0\\). Otherwise, we fail to reject \\(H_0\\). It is traditional to avoid saying that \\(H_0\\) was accepted.\n\n3.3.1 Hypothesis tests and diagnostic tests\nIf we think of \\(H_0\\) as not having the disease and rejecting \\(H_0\\) as testing positive for the disease, a hypothesis test is analogous to a diagnostic test. Table 3.1 shows the possible outcomes of a hypothesis test, and its margins show the correspondence to diagnostic testing (Diamond and Forrester 1983). A false positive occurs when we reject \\(H_0\\) when it is true, which is called a type I error. A false negative occurs when we fail to reject \\(H_0\\) when it is false, which is called type II error.\n\n\n\nTable 3.1: Truth of \\(H_0\\) and hypothesis test results.\n\n\n\n\n\n\nReject \\(H_0\\) (\\(T^+\\))\nFail to reject \\(H_0\\) (\\(T^-\\))\n\n\n\n\n\\(H_0\\) false (\\(D^+\\))\nTrue positive\nFalse negative = type II error\n\n\n\\(H_0\\) true (\\(D^-\\))\nFalse positive = type I error\nTrue negative\n\n\n\n\n\n\nA hypothesis test has analogs of sensitivity and specificity. The equivalent of specificity is \\(1 - \\alpha\\) where \\[\n  \\alpha\n  = \\Pr(\\text{reject } H_0 \\given{} H_0 \\text{ true})\n\\] is the probability of a type I error. This is also called the significance level of the test. The equivalent of sensitivity is the power of the test, which is \\(1 - \\beta\\) where \\[\n  \\beta\n  = \\Pr(\\text{fail to reject } H_0 \\given{} H_0 \\text{ false})\n\\] is the probability of a type II error.\nA hypothesis test also has analogs of positive and negative predictive values (PPV and NPV). Just like the PPV and NPV of a dignostic test depend on the prevalence of disease, the PPV and NPV of a hypothesis test depend on the prior probability that \\(H_0\\) is true, which is the probability that \\(H_0\\) is true based on what we know before we see the test result. For a hypothesis test, the PPV is \\[\n  \\Pr(H_0 \\text{ false} \\given{} H_0 \\text{ rejected})\n  = \\frac{(1 - \\beta) \\Pr(H_0 \\text{ false})}\n  {(1 - \\beta) \\Pr(H_0 \\text{ false}) + \\alpha \\Pr(H_0 \\text{ true})}\n\\tag{3.11}\\] by Bayes’ rule. Similarly, the NPV of the hypothesis test is \\[\n  \\Pr(H_0 \\text{ true} \\given H_0 \\text{ not rejected})\n  = \\frac{(1 - \\alpha) \\Pr(H_0 \\text{ true})}\n  {(1 - \\alpha) \\Pr(H_0 \\text{ true}) + \\beta \\Pr(H_0 \\text{ false})}.\n\\tag{3.12}\\] The conditional probability that \\(H_0\\) is true given the result of the hypothesis test is called the posterior probability of \\(H_0\\).\n\n\n3.3.2 Wald, score, and likelihood ratio tests\nIn a maximum likelihood framework, there are three classical tests for a null hypothesis of the form \\[\n  H_0: \\ptrue = p_0.\n\\] These tests are asymptotically equivalent, which means that they produce similar results in large samples. The best way to visualize the different tests is to look at a graph of the log likelihood function. Figure 3.4 shows the log likelihood function for a binary outcome with \\(x = 60\\) events out of \\(n = 100\\) trials and a null hypothesis \\(H_0: p_\\true = 0.5\\). All three tests generalize to null hypotheses involving multiple parameters (Boos and Stefanski 2013).\nThe Wald test (Wald 1943) of \\(H_0\\) looks at the distance between the MLE \\(\\hat{p}\\) and the hypothesized value \\(p_0\\)(Wald 1943), rejecting \\(H_0\\) when this distance is sufficiently large.11 An example is shown in Figure 3.4. The Wald test statistic is \\[\n  W = \\frac{(\\hat{p} - p_0)^2}{I(\\hat{p})}\n  = \\frac{n (\\hat{p} - p_0)^2}{\\hat{p} (1 - \\hat{p})}\n  \\approxsim \\chi^2_1\n\\tag{3.13}\\] under \\(H_0\\), where \\(I(\\hat{p})\\) is the observed information from Equation 3.6. The \\(\\chi^2_1\\) distribution is the distribution of \\(Z^2\\) if \\(Z \\sim N(0, 1)\\).\nThe score test looks at the slope of the log likelihood at \\(p_0\\), rejecting \\(H_0\\) if this slope is sufficiently far from zero (Rao 1948; Aitchison and Silvey 1958). An example is shown in Figure 3.4. It score test statistic is \\[\n  S = \\frac{U(p_0)^2}{\\mathcal{I}(p_0)}\n  = \\frac{n (\\hat{p} - p_0)^2}{p_0 (1 - p_0)}\n  \\approxsim \\chi^2_1\n\\tag{3.14}\\] under \\(H_0\\), where \\(\\mathcal{I}(p_0)\\) is the expected information from Equation 3.5. The numerator of the score statistic is the same as for the Wald statistic in Equation 3.13, but the denominator uses the expected information at \\(p_0\\) instead of the observed information at \\(\\hat{p}\\). In score tests, it generally better to use the expected information than the observed information (Freedman 2007). The most important advantage of the score test is that it only needs the hypothesized null value \\(p_0\\), so it can be done without finding the maximum likelihood estimate \\(\\hat{p}\\).\nThe likelihood ratio test looks at the vertical distance between \\(\\ell(\\hat{p})\\) (which is the maximum) and \\(\\ell(p_0)\\), rejecting \\(H_0\\) if this distance is sufficiently large Wilks (1938).12 An example is shown in Figure 3.4. The likelihood ratio test statistic is \\[\n  L = 2\\big(\\ell(\\hat{p}) - \\ell(p_0)\\big)\n  \\approxsim \\chi^2_1\n\\tag{3.15}\\] under \\(H_0\\). The Neyman-Pearson lemma (Neyman and Pearson 1933) shows that the likelihood ratio test is the most powerful of all hypothesis test for comparing two hypotheses \\(H_0: \\ptrue = p_0\\) and \\(H_1: \\ptrue = p_1\\) at a fixed significance level.\n\n\n\nCode\n\nhtests.R\n\n## Hypothesis tests based on the log likelihood\n\n# binomial log likelihood, score, and information functions\nbin_loglik &lt;- function(p, k=60, n=100) {\n  k * log(p) + (n - k) * log(1 - p)\n}\nbin_score &lt;- function(p, k=60, n=100) {\n  k / p - (n - k) / (1 - p)\n}\nbin_information &lt;- function(p, k=60, n=100) {\n  k / p^2 + (n - k) / (1 - p)^2\n}\n\n# plot showing Wald, score, and likelihood ratio tests\np &lt;- seq(0.4, 0.8, length.out = 200)\nplot(p, bin_loglik(p), type = \"n\",\n     xlim = c(0.40, 0.70), ylim = c(-72, -66),\n     main = \"Tests of the null hypothesis p = 0.5\",\n     xlab = \"p\", ylab = \"ln L(p)\")\ngrid()\nlines(p, bin_loglik(p))\nabline(v = c(0.5, 0.6), lty = \"dotted\")\nabline(h = c(bin_loglik(0.5), bin_loglik(0.6)), lty = \"dashed\")\nabline(a = bin_loglik(0.5) - bin_score(0.5) * 0.5, b = bin_score(0.5),\n       col = \"darkgray\")\ntext(c(0.5, 0.6), c(-67.05, -67),\n     labels = c(expression(p[0]), expression(hat(p))))\ntext(0.55, -70.7, labels = \"Wald test\")\narrows(0.5, -70.5, 0.6, code = 3, length = 0.1)\narrows(0.475, bin_loglik(0.5), y1 = bin_loglik(0.6),\n       code = 3, length = 0.1)\ntext(0.45, -68.3, labels = \"LRT\")\n\n# The slope is the tangent of the angle to the x-axis.\n# We also must account for the different scales on the x- and y-axes.\n# 0.3 / 6 is xdist / ydist (see xlim and ylim above)\nscore_angle &lt;- atan(bin_score(0.5) * 0.3 / 6)\nangles &lt;- seq(0, score_angle, by = 0.01)\nscore_x &lt;- 0.5 + 0.04 * cos(angles)\nscore_y &lt;- bin_loglik(0.5) + 0.04 * (6 / 0.3) * sin(angles)\nlines(score_x, score_y)\ntext(0.56, -68.8, \"Score test\")\narrows(score_x[2], score_y[2], score_x[1], score_y[1], length = 0.1)\narrows(rev(score_x)[2], rev(score_y)[2], rev(score_x)[1], rev(score_y)[1],\n       length = 0.1)\n\n\n\n\n\n\n\n\n\nFigure 3.4: Binomial log likelihood function for \\(x = 60\\) and \\(n = 100\\). The null value of \\(p\\) is \\(p_0 = 0.5\\) and the maximum likelihood estimate is \\(\\hat{p} = 0.6\\).\n\n\n\n\n\n\n\n3.3.3 Critical values and p-values\nThe Neyman-Pearson approach to hypothesis testing fixes the significance level \\(\\alpha\\) before calculating the test statistic and deciding whether to reject \\(H_0\\).13 The decision to reject the null hypothesis depends on the value of the test statistic, which is compared to a critical value calculated based on the distribution of the test statistic under \\(H_0\\). If \\(Z \\sim N(0, 1)\\) under \\(H_0\\) \\[\n  \\Pr\\big(|Z| \\geq z_{1 - \\frac{\\alpha}{2}} \\given{} H_0 \\text{ true}\\big)\n  = 1 - \\alpha.\n\\] Because \\(Z^2 \\sim \\chi^2_1\\) when \\(Z \\sim N(0, 1)\\), this is equivalent to \\[\n  \\Pr\\big(Z^2 \\geq z_{1 - \\frac{\\alpha}{2}}^2 \\given{} H_0 \\text{ true}\\big)\n  = 1 - \\alpha.\n\\] In the Wald, score, and likelihood ratio tests above, \\(H_0\\) is rejected if the test statistic is larger than the critical value \\(z_{1 - \\frac{\\alpha}{2}}^2\\). For \\(\\alpha = 0.05\\), we have \\(z_{0.975} \\approx 1.96\\) so critical value for the \\(\\chi^2_1\\) distribution is \\(1.96^2 \\approx 3.84\\). The test statistic and critical value in a hypothesis test are analogous to the clinical measurement and cutoff in a diagnostic test.\nInstead of making a binary decision, it is more informative to calculate a measure of the evidence against \\(H_0\\). The p-value for a given test statistic is the lowest value of \\(\\alpha\\) at which the test would still fail to reject \\(H_0\\). A hypothesis test with significance level \\(\\alpha\\) rejects \\(H_0\\) if the p-value is \\(\\leq \\alpha\\). For the Wald, score, or likelihood ratio tests above, \\[\n  \\text{p-value} = 1 - F_{\\chi^2_1}(\\text{test statistic})\n\\] where \\(F_{\\chi^2_1}\\) is the CDF of the \\(\\chi^2_1\\) distribution If we think of the test statistic as the clinical measurement underlying a diagnostic test, the p-value equals \\(1 - \\text{spec}_\\text{max}\\) where \\(\\text{spec}_\\text{max}\\) is the highest specificity under which we would still get a positive test (i.e., reject \\(H_0\\)).",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#sec-wslr-ci",
    "href": "mlestimation.html#sec-wslr-ci",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.4 Confidence intervals",
    "text": "3.4 Confidence intervals\nA p-value is more informative than a binary decision whether to reject \\(H_0\\), but it is still more useful to know what values of \\(p\\) are plausibly consistent with the data we observed (Rothman 1978). The \\(1 - \\alpha\\) confidence interval for \\(\\ptrue\\) is the set of all possible null values \\(p_0\\) such that we would fail to reject \\(H_0: \\ptrue = p_0\\) in a hypothesis test with significance level \\(\\alpha\\). The endpoints of the confidence interval are called are called confidence limits. Just as different clinical measurements lead to different diagnostic tests, different hypothesis tests lead to different confidence intervals.\nIf we calculate a confidence interval many times with independent data sets, the \\(1 - \\alpha\\) confidence interval should contain \\(\\ptrue\\) with probability \\(1 - \\alpha\\). The actual probability that the confidence interval contains \\(\\ptrue\\) is called the coverage probability. A good confidence interval should have a coverage probability close to \\(1 - \\alpha\\) while being as narrow as possible. The Wald, score, and likelihood ratio tests from Section 3.3.2 are large-sample tests because they rely on consistency and asymptotic normality of the maximum likelihood estimate \\(\\hat{p}\\). All three tests can be inverted to produce confidence intervals that perform well in large samples. In smaller samples, the score and likelihood ratio confidence intervals often have better coverage probability and width than the Wald confidence interval (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001).\n\n3.4.1 Wald confidence intervals and the delta method\nThe Wald confidence limits come from solving the equation \\[\n  \\frac{(\\hat{p} - p)^2}{\\hat{p} (1 - \\hat{p}) / n}\n  = z_{1 - \\frac{\\alpha}{2}}^2.\n\\tag{3.16}\\] for \\(p\\), which gives us \\[\n  \\hat{p} \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p} (1 - \\hat{p})}{n}}.\n\\tag{3.17}\\] The coverage probabilities of Wald confidence intervals can be much lower than \\(1 - \\alpha\\), especially when \\(p_\\true\\) is close to zero or one (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001).\nAnother problem with the Wald confidence interval for \\(\\ptrue\\) is that it can have bounds outside \\([0, 1]\\). One way to avoid this is to calculate confidence limits for a transformation of \\(\\hat{p}\\) using the delta method. A good transformation \\(g(p)\\) should have continuous first derivatives and be strictly increasing or decreasing, so each value of \\(g(p)\\) corresponds to a single value of \\(p\\) (i.e., \\(g\\) is one-to-one). The delta method derives the approximate normal distribution \\(g(\\hat{p})\\) using the approximation \\[\n  g(\\hat{p}) \\approx g(\\ptrue) + g'(\\ptrue) (\\hat{p} - \\ptrue).\n\\] where \\(g'(\\ptrue)\\) is the slope of \\(g\\) at \\(\\ptrue\\). An example of this approximation is shown in Figure 3.5. The key insight is that \\[\n  \\Var[g(\\hat{p})] \\approx g'(\\ptrue)^2 \\Var(\\hat{p}),\n\\] which is a generalization of the fact that \\(\\Var(c \\hat{p}) = c^2 \\Var(\\hat{p})\\) for any constant \\(c\\). If \\(\\hat{p}\\) has an approximate \\(N\\bigl(\\ptrue, \\Var(\\hat{p})\\bigr)\\) distribution in large samples, then \\[\n  g(\\hat{p}) \\approxsim N\\bigl(g(\\ptrue), g'(\\ptrue)^2 \\Var(\\hat{p})\\bigr).\n\\] in large samples. Because our estimator \\(\\hat{p}\\) is consistent, we can replace the unknown \\(\\ptrue\\) with \\(\\hat{p}\\). Because \\(g\\) is one-to-one, we can calculate confidence limits for \\(\\ptrue\\) using the confidence limits for \\(g(\\ptrue)\\).\n\n\n\nCode\n\ndelta.R\n\n## Approximation used by the delta method\n\np &lt;- seq(0.02, 0.98, by = 0.01)\nlogit &lt;- function(p) log(p) - log(1 - p)\n\n# plot\nplot(p, logit(p), type = \"n\",\n     xlab = \"p\", ylab = \"logit(p)\")\ngrid()\nlines(p, logit(p))\npoints(0.6, logit(0.6))\nabline(logit(0.6) - 2.5, 1 / 0.24, lty = \"dashed\")\ntext(0.6, -1,\n     labels = expression(paste(\"logit(p) - \", logit(0.6) %~~% logit,\n                               \"'(0.6) (p - 0.6)\")))\n\n\n\n\n\n\n\n\n\nFigure 3.5: The approximation used by the delta method using the logistic transformation for a binomial confidence interval near \\(\\hat{p} = 0.6\\). The black curve is \\(\\logit(p)\\), and the dashed line shows the tangent line at \\(p = 0.6\\).\n\n\n\n\n\nA widely used transformation for probabilities is the logit transformation \\[\n  \\logit(p) = \\ln\\Bigl(\\frac{p}{1 - p}\\Bigr).\n\\tag{3.18}\\] The odds corresponding to the probability \\(p\\) is \\(\\frac{p}{1 - p}\\), so the logit is the natural logarithm of the odds. The logit transformation maps the interval \\((0, 1)\\) onto all of \\(\\mathbb{R}\\):\n\nAs \\(p \\rightarrow 0\\), the odds \\(p / (1 - p) \\rightarrow 0\\) and \\(\\logit(p) \\rightarrow -\\infty\\).\nWhen \\(p = 1 / 2\\), the odds \\(p / (1 - p) = 1\\) and \\(\\logit(p) = 0\\).\nAs \\(p \\rightarrow 1\\), the odds \\(p / (1 - p) \\rightarrow \\infty\\) and \\(\\logit(p) \\rightarrow \\infty\\).\n\nTo use the delta method, we need to calculate the derivative of \\(\\logit(p)\\). By the chain rule, \\[\n  \\logit'(p)\n  = \\frac{1 - p}{p} \\frac{1}{(1 - p)^2}\n  = \\frac{1}{p (1 - p)},\n\\] which is continuous and strictly positive for all \\(p \\in (0, 1)\\). By the delta method, the variance of \\(\\logit(\\hat{p})\\) is approximately \\[\n  \\logit'(\\ptrue)^2 \\frac{\\ptrue (1 - \\ptrue)}{n}\n  = \\frac{1}{\\ptrue^2 (1 - \\ptrue)^2} \\frac{\\ptrue (1 - \\ptrue)}{n}\n  = \\frac{1}{n \\ptrue (1 - \\ptrue)}.\n\\] When we replace the unknown \\(\\ptrue\\) with our MLE \\(\\hat{p}\\), we get the following confidence limits for \\(\\logit(\\ptrue)\\): \\[\n  \\logit(\\hat{p}) \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\frac{1}{n \\hat{p} (1 - \\hat{p})}}.\n\\] To get confidence limits for \\(\\ptrue\\), we use the inverse function for the logit, which is \\[\n  \\expit(v) = \\frac{e^v}{1 + e^v} = \\frac{1}{1 + e^{-v}}.\n\\tag{3.19}\\] This is also called the logistic function. If the confidence limits for \\(\\logit(\\ptrue)\\) are \\(a\\) and \\(b\\), then the confidence limits for \\(\\ptrue\\) are \\(\\expit(a)\\) and \\(\\expit(b)\\). These are guaranteed to be in \\((0, 1)\\) because \\(\\expit(v) \\in (0, 1)\\) for any \\(v \\in \\mathbb{R}\\). The logit-transformed confidence interval can have narrower width and a coverage probability closer to \\(1 - \\alpha\\) than the untransformed Wald confidence interval (Agresti 2013).\n\n\n3.4.2 Score (Wilson) confidence intervals\nThe score or Wilson confidence limits come from solving the equation \\[\n  \\frac{(\\hat{p} - p)^2}{p (1 - p) / n}\n  = z_{1 - \\frac{\\alpha}{2}}^2.\n\\tag{3.20}\\] for \\(p\\) (Wilson 1927). This differs from Equation 3.16 for the Wald confidence interval because it uses \\(p\\) instead of \\(\\hat{p}\\) in the denominator. It is a quadratic equation in \\(p\\), so it has two solutions. The center of the resulting confidence interval is \\[\n  \\tilde{p}\n  = \\hat{p} \\Bigg(\\frac{n}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg)\n    + \\frac{1}{2} \\Bigg(\\frac{z_{1 - \\frac{\\alpha}{2}}^2}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg)\n  = \\frac{x + \\frac{1}{2} z_{1 - \\frac{\\alpha}{2}}^2}{n + z_{1 - \\frac{\\alpha}{2}}^2},\n\\tag{3.21}\\] where \\(x\\) is the number of diseased individuals in our sample. This is a weighted average of \\(\\hat{p}\\) and \\(1 / 2\\) with weights proportional to \\(n\\) and \\(z_{1 - \\frac{\\alpha}{2}}^2\\), respectively. The resulting confidence interval is \\[\n  \\tilde{p} \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\tilde{V}}\n\\] where \\[\n  \\tilde{V}\n  = \\frac{\\hat{p} (1 - \\hat{p})}{n + z_{1 - \\frac{\\alpha}{2}}^2} \\Bigg(\\frac{n}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg)\n    + \\frac{\\big(\\frac{1}{2}\\big)^2}{n + z_{1 - \\frac{\\alpha}{2}}^2} \\Bigg(\\frac{z_{1 - \\frac{\\alpha}{2}}^2}{n + z_{1 - \\frac{\\alpha}{2}}^2}\\Bigg).\n\\] This variance is a weighted average of the variances of sample proportions equal to \\(\\hat{p}\\) and \\(1 / 2\\) with the same weights as in \\(\\tilde{p}\\) and with \\(n + z_{1 - \\frac{\\alpha}{2}}^2\\) instead of \\(n\\) in the denominator. Wilson confidence intervals are narrower than the corresponding Wald intervals, and they have coverage probabilities much closer to \\(1 - \\alpha\\) (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001).\nThe Agresti-Coull confidence interval is a simplification of the Wilson confidence interval that replaces \\(\\hat{p}\\) with \\(\\tilde{p}\\) in the Wald confidence interval to get the confidence limits \\[\n  \\tilde{p} \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\frac{\\tilde{p} (1 - \\tilde{p})}{n}}.\n\\] Because \\(z_{0.975} \\approx 1.96\\), we have \\(\\tilde{p} \\approx \\frac{k + 2}{n + 4}\\) for a 95% confidence interval. In this case, the Agresti-Coull interval is often implemented as follows: ``Add two successes and two failures and then use the Wald formula’’ (Agresti and Coull 1998). This interval is only slightly wider than the score confidence interval, and the two intervals are nearly identical for \\(n &gt; 40\\) (Brown, Cai, and DasGupta 2001).\nThe likelihood ratio test can also be inverted to get confidence intervals, but these can only be calculated numerically. For the binomial model, the likelihood ratio and score confidence intervals are nearly identical (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001). The score intervals are more common in practice because they are easier to calculate.\n\nR\n\n\n\n\nbinconf.R\n\n## Binomial confidence intervals\n\n# using BinomCI() function from the DescTools package\nlibrary(DescTools)\nBinomCI(15, 22, method = \"wald\")            # Wald confidence interval\nBinomCI(15, 22, method = \"logit\")           # logit-transformed Wald CI\nBinomCI(15, 22, method = \"wilson\")          # score CI (default)\nBinomCI(15, 22, method = \"agresti-coull\")   # Agresti-Coull CI\nBinomCI(15, 22, method = \"lik\")             # likelihood ratio CI\n\n# using binconf() function from the Hmisc package\nlibrary(Hmisc)\nbinconf(15, 22, method = \"asymptotic\")  # Wald CI\nbinconf(15, 22, method = \"wilson\")      # score CI (default)\n\n# using prop.test in base R (stats package)\n# Wilson confidence interval with continuity correction by default\n# The continuity correction is not generally recommended. Like the exact CI,\n# it can be too wide and have a coverage probability greater than 1 - \\alpha.\nprop.test(15, 22)\nnames(prop.test(15, 22))\nprop.test(15, 22, correct = FALSE)      # score CI\n\n# using binom.test (exact confidence interval)\nbinom.test(15, 22)    # same as binconf with method = \"exact\"\nnames(binom.test(15, 22))\n\n# changing the confidence level (1 - alpha) to 80%\n# All are score (Wilson) confidence intervals by default.\nBinomCI(15, 22, conf.level = 0.8)\nbinconf(15, 22, alpha = 0.2)\nprop.test(15, 22, conf.level = 0.8, correct = FALSE)\n\n# writing a function to get Wald confidence limits\nbconf_wald &lt;- function(x, n, level=0.95) {\n  # x is number of successes out of n trials\n  p_hat &lt;- x / n\n  alpha &lt;- 1 - level\n  pvar &lt;- p_hat * (1 - p_hat) / n\n  p_int &lt;- p_hat + c(-1, 1) * qnorm(1 - alpha / 2) * sqrt(pvar)\n\n  # return named vector (names do not need quotes)\n  return(c(point = p_hat, lower = p_int[1], upper = p_int[2]))\n}\nbconf_wald(15, 22)\nbconf_wald(15, 22, level = 0.80)",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#small-sample-estimation",
    "href": "mlestimation.html#small-sample-estimation",
    "title": "3  Maximum Likelihood Estimation",
    "section": "3.5 Small-sample estimation*",
    "text": "3.5 Small-sample estimation*\nMaximum likelihood estimates are consistent, asymptotically normal, and asymptotically efficient. However, they are not guaranteed to perform well in any finite sample. For a sample of \\(n\\) independent Bernoulli(\\(p\\)) random variables, the sum has a binomial(\\(n, p\\)) distribution and this can be used to find the finite-sample distribution of the sample mean. This distribution can be used directly to calculate point estimates, p-values, and confidence limits.\nConfidence limits calculated using the finite-sample distribution of a test statistic under \\(H_0\\) are called exact confidence limits. They can often be constructed to have a coverage probability of at least \\(1 - \\alpha\\). However, their coverage probabilities are often higher than \\(1 - \\alpha\\), and they can be much wider than approximate \\(1 - \\alpha\\) confidence intervals for the same parameter (Agresti and Coull 1998).\nIf the finite-sample distribution of the test statistic is not known exactly, it is possible to calculate point estimates, p-values, or confidence limits using simulations. This is the basic idea behind the bootstrap (Efron and Tibshirani 1994) and Monte Carlo methods (Robert and Casella 2004).\n\n3.5.1 Median unbiased estimate\nThe median unbiased estimate of \\(\\ptrue\\) is the value of \\(p\\) that makes \\[\n  \\Pr\\nolimits_p(X &lt; x) = \\Pr\\nolimits_p(X &gt; x)\n\\] where we use the subscript \\(p\\) to indicate that these probabilities are calculated assuming \\(\\ptrue = p\\). If \\(p_\\text{med}\\) is the median unbiased estimate, then \\[\n  \\sum_{k = 0}^{x - 1} \\binom{n}{k} p_\\text{med}^k \\big(1 - p_\\text{med}\\big)^{n - k}\n  + \\frac{1}{2} \\binom{n}{x} p_\\text{med}^k \\big(1 - p_\\text{med}\\big)^{n - x}\n  = \\frac{1}{2},\n\\] and \\[\n  \\frac{1}{2} \\binom{n}{x} p_\\text{med}^x \\big(1 - p_\\text{med}\\big)^{n - x}\n  + \\sum_{k = x + 1}^n \\binom{n}{k} p_\\text{med}^k \\big(1 - p_\\text{med}\\big)^{n - k}\n  = \\frac{1}{2}.\n\\] The median of the distribution of \\(p_\\text{med}\\) is always \\(\\ptrue\\) (Birnbaum 1964), which is a slightly different notion of unbiasedness than the unbiasedness of \\(\\hat{p}\\) where \\(\\E(\\hat{p}) = \\ptrue\\).\n\n\n3.5.2 Exact (Clopper-Pearson) and mid-p confidence intervals\nThe exact or Clopper-Pearson confidence limits for \\(\\ptrue\\) use the finite-sample distribution of the sample mean \\(\\hat{p}\\) Clopper and Pearson (1934). When \\(x &gt; 0\\), the lower \\(1 - \\alpha\\) confidence limit is the solution to \\[\n  \\sum_{k = x}^n \\binom{n}{k} p_\\text{lower}^k (1 - p_\\text{lower})^{n - k}\n  = \\frac{\\alpha}{2},\n\\tag{3.22}\\] so the upper tail of the binomial(\\(n\\), \\(p_\\text{lower}\\)) distribution has probability \\(\\alpha / 2\\). When \\(x = 0\\), we set \\(p_\\text{lower} = 0\\). When \\(x &lt; n\\), the upper confidence limit is the solution to \\[\n  \\sum_{k = 0}^x \\binom{n}{k} p_\\text{upper}^k (1 - p_\\text{upper})^{n - k}\n  = \\frac{\\alpha}{2},\n\\tag{3.23}\\] so the lower tail of the binomial(\\(n\\), \\(p_\\text{upper}\\)) distribution has probability \\(\\alpha / 2\\). When \\(x = n\\), we set \\(p_\\text{upper} = 1\\). This interval is guaranteed to have a coverage probability of at least \\(1 - \\alpha\\), but the price for this is that it is always wider than the Wald and Wilson confidence intervals (Agresti and Coull 1998; Brown, Cai, and DasGupta 2001). In general, the score or likelihood ratio confidence intervals have better combinations of coverage probability and width.\nTo make exact confidence limits less conservative, we can include only \\(\\frac{1}{2} \\Pr(X = x)\\) instead of \\(\\Pr(X = x)\\) in the calculation of the tail probabilities in Equation 3.23 and Equation 3.22. The resulting confidence intervals are called mid-p exact confidence intervals (Lancaster 1961, berry1995mid). The lower \\(1 - \\alpha\\) mid-p exact confidence limit is the solution to \\[\n  \\frac{1}{2} \\binom{n}{x} p_\\text{lower}^x (1 - p_\\text{lower})^{n - x}\n    + \\sum_{k = x + 1}^n \\binom{n}{k} p_\\text{lower}^k (1 - p_\\text{lower})^{n - k}\n  = \\frac{\\alpha}{2}.\n\\] and the upper limit is the solution to \\[\n  \\sum_{k = 0}^{x - 1} \\binom{n}{k} p_\\text{upper}^k (1 - p_\\text{upper})^{n - k}\n    + \\frac{1}{2} \\binom{n}{x} p_\\text{upper}^x (1 - p_\\text{upper})^{n - x}\n  = \\frac{\\alpha}{2}.\n\\] The mid-p exact confidence limits are have good combinations of coverage probability and width as well as good perfomance in small samples (Brown, Cai, and DasGupta 2001).\n\nR\n\n\n\n\nbinomial-small.R\n\n## Small-sample binomial point and interval estimates\n\n# median unbiased estimate\nmedp_binom &lt;- function(k, n) {\n  # k = number of successes, n = number of trials\n\n  # binomial lower tail probability\n  lower_tail &lt;- function(p) pbinom(k, n, p) - dbinom(k, n, p) / 2\n\n  # median unbiased estimate\n  med &lt;- uniroot(function(p) lower_tail(p) - 1 / 2, interval = c(0, 1))\n  med$root\n}\nmedp_binom(15, 22)\n\n\n# exact (Clopper-Pearson) confidence intervals\nbinom.test(15, 22)                              # base R (stats)\nnames(binom.test(15, 22))\nbinom.test(15, 22, conf.level = 0.8)\n\nlibrary(Hmisc)\nbinconf(15, 22, method = \"exact\")\nbinconf(15, 22, method = \"exact\", alpha = 0.2)\n\nlibrary(DescTools)\nBinomCI(15, 22, method = \"clopper-pearson\")     # exact CI\nBinomCI(15, 22, method = \"midp\")                # mid-p exact CI\n\n\n\n\n\n\n\n\nAgresti, Alan. 2013. Categorical Data Analysis. Third. Vol. 792. John Wiley & Sons.\n\n\nAgresti, Alan, and Brent A Coull. 1998. “Approximate Is Better Than ‘Exact’ for Interval Estimation of Binomial Proportions.” The American Statistician 52 (2): 119–26.\n\n\nAitchison, John, and SD Silvey. 1958. “Maximum-Likelihood Estimation of Parameters Subject to Restraints.” The Annals of Mathematical Statistics 29: 813–28.\n\n\nBirnbaum, Allan. 1964. “Median-Unbiased Estimators.” Bulletin of Mathematical Statistics 11: 25–34.\n\n\nBoos, Dennis D, and Leonard A Stefanski. 2013. Essential Statistical Inference. Springer.\n\n\nBrown, Lawrence D, T Tony Cai, and Anirban DasGupta. 2001. “Interval Estimation for a Binomial Proportion.” Statistical Science 16 (2): 101–17.\n\n\nClopper, Charles J, and Egon S Pearson. 1934. “The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial.” Biometrika 26 (4): 404–13.\n\n\nCohen, I Bernard. 1984. “Florence Nightingale.” Scientific American 250 (3): 128–37.\n\n\nCramér, Harald. 1946. Mathematical Methods of Statistics. Princeton University Press.\n\n\nDiamond, George A, and James S Forrester. 1983. “Clinical Trials and Statistical Verdicts: Probable Grounds for Appeal.” Annals of Internal Medicine 98 (3): 385–94.\n\n\nEfron, Bradley, and David V Hinkley. 1978. “Assessing the Accuracy of the Maximum Likelihood Estimator: Observed Versus Expected Fisher Information.” Biometrika 65 (3): 457–83.\n\n\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. Chapman & Hall/CRC.\n\n\nFreedman, David A. 2007. “How Can the Score Test Be Inconsistent?” The American Statistician 61 (4): 291–95.\n\n\nKenward, Michael G, and Geert Molenberghs. 1998. “Likelihood Based Frequentist Inference When Data Are Missing at Random.” Statistical Science 13 (3): 236–47.\n\n\nLancaster, H Oliver. 1961. “Significance Tests in Discrete Distributions.” Journal of the American Statistical Association 56 (294): 223–34.\n\n\nNeyman, Jerzy, and Egon Sharpe Pearson. 1933. “On the Problem of the Most Efficient Tests of Statistical Hypotheses.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 231 (694-706): 289–337.\n\n\nRao, C Radhakrishna. 1945. “Information and Accuracy Attainable in the Estimation of Statistical Parameters.” Bulletin of the Calcutta Mathematical Society 37 (3): 81–91.\n\n\n———. 1948. “Large Sample Tests of Statistical Hypotheses Concerning Several Parameters with Applications to Problems of Estimation.” In Mathematical Proceedings of the Cambridge Philosophical Society, 44:50–57. Cambridge University Press.\n\n\nReid, Nancy. 2003. “Asymptotics and the Theory of Inference.” The Annals of Statistics 31 (6): 1695–2095.\n\n\nRobert, Christian P, and George Casella. 2004. Monte Carlo Statistical Methods. Second edition. Springer.\n\n\nRothman, Kenneth J. 1978. “A Show of Confidence.” New England Journal of Medicine 299 (24): 1362–63.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\nWald, Abraham. 1943. “Tests of Statistical Hypotheses Concerning Several Parameters When the Number of Observations Is Large.” Transactions of the American Mathematical Society 54 (3): 426–82.\n\n\nWilks, Samuel S. 1938. “The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses.” The Annals of Mathematical Statistics 9 (1): 60–62.\n\n\nWilson, Edwin B. 1927. “Probable Inference, the Law of Succession, and Statistical Inference.” Journal of the American Statistical Association 22 (158): 209–12.\n\n\nWinkelstein Jr, Warren. 2009. “Florence Nightingale: Founder of Modern Nursing and Hospital Epidemiology.” Epidemiology 20 (2): 311.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mlestimation.html#footnotes",
    "href": "mlestimation.html#footnotes",
    "title": "3  Maximum Likelihood Estimation",
    "section": "",
    "text": "John Tukey (1915-2000) was an American mathematician and statistician who worked at Bell Labs and Princeton University. He developed the box plot, Tukey’s range test for multiple comparisons, and the fast Fourier transform. In 1947, he coined the term “bit” as shorthand for “binary digit”.↩︎\n She was elected a member of the Royal Statistical Society in 1859, where she was the first woman to be a member. In 1860, she founded the world’s first modern nursing school at St. Thomas Hospital in London.↩︎\n For finite \\(N\\), \\(X\\) actually has a hypergeometric distribution because the test results are not exactly independent. If the first person in our sample has disease, the probability that the next person we sample has disease is slightly less than \\(p\\). If the first person in our sample does not have disease, the probability that the next person we sample has disease is slightly greater than \\(p\\). When \\(N \\gg n\\), this hypergeometric distribution is approximately binomial(\\(n\\), \\(p\\)).↩︎\n Euler’s number \\(e\\) is named after Leonhard Euler (1707–1783), a Swiss mathematician who introduced the notation \\(f(x)\\) for mathematical functions and the letter \\(i\\) to denote the imaginary unit \\(\\sqrt{-1}\\). He spent most of his life in Berlin and St. Petersburg, and he is widely considered the greatest mathematician of the 18th century. The number \\(e\\) was first discovered in 1683 by Jacob Bernoulli (the namesake of the Bernoulli distribution) when studying compound interest, where \\(e = \\lim_{n \\rightarrow \\infty} (1 + 1 / n)^n\\). In 1748, Euler proved that \\(e = \\frac{1}{0!} + \\frac{1}{1!} + \\frac{1}{2!} + \\frac{1}{3!} + \\cdots\\).↩︎\n This is named for Josiah Willard Gibbs (1839–1903), an American scientist who earned the first American doctorate in engineering in 1863 and went on to work on statistical mechanics, thermodynamics, optics, and vector calculus as a professor of physics at Yale. Albert Einstein called him the greatest mind in American history.↩︎\n Named after Ronald Fisher (1890–1962), who established the foundations of maximum likelihood inference between 1912 and 1922. He was the most important statistician of the 20th century, and he was one of the founders of population genetics. He had poor eyesight for his entire life, which led him to develop a formidable sense of geometry in his head. However, he was also a leading eugenicist and one of the most vocal opponents of the hypothesis that smoking causes lung cancer.↩︎\n For estimating a parameter \\(\\theta\\), the conditions are these: (1) The set of possible values of the observed data \\(X\\) does not depend on \\(\\theta\\). (2) Each \\(\\theta\\) produces a different distribution of \\(X\\). (3) The true value of \\(\\theta\\) is in the interior of the set of possible values. (4) The log likelihood \\(\\ell(\\theta)\\) has continuous first and second derivatives with respect to \\(\\theta\\) in a neighborhood of \\(\\theta_\\true\\). These conditions are met by the binomial likelihood when \\(\\ptrue \\in (0, 1)\\).↩︎\n For simplicity, we are being vague about what we mean by \\(\\hat{\\mu}_n \\rightarrow \\mu\\). Probability has several different notions of convergence/. The weak LLN guarantees convergence in probability, which means that \\(\\lim_{n \\rightarrow \\infty} \\Pr\\big(|\\hat{\\mu}_n - \\mu| &gt; \\varepsilon\\big) = 0\\) for any \\(\\varepsilon &gt; 0\\). The strong LLN guarantees convergence almost surely, which means that \\(\\Pr\\big(\\lim_{n \\rightarrow \\infty} \\hat{\\mu}_n = \\mu\\big) = 1\\).↩︎\n Named after Carl Friedrich Gauss (1777-1855), a German mathematician who is widely considered one of the greatest mathematicians of all time. He discovered the normal distribution in 1809, but the CLT itself was first proved by Laplace in 1810 (see Chapter 1).↩︎\n Named after Swedish statistician Harald Cramér (1893–1985), who was a professor at Stockholm University, and Indian-American statistician Calyampudi Radhakrishna (C. R.) Rao (1920–2023), who was a professor at the Indian Statistical Institute, the University of Cambridge, the University of Pittsburgh, and Pennsylvania State University.↩︎\n Named after Abraham Wald (1902–1950), a Jewish Hungarian mathematician who was invited to move from Vienna to the United States in 1938 after Nazi Germany annexed Austria. He worked at the Statistical Research Group at Columbia University during World War II. In 1950, he and his wife were killed in a plane crash in India, where he was visiting the Indian Statistical Institute.↩︎\n Samuel S. Wilks (1906–1964) was an American mathematician and statistician who grew up on a farm in Texas, got a Ph.D. at the University of Iowa, and went on to be a professor at Princeton University.↩︎\n This approach to hypothesis testing was pioneered in the 1929s by Jerzy Neyman (1894–1981), a Polish mathematician and statistician who founded the first department of statistics in the United States at the University of California, Berkeley in 1938, and Egon Pearson (1895–1980), a British statistician who was a professor at University College London like his father Karl Pearson. ↩︎",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html",
    "href": "bayes.html",
    "title": "4  Bayesian Estimation",
    "section": "",
    "text": "4.1 Prior and posterior distributions\nIn Bayesian inference, probability distributions are used to summarize our knowledge about the possible values of an unknown parameter \\(\\theta_\\true\\). The distribution of possible values of \\(\\theta_\\true\\) before we have seen our data is called the prior distribution, and the distribution of possible values of \\(\\theta_\\true\\) after we see the data is called the posterior distribution. Most parameters we are interested in estimating (such as probabilities) are continuous, so they have a probability density function (PDF) instead of a probability mass function (PMF). The posterior PDF is proportional to the prior PDF times the likelihood function, and the posterior distribution can be used to get point and interval estimates of an unknown parameter. The interval estimates are called credible intervals, and they have important advantages over confidence intervals. The Bayesian approach to statistics is more logically consistent and more intuitive than the frequentist approach, but it can be more computationally complex. While large-sample theory can be useful in Bayesian inference, it does not rely on asymptotic normality to the same degree that maximum likelihood estimation does.\nThe value of a PDF is not a probability (it can be greater than one), but PDFs can be handled like probabilities in terms of the addition rule and the multiplication of conditional probabilities Boos and Stefanski (2013). Let \\(\\pi(\\theta)\\) be the prior PDF of \\(\\theta\\). Before we see our data, we believe that \\(\\theta_\\true \\in [a, b]\\) with probability \\[\n  \\Pr(a \\leq \\theta \\leq b) = \\int_a^b \\pi(\\theta) \\,\\dif \\theta.\n\\] This integral is the area under the graph of \\(\\pi(\\theta)\\) between \\(\\theta = a\\) and \\(\\theta = b\\). For a given value of \\(\\theta\\), the likelihood of our data is \\(L(\\theta) = \\pi(\\data \\given{} \\theta)\\). By Bayes’ rule, the posterior PDF is \\[\n  \\pi(\\theta \\given{} \\data)\n  = \\frac{L(\\theta) \\pi(\\theta)}{\\pi(\\data)}\n  \\propto L(\\theta) \\pi(\\theta).\n\\] where \\(\\propto\\) denotes “proportional to.” Calculating \\(\\pi(\\data)\\) is difficult and almost always unnecessary. As long as we can calculate \\(\\pi(\\theta \\given{} \\data)\\) up to a constant of proportionality, we can normalize it to ensure that we have a posterior PDF whose integral over \\(\\mathbb{R}\\) equals one. After we see our data, we believe that \\(\\theta_\\true \\in [a, b]\\) with probability \\[\n  \\Pr(a \\leq \\theta \\leq b \\given{} \\data)\n  = \\int_a^b \\pi(\\theta \\given{} \\data) \\,\\dif \\theta.\n\\] Bayesian point and interval estimates of \\(\\theta_\\true\\) are based on the posterior PDF.\nBayesian methods are often simpler, easier to interpret, and more robust to small sample sizes than frequentist methods like maximum likelihood estimation. In the limit of a large sample size, Bayesian and frequentist methods almost always give equivalent results. The Bayesian approach is also valuable because it emphasizes estimation and the accumulation of knowledge rather than binary decisions (Tukey 1960). However, the adoption of Bayesian methods has been impeded by a historical lack of the computational power needed to use them and by the widespread hesitation to specify prior distributions among epidemiologists, statisticians, and other scientists. The first problem is largely solved, but the latter problem remains with us today.\nA common approach to specifying a prior distribution is to use a noninformative prior that places few or no restrictions on the value of \\(\\theta\\). This lets the data “speak for itself” at the price of ignoring existing knowledge about the underlying scientific question. The ability to incorporate an informative prior distribution in the Bayesian approach to statistical inference should be viewed as a feature, not a bug (Greenland 2006; Greenland and Poole 2013).",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html#prior-and-posterior-distributions",
    "href": "bayes.html#prior-and-posterior-distributions",
    "title": "4  Bayesian Estimation",
    "section": "",
    "text": "4.1.1 Posterior point and interval estimation\nThe mean, median, or mode of the posterior distribution of \\(\\theta\\) can be used as a point estimate of \\(\\theta_\\true\\). We will use \\(\\bar{\\theta}\\) to denote the posterior mean and \\(\\tilde{\\theta}\\) to denote the posterior median. In large samples, the posterior distribution converges to a normal distribution with mean \\(\\theta_\\true\\) and variance \\(I(\\theta_\\true)^{-1}\\) (Le Cam 1953; Gelman et al. 2014),2 which is the same as the limiting normal distribution of the MLE \\(\\hat{\\theta}\\). In this limit, the posterior mean, median, and mode are all equal to \\(\\hat{\\theta}\\).\nA \\(1 - \\alpha\\) credible interval is an interval \\([a, b]\\) such that \\[\n  \\Pr(a \\leq \\theta \\leq b \\given{} \\data)\n  = \\int_a^b \\pi(\\theta \\given{} \\data) \\,\\dif \\theta\n  = 1 - \\alpha.\n\\] Given our data, we believe that \\(\\theta_\\true \\in [a, b]\\) with probability \\(1 - \\alpha\\). There are many different ways that a credible interval can be defined. The one that is conceptually closest to a confidence interval is the central posterior interval or equal-tailed interval, where \\[\n  \\Pr(\\theta &lt; a \\given{} \\data) = \\Pr(\\theta &gt; b \\given{} \\data)\n  = \\frac{\\alpha}{2}.\n\\] Thus, the \\(1 - \\alpha\\) confidence limits are the \\(\\alpha / 2\\) and \\(1 - \\alpha / 2\\) quantiles of the posterior distribution of \\(\\theta_\\true\\). Unlike confidence intervals, credible intervals can accurately be interpreted as containing \\(\\theta_\\true\\) with probability \\(1 - \\alpha\\). Like confidence intervals, credible intervals are only reliable if the likelihood is approximately correct. An equal-tailed credible interval is guaranteed to contain the posterior median \\(\\tilde{\\theta}\\). It will usually contain the posterior mean \\(\\bar{\\theta}\\), but this is not guaranteed in small samples.\n\n\n4.1.2 Bayesian interpretation of confidence intervals\nBayesian credible intervals actually have the properties that people intuitively but naively expect of frequentist confidence intervals. In finite samples, a \\(1 - \\alpha\\) equal-tailed credible interval and a \\(1 - \\alpha\\) confidence interval will be similar when the posterior density \\(\\pi(\\theta \\given{} \\data)\\) is proportional to the likelihood \\(L(\\theta) = \\pi(\\theta \\given{} \\data)\\). This occurs when \\(\\pi(\\theta)\\) is constant, as in some uninformative priors. In the limit of a large sample, the credible interval and the confidence interval are nearly identical. In general, a frequentist confidence interval can be interpreted as an approximation to a Bayesian credible interval when there is a large sample and a prior distribution that is flat across the range of the confidence interval (Pratt 1965; Greenland and Poole 2013). However, credible intervals do not rely on large-sample approximations, and they are able to incorporate prior knowledge about the possible values of \\(\\theta_\\true\\).\n\n\n4.1.3 Posterior probability of \\(H_0\\) and p-values\nThe idea of prior and posterior probabilities from Bayesian inference gives us a useful perspective on the interpretation of p-values, which is a source of much confusion in epidemiology (Diamond and Forrester 1983; Greenland et al. 2016; Baduashvili, Evans, and Cutler 2020). The most common error is to interpret the p-value as the posterior probability that \\(H_0\\) is true. By Bayes’ rule, \\[\n  \\Pr(H_0 \\ \\true \\given{} \\data)\n  = \\frac{\\Pr(\\data \\given{} H_0 \\ \\true) \\Pr(H_0 \\ \\true)}{\\Pr(\\data)}.\n\\] The p-value is analogous to \\(\\Pr(\\data \\given{} H_0 \\ \\true)\\), but the posterior probability \\(\\Pr(H_0 \\text{ true} \\given{} \\ \\data)\\) depends on its prior probability \\(\\Pr(H_0)\\). It should take more data to convince us of a null hypothesis that seemed very unlikely than to convince us of a null hypothesis that seemed very likely.\nFor a null hypothesis of the form \\(H_0: \\theta_\\true = \\theta_0\\) where \\(\\theta\\) ranges over an interval, it can be difficult to assign a prior probability to \\(H_0\\). For any continuous distribution of \\(\\theta\\), the probability that it takes any particular value is zero. One way around this difficulty is to assign a prior probability mass to the null value \\(\\theta_0\\). A more difficult problem is to assign a prior probability to the alternative hypothesis \\(H_1\\), which is often not clearly specified. This means that the posterior probability of the null hypothesis is often not clearly defined.\nHowever, it is not difficult to calculate a lower bound on the probability that \\(H_0\\) is true given the data (Edwards, Lindman, and Savage 1963; Berger and Sellke 1987). Let \\(\\pi_0\\) be the prior probability of \\(H_0\\), and suppose we have a single alternative hypothesis \\(H_1: \\theta_\\true = \\theta_1\\) with prior probability \\(1 - \\pi_0\\). Then the posterior probability of \\(H_0\\) is \\[\n  \\Pr(H_0 \\given{} \\data)\n  = \\frac{L(\\theta_0) \\pi_0}{L(\\theta_0) \\pi_0 + L(\\theta_1) (1 - \\pi_0)}\n  = \\bigg(1 + \\frac{1 - \\pi_0}{\\pi_0} \\frac{L(\\theta_1)}{L(\\theta_0)}\\bigg)^{-1}.\n\\] Given the data, the posterior probability of \\(H_0\\) is minimized if we happen to get the MLE \\(\\hat{\\theta} = \\theta_1\\), so \\[\n  \\Pr(H_0 \\ \\true \\given{} \\data)\n  \\geq \\bigg(1 + \\frac{1 - \\pi_0}{\\pi_0}\\frac{L(\\hat{\\theta})}{L(\\theta_0)}\\bigg)^{-1}.\n\\] From Wilk’s theorem (Wilks 1938) for the likelihood ratio test, twice the log likelihood ratio has an approximate \\(\\chi^2_1\\) distribution in large samples. To get a p-value of \\(\\alpha\\) from the likelihood ratio test, we need \\[\n  2\\big(\\ell(\\hat{\\theta}) - \\ell(\\theta_0)\\big)\n  = z_{1 - \\frac{\\alpha}{2}}^2\n  \\;\\Rightarrow\\;\n  \\frac{L(\\hat{\\theta})}{L(\\theta_0)}\n   = e^{\\frac{1}{2} z_{1 - \\frac{\\alpha}{2}}^2}.\n\\] Figure 4.1 shows the minimum posterior probability of \\(H_0\\) if \\(\\pi_0 = 0.5\\) for different p-values. The p-value is almost always much lower than the lower bound of the posterior probability of the null. For \\(\\pi_0 = 0.5\\), the lower bounds for \\(\\Pr(H_0 \\ \\true \\given{} \\data)\\) are approximately 0.205 for \\(p = 0.10\\), 0.128 for \\(p = 0.05\\), and 0.035 for \\(p = 0.01\\). In practice, the posterior probability of \\(H_0\\) can be much larger than its lower bound (Berger and Sellke 1987).\n\n\n\nCode\n\npostH0.R\n\n## Posterior probability of the null hypothesis (H0)\n\n# function to calculate lower bound\nlowerb &lt;- function(pi0=0.5, pval=0.05) {\n  # args: pi0 = prior probability of H0, pval = p-value\n  # return: lower bound on posterior probability of H0\n  z &lt;- qnorm(1 - pval / 2)\n  1 / (1 + (1 - pi0) / pi0 * exp(0.5 * z^2))\n}\n\n# plot of lower bounds for p-value = 0.01, 0.05, and 0.1\nx &lt;- seq(0, 1, by = .01)\nplot(x, lowerb(x), type = \"n\", xlim = c(0, 1), ylim = c(0, 1),\n     xlab = expression(\"Prior probability of H\"[0]),\n     ylab = expression(\"Minimum posterior probability of H\"[0]))\ngrid()\nabline(0, 1, col = \"gray\")\nlines(x, lowerb(x))\nlines(x, lowerb(x, pval = .01), lty = \"dashed\")\nlines(x, lowerb(x, pval = .1), lty = \"dotted\")\nlegend(\"topleft\", bg = \"white\", lty = c(\"dotted\", \"solid\", \"dashed\"),\n       legend = c(\"p = 0.10\", \"p = 0.05\", \"p = 0.01\"))\n\n\n\n\n\n\n\n\n\nFigure 4.1: The minimum posterior probability of \\(H_0\\) as a function of its prior probability \\(\\pi_0\\) for p-values of 0.01, 0.05, and 0.1. These posterior probabilities can be much higher than the p-value.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html#bayesian-estimation-of-a-probability",
    "href": "bayes.html#bayesian-estimation-of-a-probability",
    "title": "4  Bayesian Estimation",
    "section": "4.2 Bayesian estimation of a probability",
    "text": "4.2 Bayesian estimation of a probability\nWhen estimating a probability \\(\\ptrue\\), our likelihood will be the binomial likelihood \\[\n  L(p) = \\binom{n}{k} p^k (1 - p)^{n - x}\n\\] when \\(k\\) out of \\(n\\) samples equal one. Because we only need to calculate the likelihood up to a constant of proportionality, we can safely ignore the \\(\\binom{n}{k}\\) because it does not depend on \\(p\\). The posterior distribution of \\(p\\) will be \\[\n  \\pi(p \\given{} \\data) \\propto p^k (1 - p)^{n - k} \\pi(p)\n\\] where \\(\\pi(p)\\) is the prior distribution of \\(p\\). When \\(\\pi(p \\given{} \\data)\\) and \\(\\pi(p)\\) are from the same family of distributions, the prior \\(\\pi(p)\\) is said to be a conjugate distribution for the binomial likelihood. Conjugate distributions exist for many likelihoods used in epidemiology.\n\n4.2.1 Beta distribution\nThe conjugate distribution for the binomial likelihood is the beta distribution. It has is a support on \\([0, 1]\\) (where \\(p_\\true\\) must live) with the PDF \\[\n  f(x, \\alpha, \\beta)\n  = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}\n    x^{\\alpha - 1} (1 - x)^{\\beta - 1}\n  \\propto x^{\\alpha - 1} (1 - x)^{\\beta - 1}.\n\\] where \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\) are shape parameters and \\(\\Gamma(v)\\) is the gamma function.3 Because we will only need to calculate PDFs up to a mutiplicative constant, the gamma function term can be safely ignored. If \\(X \\sim \\text{beta}(\\alpha, \\beta)\\), then \\[\n  \\E[X] = \\frac{\\alpha}{\\alpha + \\beta}\n\\] and \\[\n  \\Var(X) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}.\n\\] The beta distribution has a number of important special cases:\n\nWhen \\(\\alpha = \\beta = 1\\), it is a uniform(\\(0, 1\\)) distribution.\nThe beta distribution with \\(\\alpha = \\beta = 1 / 2\\) is the Jeffreys prior for the binomial likelihood.4 A Jeffreys prior is proportional to the square root of the expected information \\(\\mathcal{I}_1(\\theta)\\) of a single observation(Jeffreys 1946). They are widely used as noninformative priors. For the binomial likelihood, \\(\\mathcal{I}_1(p) = \\frac{1}{p (1 - p)}\\).\n\n\n\n4.2.2 Posterior point and interval estimates\nIf the prior distribution of \\(p\\) is a beta(\\(\\alpha, \\beta\\)) distribution, then \\[\n  \\begin{aligned}\n    \\pi(p \\given \\data)\n    &\\propto p^k (1 - p)^{n - k} \\times p^{\\alpha - 1} (1 - p)^{\\beta - 1} \\\\\n    &= p^{k + \\alpha - 1} (1 - p)^{n - k + \\beta - 1}\n  \\end{aligned}\n\\] so the posterior distribution of \\(p\\) is a beta(\\(k + \\alpha, n - k + \\beta\\)) distribution. The posterior mean is \\[\n  \\bar{p} = \\frac{k + \\alpha}{(k + \\alpha) + (n - k + \\beta)}\n  = \\frac{k + \\alpha}{n + \\alpha + \\beta},\n\\] and the posterior variance is \\[\n  \\frac{(k + \\alpha) (n - k + \\beta)}{(n + \\alpha + \\beta)^2 (n + \\alpha + \\beta + 1)}\n  = \\frac{\\bar{p} (1 - \\bar{p})}{n + \\alpha + \\beta + 1}.\n\\] The endpoints of the \\(1 - \\alpha\\) central credible interval are the \\(\\alpha / 2\\) and \\(1 - \\alpha / 2\\) quantiles of the beta(\\(k + \\alpha, n - k + \\beta)\\) distribution. Figure 4.2 shows prior and posterior distributions for 15 successes out of 22 trials. Although the prior distributions are quite different, the posterior distributions are quite similar. With large samples, the prior distribution disappears into the likelihood.\n\n\n\nCode\n\nnormplots.R\n\n## Bayesian prior and posterior distributions for a probability\n\n# beta prior and posterior distributions\n# The prior is beta(1, 1), which is the uniform(0, 1) distribution.\n# Because k = 15 and n = 22, the posterior is beta(15 + 1, 22 - 15 + 1)\np &lt;- seq(0.01, 0.99, by = 0.01)\nplot(p, dbeta(p, 16, 8), type = \"n\", ylim = c(0, 5),\n     xlab = \"p\", ylab = \"Probability density\")\ngrid()\nlines(p, dbeta(p, 1, 1), lty = \"dashed\")  # prior PDF\nlines(p, dbeta(p, 16, 8))                 # posterior PDF\npostlower &lt;- qbeta(0.025, 16, 8)          # 95% credible interval lower bound\npostupper &lt;- qbeta(0.975, 16, 8)          # 95% credible interval upper bound\npolygon(c(0, seq(0, postlower, by = 0.01), postlower),\n        c(0, dbeta(seq(0, postlower, by = 0.01), 16, 8), 0),\n        col = \"darkgray\")\npolygon(c(postupper, seq(postupper, 1, by = 0.01), 1),\n        c(0, dbeta(seq(postupper, 1, by = 0.01), 16, 8), 0),\n        col = \"darkgray\")\nlegend(\"topleft\", bg = \"white\",\n       lty = c(\"dashed\", \"solid\", NA, \"dashed\", \"solid\"),\n       col = c(\"black\", \"black\", NA, \"darkgray\", \"darkgray\"),\n       fill = c(NA, NA, \"darkgray\", NA, NA),\n       border = c(NA, NA, \"black\", NA, NA),\n       legend = c(\"uniform prior\", \"posterior (uniform)\",\n                  \"2.5% tails (uniform)\", \"Jeffreys prior\",\n                  \"posterior (Jeffreys)\"))\nlines(p, dbeta(p, 0.5, 0.5), lty = \"dashed\", col = \"darkgray\")\nlines(p, dbeta(p, 15.5, 7.5), col = \"darkgray\")\n\n\n\n\n\n\n\n\n\nFigure 4.2: Prior and posterior distributions for \\(p\\) after observing 15 successes out of 22 trials. The uniform prior and the resulting posterior distribution are shown in black with the 2.5% tails shaded. The central 95% credible interval includes all values of \\(p\\) between the two tails. The Jeffreys prior and the corresponding posterior distribution are shown in dark gray.\n\n\n\n\n\nAs \\(n \\rightarrow \\infty\\), we have \\(\\bar{p} - \\hat{p} \\rightarrow 0\\) and \\[\\frac{\\bar{p} (1 - \\bar{p})}{n + \\alpha + \\beta + 1}\n    - \\frac{\\hat{p} (1 - \\hat{p})}{n} \\rightarrow 0.\\] Thus, the posterior distribution approaches approximate normal distribution of the maximum likelihood estimate \\(\\hat{p}\\) in large samples. However, the Bayesian posterior distribution is valid for any sample size, not just large samples.\n\n\n4.2.3 Jeffreys confidence interval\nThe \\(1 - \\alpha\\) Jeffreys confidence interval is the central credible interval with a beta(\\(1 / 2, 1 / 2)\\) prior, which is the Jeffreys prior for a binomial model. When \\(k &gt; 0\\) and \\(k &lt; n\\), its endpoints are the \\(\\alpha / 2\\) and \\(1 - \\alpha / 2\\) quantiles of the beta(\\(k + 1 / 2, n - k + 1 / 2\\)) posterior distribution that we get if we see \\(k\\) successes in \\(n\\) trials. When \\(k = 0\\), the lower endpoint is 0. When \\(k = n\\), the upper endpoint is 1. For a binomial proportion, the Jeffreys confidence interval has width and coverage probability similar to the score (Wilson) and mid-p exact confidence intervals (Brown, Cai, and DasGupta 2001).",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html#comparison-of-binomial-confidence-intervals",
    "href": "bayes.html#comparison-of-binomial-confidence-intervals",
    "title": "4  Bayesian Estimation",
    "section": "4.3 Comparison of binomial confidence intervals",
    "text": "4.3 Comparison of binomial confidence intervals\nTable 4.1 shows eight types of confidence intervals for a probability that we have discussed. These are calculated for the sensitivity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV) of the diabetes test one hour after the meal, where a positive test was defined as a blood glucose concentration above 130 mg/dL.\n\n\n\nTable 4.1: 95% confidence intervals for the sensitivity, specificity, PPV, and NPV of diabetes test in Remein and Wilkerson (1961).\n\n\n\n\n\n\nSensitivity\nSpecificity\nPPV\nNPV\n\n\n\n\n95% CI type\n\\(55 / 70 \\approx 0.786\\)\n\\(462 / 510 \\approx 0.906\\)\n\\(55 / 103 \\approx 0.534\\)\n\\(462 / 477 \\approx 0.969\\)\n\n\nWald\n(0.690, 0.882)\n(0.881, 0.931)\n(0.438, 0.630)\n(0.953, 0.984)\n\n\nLogit\n(0.674, 0.866)\n(0.877, 0.928)\n(0.438, 0.628)\n(0.949, 0.981)\n\n\nAgresti-Coull\n(0.675, 0.867)\n(0.877, 0.928)\n(0.438, 0.627)\n(0.948, 0.981)\n\n\nScore\n(0.676, 0.866)\n(0.877, 0.928)\n(0.438, 0.627)\n(0.949, 0.981)\n\n\nLikelihood ratio\n(0.680, 0.871)\n(0.879, 0.929)\n(0.438, 0.629)\n(0.950, 0.982)\n\n\nExact\n(0.671, 0.875)\n(0.877, 0.930)\n(0.433, 0.633)\n(0.949, 0.982)\n\n\nMid-p\n(0.678, 0.870)\n(0.878, 0.929)\n(0.437, 0.629)\n(0.950, 0.982)\n\n\nJeffreys\n(0.679, 0.869)\n(0.878, 0.929)\n(0.438, 0.628)\n(0.950, 0.982)\n\n\n\n\n\n\nBecause of their good coverage probabilities and narrow widths, the score (Wilson), likelihood ratio, and Jeffreys intervals are recommended by Agresti and Coull (1998) and Brown, Cai, and DasGupta (2001). The Agresti-Coull intervals are slightly wider than these intervals, but they have slightly higher coverage probabilities and are simpler to calculate. Mid-p exact confidence intervals are similar to the Jeffreys intervals, but they are more difficult to calculate. Exact intervals are too wide, and the Wald intervals have coverage probabilities that are often too low. The logit-transformed Wald interval has good coverage probabilities, but it can be even wider than the exact interval. In large samples (with larger samples required for probabilities near zero or one), all of the intervals are similar.\n\nR\n\n\n\n\nbinconf-table.R\n\n## Comparison of binomial confidence limits\nlibrary(DescTools)\n\n# Sensitivity (55 / 70)\n# Use round() to get rounded numbers shown in the table.\nround(BinomCI(55, 70, method = \"wald\"), 3)\nround(BinomCI(55, 70, method = \"logit\"), 3)\nround(BinomCI(55, 70, method = \"agresti-coull\"), 3)\nround(BinomCI(55, 70, method = \"wilson\"), 3)\nround(BinomCI(55, 70, method = \"lik\"), 3)\nround(BinomCI(55, 70, method = \"clopper-pearson\"), 3)\nround(BinomCI(55, 70, method = \"midp\"), 3)\nround(BinomCI(55, 70, method = \"jeffreys\"), 3)\n\n# Specificity (462 / 510)\nround(BinomCI(462, 510, method = \"wald\"), 3)\nround(BinomCI(462, 510, method = \"logit\"), 3)\nround(BinomCI(462, 510, method = \"agresti-coull\"), 3)\nround(BinomCI(462, 510, method = \"wilson\"), 3)\nround(BinomCI(462, 510, method = \"lik\"), 3)\nround(BinomCI(462, 510, method = \"clopper-pearson\"), 3)\nround(BinomCI(462, 510, method = \"midp\"), 3)\nround(BinomCI(462, 510, method = \"jeffreys\"), 3)\n\n# PPV (55 / 103)\nround(BinomCI(55, 103, method = \"wald\"), 3)\nround(BinomCI(55, 103, method = \"logit\"), 3)\nround(BinomCI(55, 103, method = \"agresti-coull\"), 3)\nround(BinomCI(55, 103, method = \"wilson\"), 3)\nround(BinomCI(55, 103, method = \"lik\"), 3)\nround(BinomCI(55, 103, method = \"clopper-pearson\"), 3)\nround(BinomCI(55, 103, method = \"midp\"), 3)\nround(BinomCI(55, 103, method = \"jeffreys\"), 3)\n\n# NPV (462 / 477)\nround(BinomCI(462, 477, method = \"wald\"), 3)\nround(BinomCI(462, 477, method = \"logit\"), 3)\nround(BinomCI(462, 477, method = \"agresti-coull\"), 3)\nround(BinomCI(462, 477, method = \"wilson\"), 3)\nround(BinomCI(462, 477, method = \"lik\"), 3)\nround(BinomCI(462, 477, method = \"clopper-pearson\"), 3)\nround(BinomCI(462, 477, method = \"midp\"), 3)\nround(BinomCI(462, 477, method = \"jeffreys\"), 3)\n\n\n\n\n\n\n\n\nAgresti, Alan, and Brent A Coull. 1998. “Approximate Is Better Than ‘Exact’ for Interval Estimation of Binomial Proportions.” The American Statistician 52 (2): 119–26.\n\n\nBaduashvili, Amiran, Arthur T Evans, and Todd Cutler. 2020. “How to Understand and Teach p-Values: A Diagnostic Test Framework.” Journal of Clinical Epidemiology 122: 49–55.\n\n\nBerger, James O, and Thomas Sellke. 1987. “Testing a Point Null Hypothesis: The Irreconcilability of p Values and Evidence.” Journal of the American Statistical Association 82 (397): 112–22.\n\n\nBerkson, Joseph. 1942. “Tests of Significance Considered as Evidence.” Journal of the American Statistical Association 37 (219): 325–35.\n\n\nBoos, Dennis D, and Leonard A Stefanski. 2013. Essential Statistical Inference. Springer.\n\n\nBrown, Lawrence D, T Tony Cai, and Anirban DasGupta. 2001. “Interval Estimation for a Binomial Proportion.” Statistical Science 16 (2): 101–17.\n\n\nDiamond, George A, and James S Forrester. 1983. “Clinical Trials and Statistical Verdicts: Probable Grounds for Appeal.” Annals of Internal Medicine 98 (3): 385–94.\n\n\nEdwards, Ward, Harold Lindman, and Leonard J Savage. 1963. “Bayesian Statistical Inference for Psychological Research.” Psychological Review 70 (3): 193–242.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2014. Bayesian Data Analysis. Third. CRC press.\n\n\nGreenland, Sander. 2006. “Bayesian Perspectives for Epidemiological Research: I. Foundations and Basic Methods.” International Journal of Epidemiology 35 (3): 765–75.\n\n\nGreenland, Sander, and Charles Poole. 2013. “Living with p Values: Resurrecting a Bayesian Perspective on Frequentisi Statistics.” Epidemiology 24 (1): 62–68.\n\n\nGreenland, Sander, Stephen J Senn, Kenneth J Rothman, John B Carlin, Charles Poole, Steven N Goodman, and Douglas G Altman. 2016. “Statistical Tests, p Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31 (4): 337–50.\n\n\nJeffreys, Harold. 1946. “An Invariant Form for the Prior Probability in Estimation Problems.” Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences 186 (1007): 453–61.\n\n\nLe Cam, Lucien. 1953. “On Some Asymptotic Properties of Maximum Likelihood Estimates and Related Bayes’ Estimates.” University of California Publications in Statistics 1 (11): 277–330.\n\n\nPratt, John W. 1965. “Bayesian Interpretation of Standard Inference Statements.” Journal of the Royal Statistical Society: Series B (Methodological) 27 (2): 169–203.\n\n\nRemein, Quentin R, and Hugh LC Wilkerson. 1961. “The Efficiency of Screening Tests for Diabetes.” Journal of Chronic Diseases 13 (1): 6–21.\n\n\nTukey, John W. 1960. “Conclusions Vs Decisions.” Technometrics 2 (4): 423–33.\n\n\nWilks, Samuel S. 1938. “The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses.” The Annals of Mathematical Statistics 9 (1): 60–62.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "bayes.html#footnotes",
    "href": "bayes.html#footnotes",
    "title": "4  Bayesian Estimation",
    "section": "",
    "text": "Joseph Berkson (1899–1982) was an American physician and statistician at the Mayo Clinic in Rochester, Minnesota. He helped develop and popularize the use of logistic regression for binary outcomes, coining the term “logit” for the log odds in 1944. He also pioneered the study of selection bias, a special case of which is called “Berkson’s bias”. In the late 1950s and the 1960s, he argued that scientific evidence did not establish that smoking causes lung cancer.↩︎\n This convergence follows from the Laplace approximation to the posterior distribution (Gelman et al. 2014). It occurs when the likelihood \\(L(\\theta)\\) has a continuous second derivative with respect to \\(\\theta\\) and \\(\\theta_\\true\\) is not on the boundary of the support of the prior distribution. These are similar to the regularity conditions for maximum likelihood estimation.↩︎\n The gamma function is \\(\\Gamma(v) = \\int_0^\\infty y^{v - 1} e^{-y} \\dif y\\). It is used to define the gamma distribution, of which the chi-squared distributions (including \\(\\chi^2_1\\)) are special cases. If \\(v\\) is a positive integer, then \\(\\Gamma(v) = (v - 1)!\\).↩︎\n Harold Jeffreys (1891–1989) was an English mathematician, statistician, geophysicist, and astronomer. He helped revive the Bayesian notion of probability as an expression of our knowledge about an unknown quantity, wrote a classic textbook on mathematical physics with his wife Bertha (also a mathematician and physicist), and was a prominent opponent of the theory of plate tectonics.↩︎",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Estimation</span>"
    ]
  },
  {
    "objectID": "longitudinal.html",
    "href": "longitudinal.html",
    "title": "5  Longitudinal Data, Rates, and Counts",
    "section": "",
    "text": "5.1 Incomplete follow-up\nThe meaning of a risk or cumulative incidence depends on the time interval over which it is observed. William Farr observed that a risk of death slightly less than 50% over one week for cholera patients was widely considered more alarming than a risk of death greater than 90% over two years for tuberculosis patients. In public health, it matters a great deal how quickly things happen. How rapidly an event occurs is measured using a rate, which has units of events per unit time (Elandt-Johnson 1975; Morgenstern, Kleinbaum, and Kupper 1980). To estimate a risk or a rate, we use longitudinal data where individuals are followed over time. The analysis of longitudinal data is complicated by the fact that individuals can come and go during the study period.\nTo estimate the risk of disease onset in a time interval \\((t_a, t_b]\\), we would follow individuals from time \\(t_a\\) to time \\(t_b\\) to ascertain disease onset. From Section 1.5.2, the risk of disease onset in the time interval \\((t_a, t_b]\\) is \\[\n  \\Pr\\bigl(\\{\\omega \\in \\Omega: D(\\omega) = 1\\}\\bigr)\n\\] where \\[\n  D(\\omega) =\n  \\begin{cases}\n    1 & \\text{if } \\omega \\text{ has } \\tonset \\in (t_a, t_b], \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] and \\(\\tonset\\) denotes the onset time of the disease. In practice, the population \\(\\Omega\\) is often defined to consist only of individuals who are at risk of disease at time \\(t_a\\). If we have complete follow-up of the entire population \\(\\Omega\\) over the entire interval \\((t_a, t_b]\\), then we know the cumulative incidence exactly.\nMore often, we follow a sample of individuals from the population. If selection into the sample is independent of disease onset during \\((t_a, t_b]\\) and we have complete follow-up over the entire interval, then we can get point and interval estimates of the true cumulative incidence using methods for a binomial proportion from Chapter 3 or ?sec-bayes. In practice, almost all longitudinal studies have individuals entering or leaving the study during the follow-up period. When this occurs, methods for binomial proportions can produce inefficient or biased estimates of risk.\nThe analysis of incomplete longitudinal data is called survival analysis, and it is the theoretical foundation for many epidemiologic methods. To analyze survival data, it is important to have clear definitions of the following that can be applied equally to all study participants:\nIn complex data, an individual can have multiple entry, exit, and failure times and even multiple time origins (e.g., time to heart attack after vigorous exercise or ingestion of cocaine). The time scale is usually defined so that \\(t = 0\\) at the origin time, and we will assume that the origin and population are defined so that all individuals in the population are at risk of the event at \\(t = 0\\).\nLet \\(\\tevent\\) be the failure time of individual \\(i\\) and \\(\\tcens\\) be the last time at which they would be under observation if they had no disease onset. We assume all times are defined so that \\(t = 0\\) at the time origin.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Longitudinal Data, Rates, and Counts</span>"
    ]
  },
  {
    "objectID": "longitudinal.html#sec-incomplete",
    "href": "longitudinal.html#sec-incomplete",
    "title": "5  Longitudinal Data, Rates, and Counts",
    "section": "",
    "text": "5.1.1 Right censoring\nWhen \\(\\tcens_i &lt; \\tevent_i\\), the outcome in person \\(i\\) is right censored at time \\(\\tcens_i\\). We know that individual \\(i\\) does not have an event at or before \\(\\tcens_i\\), but we do not know when or if the event will occur after that. In right-censored data, we see the exit time \\[\n    t_i = \\min\\bigl(\\tevent_i, \\tcens_i\\bigr)\n\\] and the event indicator \\[\n    \\delta_i\n    = \\begin{cases}\n        0 & \\text{if } \\texit_i = \\tcens_i,\\\\\n        1 & \\text{if } \\texit_i = \\tevent_i.\n    \\end{cases}\n\\] Right censoring is often just called censoring. There are many reasons that person \\(i\\) might be censored: Perhaps we can no longer find person \\(i\\) (loss to follow-up), person \\(i\\) is no longer at risk of failure (e.g., a woman who has a hysterectomy is no longer at risk of uterine cancer), or observation ends. Different individuals in the same study can be censored for different reasons.\nIndependent right censoring occurs if those who remain under observation at any given time are a random sample of all of those who would be under observation if there were no right censoring. Censoring is not independent if those who remain under observation have systematically different failure times than those who are censored. There are three canonical types of independent censoring:\n\nType I censoring occurs when observation of each individual ends at a predetermined time under the control of the investigators.\nType II censoring occurs when observation ends after a predetermined number of events have been observed.\nRandom censoring occurs when each person \\(i\\) has a random censoring time \\(\\tcens_i\\) that is independent of his or her failure time \\(\\tevent_i\\) and not under the control of the investigators.\n\nAs long as all censoring is independent, different censoring mechanisms can occur within the same study. All of the methods we will discuss assume independent censoring, and they become biased under dependent or informative censoring.\n\n\n5.1.2 Delayed entry (left truncation)\nWhereas right censoring concerns the observation of failure times, truncation concerns the selection of study participants. Delayed entry or left truncation occurs when an individual \\(i\\) has an entry time \\(\\tentry_i &gt; 0\\) where \\(t = 0\\) denotes the origin. If person \\(i\\) had disease onset before \\(\\tentry_i\\), he or she would have been excluded from the study. Person-time prior to entry cannot be included as time at risk; it is called immortal person-time. Delayed entry can be handled easily by all of the methods we will discuss. As with independent right censoring, we require that the set of individuals under observation are a random sample of the individuals who would be under observation if we had no right censoring or left truncation. You can think of this as independent left truncation\n\n\n5.1.3 Left censoring and right truncation\nGiven that we have right censoring and left truncation (delayed entry), it is natural to wonder whether there is also left censoring and right truncation. Unfortunately, the answer is yes:\n\nLeft censoring means that we know an individual had an event before a left-censoring time but we do not know when.\nRight truncation means that our sample contains only individuals who have already had the event.\n\nBoth of these must be handled using strong assumptions or specialized methods. Some areas of epidemiology must contend with left censoring (e.g., we know that a birth defect occurred prior to birth but not exactly when) or right truncation. Usually, they can be prevented or minimized by good study design.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Longitudinal Data, Rates, and Counts</span>"
    ]
  },
  {
    "objectID": "longitudinal.html#failure-time-distributions",
    "href": "longitudinal.html#failure-time-distributions",
    "title": "5  Longitudinal Data, Rates, and Counts",
    "section": "5.2 Failure time distributions",
    "text": "5.2 Failure time distributions\nFailure time distributions are descibed in terms of survival functions, cumulative hazard functions, and hazard functions. Any one of these is sufficient to specify the distribution of the time to an event, but each has a different and useful interpretation. We denote the failure time as a positive random variable \\(T\\). For simplicity, we will assume that \\(T\\) is continuous.\n\n5.2.1 Survival function\nThe survival function for a failure time \\(T\\) is \\[\n  S(t) = \\Pr(T &gt; t),\n\\] which is the probability that the event does not occur up to and including time \\(t\\). Several properties follow from this definition:\n\nBecause it is a probability, \\(S(t) \\in [0, 1]\\) for all \\(t\\).\nBecause \\(T &gt; 0\\), \\(S(0) = 1\\).\n\\(S(t)\\) is monotonically decreasing, which means that it cannot increase. If \\(u &gt; t\\), you can survive to time \\(u\\) only if you survive to time \\(t\\), so \\(S(t) \\geq S(u)\\).\n\nThe survival function \\(S(t)\\) tells us everything there is to know about the distribution of the time-to-event \\(T\\). The cumulative incidence function is \\[\n  F(t) = 1 - S(t) = \\Pr(T \\leq t),\n\\] which is also the cumulative distribution function (CDF) for \\(T\\). If \\(T\\) is a continuous random variable, then \\[\n  -S'(t) = F'(t) = f(t),\n\\tag{5.1}\\] where \\(f(t)\\) is the probability density function (PDF) of \\(T\\). If \\(T\\) is a discrete or continuous failure time with survival function \\(S(t)\\), it turns out that \\[\n  \\E[T] = \\int_0^\\infty S(t) \\,\\dif t.\n\\tag{5.2}\\] This is often easier to calculate than the integral \\(\\E[T] = \\int_0^t t f(t) \\dif t\\) that is normally used to define the mean of a positive random variable.3 The \\(p\\)th quantile of the failure time distribution is the solution to the equation \\[\n  p = F(t_p) = 1 - S(t_p),\n\\tag{5.3}\\] so higher quantiles correspond to longer times to events.4 When \\(p = 0.5\\), we get the median time-to-event. Much of survival analysis is dedicated to calculating and comparing survival functions.\n\n\n5.2.2 Hazard function\nFor any \\(\\Delta &gt; 0\\), the probability that you have an event in the time interval \\((t, t + \\Delta]\\) is \\[\n  \\Pr\\bigl(\\text{event in } (t, t + \\Delta]\\bigr)\n  = S(t) - S(t + \\Delta)\n\\] and the conditional probability that you have an event in the interval given that you survived until time \\(t\\) is \\[\n  \\Pr\\bigl(\\text{event in } (t, t + \\Delta] \\,\\big|\\, \\text{survival until } t\\bigr)\n   = \\frac{S(t) - S(t + \\Delta)}{S(t)}.\n\\tag{5.4}\\] The numerator is the expected number of events in \\((t, t + \\Delta]\\) given that you remain at risk at time \\(t\\). Dividing by \\(\\Delta\\) gives us \\[\n  \\frac{S(t) - S(t + \\Delta)}{S(t) \\Delta},\n\\] which is the expected number of events per unit time in the interval \\((t, t + \\Delta]\\). The hazard function is the limit of this expected number of events per unit time as \\(\\Delta \\downarrow 0\\) (i.e., as \\(\\Delta\\) decreases to zero): \\[\n  h(t) = \\lim_{\\Delta \\downarrow 0} \\frac{S(t) - S(t + \\Delta)}{S(t) \\Delta}.\n\\tag{5.5}\\] Because the numerator is nonnegative and the denominator is positive, \\(h(t) \\geq 0\\). The hazard function \\(h(t)\\) measures the instantaneous expected number of events per unit time like a speedometer measures the instantaneous speed of a vehicle. When \\(h(t)\\) is high, you are likely to have an event soon after time \\(t\\) if you have survived event-free until \\(t\\). When \\(h(t)\\) is low, you are relatively unlikely to have an event soon after \\(t\\).\nUnlike the survival function, the hazard function has units. Because we divide an expected number of events by a time interval \\(\\Delta\\), the hazard has units of \\(\\text{events} / \\text{time}\\). Just like the same speed can be expressed in miles per hour or kilometers per hour, using different measures of time (e.g., month, week, day, hour, minute, or second) changes the numerical value of the hazard but not its meaning.\nWhen \\(\\Delta\\) is small, then Equation 5.4 and Equation 5.5 give us \\[\n  h(t) \\approx\n  \\frac{\\Pr\\bigl(\\text{event in } (t, t + \\Delta] \\,\\big|\\, \\text{survival until } t\\bigr)}{\\Delta}.\n\\] Rearranging, we get \\[\n  h(t) \\Delta \\approx \\Pr\\bigl(\\text{event in } (t, t + \\Delta] \\,\\big|\\, \\text{survival until } t\\bigr).\n\\] Notice how the units work: \\[\n  h(t) \\frac{\\text{events}}{\\text{time unit}} \\Delta \\text{ time units} = h(t) \\Delta \\text{ events}.\n\\tag{5.6}\\] Multiplying the hazard by a time interval gives the expected number of events that would occur in that interval if the hazard remained constant.\nWhen times to failure are continuous, the survival function is differentiable. By definition of the derivative of \\(S(t)\\), \\[\n  \\lim_{\\Delta \\downarrow 0} \\frac{S(t) - S(t + \\Delta)}{\\Delta}\n  = -S'(t).\n\\] Putting this back into Equation 5.5, we get \\[\n  h(t)\n  = \\frac{1}{S(t)} \\lim_{\\Delta \\downarrow 0} \\frac{S(t) - S(t + \\Delta)}{\\Delta}\n  = \\frac{-S'(t)}{S(t)}.\n\\tag{5.7}\\] Because \\(-S'(t) = f(t)\\) from Equation 5.1, we can multiply both sides by \\(S(t)\\) to get \\[\n  f(t) = h(t) S(t).\n\\tag{5.8}\\] Thus, the PDF is the product of the hazard and survival functions. This is used to write likelihoods for right-censored and left-truncated data.\n\n\n5.2.3 Cumulative hazard function\nThe cumulative hazard function for a positive random variable \\(T\\) is \\[\n  H(t) = -\\ln S(t).\n\\tag{5.9}\\] Several properties follow from this definition:\n\n\\(H(0) = 0\\) because \\(S(0) = 1\\).\n\\(H(t)\\) is monotonically increasing, which means that it cannot decrease. If \\(u &gt; t\\), then \\(H(u) \\geq H(t)\\).\nWhen \\(S(t) &gt; 0\\), \\(H(t) \\in [0, \\infty)\\).\n\\(S(t) = e^{-H(t)}\\).\n\nTaking the derivative with respect to \\(t\\) on both sides of Equation 5.9 (using the chain rule on the right-hand side), we get \\[\n  H'(t) = \\frac{-S'(t)}{S(t)} = h(t)\n\\] where the final equality follows from Equation 5.7. By the fundamental theorem of calculus and the fact that \\(H(0) = 0\\), \\[\n  H(t) = \\int_0^t h(u) \\dif u\n\\tag{5.10}\\] so the cumulative hazard \\(H(t)\\) is the area under the graph of \\(h\\) over \\((0, t)\\).\nEquation 5.10 gives us an interesting way to interpret the cumulative hazard function. If the event is something that can be repeated (e.g., clicks on a Geiger counter), the expected number of events in \\((0, t]\\) is the sum of the expected numbers of events in a series of intervals \\((u_0, u_1], (u_1, u_2], \\ldots, (u_{n - 1}, u_n]\\) where \\(u_0 = 0\\) and \\(u_n = t\\). Taking a limit as the number of subintervals grows larger and each subinterval becomes smaller, \\[\n  \\E\\bigl[\\text{number of events in} (0, t]\\bigr] = \\int_0^t h(u) \\,\\dif u = H(t).\n\\] The units in the integral work in the same way as in Equation 5.6. For an event that cannot be repeated (e.g., death), \\(H(t)\\) can be interpreted as the expected number of events that would occur if the event were made repeatable. After an event at time \\(t\\), you would be brought back to being at risk at time \\(t\\) to wait for the next event to occur.\n\n\n5.2.4 Likelihoods for right-censored and left-truncated data\nSuppose our survival time distribution has hazard function \\(h(t, \\theta_\\true)\\) and survival function \\(S(t, \\theta_\\true)\\), where \\(\\theta_\\true\\) is an unknown parameter (or parameter vector). If individual \\(i\\) has entry time \\(\\tentry_i\\), exit time \\(t_i\\), and event indicator \\(\\delta_i\\), then their likelihood contribution is \\[\n  L_i(\\theta) = \\frac{h(t_i, \\theta)^{\\delta_i} S(t_i, \\theta)}{S\\bigl(\\tentry_i, \\theta\\bigr)}.\n\\tag{5.11}\\] In the numerator, every observation contributes a survival term, but only the observed failure times (\\(\\delta_i = 1\\)) contribute a hazard term. The survival term in the denominator accounts for the fact that they survived until time \\(\\tentry_i\\). When \\(\\tentry_i = 0\\), then the denominator equals one. Table 5.2 shows the likelihood contributions for all four possible combinations of right censoring and delayed entry (left truncation).\n\n\n\nTable 5.2: Possible likelihood contributions for \\((\\tentry_i, t_i, \\delta_i)\\).\n\n\n\n\n\n\nRight censoring (\\(\\delta_i = 0\\))\nNo right censoring (\\(\\delta_i = 1\\))\n\n\n\n\nDelayed entry (\\(\\tentry &gt; 0\\))\n\\(\\frac{S(t_i, \\theta)}{S(\\tentry_i, \\theta)}\\)\n\\(\\frac{h(t_i, \\theta) S(t_i, \\theta)}{S(\\tentry_i, \\theta)}\\)\n\n\nNo delayed entry (\\(\\tentry = 0\\))\n\\(S(t_i, \\theta)\\)\n\\(h(t_i, \\theta) S(t_i, \\theta)\\)\n\n\n\n\n\n\nTaking logarithms on both sides of Equation 5.11 gives us the log likelihood contribution \\[\n  \\ell_i(\\theta) =\n  \\delta_i \\ln h(t_i, \\theta) - [H(t_i, \\theta) - H(\\tentry_i, \\theta)].\n\\] All observations contribute a cumulative hazard term, but only observed event times contribute a hazard term. When \\(\\tentry_i = 0\\), then \\(H(0, \\theta) = 0\\) for all \\(\\theta\\). When we have \\(n\\) independent observations, the log likelihood is the sum of the individual log likelihood contributions: \\[\n  \\ell(\\theta) = \\sum_{i = 1}^n \\ell_i(\\theta).\n\\] The maximum likelihood estimate \\(\\hat{\\theta}\\) can be found using the score function \\(U(\\theta) = \\ell'(\\theta)\\), and its approximate variance is \\(I(\\hat{\\theta})^{-1} = [-\\ell''(\\hat{\\theta})]^{-1}\\). The log likelihood can be used to do Wald, score, and likelihood ratio hypothesis tests and obtain the corresponding confidence intervals. For example, the Wald 95% confidence interval in large samples is \\[\n  \\hat{\\theta} \\pm 1.96 \\sqrt{I(\\hat{\\theta})^{-1}}.\n\\] If necessary, the delta method can be used to get confidence intervals that keep \\(\\hat{\\theta}\\) within an appropriate range of values. Bayesian estimation can be done using the corresponding likelihood \\(L(\\theta) = \\exp\\bigl(\\ell(\\theta)\\bigr)\\)",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Longitudinal Data, Rates, and Counts</span>"
    ]
  },
  {
    "objectID": "longitudinal.html#exponential-distribution",
    "href": "longitudinal.html#exponential-distribution",
    "title": "5  Longitudinal Data, Rates, and Counts",
    "section": "5.3 Exponential distribution",
    "text": "5.3 Exponential distribution\nThe exponential distribution is the simplest and most important failure time distribution. It has a constant hazard function \\[\n  h(t) = \\lambda\n\\] where \\(\\lambda\\) is called the rate parameter and has units of \\(\\text{events} / \\text{time}\\). Its cumulative hazard function is \\[\n  H(t)\n  = \\int_0^t h(u) \\,\\dif u\n  = \\int_0^t \\lambda \\dif u\n  = \\lambda t,\n\\] its the survival function is \\[\n    S(t) = e^{-H(t)} = e^{-\\lambda t}.\n\\] A higher rate parameter \\(\\lambda\\) implies shorter survival times. The scale parameter is \\(\\sigma = \\lambda^{-1}\\). A smaller scale parameter corresponds to shorter survival times.\n\n5.3.1 Mean and variance\nThe mean of an exponential random variable \\(T\\) is found most easily by integrating the survival function: \\[\n  \\E(T)\n  = \\int_0^\\infty e^{-\\lambda t} \\,\\dif t\n  = \\frac{1}{\\lambda} \\int_0^\\infty \\lambda e^{-\\lambda t} \\,\\dif t\n  = \\frac{1}{\\lambda},\n\\tag{5.12}\\] In that integral, we ``creatively multiplied by one’’ to turn the integrand into a exponential PDF and then used the fact that the total area under the PDF is one. Integration by parts can be used to show that \\[\n  \\E(T^2)\n  = \\int_0^\\infty t^2 \\lambda e^{-\\lambda t} \\,\\dif t\n  = \\frac{2}{\\lambda^2},\n\\] so \\[\n    \\Var(T)\n    = \\E(T^2) - \\E(T)^2\n    = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2}\n    = \\frac{1}{\\lambda^2}.\n\\] by Equation 1.10. Thus, the standard deviation \\(\\sqrt{\\Var(T)} = 1 / \\lambda\\) is equal to the mean.\n\n\n5.3.2 Incidence rates\nNow imagine that we want to estimate an unknown rate parameter \\(\\lambda_\\true\\) for an exponential distribution. Let \\((\\tentry_1, t_1, \\delta_1), \\ldots (\\tentry_n, t_n, \\delta_n)\\) denote a set of entry times, exit times, and event indicators in a sample of size \\(n\\). The likelihood is \\[\n  L(\\lambda) = \\prod_{i = 1}^n \\lambda^{\\delta_i} e^{-\\lambda (t_i - \\tentry_i)},\n\\] and the log likelihood is \\[\n  \\ell(\\lambda) =\n  \\sum_{i = 1}^n \\big(\\delta_i \\ln \\lambda - \\lambda (t_i - \\tentry_i)\\big)\n  = m \\ln \\lambda - \\lambda T,\n\\tag{5.13}\\] where \\[\n  m = \\sum_{i = 1}^n \\delta_i\n\\] is the total number of observed events and \\[\n  T = \\sum_{i = 1}^n \\big(t_i - \\tentry_i\\big)\n\\] is the total person-time. This is the total time under observation added up over all participants in the study. The likelihood assumes that the same rate \\(\\lambda_\\true\\) of events per unit time occurs in all of this person-time.\nMaximum likelihood estimation of the rate \\(\\lambda_\\true\\) proceeds in the same way as it did for a probability. The only difference is that we are working with an exponential likelihood for times to events rather than a binomial likelihood for a binary outcome. Differentiating with \\(\\ell(\\lambda)\\) with respect to \\(\\lambda\\), we get the score function \\[\n  U(\\lambda) = \\frac{m}{\\lambda} - T.\n\\tag{5.14}\\] Solving the score equation \\(U(\\hat{\\lambda}) = 0\\) gives us the point estimate \\[\n  \\hat{\\lambda} = \\frac{m}{T}.\n\\tag{5.15}\\] This is called the incidence rate in epidemiology. The incidence rate is the maximum likelihood estimate of an exponential rate parameter.\nDifferentiating \\(U(\\lambda)\\) with respect to \\(\\lambda\\), we get the observed information function \\(I(\\lambda) = m / \\lambda^2\\). The estimated variance is \\[\n  I\\big(\\hat{\\lambda}\\big)^{-1}\n  = \\Bigg(\\frac{m}{\\big(\\frac{m}{T}\\big)^2}\\Bigg)^{-1}\n  = \\frac{m}{T^2},\n\\tag{5.16}\\] so the Wald 95% confidence interval for \\(\\lambda_\\true\\) is \\[\n    \\hat{\\lambda} \\pm 1.96 \\sqrt{\\frac{m}{T^2}}.\n\\tag{5.17}\\] It has relatively poor performance in terms of width and coverage probability. The performance of the Wald confidence interval can be improved using a log transformation, which ensures that the lower bound is greater than zero. Using the delta method, the variance of \\(\\ln \\hat{\\lambda}\\) is approximately \\[\n  \\bigg(\\frac{1}{\\hat{\\lambda}}\\bigg)^2 \\frac{m}{T^2}\n  = \\frac{1}{m},\n\\] which depends only on the number of events observed. An approximate 95% confidence interval for \\(\\ln(\\lambda_0)\\) is \\[\n  \\ln\\Big(\\frac{m}{T}\\Big) \\pm 1.96 \\sqrt{\\frac{1}{m}}.\n\\] Exponentiating, we get the log-transformed Wald 95% confidence interval \\[\n  \\frac{m}{T} e^{\\pm 1.96 \\sqrt{\\frac{1}{m}}}.\n\\] Better interval estimates can be obtained by inverting the score or likelihood ratio test or from Bayesian credible intervals. Among the frequentist large-sample confidence intervals, the likelihood ratio interval has the best performance (Brown, Cai, and DasGupta 2003).\n\nR\n\n\n\n\nexponential.R\n\n## Exponential rate parameter estimation\n\n# generate right-censored exponential distribution\ntevent &lt;- rexp(1000, rate = 2)\ntcens &lt;- rexp(1000)             # default rate = 1\nsdat &lt;- data.frame(texit = pmin(tcens, tevent),\n                   event = ifelse(tcens &lt; tevent, 0, 1))\n\n# calculating incidence rate and log-transformed confidence interval\nm &lt;- sum(sdat$event)\nT &lt;- sum(sdat$texit)\nm / T\nm / T * exp(c(-1, 1) * qnorm(.975) * sqrt(1 / m))\n\n# fitting intercept-only exponential regression model\n# This uses the survreg() function from the survival package.\nlibrary(survival)\nexpfit &lt;- survreg(Surv(texit, event) ~ 1, data = sdat,\n                  dist = \"exponential\")\nsummary(expfit)\ncoef(expfit)\nconfint(expfit)\n\n# log-transformed Wald CI for the exponential rate\n# The intercept is ln(scale), which is -ln(rate).\n# The rate is exp(-intercept).\nexp(-coef(expfit))\nexp(-confint(expfit))\n\n# add delayed entry (left truncation) to sdat\nsdat2 &lt;- sdat\nsdat2$tentry &lt;- rexp(1000, rate = 5)\nsdat2 &lt;- subset(sdat2, tentry &lt; texit)\n\n# incidence rate and log-transformed confidence interval\nm2 &lt;- sum(sdat2$event)\nT2 &lt;- sum(sdat2$texit - sdat2$tentry)\nm2 / T2\nm2 / T2 * exp(c(-1, 1) * qnorm(.975) * sqrt(1 / m2))\n\n# survreg() does not handle delayed entry, so use flexsurv::flexsurvreg()\nlibrary(flexsurv)\nexpfit2 &lt;- flexsurvreg(Surv(tentry, texit, event) ~ 1, data = sdat2,\n                       dist = \"exp\")\n# The summary() function does not work with flexsurvreg objects.\n# Type \"expfit2\" or \"expfit2$res\" to get point and interval estimates.\n# The \"se\" in expfit2$res is the delta method standard error.\nexpfit2\nexpfit2$res       # rate parameter scale\nexpfit2$res.t     # log rate parameter scale\n\n\n\n\n\n\n5.3.3 Memoryless property\nGiven that you have reached age \\(a\\), your life expectancy at age \\(a\\) is how many years you are expected to live past age \\(a\\). According to the Social Security Administration’s 2019 life table, life expectancy at birth in the United States in 2019 was 76.22 years for males and 81.28 years for females. Life expectancy at age 40 was 38.74 years for males and 42.76 years for females, which means that the average age at death for those who survive to age 40 was 78.74 years and 82.76 years, respectively. Life expectancy at age 80 was 8.43 years for males and 9.83 years for females, so the average ages at death were 88.43 and 89.83 years, respectively. For humans, remaining life expectancy decreases with age.\nHumans do not have exponential lifetimes. If your lifetime has an exponential distribution with rate \\(\\lambda\\) and you survive to age \\(t\\), the probability that you survive to age \\(t + u\\) is \\[\n  \\Pr(\\text{lifetime} &gt; t + u \\given{} \\text{lifetime} &gt; t)\n  = \\frac{e^{-\\lambda (t + u)}}{e^{-\\lambda t}}\n  = e^{-u}.\n\\] This does not depend on \\(t\\), so your life expectancy would be constant with age. This is called the memoryless property.\nIn a population with exponential lifetimes, the old and the young would have equal hazards of death at any given time and equal risks of death over any time interval. This seems to be true of decaying radioactive isotopes and other processes from physics and chemistry, but humans are more complex. The hazard of death is typically high right after birth, drops rapidly to a minimum between the ages of roughly 5 and 30, and then slowly increases. This is called the bathtub-shaped hazard or the Gompertz-Makeham law of mortality (Gompertz 1825; Makeham 1860). Figure 5.1 shows The bathtub-shaped hazard of death for the United States population in 2019.\n\n\n\nCode\n\nbathtub.R\n\n# life table for male and female mortality in the United State, 2019\nlifetab &lt;- read.csv(file = \"R/lifetable-2019.csv\")\nhdat &lt;- subset(lifetab, age &lt;= 80)\nhdat$surv_male &lt;- 1 - hdat$mortality_male\nhdat$surv_female &lt;- 1 - hdat$mortality_female\n\n# plot hazard (events per year) for ages 0-80\nplot(hdat$age, -log(hdat$surv_male), type = \"l\",\n     xlab = \"Age (years)\", ylab = \"Mortality hazard (deaths per year)\")\nlines(hdat$age, -log(hdat$surv_female), lty = \"dashed\")\ngrid()\nlegend(\"topleft\", bg = \"white\", lty = c(\"solid\", \"dashed\"),\n       legend = c(\"Male\", \"Female\"))\n\n\n\n\n\n\n\n\n\nFigure 5.1: Mortality hazard (deaths per year) by age in the United States based on the Social Security Administration 2019 life table.\n\n\n\n\n\n\n\n5.3.4 Prevalence, incidence, and duration of disease*\nSuppose the time to disease onset in healthy individuals has an exponential(\\(\\lambda\\)) distribution and the time to recovery in diseased individuals has an exponential(\\(\\gamma\\)) distribution. Then the incidence rate of disease is \\(\\lambda\\) and the mean duration of disease is \\(1 / \\gamma\\). For simplicity, imagine a closed population where individuals move between the healthy and diseased states.\nIf the prevalence of disease at time \\(t\\) is \\(p\\) and the population has total size \\(n\\), then the expected number of disease onsets in any time interval \\((t, t + \\dif t]\\) is \\(n (1 - p) \\lambda \\,\\dif t\\). The expected number of recoveries in the same interval is \\(n p \\gamma \\dif t\\). For the prevalence to remain roughly constant over time (i.e., to randomly fluctuate around an equilibrium), we need the expected number of onsets and recoveries in each time interval to be the same: Thus, we need \\[\n  n (1 - p) \\lambda = n p \\gamma\n\\] The left-hand side is the expected number of disease onsets per time unit, and the right-hand side is the expected number of recoveries per time unit. It is critical to use the same time units (e.g., day, week, month, year) for the incidence rate and the duration of disease. This equation can be rearranged to get \\[\n  \\frac{\\lambda}{\\gamma} = \\frac{p}{1 - p},\n\\] so \\[\n  \\text{incidence rate} \\times \\text{mean duration}\n  = \\text{prevalence odds}\n\\] When the prevalence \\(p\\) is low, the prevalence and prevalence odds are roughly equal and we get \\[\n  \\text{incidence rate} \\times \\text{mean duration}\n  \\approx \\text{prevalence}.\n\\] Under more realistic conditions, the relationship between prevalence, incidence, and the duration of disease is more complex (Freeman and Hutchison 1980; Preston 1987; Keiding 1991; Alho 1992).",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Longitudinal Data, Rates, and Counts</span>"
    ]
  },
  {
    "objectID": "longitudinal.html#sec-Poisson",
    "href": "longitudinal.html#sec-Poisson",
    "title": "5  Longitudinal Data, Rates, and Counts",
    "section": "5.4 Poisson distribution",
    "text": "5.4 Poisson distribution\nThe Poisson distribution5 is one of the most important distributions in probability and statistics, and it has many applications in epidemiology. Section 5.2.3 showed that the cumulative hazard \\(H(t)\\) is the expected number of events that occur in \\((0, t]\\) when the event is repeatable. The number of events has a Poisson distribution with mean \\(H(t)\\).\nIf events occur at a constant rate \\(\\lambda\\), then the times between events have an exponential(\\(\\lambda\\)) distribution. The number \\(X\\) of events that occur in a time interval of length \\(T\\) will have a Poisson distribution with mean \\(\\lambda T\\). The probability mass function (PMF) of a Poisson(\\(\\lambda T\\)) distribution is \\[\n  \\Pr(X = x) = \\frac{(\\lambda T)^k}{k!} e^{-\\lambda T}\n  \\text{ for } x = 0, 1, 2, \\ldots.\n\\tag{5.18}\\] and \\(\\supp(X) = \\{0, 1, 2, \\ldots\\}\\). This is a PMF because, by definition, \\[\n  e^{\\lambda T}\n  = \\sum_{k = 0}^\\infty \\frac{(\\lambda T)^k}{k!}.\n\\] Multiplying \\((\\lambda T)^k / k!\\) by \\(e^{-\\lambda T}\\) ensures that the PMF over all possible values of \\(X\\) equals one. The relationship between the exponential and Poisson distributions can be seen easily for \\(X = 0\\): \\[\n  \\Pr(X = 0)\n  = \\frac{(\\lambda T)^0}{0!} e^{-\\lambda T}\n  = e^{-\\lambda T},\n\\] which is the probability that no event occurs in \\((0, T]\\) when the time-to-event has an exponential(\\(\\lambda\\)) distribution.\n\n5.4.1 Mean and variance\nThe mean of the Poisson(\\(\\lambda T\\)) distribution is \\[\n  \\E[X]\n  = \\sum_{k = 0}^\\infty k \\frac{(\\lambda T)^k}{k!} e^{-\\lambda T}\n  = \\lambda T e^{\\lambda T} e^{-\\lambda T}\n  = \\lambda T.\n\\] A similar calculation yields \\[\n  \\E[X^2]\n  = \\sum_{k = 0}^\\infty k^2 \\frac{(\\lambda T)^k}{k!} e^{-\\lambda T}\n  = \\lambda T + (\\lambda T)^2.\n\\] Using Equation 1.10, we get \\[\n  \\Var(X) = [\\lambda T + (\\lambda T)^2] - (\\lambda T)^2 = \\lambda T.\n\\] Thus, both the mean and the variance are \\(\\lambda T\\). Because it equals the mean, the parameter \\(\\lambda T\\) is often written \\(\\mu\\). Both \\(\\lambda\\) and \\(\\mu\\) can be estimated using maximum likelihood or Bayesian methods.\n\n\n5.4.2 Incidence rates via count data\nSuppose we observe \\(n\\) individuals in whom the time to disease onset is exponential(\\(\\lambda_\\true\\)), observing a total person-time of \\(T\\) during which \\(m\\) disease onsets occur. If we string together all of the observations, the memoryless property of the exponential distribution guarantees that we get an interval with total length \\(T\\) in which \\(m\\) events occurred and the times between events were exponential(\\(\\lambda_\\true\\)). Therefore, the total number of events that we see has a Poisson distribution with mean \\(\\lambda_\\true T\\). Using the Poisson PMF in Equation 5.18, we get the likelihood \\[\n  L(\\lambda) = \\frac{(\\lambda T)^m}{m!} e^{-\\lambda T}.\n\\] The corresponding log likelihood is \\[\n  \\ell(\\lambda) = \\ln L(\\lambda) = m (\\ln\\lambda + \\ln T)\n  - \\ln(m!) - \\lambda T.\n\\] Taking the derivative with respect to \\(\\lambda\\), we get the score function \\[\n  U(\\lambda) = \\frac{m}{\\lambda} - T,\n\\] which is exactly the same as the score function for \\(\\lambda\\) that we got using an exponential likelihood in Equation 5.14. Taking another derivative with respect to \\(\\lambda\\), we also get the same estimated variance \\[\n  I(\\hat{\\lambda})^{-1} = \\frac{m}{T^2}\n\\] that we got in Equation 5.16. Under the Poisson model for the number of events, the Wald and log-transformed Wald confidence intervals for the unknown incidence rate \\(\\lambda_\\true\\) are exactly the same as those from the exponential model for the times to events. The score and likelihood ratio tests can be inverted to get confidence intervals that perform better in terms of coverage probability and width than the Wald interval (Brown, Cai, and DasGupta 2003).\n\nR\n\n\n\n\nPoisson-rate.R\n\n## Poisson regression for incidence rates\n\n# generate right-censored exponential distribution\ntevent &lt;- rexp(1000, rate = 2)\ntcens &lt;- rexp(1000)             # default rate = 1\nsdat &lt;- data.frame(texit = pmin(tcens, tevent),\n                   event = ifelse(tcens &lt; tevent, 0, 1))\n\n# Poisson regression model\n# Use log(time) offset to get incidence rate from Poisson(time * incidence rate)\npoisreg &lt;- glm(event ~ offset(log(texit)), data = sdat, family = poisson())\nexp(coef(poisreg))\n# GLMs use likelihood ratio confidence intervals in R.\nexp(confint(poisreg))\n\n# exponential regression for comparison (log-transformed Wald CI)\nlibrary(survival)\nexpreg &lt;- survreg(Surv(texit, event) ~ 1, data = sdat, dist = \"exponential\")\nexp(-coef(expreg))\nexp(-confint(expreg))\n\n# add delayed entry to sdat\nsdat2 &lt;- sdat\nsdat2$tentry &lt;- rexp(1000, rate = 5)\nsdat2 &lt;- subset(sdat2, tentry &lt; texit)\n\n# Poisson regression with delayed entry\npoisreg2 &lt;- glm(event ~ offset(log(texit - tentry)), data = sdat2,\n                family = poisson())\nexp(coef(poisreg2))\nexp(confint(poisreg2))\n\n# exponential regression with delayed entry for comparison\nlibrary(flexsurv)\nexpreg2 &lt;- flexsurvreg(Surv(tentry, texit, event) ~ 1, data = sdat2,\n                       dist = \"exp\")\nexpreg2$res\n\n\n\n\n\n\n5.4.3 Small-sample estimation of incidence rates\nThe Poisson distribution can be used to calculate confidence limits for the incidence rate \\(\\lambda_\\true\\) that do not rely on the approximate normality in large samples that is guaranteed by the central limit theorem (CLT). Exact confidence limits can be calculated in a manner similar to the Clopper-Pearson confidence limits for a binomial probability in Section 3.5.2. Bayesian estimation, which does not require asymptotic normality, can also be used for small samples.\nIf we observe \\(m\\) events in a total person-time \\(T\\), the median unbiased estimate of \\(\\lambda_\\true\\) is the value of \\(\\lambda\\) that makes \\[\n  \\Pr\\nolimits_\\lambda(X \\leq m) = \\Pr\\nolimits_\\lambda(X \\geq m)\n\\] where we use the subscript \\(\\lambda\\) to indicate that the probabilities are calculated assuming \\(\\lambda_\\true = \\lambda\\). If \\(\\lambda_\\text{med}\\) is the median unbiased estimate, then \\[\n  \\sum_{k = 0}^{m - 1} \\frac{\\big(\\lambda_\\text{med} T\\big)^k}{k!} e^{-\\lambda_\\text{med} T}\n    + \\frac{1}{2} \\frac{\\big(\\lambda_\\text{med} T\\big)^m}{m!} e^{-\\lambda_\\text{med} T}\n  = \\frac{1}{2}.\n\\] It is the value of \\(\\lambda\\) that makes the tail probabilities equal. The median of the distribution of \\(\\lambda_\\text{med}\\) is always \\(\\lambda_\\true\\) (Birnbaum 1964).\nThe lower exact \\(1 - \\alpha\\) confidence limit for \\(\\lambda_\\true\\) gives the upper tail of the Poisson distribution a probability of \\(\\alpha / 2\\) (Garwood 1936). It solves the equation \\[\n  \\sum_{k = m}^\\infty \\frac{\\big(\\lambda_\\text{lower} T\\big)^k}{k!} e^{-\\lambda_\\text{lower} T}\n  = 1 - \\Bigg[\\sum_{k = 0}^{m - 1} \\frac{\\big(\\lambda_\\text{lower} T\\big)^k}{k!} e^{-\\lambda_\\text{lower} T}\\Bigg]\n  = \\frac{\\alpha}{2}.\n\\] The upper exact \\(1 - \\alpha\\) confidence limit for \\(\\lambda_\\true\\) gives the lower tail of the Poisson distribution a probability of \\(\\alpha / 2\\). It solves the equation \\[\n  \\sum_{k = 0}^m \\frac{\\big(\\lambda_\\text{upper} T\\big)^k}{k!} e^{-\\lambda_\\text{upper} T}\n  = \\frac{\\alpha}{2}.\n\\] As with the Clopper-Pearson confidence limits for a binomial probability, the exact Poisson confidence limits guarantee a coverage probability (i.e., probability that the confidence interval contains \\(\\lambda_\\true\\)) of at least \\(1 - \\alpha\\). However, these confidence intervals can be wide and have a coverage probability much higher than \\(1 - \\alpha\\) (Cohen and Yang 1994; Swift 2009).\nMid-p confidence limits can produce confidence intervals that are narrower and have a coverage probability closer to \\(1 - \\alpha\\) (Lancaster 1961). The lower \\(1 - \\alpha\\) mid-p exact confidence limit for the incidence rate \\(\\lambda_\\true\\) solves the equation \\[\n  1 - \\Bigg[\\sum_{k = 0}^{m - 1}\n    \\frac{\\big(\\lambda_\\text{lower} T\\big)^k}{k!} e^{-\\lambda_\\text{lower} T}\n    + \\frac{1}{2} \\frac{\\big(\\lambda_\\text{lower} T\\big)^m}{m!} e^{-\\lambda_\\text{lower} T}\\Bigg]\n  = \\frac{\\alpha}{2}.\n\\] The upper \\(1 - \\alpha\\) mid-p exact confidence limit solves the equation \\[\n  \\sum_{k = 0}^{m - 1} \\frac{\\big(\\lambda_\\text{upper} T\\big)^k}{k!} e^{-\\lambda_\\text{upper} T} \\\\\n    + \\frac{1}{2} \\frac{\\big(\\lambda_\\text{upper} T\\big)^m}{m!} e^{-\\lambda_\\text{upper} T}\n  = \\frac{\\alpha}{2}.\n\\] The coverage probability of the mid-p confidence interval is usually very close to \\(1 - \\alpha\\) (Cohen and Yang 1994; Swift 2009).\n\nR\n\n\n\n\nPoisson-small.R\n\n## Small-sample Poisson point and interval estimation\n\n# median unbiased estimate\nmedrate_pois &lt;- function(m, T) {\n  # m = number of events, T = total person-time\n\n  # Poisson lower tail probability\n  lower_tail &lt;- function(rate) {\n    mu = rate * T\n    ppois(m, mu) - dpois(m, mu) / 2\n  }\n\n  # median unbiased estimate\n  med &lt;- uniroot(function(rate) lower_tail(rate) - 1 / 2, interval = c(0, 1))\n  med$root\n}\nmedrate_pois(7, 22)\n\n# exact confidence limits\n# The point estimate is the incidence rate m / T, not the median unbiased rate.\nlibrary(DescTools)\nPoissonCI(7, 22, method = \"exact\")\nPoissonCI(7, 22, method = \"exact\", conf.level = 0.8)\n\n# mid-p confidence limits\nmidp_pois &lt;- function(m, T, level=0.95) {\n  # m = number of events, T = total person-time\n  # The default confidence level (1 - type I error probability) is 0.95.\n  \n  # Poisson mid-p lower tail probability\n  lower_tail &lt;- function(rate) {\n    mu = rate * T\n    ppois(m, mu) - dpois(m, mu) / 2\n  }\n\n  # lower confidence limit\n  alpha &lt;- 1 - level\n  lower &lt;- uniroot(function(rate) lower_tail(rate) - (1 - alpha / 2),\n                   interval = c(0, 100), extendInt = \"yes\")\n  # upper confidence limit\n  upper &lt;- uniroot(function(rate) lower_tail(rate) - alpha / 2,\n                   interval = c(0, 100), extendInt = \"yes\")\n\n  # names for confidence limits\n  lower_perc &lt;- paste(round(alpha / 2 * 100, 3), \"%\", sep = \"\")\n  upper_perc &lt;- paste(round((1 - alpha / 2) * 100, 3), \"%\", sep = \"\")\n  \n  # return named vector of confidence limits\n  conflimits &lt;- c(lower$root, upper$root)\n  names(conflimits) &lt;- c(lower_perc, upper_perc)\n  conflimits\n}\nmidp_pois(7, 22)\nmidp_pois(7, 22, level = 0.8)\n\n\n\n\n\n\n5.4.4 Poisson approximation to the binomial for rare events*\nMost applications of the Poisson distribution in epidemiology come from its relationship with the exponential distribution, but the Poisson distribution also has a useful relationship with the binomial distribution. When \\(n\\) is large and \\(p\\) is small, the binomial(\\(n, p\\)) distribution can be approximated by a Poisson(\\(n p\\)) distribution. More specifically, imagine that \\(n\\) increases and \\(p\\) decreases such that \\(n p = \\mu\\) stays constant. The binomial(\\(n, p\\)) probability mass function is \\[\n  \\Pr(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k} = \\frac{n!}{(n - k)! k!} p^k (1 - p)^{n - k}.\n\\] When \\(n\\) is much larger than \\(k\\), we have \\[\n  \\frac{n!}{(n - k)! k!} \\approx \\frac{n^k}{k!}.\n\\] Because \\(p = \\mu / n\\), we also have \\[\n  p^k = \\frac{\\mu^k}{n^k}\n\\] for each \\(n\\) and \\[\n  (1 - p)^{n - k} = \\Big(1 - \\frac{\\mu}{n}\\Big)^{n - k}\n  \\longrightarrow e^{-\\mu}\n\\] as \\(n \\rightarrow \\infty\\). Putting all of this together, we get the following approximation to the binomial PMF \\[\n  \\Pr(X = k) \\approx \\frac{n^k \\mu^k}{k! n^k} e^{-\\mu}\n  = \\frac{\\mu^k}{k!} e^{-\\mu},\n\\] which is the Poisson(\\(\\mu\\)) PMF.\nAs an approximation to the binomial distribution, the Poisson distribution can be used for rare events in many different contexts. The number of events such as automobile accidents or number of onsets of a rare disease in a given time period or area (or both) will often have a Poisson distribution. Clarke (1946) describes an astonishing application of the Poisson distribution that occurred in London in World War II. The V1 flying bomb was a German cruise missile with an 850 kg warhead that was fired at London (and later at Belgium) in 1944 and 1945. Soon after the attacks began, many people felt that the bomb impacts were clustered in particular areas of London. British investigators used the Poisson distribution to determine whether the V1s were being aimed or fell randomly within the city.\nThey divided 144 square kilometers of central London into 576 squares of \\(0.25\\) square kilometers each. By then, the total number of bombs that had fallen on the entire area was 537. If the bombs were falling randomly, the number of bombs in each square should have a Poisson distribution with mean \\(537 / 576 \\approx  0.932\\). Grouping the squares by the number of bombs that had fallen on them yielded Table 5.3. The close fit to the Poisson distribution suggested that the bombs were falling randomly over the entire 144 square kilometers, not being aimed at particular targets within the city. Near the end of the war, analysis of captured V1s revealed that the guidance system was only accurate to a radius of about 6 kilometers, a circle large enough to encompass almost all of London at the time.\n\n\n\nTable 5.3: Distribution of V1 bomb impacts in London (Clarke 1946).}\n\n\n\n\n\nNumber of\nExpected (Poisson)\nActual number\n\n\nimpacts\nnumber of squares\nof squares\n\n\n0\n226.74\n229\n\n\n1\n211.39\n211\n\n\n2\n98.54\n93\n\n\n3\n30.62\n35\n\n\n4\n7.14\n7\n\n\n5+\n1.57\n1\n\n\nTotal squares\n576.00\n576",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Longitudinal Data, Rates, and Counts</span>"
    ]
  },
  {
    "objectID": "longitudinal.html#bayesian-estimation-of-incidence-rates",
    "href": "longitudinal.html#bayesian-estimation-of-incidence-rates",
    "title": "5  Longitudinal Data, Rates, and Counts",
    "section": "5.5 Bayesian estimation of incidence rates",
    "text": "5.5 Bayesian estimation of incidence rates\nAs with a binomial probability, Bayesian methods can be used to estimate an incidence rate without making any large-sample assumptions. They also allow a prior distribution to be used to incorporate background knowledge about the possible values of \\(\\lambda_\\true\\).\n\n5.5.1 Gamma conjugate distribution\nThe conjugate distribution for the exponential(\\(\\lambda\\)) and Poisson(\\(\\lambda T\\)) distributions is the gamma distribution, which has the PDF \\[\n  f(x, \\alpha, \\beta)\n  = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-\\beta x}\n\\tag{5.19}\\] where \\(\\alpha &gt; 0\\) is the shape parameter, \\(\\beta &gt; 0\\) is the rate parameter, and \\(\\Gamma(\\alpha) = \\int_0^\\infty x^{\\alpha - 1} \\dif x\\) is the gamma function. If \\(X\\) has a gamma(\\(\\alpha, \\beta\\)) distribution, then \\[\n  \\E[X] = \\frac{\\alpha}{\\beta}\n\\] and variance \\[\n  \\Var(X) = \\frac{\\alpha}{\\beta^2}\n\\] The exponential(\\(\\beta\\)) distribution is a special case of the gamma distribution with shape \\(\\alpha = 1\\).\nIf the prior PDF of an unknown exponential rate \\(\\lambda_\\true\\) has a gamma(\\(\\alpha, \\beta\\)) distribution, then \\[\n  p(\\lambda \\given \\data) \\propto\n  \\big(\\lambda^m e^{-\\lambda T}\\big) \\big(\\lambda^{\\alpha - 1} e^{-\\beta \\lambda}\\big)\n  = \\lambda^{m + \\alpha - 1} e^{-\\lambda (T + \\beta)}.\n\\] After normalizing, this is a gamma(\\(m + \\alpha, T + \\beta\\)) distribution. The posterior mean is \\[\n  \\bar{\\lambda} = \\frac{m + \\alpha}{T + \\beta}\n\\] and the posterior variance is \\[\n  \\frac{m + \\alpha}{(T + \\beta)^2} = \\frac{\\bar{\\lambda}}{T + \\beta}\n\\] The equal-tailed \\(1 - \\alpha\\) credible interval has its limits at the \\(\\alpha / 2\\) and \\(1 - \\alpha / 2\\) quantiles of this distribution. Figure 5.2 shows an example of an uninformative Bayesian prior and a posterior distribution for the incidence rate.\n\n\n\nCode\n\nincrate-Bayes-plot.R\n\n## Bayesian estimation of incidence rates\n\nx &lt;- seq(0, 1, by = 0.002)\nm &lt;- 4\nT &lt;- 25\n\n# plot of prior and posterior distributions\nplot(x, dgamma(x, shape = 0.5 + m, rate = 0.1 + T), type = \"n\",\n     xlab = expression(paste(\"Incidence rate (\", lambda, \")\")),\n     ylab = \"Probability density\")\ngrid()\nlines(x, dgamma(x, shape = 0.5, rate = 0.1), lty = \"dashed\")\nlines(x, dgamma(x, shape = 0.5 + m, rate = 0.1 + T))\nlegend(\"topright\", lty = c(\"dashed\", \"solid\"),\n       legend = c(\"gamma(0.5, 0.1) prior\", \"gamma (4.5, 25.1) posterior\"))\n\n\n\n\n\n\n\n\n\nFigure 5.2: The PDFs of a gamma(0.5, 0.1) prior distribution and the gamma(4.5, 25.1) posterior distribution that results from seeing 4 events in 25 person-years of observation. The units of the incidence rate events per year.\n\n\n\n\n\n\n\n5.5.2 Jeffreys confidence interval\nIf we fix the total person-time \\(T\\) and model the number of events as a Poisson(\\(\\lambda T\\)) random variable, the Jeffreys prior has \\(\\alpha = 1 / 2\\) and \\(\\beta = 0\\) (Jeffreys 1946). This is an improper prior because the PDF has a total area of \\(\\infty\\) under the curve, but we get a proper posterior distribution (i.e., a posterior PDF with a total area of \\(1\\)) as long as \\(T &gt; 0\\). The Jeffreys confidence interval for \\(\\lambda_\\true\\) is the equal-tailed credible interval from the gamma(\\(m + 1 / 2, T\\)) posterior distribution. If \\(m = 0\\), the lower limit can be set to zero. This confidence interval has good coverage probabilities and narrow widths similar to the likelihood ratio confidence interval (Brown, Cai, and DasGupta 2003; Swift 2009).\n\nR\n\n\n\n\nincrate-Bayes.R\n\n## Bayesian estimation of incidence rates with gamma conjugate distribution\n\n# incidence rate posterior mean, median, and equal-tailed credible limits\nincrate_bayes &lt;- function(m, T, level=0.95, priora=0.5, priorb=0) {\n  # default arguments are for the Jeffreys confidence interval\n  alpha &lt;- 1 - level\n  posta  &lt;- priora + m\n  postb &lt;- priorb + T\n  if (m == 0) {\n    lower &lt;- 0\n  } else {\n    lower &lt;- qgamma(alpha / 2, shape = posta, rate = postb)\n  }\n  upper &lt;- qgamma(1 - alpha / 2, shape = posta, rate = postb)\n  postmean &lt;- posta / postb\n  postmedian &lt;- qgamma(0.5, shape = posta, rate = postb)\n  return(c(postmean = postmean, postmedian = postmedian,\n           lower = lower, upper = upper,\n           priora = priora, priorb = priorb, level = level))\n}\n\n# 7 events in 22 units of person-time\nincrate_bayes(7, 22)                          # Jeffreys 95% confidence interval\nincrate_bayes(7, 22, level = 0.8)             # Jeffreys 80% confidence interval\nincrate_bayes(7, 22, priora = 1, priorb = 1)  # uniform prior\n\n\n\n\n\n\n\n\nAlho, Juha M. 1992. “On Prevalence, Incidence, and Duration in General Stable Populations.” Biometrics 48 (2): 587–92.\n\n\nBirnbaum, Allan. 1964. “Median-Unbiased Estimators.” Bulletin of Mathematical Statistics 11: 25–34.\n\n\nBrown, Lawrence D, T Tony Cai, and Anirban DasGupta. 2003. “Interval Estimation in Exponential Families.” Statistica Sinica 13: 19–49.\n\n\nClarke, RD. 1946. “An Application of the Poisson Distribution.” Journal of the Institute of Actuaries 72 (3): 481–81.\n\n\nClayton, David, and Michael Hills. 1993. Statistical Models in Epidemiology. Oxford University Press.\n\n\nCohen, Geoffrey R, and Shu-Ying Yang. 1994. “Mid-p Confidence Intervals for the Poisson Expectation.” Statistics in Medicine 13 (21): 2189–2203.\n\n\nElandt-Johnson, Regina C. 1975. “Definition of Rates: Some Remarks on Their Use and Misuse.” American Journal of Epidemiology 102 (4): 267–71.\n\n\nFarr, William. 1838. “On Prognosis.” British Medical Almanack Supplement: 199–216.\n\n\nFreeman, Jonathan, and George B Hutchison. 1980. “Prevalence, Incidence and Duration.” American Journal of Epidemiology 112 (5): 707–23.\n\n\nGarwood, F. 1936. “Fiducial Limits for the Poisson Distribution.” Biometrika 28 (3/4): 437–42.\n\n\nGompertz, Benjamin. 1825. “XXIV. On the Nature of the Function Expressive of the Law of Human Mortality, and on a New Mode of Determining the Value of Life Contingencies.” Philosophical Transactions of the Royal Society of London 115: 513–83.\n\n\nJeffreys, Harold. 1946. “An Invariant Form for the Prior Probability in Estimation Problems.” Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences 186 (1007): 453–61.\n\n\nKeiding, Niels. 1991. “Age-Specific Incidence and Prevalence: A Statistical Perspective.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 154 (3): 371–96.\n\n\nLancaster, H Oliver. 1961. “Significance Tests in Discrete Distributions.” Journal of the American Statistical Association 56 (294): 223–34.\n\n\nMakeham, William Matthew. 1860. “On the Law of Mortality and the Construction of Annuity Tables.” The Assurance Magazine, and Journal of the Institute of Actuaries 8 (6): 301–10.\n\n\nMorgenstern, Hal, David G Kleinbaum, and Lawrence L Kupper. 1980. “Measures of Disease Incidence Used in Epidemiologic Research.” International Journal of Epidemiology 9 (1): 97–104.\n\n\nPreston, Samuel H. 1987. “Relations Among Standard Epidemiologic Measures in a Population.” American Journal of Epidemiology 126 (2): 336–45.\n\n\nSwift, Michael Bruce. 2009. “Comparison of Confidence Intervals for a Poisson Mean–Further Considerations.” Communications in Statistics–Theory and Methods 38 (5): 748–59.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Longitudinal Data, Rates, and Counts</span>"
    ]
  },
  {
    "objectID": "longitudinal.html#footnotes",
    "href": "longitudinal.html#footnotes",
    "title": "5  Longitudinal Data, Rates, and Counts",
    "section": "",
    "text": "William Farr (1807–1883) was a British pioneer of epidemiology. As the first statistician in the General Register Office, he was responsible for the collection of medical statistics in England and Wales He set up a system for recording causes of death that allowed comparison of mortality rates. In the quote, “consumption” and “phthisis” both refer to tuberculosis.↩︎\n To be predictable, it is sufficient for a process to be left-continuous. The disease onset process is assumed to be right-continuous with left-hand limits (cadlag) and hence unpredictable.↩︎\n Using integration by parts, we get that \\[\n    \\int_0^\\infty t f(t) \\,\\dif t = -t S(t) \\Big|_0^\\infty + \\int_0^\\infty S(t) \\,\\dif t = \\int_0^\\infty S(t) \\,\\dif t\n  \\] when \\(t S(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\).↩︎\n When \\(T\\) is discrete (or a mixture of continuous and discrete components), \\[\n    t_p = \\inf\\{t: F(t) \\geq p\\} = \\inf\\{t: S(t) \\leq 1 - p\\}.\n  \\] When \\(T\\) is continuous, this reduces to Equation 5.3.↩︎\n Named after Sim'{e}on Denis Poisson (1781-1840), a French mathematician, physicist, and astronomer. He introduced the distribution in an 1837 paper in which he estimated the number of wrongful convictions that would occur over a given time period. His is one of the 72 names inscribed on the Eiffel Tower.↩︎",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Longitudinal Data, Rates, and Counts</span>"
    ]
  },
  {
    "objectID": "survival.html",
    "href": "survival.html",
    "title": "6  One-Sample Survival Analysis",
    "section": "",
    "text": "6.1 Empirical cumulative distribution function\nIn nonparametric survival analysis, we do not assume that the failure time distributions are defined by a small number of parameters, such as the rate parameter in an exponential model for times to events or a Poisson model for the number of events. Whenever possible, it is good to incorporate existing knowledge into the estimation of unknown parameters, and the use of parametric models and Bayesian methods accomplishes this. When such knowledge is not available, nonparametric methods allow us to avoid making assumptions we cannot defend.\nHowever, this flexibility comes at a price. For example, suppose we know that our time-to-event has an exponential distribution. If we use a nonparametric model anyway, then:\nParametric and nonparametric methods are at opposite ends of the bias-variance tradeoff. The assumptions of parametric models can induce bias, but they produce estimates with low variance when the assumptions are approximately correct. The flexibility of nonparametric models avoids bias, but they produce estimates with higher variance than an equivalent parametric method based on sound assumptions. Survival analysis has nonparametric estimators of the survival and cumulative hazard functions that can be used with relatively little loss of efficiency. Because of this combination of flexibility and efficiency, they are widely used in epidemiologic research.\nThe cumulative distribution function (CDF) of a random variable \\(X\\) is the function \\[F(x) = \\Pr(X \\leq x).\\] For each value of \\(x\\), \\(F(x)\\) is a probability that we can estimate using methods for a binomial proportion. However, we can get a more complete picture of the distribution of \\(X\\) by linking the estimates for different \\(x\\) together to estimate the whole function \\(F(x)\\).\nIf \\(x_1, \\ldots, x_n\\) are observations of a random variable \\(X\\), the empirical CDF is the function \\[\n  \\hat{F}_n(x) = \\frac{1}{n} \\sum_{i = 1}^n \\indicator_{x_i \\leq x}\n\\tag{6.1}\\] where \\(\\indicator_{x_i \\leq x} = 1\\) if \\(x_i \\leq x\\) is true and zero otherwise. For a fixed value of \\(x\\), \\(\\hat{F}_n(x)\\) is just the proportion of the observations that are \\(\\leq x\\). At each \\(x\\), the number of observations with \\(X_i \\leq x\\) is a binomial(\\(n\\), \\(F(x)\\)) random variable with expected value \\(n F(x)\\) and variance \\(n F(x) \\big(1 - F(x)\\big)\\). Thus, \\[\n  \\E\\bigl(\\hat{F}_n(x)\\bigr) = F(x)\n\\] and \\[\n  \\Var\\bigl(\\hat{F}_n(x)\\bigr)\n  = \\frac{1}{n} F(x) \\big(1 - F(x)\\big).\n\\tag{6.2}\\] As \\(n \\rightarrow \\infty\\), we have \\(\\hat{F}_n(x) \\rightarrow F(x)\\) by the law of large numbers (LLN) and By the central limit theorem (CLT), \\[\n    \\frac{\\sqrt{n}\\big(\\hat{F}_n(x) - F(x)\\big)}{\\sqrt{F(x) \\big(1 - F(x)\\big)}} \\approxsim N(0, 1)\n\\tag{6.3}\\] by the central limit theorem (CLT).1 Unlike a single proportion estimate \\(\\hat{p}\\), the empirical CDF links all of these estimated probabilities together—like beads on a necklace—through the variable \\(x\\).\nAt any given \\(x\\), interval estimates for \\(F(x)\\) can be obtained using any of the methods we have discussed for probabilities, including the Wald, score (Wilson), likelihood ratio, exact (Clopper-Pearson), mid-p, or Jeffreys confidence intervals as well as Bayesian credible intervals. For example, \\[\n    \\hat{F}_n(x) \\pm 1.96 \\sqrt{\\frac{1}{n} \\hat{F}_n(x) \\big(1 - \\hat{F}_n(x)\\big)}\n\\tag{6.4}\\] is the 95% Wald confidence interval. To force this confidence interval to stay inside \\((0, 1)\\), we can use the delta method with a logit or log-log transformation. These are called pointwise confidence intervals because we have a separate confidence interval for \\(F(x)\\) at each \\(x\\).",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-Sample Survival Analysis</span>"
    ]
  },
  {
    "objectID": "survival.html#sec-KM",
    "href": "survival.html#sec-KM",
    "title": "6  One-Sample Survival Analysis",
    "section": "6.2 Kaplan-Meier estimator",
    "text": "6.2 Kaplan-Meier estimator\nIn data with right censoring and left truncation, we cannot calculate the empirical CDF directly. The Kaplan-Meier estimator (Kaplan and Meier 1958) uses conditional probabilities to estimate the survival function \\(S(t) = 1 - F(t)\\) for failure time data.2 The basic idea behind the Kaplan-Meier estimator is to solve the problems of right censoring and left truncation (delayed entry) by breaking analysis time into periods where no one enters or leaves the study. In each such interval \\((t_a, t_b]\\), we can estimate the conditional probability of surviving to time \\(t_b\\) given that you were at risk of disease at time \\(t_a\\).\nIf there are \\(n\\) individuals at risk throughout the interval $(t_a, t_b], then the number of events at time \\(t_b\\) can be treated like a binomial(\\(n, p\\)) random variable where \\(p\\) is the risk of the event in \\((t_a, t_b]\\). Our point estimate of this conditional probability is \\[\n  \\hat{p} = \\frac{d}{n}\n\\] where \\(d\\) is the number of failures at time \\(t_b\\). Its variance is approximately \\[\n  \\Var(\\hat{p}) = \\frac{1}{n} \\hat{p} (1 - \\hat{p}).\n\\] Given that you were at risk of disease at time \\(t_a\\), \\[\n  \\hat{q} = 1 - \\hat{p}\n\\] is the conditional probability of surviving past time \\(t_b\\).\n\n6.2.1 At-risk process and risk sets\nTo estimate the risk in an interval \\((t_a, t_b]\\), it is critical to define who is at risk of failure at time \\(t_a\\). We assume that all times are defined relative to a time origin that can differ between individuals. The at-risk process for individual \\(i\\) is \\[\n  Y_i(t) =\n  \\begin{cases}\n    1 &\\text{ if $i$ is at risk and under observation at time $t$},\\\\\n    0 &\\text{ otherwise}.\n  \\end{cases}\n\\] The at-risk process is assumed to be predictable (i.e., its value at \\(t\\) is determined just before time \\(t\\)), so \\(Y_i(t) = 1\\) even if \\(i\\) fails or is censored at time \\(t\\). Note that a person is only at risk if they are both at risk of failure and under observation. If person \\(i\\) is not under observation until an entry time \\(\\tentry_i &gt; 0\\), then \\(Y_i(t) = 0\\) when \\(t \\leq \\tentry_i\\).\nThe set of individuals under observation and at risk of failure at time \\(t\\) is called the risk set at time \\(t\\) and written \\[\n  \\mathcal{R}(t) = \\{i : Y_i(t) = 1\\}\n\\] The risk set \\(\\mathcal{R}(t)\\) includes everyone under observation who fails at time \\(t\\), who is censored at time \\(t\\), or who survives past time \\(t\\).\n\n\n6.2.2 Survival via multiplication of conditional probabilities\nLet \\(T\\) denote the random failure time in the analysis time scale (i.e., with the origin as time zero), and suppose we have times \\(0 &lt; t_1 &lt; t_2\\). To have \\(T &gt; t_2\\), we must also have \\(T &gt; t_1\\), so \\[\n  \\begin{aligned}\n    \\Pr\\bigl(T &gt; t_2\\bigr)\n    &= \\Pr\\bigl(T &gt; t_2 \\text{ and } T &gt; t_1\\bigr) \\\\\n    &= \\Pr\\bigl(T &gt; t_2 \\,\\big|\\, T &gt; t_1\\bigr) \\Pr\\bigl(T &gt; t_1\\bigr).\n  \\end{aligned}\n\\] In other words, the probability of survival in \\((0, t_2]\\) is the product of the survival probabilities in the intervals \\((0, t_1]\\) and \\((t_1, t_2]\\). If \\(t_3 &gt; t_2\\), then \\[\n  \\begin{aligned}\n    \\Pr\\bigl(T &gt; t_3\\bigr)\n    &= \\Pr\\bigl(T &gt; t_3 \\,\\big|\\, T &gt; t_2\\bigr) \\Pr\\bigl(T &gt; t_2\\bigr) \\\\\n    &= \\Pr\\bigl(T &gt; t_3 \\,\\big|\\, T &gt; t_2\\bigr)\n      \\Pr\\bigl(T &gt; t_2 \\,\\big|\\, T &gt; t_1\\bigr) \\Pr\\bigl(T &gt; t_1\\bigr)\n  \\end{aligned}\n\\] which is the product of the survival probabilities in the intervals \\((0, t_1]\\), \\((t_1, t_2]\\), and \\((t_2, t_3]\\). This logic extends to any number of intervals. If we have distinct times \\(0 = t_0 &lt; t_1 &lt; \\cdots &lt; t_m\\), then \\[\n  \\Pr(T &gt; t_m)\n  = \\prod_{i = 1}^{m} \\Pr\\big(T &gt; t_i \\,\\big|\\, T &gt; t_{i - 1}\\big).\n\\] This uses the multiplication rule for conditional probabilities, so it does not assume that failures in different intervals are independent. In single-failure data, an individual who fails in one interval cannot fail in a later interval, so failures in different intervals cannot be independent.\nLet \\(0 = t_0 &lt; t_1 &lt; t_2 &lt; \\ldots &lt; t_m\\) be the endpoints of intervals \\((t_{i-1}, t_i]\\) within which there are no entries or exits from the study. Let \\(n_j = \\sum_{i = 1}^n Y_i(t_j)\\) be the number of people in the risk set \\(\\mathcal{R}(t_j)\\) and let \\(d_j \\geq 0\\) be the number of failures that occur at time \\(t_j\\). The estimated conditional probability \\(q_j\\) of surviving through the interval \\((t_{j - 1}, t_j]\\) given survival to \\(t_{j - 1}\\) is \\[\n  \\hat{q}_j = 1 - \\frac{d_j}{n_j},\n\\] and the Kaplan-Meier estimator of \\(S(t)\\) is \\[\n  \\hat{S}(t)\n  = \\prod_{j: t_j \\leq t} \\hat{q}_j\n  = \\prod_{j: t_j \\leq t} \\bigg(1 - \\frac{d_j}{n_j}\\bigg).\n\\tag{6.5}\\] This is the product of the conditional survival probabilities in \\((t_{j - 1}, t_j]\\) for all intervals such that \\(t_j \\leq t\\). To survive to time \\(t\\), you need to survive through all intervals up to and including time \\(t\\). This makes it easier to calculate the survival function than to calculate cumulative incidence directly. The Kaplan-Meier estimator is a consistent and asymptotically normal estimator of the true survival function (Fleming and Harrington 2005; Aalen, Borgan, and Gjessing 2008). Figure 6.1 shows an example based on a right-censored sample of size 500 from a log-logistic distribution with shape \\(\\alpha = 1\\) and rate \\(\\lambda = 2\\).\n\nR\n\n\n\n\nloglogistic.R\n\n## Fitting a log-logistic distribution\n# To sample a log-logistic random variable in R, you sample a logistic random\n# variable with location = -log(rate) and scale = 1 / shape.\n# The exponential of the logistic variable has a log-logistic distribution\n# with the correct rate and shape parameters.\nlibrary(survival)\n\n# Log-logistic distribution and regression (rate = 2, shape = 3)\nllsample &lt;- exp(rlogis(1000, location = -log(2), scale = 1 / 3))\nllogdat &lt;- data.frame(time = llsample, event = 1)\nllogreg &lt;- survreg(Surv(time, event) ~ 1, data = llogdat,\n                   dist = \"loglogistic\")\nexp(-coef(llogreg))       # point estimate of rate\nexp(-confint(llogreg))    # 95% confidence interval for rate\n1 / llogreg$scale         # point estimate of shape\n\n# log-transformed 95% confidence interval for the shape parameter\nexp(-log(llogreg$scale) + c(-1, 1)\n    * qnorm(.975) * sqrt(vcov(llogreg)[\"Log(scale)\", \"Log(scale)\"]))\n\n# plot of true and estimated log-logistic hazard functions\nllrate_est &lt;- exp(-coef(llogreg))\nllshape_est &lt;- 1 / llogreg$scale\nllrate_true &lt;- 2\nllshape_true &lt;- 3\nh_llog &lt;- function(time, rate, shape) {\n  # returns last expression if there is no return() statement\n  (rate * shape * (time * rate)^(shape - 1) /\n   (1 + (time* rate)^shape))\n}\nt &lt;- seq(0, 4, by = 0.01)\nplot(t, h_llog(t, llrate_true, llshape_true), type = \"n\",\n     xlab = \"Time\", ylab = \"Hazard (Log-logistic)\")\ngrid()\nlines(t, h_llog(t, llrate_est, llshape_est))\nlines(t, h_llog(t, llrate_true, llshape_true), lty = \"dashed\")\nlegend(\"topright\", lty = c(\"solid\", \"dashed\"), bg = \"white\",\n       legend = c(\"Estimated hazard function\", \"True hazard function\"))\n\n\n\n\n\n\n\nCode\n\nKMcurve.R\n\n## Kaplan-Meier survival curve\nlibrary(survival)\n\n# right-censored sample from log-logistic dist with rate = 2 and shape = 1\n# Uses samples from logistic distribution with location = -log(rate) and\n# scale = 1 / shape.\nset.seed(42)\nllog_f &lt;- exp(rlogis(500, location = -log(2), scale = 1))\nllog_c &lt;- exp(rlogis(500, location = -log(2), scale = 2))\nt &lt;- pmin(llog_f, llog_c)\nd &lt;- ifelse(llog_c &lt; llog_f, 0, 1)\nllogdat &lt;- data.frame(time = t, delta = d)\n\n# Kaplan-Meier estimate with complementary log-log confidence intervals\nllog_km &lt;- survfit(Surv(time, delta) ~ 1, data = llogdat,\n                   conf.type = \"log-log\")\n\n# Log-logistic survival function\nllog_surv &lt;- function(t, lambda=1, gamma=1) 1 / (1 + (lambda * t)^gamma)\n\n# Kaplan-Meier curve and log-logistic survival curve\nt &lt;- seq(0, max(llogdat$time), .01)\nplot(llog_km, xlim = c(0, 15),\n     xlab = \"Time\", ylab = \"Survival probability\")\ngrid()\nlines(t, llog_surv(t, 2, 1), col = \"darkgray\")\nlegend(\"topright\", bg = \"white\",\n       lty = c(\"solid\", \"dashed\", \"solid\"),\n       col = c(\"black\", \"black\", \"darkgray\"),\n       legend = c(\"Kaplan-Meier estimate\", \"95% confidence limits\",\n                  \"True survival function\"))\n\n\n\n\n\n\n\n\n\nFigure 6.1: True log-logistic survival function and Kaplan-Meier estimate with complementary log-log 95% confidence limits.\n\n\n\n\n\n\n\n6.2.3 Greenwood formula and confidence intervals\nCalculating the variance of a product is difficult and tedious, but calculating the variance of a sum is easy. Taking logarithms in Equation 6.5, we get \\[\n  \\ln \\hat{S}(t) = \\sum_{j: t_j \\leq t} \\ln \\hat{q}_j.\n\\] For each \\(j\\), the estimated variance of \\(\\hat{q}_j\\) is \\[\n  \\Var(\\hat{q}_j) = \\frac{1}{n_j}\\hat{q}_j \\big(1 - \\hat{q}_j\\big).\n\\] Since \\(\\ln x\\) has the derivative \\(\\frac{1}{x}\\), \\[\n    \\Var(\\ln \\hat{q}_j)\n    \\approx \\frac{1}{\\hat{q}_j^2} \\Var\\big(\\hat{q}_j\\big)\n    = \\frac{d_j}{n_j(n_j - d_j)}.\n\\] by the delta method from Section 3.4.1.\nThe estimated survival probabilities in each time interval are conditionally independent, so \\[\n  \\Var\\bigl(\\ln \\hat{S}(t)\\bigr)\n  = \\sum_{t_j \\leq t} \\Var(\\ln \\hat{q}_j)\n  = \\sum_{t_j \\leq t} \\frac{d_j}{n_j(n_j - d_j)}.\n\\] by Equation 1.12. Since \\(\\hat{S}(t) = \\exp\\big(\\ln \\hat{S}(t)\\big)\\), we can use the delta method again to get an estimated variance for \\(\\hat{S}(t)\\). The function \\(\\exp(x) = e^x\\) is its own derivative, so we get \\[\n  \\Var\\bigl(\\hat{S}(t)\\bigr)\n  = \\hat{S}(t)^2 \\Var\\bigl(\\ln \\hat{S}(t)\\bigr)\n  = \\hat{S}(t)^2 \\sum_{t_j \\leq t} \\frac{d_j}{n_j(n_j - d_j)}.\n\\tag{6.6}\\] This is called the Greenwood formula (Greenwood 1926).3 It was developed originally for life tables and applied later to the Kaplan-Meier estimator.\nFor each \\(t\\), a pointwise Wald \\(95\\%\\) confidence interval for \\(S(t)\\) is \\[\n  \\hat{S}(t) \\pm 1.96 \\sqrt{\\Var\\bigl(\\hat{S}(t)\\bigr)}.\n\\] This confidence interval can have a lower bound less than zero or an upper bound greater than one, outside the possible values of \\(S(t)\\). Better confidence intervals can be obtained using a complementary log-log transformation, which is \\[\n  \\ln\\bigl(-\\ln S(t)\\bigr) = \\ln H(t)\n\\] where \\(H(t)\\) is the cumulative hazard. The logit (log odds) transformation can also be used.\n\n\n6.2.4 Cumulative incidence and cumulative hazard\nThe Kaplan-Meier estimator of the survival function can also be used to estimate the cumulative hazard function \\(H(t) = -\\ln S(t)\\) and the cumulative incidence function \\(F(t) = 1 - S(t)\\), which is the CDF of the time-to-event distribution. The estimated cumulative hazard function is \\[\n  \\hat{H}_\\text{KM}(t)\n  = -\\ln \\hat{S}(t),\n\\tag{6.7}\\] which is defined whenever \\(\\hat{S}(t) &gt; 0\\). The estimated cumulative incidence function is \\[\n  \\hat{F}_\\text{KM}(t)\n  = 1 - \\hat{S}(t).\n\\] When there is no right censoring or left truncation (delayed entry), \\(\\hat{F}(t)\\) equals the empirical CDF of the times to events as in Equation 6.1 and the Greenwood variance equals the corresponding variance in Equation 6.2. Confidence limits for \\(F(t)\\) or \\(H(t)\\) can be obtained from the corresponding confidence limits for \\(S(t)\\).",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-Sample Survival Analysis</span>"
    ]
  },
  {
    "objectID": "survival.html#sec-NA",
    "href": "survival.html#sec-NA",
    "title": "6  One-Sample Survival Analysis",
    "section": "6.3 Nelson-Aalen estimator",
    "text": "6.3 Nelson-Aalen estimator\nThe Kaplan-Meier estimator is based on estimating conditional survival probabilities in intervals within which there are no entries or exits. The Nelson-Aalen esimator uses the same time intervals, but it estimates an expected number of events in each interval (Nelson 1969, 1972; Altshuler 1970; Aalen 1978). It is based on the interpretation of the cumulative hazard \\(H(t)\\) as an expected number of events in \\((0, t]\\) if the event could be made repeatable, and it uses the fact that the expected number of events in different intervals can be added together by Equation 1.11. The at-risk process and risk sets are defined exactly as in Section 6.2.1.\n\n6.3.1 Cumulative hazard via addition of expected values\nAs above, let \\(0 = t_0 &lt; t_1 &lt; t_2 &lt; \\ldots &lt; t_m\\) be the endpoints of intervals \\((t_{i-1}, t_i]\\) within which there are no entries or exits from the study. Let \\(n_j = \\sum_{i = 1}^n Y_i(t_j)\\) be the number of people in the risk set \\(\\mathcal{R}(t_j)\\) and let \\(d_j \\geq 0\\) be the number of failures that occur at time \\(t_j\\). The estimated expected number of events per individual under observation in this time interval is \\[\n  \\frac{1}{n_j} \\sum_{i \\in \\mathcal{R}_j} \\indicator_{\\tevent_i \\in (t_{j - 1}, t_j]}\n  = \\frac{d_j}{n_j}.\n\\] Adding these up over all time intervals with endpoints at or before time \\(t\\), we get the Nelson-Aalen estimator \\[\n  \\hat{H}(t)\n  = \\sum_{j: t_j \\leq t} \\frac{d_j}{n_j}.\n\\tag{6.8}\\] The Nelson-Aalen estimator is an unbiased, consistent, and asymptotically normal estimator of the true cumulative hazard (Fleming and Harrington 2005; Aalen, Borgan, and Gjessing 2008). Figure 6.2 shows an example based on a right-censored sample of size 500 from a log-logistic distribution with shape \\(\\alpha = 1\\) and rate \\(\\lambda = 2\\).\nThe Fleming-Harrington correction for ties (Fleming and Harrington 1984) replaces each \\(d_j / n_j\\) in Equation 6.8 with \\[\n  \\frac{1}{n_j} + \\frac{1}{n_j - 1} + \\cdots + \\frac{1}{n_j - (d_j - 1)}\n  &gt; \\frac{d_j}{n_j}.\n\\tag{6.9}\\] The resulting estimator of the cumulative hazard is sometimes called the Fleming-Harrington estimator. It accounts for the fact that the \\(d_j\\) events did not really happen at the same time. They appear to be tied because the times were defined and measured with the limited precision possible in a real study. The example in Figure 6.2 uses this correction.\n\nR\n\n\n\n\nNelsonAalen.R\n\n## Nelson-Aalen estimator\n# The Nelson-Aalen estimator is calculated using survival::survfit().\n# Use the argument \"stype = 2\" to get the survival function estimated from the\n# Nelson-Aalen estimate of the cumulative hazard.\n# Use the argument \"ctype = 2\" to get the Fleming-Harrington correction.\n\nlibrary(survival)\n?survfit          # get general help about survfit\n?survfit.formula  # get help with the specific version we use below\n\n# right-censored sample from log-logistic dist with rate = 2 and shape = 1\n# Uses samples from logistic distribution with location = -log(rate) and\n# scale = 1 / shape.\nset.seed(42)\nllog_f &lt;- exp(rlogis(500, location = -log(2), scale = 1))\nllog_c &lt;- exp(rlogis(500, location = -log(2), scale = 2))\nt &lt;- pmin(llog_f, llog_c)\nd &lt;- ifelse(llog_c &lt; llog_f, 0, 1)\nllogdat &lt;- data.frame(time = t, delta = d)\n\n# Nelson-Aalen estimator with log-transformed confidence intervals\n# The log transformation of H is the log-log transformation of S, so we use\n# the argument conf.type = \"log-log\".\nllog_na &lt;- survfit(Surv(time, delta) ~ 1, data = llogdat,\n                   conf.type = \"log-log\", stype = 2, ctype = 2)\n\n# point and interval estimates of the survival function\nsummary(llog_na, times = 1:15)\n\n# calculate point and interval estimates of the cumulative hazard function\nnames(summary(llog_na, times = 1:15))\nsummary(llog_na, times = 1:15)$cumhaz\n-log(summary(llog_na, times = 1:15)$surv)\n-log(summary(llog_na, times = 1:15)$lower)\n-log(summary(llog_na, times = 1:15)$upper)\n\n\n\n\n\n\n\nCode\n\nNAcurve.R\n\n## Nelson-Aalen cumulative hazard curve\nlibrary(survival)\n\n# right-censored sample from log-logistic dist with rate = 2 and shape = 1\n# Uses samples from logistic distribution with location = -log(rate) and\n# scale = 1 / shape.\nset.seed(42)\nllog_f &lt;- exp(rlogis(500, location = -log(2), scale = 1))\nllog_c &lt;- exp(rlogis(500, location = -log(2), scale = 2))\nt &lt;- pmin(llog_f, llog_c)\nd &lt;- ifelse(llog_c &lt; llog_f, 0, 1)\nllogdat &lt;- data.frame(time = t, delta = d)\n\n# Nelson-Aalen estimate of the survival function with FH correction\nllog_na &lt;- survfit(Surv(time, delta) ~ 1, data = llogdat,\n                   conf.type = \"log-log\", stype = 2, ctype = 2)\n\n# log-logistic cumulative hazard function\nllog_cumhaz &lt;- function(t, lambda, gamma) log(1 + (lambda * t)^gamma)\n\n# Nelson-Aalen curve and log-logistic cumulative hazard curve\nt &lt;- seq(0, max(llogdat$time), .01)\nplot(llog_na, fun = \"cumhaz\", xlim = c(0, 20),\n     xlab = \"Time\", ylab = \"Cumulative hazard\")\ngrid()\nlines(t, llog_cumhaz(t, 2, 1), col = \"darkgray\")\nlegend(\"bottomright\", bg = \"white\", lty = c(\"solid\", \"dashed\", \"solid\"),\n       col = c(\"black\", \"black\", \"darkgray\"),\n       legend = c(\"Nelson-Aalen estimate\", \"95% confidence limits\",\n                  \"True cumulative hazard function\"))\n\n\n\n\n\n\n\n\n\nFigure 6.2: True log-logistic cumulative hazard function and Nelson-Aalen estimate with log-transformed 95% confidence limits.\n\n\n\n\n\n\n\n6.3.2 Variance and confidence intervals\nLet \\(D_j\\) be the random number of events in the interval \\((t_{j - 1}, t_j]\\) whose observed value is \\(d_j\\). If an event is repeatable, the number of events per individual in any interval \\((t_{j - 1}, t_j]\\) has a Poisson distribution with mean \\[\n  \\Delta H_j\n  = H(t_j) - H(t_{j - 1}).\n\\] When an event cannot be repeated, this is true in an interval sufficiently small that \\(\\Delta H_j\\) is much smaller than one (i.e., \\(\\Delta H_j \\ll 1\\)). Because our time intervals are defined to be small enough that no one leaves the study by having an event, \\(\\Delta H_j \\ll 1\\) in each interval where \\(n_j\\) is large.\nBecause we have \\(n_j\\) individuals at risk in this interval, \\[\n  D_j \\sim \\text{Poisson}(n_j \\Delta H_j).\n\\] because \\(D_j\\) is the sum of \\(n_j\\) Poisson(\\(\\Delta H_j\\)) random variables. Because \\(\\Var(D_j) = n_j \\Delta H_j\\), \\[\n  \\Var\\biggl(\\frac{D_j}{n_j}\\biggr)\n  = \\frac{1}{n_j^2} \\Var(D_j)\n  = \\frac{\\Delta H_j}{n_j}.\n\\] Replacing the unknown \\(\\Delta H_j\\) with \\[\n  \\Delta \\hat{H}_j\n  = \\hat{H}(t_j) - \\hat{H}(t_{j - 1})\n  = \\frac{d_j}{n_j},\n\\] we get the estimated variance \\[\n  \\Var\\bigl(\\Delta \\hat{H}_j\\bigr)\n  = \\frac{d_j}{n_j^2}.\n\\] The variance of \\(\\hat{H}(t)\\) is the sum of these variances over all time intervals with endpoints on or before time \\(t\\), so \\[\n  \\Var\\bigl(\\hat{H}(t)\\bigr)\n  = \\sum_{j: t_j \\leq t} \\Var\\bigl(\\Delta \\hat{H}_j\\bigr)\n  = \\sum_{j: t_j \\leq t} \\frac{d_j}{n_j^2}.\n\\] With the Fleming-Harrington correction for ties from Equation 6.9, each \\(d_j / n_j^2\\) is replaced by \\[\n  \\frac{1}{n_j^2} + \\frac{1}{(n_j - 1)^2} + \\cdots + \\frac{1}{\\big(n_j - (d_j - 1)\\big)^2}\n  &gt; \\frac{d_j}{n_j^2}.\n\\] This estimator of the variance (with or without the Fleming-Harrington correction) can be justified more rigorously using the theory of counting processes and martingales (Fleming and Harrington 2005; Aalen, Borgan, and Gjessing 2008). For our purposes, it is enough to highlight its connection to the Poisson distribution.\nFor each \\(t\\), a pointwise Wald 95% confidence interval for \\(H(t)\\) is \\[\n  \\hat{H}(t) \\pm 1.96 \\sqrt{\\Var\\bigl(\\hat{H}(t)\\bigr)}.\n\\] This can produce confidence intervals with negative lower bounds, outside the possible values of \\(H(t)\\). A better confidence interval is produced using a log transformation. By the delta method, \\[\n  \\Var\\bigl(\\ln \\hat{H}(t)\\bigr) = \\frac{1}{\\hat{H}(t)^2} \\Var\\bigl(\\hat{H}(t)\\bigr).\n\\] The corresponding confidence interval for \\(\\hat{H}(t)\\) has the endpoints \\[\n  \\hat{H}(t) e^{\\pm 1.96 \\sqrt{\\Var\\bigl(\\ln \\hat{H}(t)\\bigr)}}.\n\\tag{6.10}\\] Both endpoints of this confidence interval are nonnegative, and they are strictly positive for all \\(t\\) such that \\(\\hat{H}(t) &gt; 0\\). Because \\(H(t) = -\\ln S(t)\\), the log transformation for the cumulative hazard function \\(H(t)\\) corresponds to the log-log transformation for the survival function \\(S(t)\\).\n\n\n6.3.3 Survival and cumulative incidence functions\nThe Nelson-Aalen estimate \\(\\hat{H}(t)\\) can be used to estimate the survival function \\[\nS(t) = e^{-H(t)},\n\\] and the cumulative incidence function \\[\n  F(t) = 1 - S(t) = 1 - e^{-H(t)}.\n\\] The estimated survival function is \\[\n  \\hat{S}_\\text{NA}(t) = e^{-\\hat{H}(t)},\n\\] and the estimated cumulative incidence function is \\[\n  \\hat{F}_\\text{NA}(t) = 1 - \\hat{S}_\\text{NA}(t) = 1 - e^{-\\hat{H}(t)}.\n\\] In both of these estimators, \\(\\hat{H}(t)\\) can incorporate the Fleming-Harrington correction for ties from Equation 6.9. Confidence limits for \\(S(t)\\) and \\(F(t)\\) can be obtained from the corresponding confidence limits for \\(H(t)\\).\nIf there is any \\(t_j\\) where all individuals at risk have an event, the Kaplan-Meier estimator \\(\\hat{S}(t) = 0\\) for all \\(t &gt; t_j\\). Once you multiply by zero, there is no going back. This never happens for the estimate of \\(S(t)\\) based on the Nelson-Aalen estimator. Because \\(\\hat{H}(t) &lt; \\infty\\), we always have \\[\n  \\hat{S}_\\text{NA}(t) = \\exp\\bigl(-\\hat{H}(t)\\bigr) &gt; 0.\n\\] This is a practical advantage of the Nelson-Aalen estimator over the Kaplan-Meier estimator.\nMore generally, the Kaplan-Meier estimator produces smaller estimates of \\(S(t)\\) than the Nelson-Aalen estimator does. Similar inequalities exist for the estimated \\(H(t)\\) and \\(F(t)\\): \\[\n  \\begin{aligned}\n    \\hat{S}(t)            &\\leq \\hat{S}_\\text{NA}(t) \\\\\n    \\hat{H}_\\text{KM}(t)  &\\geq \\hat{H}(t) \\\\\n    \\hat{F}_{KM}(t)       &\\geq \\hat{F}_\\text{NA}(t).\n  \\end{aligned}\n\\] Each inequality implies the others, but the cumulative hazard inequality is the simplest. As shown in Figure 6.3, \\[\n  -\\ln\\biggl(1 - \\frac{d_j}{n_j}\\biggr)\n  \\geq \\frac{d_j}{n_j}\n\\] with equality only if \\(d_j &gt; 0\\). By Equation 6.5 and Equation 6.7, \\[\n  \\hat{H}_{KM}(t)\n  = -\\sum_{j: t_j \\leq t} \\ln\\biggl(1 - \\frac{d_j}{n_j}\\biggr)\n  \\geq \\sum_{j: t_j \\leq t} \\frac{d_j}{n_j}\n  = \\hat{H}(t).\n\\] with equality only when all \\(d_j = 0\\) in the sums. The Nelson-Aalen estimate with the Fleming-Harrington correction for ties is greater than the uncorrected \\(\\hat{H}(t)\\) and less than \\(\\hat{H}_\\text{KM}(t)\\). Although not equal, the Nelson-Aalen estimator and Kaplan-Meier estimators of the survival \\(S(t)\\), cumulative hazard \\(H(t)\\), and the cumulative incidence \\(F(t)\\) produce similar results in large samples, where \\(d_j / n_j\\) is small for each \\(j\\).\n\n\n\nCode\n\nestHineq.R\n\n## Kaplan-Meier H(t) &gt;= Nelson-Aalen H(t)\n\n# plot of estimated cumulative hazard function increments\nx &lt;- seq(0, 1, by = 0.01)\nplot(x, x, type = \"l\", ylim = c(0, 1),\n     xlab = expression(d[j] / n[j]),\n     ylab = expression(paste(\"Estimated \", Delta, H[j], \" = \",\n                             H(t[j]) - H(t[j - 1]))))\nlines(x, -log(1 - x), lty = \"dashed\")\ngrid()\ntext(0.75, 0.55, expression(paste(\"NA estimator: \", d[j] / n[j])))\ntext(0.25, 0.7, expression(paste(\"KM estimator: -ln(\", 1 - d[j] / n[j], \")\",\n                                 sep = \"\")))\n\n\n\n\n\n\n\n\n\nFigure 6.3: Increments in the estimated cumulative hazard in the interval \\((t_{j - 1}, t_j]\\) based on the Kaplan-Meier and Nelson-Aalen estimators.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-Sample Survival Analysis</span>"
    ]
  },
  {
    "objectID": "survival.html#parametric-failure-time-distributions",
    "href": "survival.html#parametric-failure-time-distributions",
    "title": "6  One-Sample Survival Analysis",
    "section": "6.4 Parametric failure time distributions",
    "text": "6.4 Parametric failure time distributions\nMany times to events that are important in epidemiology cannot be accurately described using an exponential distribution. In particular, it is important to allow the hazard function \\(h(t)\\) to change over time. Here, we introduce two simple failure time distributions where \\(h(t)\\) is not constant. They each have a shape parameter \\(\\alpha\\) and a rate parameter \\(\\lambda\\). As with the exponential distribution, the scale parameter is \\(\\sigma = 1 / \\lambda\\). The gamma distribution from equation Equation 5.19 is also used as a failure time distribution, but its survival and hazard functions do not have a simple closed form.\nFor all parametric failure time models, likelihoods are constructed as in Section 5.2.4. The rate parameter \\(\\lambda\\) and shape parameter \\(\\alpha\\) can be estimated using frequentist methods such as maximum likelihood or Bayesian methods. When using parametric methods, it is important to evaluate goodness-of-fit to check whether the underlying assumptions are reasonable.\n\n6.4.1 Weibull distribution\nThe Weibull distribution (Weibull et al. 1951) is a two-parameter generalization of the exponential distribution.4 It has the survival function \\[\n  S(t, \\alpha, \\lambda) = \\exp\\big(-(\\lambda t)^\\alpha\\big),\n\\] where \\(\\alpha &gt; 0\\) is the shape parameter and \\(\\lambda &gt; 0\\) is the rate parameter. The Weibull cumulative hazard function is \\[\n  H(t, \\alpha, \\lambda) = -\\ln S(t) = (\\lambda t)^\\alpha,\n\\tag{6.11}\\] and its hazard function is \\[\n  h(t, \\alpha, \\lambda)\n  = \\frac{\\partial}{\\partial t} H(t, \\alpha, \\lambda)\n  = \\alpha \\lambda^\\alpha t^{\\alpha - 1}.\n\\tag{6.12}\\] The notation \\(\\partial / \\partial t\\) means that we take a derivative with respect to \\(t\\) while holding the other arguments, \\(\\alpha\\) and \\(\\lambda\\), constant. Figure 6.4 shows examples of these hazard functions. The exponential distribution is a special case of the Weibull distribution with shape \\(\\alpha = 1\\).\n\n\n\nCode\n\nWeibull-haz.R\n\n## Weibull hazard functions\n\n# hazard function\nhweib &lt;- function(t, shape=1, rate=1) shape * rate^shape * t^(shape - 1)\n\n# hazard plots for shapes 2, 1, and 1 / 2\nt &lt;- seq(0, 2, by = 0.01)\nplot(t, hweib(t, 2), type = \"l\", lty = \"dashed\",\n     xlab = \"Time\", ylab = \"Weibull hazard (rate = 1)\")\nlines(t, hweib(t))\nlines(t, hweib(t, 0.5), lty = \"dotted\")\ngrid()\ntext(1.6, 1.2, \"shape = 1 (exponential)\")\ntext(1.25, 0.2, \"shape = 1 / 2\")\ntext(0.7, 1.9, \"shape = 2\")\n\n\n\n\n\n\n\n\n\nFigure 6.4: Hazard functions for the Weibull distribution with different shape parameters \\(\\alpha\\). All have rate \\(\\lambda = 1\\).\n\n\n\n\n\n\nR\n\n\n\n\nWeibull.R\n\n## Fitting a Weibull distribution\n# In R, the shape is 1 / the \"scale\" parameter.\n# In standard terminology, the scale is 1 / rate.\nlibrary(survival)\n\n# Weibull distribution and regression (rate = 2, shape = 3)\n# Weibull is the default distribution for survival::survreg().\nwsample &lt;- rweibull(1000, shape = 3, scale = 1 / 2)\nweibdat &lt;- data.frame(time = wsample, event = 1)\nweibreg &lt;- survreg(Surv(time, event) ~ 1, data = weibdat)\nsummary(weibreg)\nexp(-coef(weibreg))       # point estimate of rate\nexp(-confint(weibreg))    # 95% confidence interval for rate\n1 / weibreg$scale         # point estimate of shape\n\n# log-transformed Wald confidence interval for the shape parameter\n# vcov() returns the estimated covariance matrix from the model\nexp(-log(weibreg$scale) + c(-1, 1)\n    * qnorm(.975) * sqrt(vcov(weibreg)[\"Log(scale)\", \"Log(scale)\"]))\n\n# plot of true and estimated Weibull hazard functions\nwrate_est &lt;- exp(-coef(weibreg))\nwshape_est &lt;- 1 / weibreg$scale\nwrate_true &lt;- 2\nwshape_true &lt;- 3\nh_weib &lt;- function(time, rate, shape) rate * shape * (time * rate)^(shape - 1)\nt &lt;- seq(0, 4, by = 0.01)\nplot(t, h_weib(t, wrate_true, wshape_true), type = \"n\",\n     xlab = \"Time\", ylab = \"Hazard (Weibull)\")\ngrid()\nlines(t, h_weib(t, wrate_est, wshape_est))\nlines(t, h_weib(t, wrate_true, wshape_true), lty = \"dashed\")\nlegend(\"topleft\", lty = c(\"solid\", \"dashed\"), bg = \"white\",\n       legend = c(\"Estimated hazard function\", \"True hazard function\"))\n\n\n\n\n\n\n6.4.2 Log-logistic distribution\nThe exponential distribution has a constant hazard, and the Weibull hazard function is increasing, decreasing, or constant. The log-logistic distribution has a more flexible hazard function. Its survival function is \\[\n  S(t) = \\frac{1}{1 + (\\lambda t)^\\alpha}.\n\\] where \\(\\lambda &gt; 0\\) is the rate pameter and \\(\\alpha &gt; 0\\) is the shape parameter. Its cumulative hazard function is \\[\n  H(t, \\alpha, \\lambda) = -\\ln S(t) = \\ln \\big(1 + (\\lambda t)^\\alpha\\big),\n\\] and its the hazard function is \\[\n  h(t, \\alpha, \\lambda)\n  = \\frac{\\partial}{\\partial t} H(t, \\alpha, \\lambda)\n  = \\frac{\\lambda \\alpha (\\lambda t)^{\\alpha - 1}}{1 + (\\lambda t)^\\alpha}.\n\\] As before, we differentiate \\(H(t, \\alpha, \\lambda)\\) with respect to \\(t\\) while holding \\(\\alpha\\) and \\(\\lambda\\) constant. There are three general shapes that the hazard function can take depending on the shape parameter \\(\\alpha\\): \\[\n  h(t)\n  \\begin{cases}\n      \\text{decreases from } \\infty &\\text{if } \\alpha  &lt; 1, \\\\\n      \\text{decreases from } \\lambda &\\text{if } \\alpha = 1, \\\\\n      \\text{increases then decreases} &\\text{if } \\alpha &gt; 1.\n  \\end{cases}\n\\] Figure 6.5 shows examples of these hazard functions. The exponential distribution is not a special case of the log-logistic distribution for any shape \\(\\alpha\\).\n\n\n\nCode\n\nloglogistic-haz.R\n\n## Log-logistic hazard functions\n\n# hazard function\nhllog &lt;- function(t, shape=1, rate=1) {\n  shape * rate^shape * t^(shape - 1) / (1 + (rate * t)^shape)\n}\n\n# hazard plots for shape = 2, 1, 1 / 2\nt &lt;- seq(0, 4, by = 0.01)\nplot(t, hllog(t, 2), type = \"l\", lty = \"dashed\",\n     xlab = \"Time\", ylab = \"Log-logistic hazard (rate = 1)\")\nlines(t, hllog(t))\nlines(t, hllog(t, 0.5), lty = \"dotted\")\ngrid()\ntext(1.5, 0.5, \"shape = 1\")\ntext(3, 0.7, \"shape = 2\")\ntext(1.5, 0.1, \"shape = 1 / 2\")\n\n\n\n\n\n\n\n\n\nFigure 6.5: Hazard functions for the log-logistic distribution with different shape parameters \\(\\alpha\\). All have rate \\(\\lambda = 1\\).\n\n\n\n\n\nThe name of the log-logistic distribution comes from the fact that the logarithm of a log-logistic random variable has a logistic distribution (just like the logarithm of a log-normal random variable has a normal distribution). The log-logistic distribution is used in economics to model the distribution of wealth or income (Fisk 1961), where it is known as the Fisk distribution.\n\nR\n\n\n\n\nloglogistic.R\n\n## Fitting a log-logistic distribution\n# To sample a log-logistic random variable in R, you sample a logistic random\n# variable with location = -log(rate) and scale = 1 / shape.\n# The exponential of the logistic variable has a log-logistic distribution\n# with the correct rate and shape parameters.\nlibrary(survival)\n\n# Log-logistic distribution and regression (rate = 2, shape = 3)\nllsample &lt;- exp(rlogis(1000, location = -log(2), scale = 1 / 3))\nllogdat &lt;- data.frame(time = llsample, event = 1)\nllogreg &lt;- survreg(Surv(time, event) ~ 1, data = llogdat,\n                   dist = \"loglogistic\")\nexp(-coef(llogreg))       # point estimate of rate\nexp(-confint(llogreg))    # 95% confidence interval for rate\n1 / llogreg$scale         # point estimate of shape\n\n# log-transformed 95% confidence interval for the shape parameter\nexp(-log(llogreg$scale) + c(-1, 1)\n    * qnorm(.975) * sqrt(vcov(llogreg)[\"Log(scale)\", \"Log(scale)\"]))\n\n# plot of true and estimated log-logistic hazard functions\nllrate_est &lt;- exp(-coef(llogreg))\nllshape_est &lt;- 1 / llogreg$scale\nllrate_true &lt;- 2\nllshape_true &lt;- 3\nh_llog &lt;- function(time, rate, shape) {\n  # returns last expression if there is no return() statement\n  (rate * shape * (time * rate)^(shape - 1) /\n   (1 + (time* rate)^shape))\n}\nt &lt;- seq(0, 4, by = 0.01)\nplot(t, h_llog(t, llrate_true, llshape_true), type = \"n\",\n     xlab = \"Time\", ylab = \"Hazard (Log-logistic)\")\ngrid()\nlines(t, h_llog(t, llrate_est, llshape_est))\nlines(t, h_llog(t, llrate_true, llshape_true), lty = \"dashed\")\nlegend(\"topright\", lty = c(\"solid\", \"dashed\"), bg = \"white\",\n       legend = c(\"Estimated hazard function\", \"True hazard function\"))\n\n\n\n\n\n\n6.4.3 Cox-Snell residuals\nOne way to check goodness-of-fit for a parametric failure time model is to use Cox-Snell residuals (Cox and Snell 1968). For an observation \\((\\tentry_i, \\texit_i, \\delta_i)\\), the Cox-Snell residual is \\[\n  \\Big(\\hat{H}\\bigl(\\tentry_i\\bigr), \\hat{H}\\bigl(\\texit_i\\bigr), \\delta_i\\Big),\n\\] where the estimated cumulative hazards are playing the role of entry and exit times. When the parametric model is correct, the Cox-Snell residuals are a right-censored and left-truncated sample from an exponential distribution with rate \\(\\lambda = 1\\). To check this, we can plot the Nelson-Aalen estimate of the cumulative hazard for the Cox-Snell residuals and compare it to the exponential(1) cumulative hazard, which is \\(H(t) = t\\).\n\n\n\n\nAalen, Odd. 1978. “Nonparametric Inference for a Family of Counting Processes.” The Annals of Statistics 6: 701–26.\n\n\nAalen, Odd, Ørnulf Borgan, and Håkon Gjessing. 2008. Survival and Event History Analysis: A Process Point of View. Springer Science & Business Media.\n\n\nAltshuler, Bernard. 1970. “Theory for the Measurement of Competing Risks in Animal Experiments.” Mathematical Biosciences 6: 1–11.\n\n\nCox, David R, and E Joyce Snell. 1968. “A General Definition of Residuals.” Journal of the Royal Statistical Society: Series B (Methodological) 30 (2): 248–65.\n\n\nFisk, Peter R. 1961. “The Graduation of Income Distributions.” Econometrica: Journal of the Econometric Society 29 (2): 171–85.\n\n\nFleming, Thomas R, and David P Harrington. 1984. “Nonparametric Estimation of the Survival Distribution in Censored Data.” Communications in Statistics-Theory and Methods 13 (20): 2469–86.\n\n\n———. 2005. Counting Processes and Survival Analysis. Vol. 625. John Wiley & Sons.\n\n\nGreenwood, Major. 1926. “The Natural Duration of Cancer.” Reports on Public Health and Medical Subjects 33: 1–26.\n\n\nKaplan, Edward L, and Paul Meier. 1958. “Nonparametric Estimation from Incomplete Observations.” Journal of the American Statistical Association 53 (282): 457–81.\n\n\nNelson, Wayne. 1969. “Hazard Plotting for Incomplete Failure Data.” Journal of Quality Technology 1: 27–52.\n\n\n———. 1972. “Theory and Applications of Hazard Plotting for Censored Failure Data.” Technometrics 14 (4): 945–66.\n\n\nVan der Vaart, Aad W. 2000. Asymptotic Statistics. Cambridge University Press.\n\n\nWeibull, Waloddi et al. 1951. “A Statistical Distribution Function of Wide Applicability.” Journal of Applied Mechanics 18 (3): 293–97.",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-Sample Survival Analysis</span>"
    ]
  },
  {
    "objectID": "survival.html#footnotes",
    "href": "survival.html#footnotes",
    "title": "6  One-Sample Survival Analysis",
    "section": "",
    "text": "The Glivenko-Cantelli theorem guarantees that \\(\\sup_{x \\in \\mathbb{R}} |\\hat{F}_n(x) - F(x)| \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\), so the convergence happens simultaneously for all \\(x\\) (Van der Vaart 2000).↩︎\n Named after American statisticians Edward L. Kaplan (1920–2006) and Paul Meier (1924–2011). Kaplan worked at Bell Telephone Laboratories, the Lawrence Radiation Laboratory (now the Lawrence Berkeley National Laboratory), and Oregon State University. Meier worked at Johns Hopkins and the University of Chicago and was an early advocate for the use of randomization in clinical trials. Both were doctoral students of John Tukey at Princeton. Their 1958 paper is one of the most-cited papers in statistics, with 66,740 citations as of 22 January 2025.↩︎\n Major Greenwood (1880–1949) was an English epidemiologist and statistician. He worked at the Lister Institute (now part of the University of London) and joined the newly-created Ministry of Health after serving in the Royal Army Medical Corps in World War I. He studied the health effects of factory work and developed early models of infectious disease transmission. In 1928, he became the first professor of epidemiology at the London School of Hygiene and Tropical Medicine. In an obituary, Austin Bradford Hill wrote that one of Greenwood’s greatest contributions “lay merely in his outlook, in his statistical approach to medicine, then a new approach and one long regarded with suspicion. And he fought this fight continuously and honestly.”↩︎\n Named after Waloddi Weibull (1887-1979), a Swedish engineer and applied mathematician who was a member of the Swedish Coast Guard and invented a technique of using explosives to determine the type and thickness of sediments beneath the sea floor.↩︎",
    "crumbs": [
      "Defining and Measuring Disease Occurrence",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-Sample Survival Analysis</span>"
    ]
  },
  {
    "objectID": "studydesign.html",
    "href": "studydesign.html",
    "title": "7  Cohort and Case-Control Studies",
    "section": "",
    "text": "7.1 Sampling from a population\nSome the most important questions in public health involve the association between a disease and a possible predictor or cause, which we call an exposure. Here, we consider testing the null hypothesis that exposure and disease are independent in a population based on a sample from that population. If exposure and disease are independent, an individual’s exposure status contains no information about their risk of disease and vice versa. For simplicity, we focus on a binary exposure and a binary disease outcome and we focus on association, not causation. It turns out this null hypothesis can be tested most efficiently when we sample study participants according to exposure or according to disease (but not both). Sampling by exposure leads to the cohort study design, and sampling by disease leads to the case-control study design.\nSuppose we take a random sample of size \\(n\\) from a population of size \\(N \\gg n\\) (i.e., \\(N\\) is much greater than \\(n\\)) and classify each individual in the sample by exposure and disease in a contingency table. We assume that each possible sample of size \\(n\\) is equally likely. Each of the cell counts in the resulting 2x2 table is a random variable in the sample space \\(\\Omega_n\\) that consists of all possible samples of size \\(n\\) from the population \\(\\Omega\\). In Table 7.1, these random variables are \\(\\A\\), \\(\\B\\), \\(\\C\\), and \\(\\D\\). The random row totals are \\(\\R_1 = \\A + \\B\\) and \\(\\R_0 = \\C + \\D\\), and the random column totals are \\(\\K_1 = \\A + \\C\\) and \\(\\K_0 = \\B + \\D\\). The total sample size \\(n\\) is fixed, which means that it is the same for every sample \\(\\omega_n \\in \\Omega_n\\).\nTable 7.2 shows the observed values of these random variables from a single sample. These are the values available to us for statistical inference about the independence of exposure and disease.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cohort and Case-Control Studies</span>"
    ]
  },
  {
    "objectID": "studydesign.html#sampling-from-a-population",
    "href": "studydesign.html#sampling-from-a-population",
    "title": "7  Cohort and Case-Control Studies",
    "section": "",
    "text": "Table 7.1: Random 2x2 table of exposure (\\(X\\)) and disease (\\(D\\)).\n\n\n\n\n\n\n\\(D = 1\\)\n\\(D = 0\\)\nTotal\n\n\n\n\n\\(X = 1\\)\n\\(\\A\\)\n\\(\\B\\)\n\\(\\R_1\\)\n\n\n\\(X = 0\\)\n\\(\\C\\)\n\\(\\D\\)\n\\(\\R_0\\)\n\n\nTotal\n\\(\\K_1\\)\n\\(\\K_0\\)\n\\(n\\)\n\n\n\n\n\n\n\n\n\n\nTable 7.2: Observed 2x2 table of exposure (\\(X\\)) and disease (\\(D\\)).\n\n\n\n\n\n\n\\(D = 1\\)\n\\(D = 0\\)\nTotal\n\n\n\n\n\\(X = 1\\)\n\\(a\\)\n\\(b\\)\n\\(r_1\\)\n\n\n\\(X = 0\\)\n\\(c\\)\n\\(d\\)\n\\(r_0\\)\n\n\nTotal\n\\(k_1\\)\n\\(k_0\\)\n\\(n\\)\n\n\n\n\n\n\n\n7.1.1 Hypergeometric distribution*\nOver all possible samples from the population \\(\\Omega\\), the joint distribution of the cell counts \\(\\A\\), \\(\\B\\), \\(\\C\\), and \\(\\D\\) in Table 7.1 is a multivariate hypergeometric distribution. Its probability mass function (PMF) is \\[\n  \\Pr(\\A = a, \\B = b, \\C = c, D = d)\n  = \\frac{\\binom{N p_{\\A}}{a} \\binom{N p_{\\B}}{b} \\binom{N p_{\\C}}{c} \\binom{N p_{\\D}}{d}}{\\binom{N}{n}}\n\\tag{7.1}\\] for all \\(a, b, c, d \\geq 0\\) such that \\(a + b + c + d = n\\), where \\[\n  \\begin{aligned}\n    p_{\\A}  &= \\Pr(X = 1 \\text{ and } D = 1) \\\\\n    p_{\\B}  &= \\Pr(X = 1 \\text{ and } D = 0) \\\\\n    p_{\\C}  &= \\Pr(X = 0 \\text{ and } D = 1) \\\\\n    p_{\\D}  &= \\Pr(X = 0 \\text{ and } D = 0)\n  \\end{aligned}\n\\] in the underlying population (i.e., where \\(\\Omega\\) is the population and we sample a single individual \\(\\omega\\) at random). The numerator in Equation 7.1 is the number of ways of getting cell counts \\(\\A = a\\), \\(\\B = b\\), \\(\\C = c\\), and \\(\\D = d\\) in a sample of size \\(n\\), and the denominator is the number of samples of size \\(n\\) that can be chosen from our population \\(\\Omega\\) of size \\(N \\geq n\\).\nThe marginal distribution of each cell count is a hypergeometric distribution. The PMF of \\(\\A\\), which is the number of individuals who are exposed and have disease (or disease onset), is \\[\n  \\Pr(\\A = a) = \\frac{\\binom{N p_{\\A}}{a} \\binom{N (1 - p_{\\A})}{n - a}}{\\binom{N}{n}}.\n\\] where \\(a \\geq 0\\) and \\(a \\leq n\\). Its mean is \\[\n  \\E(\\A) = n p_{\\A},\n\\] which is identical to the binomial(\\(n\\), \\(p_{\\A}\\)) mean. Its variance is \\[\n  \\Var(\\A) = n p_{\\A} (1 - p_{\\A}) \\frac{N - n}{N - 1},\n\\] which is smaller than the binomial(\\(n\\), \\(p_{\\A}\\)) variance for all \\(n &gt; 1\\). The factor \\((N - n) / (N - 1)\\) is called the finite population correction.\nThe row totals \\(\\R_1\\) and \\(\\R_0\\) and the column totals \\(\\K_1\\) and \\(\\K_0\\) from Table 7.1 also have hypergeometric distributions. For \\(\\R_1\\), we have \\[\n  \\Pr(\\R_1 = r_1) = \\frac{\\binom{N \\pi}{r_1} \\binom{N (1 - \\pi)}{n - r_1}}{\\binom{N}{n}}\n\\] where \\(\\pi = \\Pr(X = 1)\\) is the marginal probability of exposure in the population. For \\(\\K_1\\), we have \\[\n  \\Pr(\\K_1 = k_1) = \\frac{\\binom{N p}{k_1} \\binom{N (1 - p)}{n - k_1}}{\\binom{N}{n}}\n\\] where \\(p = \\Pr(D = 1)\\) is the marginal prevalence or risk of disease in the population.\nAs the population size \\(N \\rightarrow \\infty\\), the distribution of \\(\\A\\) converges to a binomial(\\(n\\), \\(p_{\\A}\\)) distribution. If \\(N \\rightarrow \\infty\\) and \\(n \\rightarrow \\infty\\) such that \\(n^2 / N \\rightarrow 0\\), the distribution of \\[\n  \\frac{\\A - \\E(\\A)}{\\sqrt{\\Var(\\A)}}\n\\] converges to the standard normal distribution \\(N(0, 1)\\). The hypergeometric distributions of the other cell counts and marginal totals also converge to binomial or normal distributions.\n\n\n7.1.2 Multinomial distribution\nIf we fix the sample size \\(n\\) and let the population size \\(N \\rightarrow \\infty\\), the multivariate hypergeometric distribution converges to the multinomial distribution. Its PMF is \\[\n  \\Pr(\\A = a, \\B = b, \\C = c, \\D = d)\n  = \\frac{n!}{a! b! c! d!} p_{\\A}^a p_{\\B}^b p_{\\C}^c p_{\\D}^d.\n\\] for \\(a, b, c, d \\geq 0\\) such that \\(a + b + c + d = n\\). This PMF is written in terms of four probabilities, but there are only three degrees of freedom because \\(p_{\\A} + p_{\\B} + p_{\\C} + p_{\\D} = 1\\). In the multinomial distribution, the covariance of \\(\\A\\) and \\(\\B\\) is \\[\n  \\Cov(\\A, \\B) = -n p_{\\A} p_{\\B},\n\\] and the covariances for the other five pairs of cell counts follow the same pattern. The multinomial approximation to the multivariate hypergeometric distribution and the binomial approximation to the hypergeometric distribution can be used when \\(N\\) is much larger than \\(n\\) (i.e., \\(N \\gg n\\)), which is a common situation in epidemiologic studies.1\nWhen the joint distribution of the cell counts is multinomial, the marginal distribution of each cell count is binomial. For example, the distribution of \\(\\A\\) is binomial(\\(n\\), \\(p_{\\A}\\)), so its mean is \\(n p_{\\A}\\) and its variance is \\(n p_{\\A} (1 - p_{\\A})\\). The row and column sums also have binomial distributions. The distribution of \\(\\R_1\\) is binomial(\\(n\\), \\(\\pi\\)) where \\[\n  \\pi = \\Pr(X = 1)\n\\] is the marginal prevalence of exposure. The distribution of \\(\\K_1\\) is binomial(\\(n\\), \\(p\\)) where \\[\n  p = \\Pr(D = 1)\n\\] is the marginal prevalence or risk of disease.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cohort and Case-Control Studies</span>"
    ]
  },
  {
    "objectID": "studydesign.html#sec-2x2independence",
    "href": "studydesign.html#sec-2x2independence",
    "title": "7  Cohort and Case-Control Studies",
    "section": "7.2 Hypothesis tests for independence in a 2x2 table",
    "text": "7.2 Hypothesis tests for independence in a 2x2 table\nWhen exposure and disease are independent, the multiplication rule for independent events implies that \\[\n  \\Pr(X = x \\text{ and } D = d) = \\Pr(X = x) \\Pr(D = d)\n\\] for all possible values \\(x\\) of \\(X\\) and \\(d\\) of \\(D\\). There are two equivalent ways to express this null hypothesis that will prove useful in thinking about epidemiologic study design: one in terms of conditional risks of disease given exposure and one in terms of conditional prevalences of exposure given disease.\n\n7.2.1 Equality of conditional probabilities\nIndependence of exposure and disease can be expressed in terms of equality of conditional probabilities of disease (or disease onset) given exposure. Let \\[\n  p_1 = \\Pr(D = 1 \\given{} X = 1)\n\\] be the risk of disease among the exposed and \\[\n  p_0 = \\Pr(D = 1 \\given{} X = 0)\n\\] be the prevalence or risk of disease among the unexposed. If exposure and disease are independent, then \\[\n  \\Pr(D = 1 \\given{} X = x)\n  = \\frac{\\Pr(D = 1) \\Pr(X = x)}{\\Pr(X = x)} = \\Pr(D = 1)\n\\] for \\(x = 1\\) and \\(x = 0\\). Therefore, \\(p_1 = p_0\\) if exposure and disease are independent. Conversely, suppose \\(p_1 = p_0\\). By definition of \\(p_1\\) and \\(p_0\\), \\[\n  \\Pr(D = 1 \\given{} X = 1)\n  = \\Pr(D = 1 \\given{} X = 0).\n\\] Expanding the conditional probabilities, we get \\[\n  \\frac{\\Pr(D = 1 \\text{ and } X = 1)}{\\Pr(X = 1)}\n  = \\frac{\\Pr(D = 1 \\text{ and } X = 0)}{\\Pr(X = 0)}.\n\\] This can be rewritten as \\[\n  \\frac{\\Pr(D = 1 \\vand X = 1)}{\\Pr(X = 1)}\n  = \\frac{\\Pr(D = 1) - \\Pr(D = 1 \\vand X = 1)}{1 - \\Pr(X = 1)}.\n\\] Cross-multiplying the numerators and denominators shows that this equality holds if and only if \\[\n  \\Pr(D = 1 \\vand X = 1) = \\Pr(D = 1) \\Pr(X = 1).\n\\] Because \\(D\\) and \\(X\\) are binary, this is establishes that \\(D\\) and \\(X\\) are independent random variables. Therefore, \\(p_1 = p_0\\) implies that exposure and disease are independent. Combining both results shows that \\(H_0: p_1 = p_0\\) is equivalent to the null hypothesis that exposure and disease are independent.\nA similar argument applies to the conditional prevalence of exposure given disease status. Let \\[\n  \\pi_1 = \\Pr(X = 1 \\given D = 1)\n\\] be the prevalence of exposure among cases and \\[\n  \\pi_0 = \\Pr(X = 1 \\given D = 0)\n\\] be the prevalence of exposure among controls. The null hypothesis \\(H_0: \\pi_1 = \\pi_0\\) is equivalent to the null hypothesis that exposure and disease are independent.\n\n\n7.2.2 Hypergeometric chi-squared test\nUnder the null hypothesis that exposed and disease are independent, we have \\[\n  \\begin{aligned}\n    p_{\\A}  &= \\Pr(X = 1) \\Pr(D = 1) = \\pi p, \\\\\n    p_{\\B}  &= \\Pr(X = 1) \\Pr(D = 0) = \\pi (1 - p), \\\\\n    p_{\\C}  &= \\Pr(X = 0) \\Pr(D = 1) = (1 - \\pi) p, \\\\\n    p_{\\D}  &= \\Pr(X = 0) \\Pr(D = 0) = (1 - \\pi) p.\n  \\end{aligned}\n\\] The marginal prevalence of exposure \\(\\pi\\) and the marginal risk of disease \\(p\\) are both unknown. In a score test of the null hypothesis, these are nuisance parameters that can be replaced by maximum likelihood estimates (Rao 1948; Boos and Stefanski 2013). Because \\(\\R_1\\) has an approximate binomial(\\(n\\), \\(\\pi\\)) distribution when \\(N \\gg n\\), \\[\n  \\hat{\\pi} = \\frac{r_1}{n}.\n\\tag{7.2}\\] is the maximum likelihood estimate of \\(\\pi\\) based on Table 7.2. Because \\(\\K_1\\) has an approximate binomial(\\(n\\), \\(p\\)) distribution when \\(N \\gg n\\), \\[\n  \\hat{p} = \\frac{k_1}{n}\n\\tag{7.3}\\] is the maximum likelihood estimate of \\(p\\) based on Table 7.2. When we use these maximum likelihood estimates of \\(\\pi\\) and \\(p\\) to test independence, we are conditioning on the row and column totals in the observed 2x2 table in Table 7.2.\nGiven the margins of a 2x2 table, the entire table is determined by any one of the four cell counts. Table 7.3 shows how the cell counts in Table 7.2 are determined by \\(\\A\\) and the margins. Because all cell counts must be nonnegative, we must have \\(\\A \\geq 0\\), \\(\\A \\leq r_1\\), and \\(\\A \\leq k_1\\). In the bottom right cell of the \\(2 \\times 2\\) table, we must have \\[\n  \\A - (a - d) \\geq 0.\n\\] Therefore, \\[\n  a_\\text{min} = \\max(0, a - d) \\leq \\A \\leq \\min(r_1, k_1) = a_\\text{max}.\n\\] Note that the cells along the diagonal of the \\(2\\times 2\\) table (the \\(\\A\\) and \\(\\D\\) cells) both increase with \\(\\A\\), while the cells off the diagonal (the \\(\\B\\) and \\(\\C\\) cells) both decrease with \\(\\A\\). Any of the other cells could also determine the entire table given the margins, and constraints on the possible values of \\(\\B\\), \\(\\C\\), and \\(\\D\\) given the margins could be found in a similar way.\n\n\n\nTable 7.3: 2x2 table determined by \\(\\A\\) and the margins.\n\n\n\n\n\n\n\\(D = 1\\)\n\\(D = 0\\)\nTotal\n\n\n\n\n\\(X = 1\\)\n\\(\\A\\)\n\\(r_1 - \\A\\)\n\\(r_1\\)\n\n\n\\(X = 0\\)\n\\(k_1 - \\A\\)\n\\(\\A - (a - d)\\)\n\\(r_0\\)\n\n\nTotal\n\\(k_1\\)\n\\(k_0\\)\n\\(n\\)\n\n\n\n\n\n\nThe conditional distribution of the cell count \\(\\A\\) given the margins of Table 7.2 is hypergeometric. Imagine our sample as a bowl of \\(n\\) marbles, \\(r_1\\) of which are exposed and \\(r_0\\) of which are unexposed. If we randomly choose \\(k_1\\) marbles without replacement to represent the individuals with disease, then \\(\\A\\) is the number of exposed marbles in our sample. The probability that we get \\(a\\) exposed marbles and \\(k_1 - a\\) unexposed marbles is \\[\n  \\Pr(\\A = a \\given{} \\margins)\n  = \\frac{\\binom{r_1}{a} \\binom{r_0}{k_1 - a}}{\\binom{n}{k_1}}\n  = \\frac{\\binom{r_1}{a} \\binom{r_0}{c}}{\\binom{n}{k_1}}\n  = \\frac{r_1!\\, r_0!\\, k_1!\\, k_0!}{a!\\, b!\\, c!\\, d!\\, n!}\n\\] We could also view our sample as a bowl of \\(n\\) marbles of which \\(k_1\\) have disease (or disease onset) and \\(k_0\\) do not. In that case, \\(A\\) is the number of diseased marbles in a sample of \\(r_1\\) marbles that represent exposed individuals and we get exactly the same hypergeometric distribution of \\(\\A\\). The cell counts \\(\\B\\), \\(\\C\\), and \\(\\D\\) also have hypergeometric distributions given the margins of the table.\nUnder the null hypothesis that exposure and disease are independent, the conditional mean of \\(\\A\\) given the margins of Table 7.2 is \\[\n  \\E(\\A \\given{} \\margins) = n \\hat{\\pi} \\hat{p} = \\frac{r_1 k_1}{n}\n\\] and its conditional variance is \\[\n  \\Var(\\A \\given{} \\margins) = \\frac{r_1 r_0 k_1 k_0}{n^2 (n - 1)}.\n\\] For large \\(n\\), the hypergeometric distribution is approximately normal so the hypergeometric chi-squared statistic is \\[\n  \\chisqH = \\frac{\\big(a - \\E(\\A \\given{} \\margins)\\big)^2}{\\Var(\\A \\given{} \\margins)}\n  = \\frac{(n - 1) (a d - b c)^2}{r_1 r_0 k_1 k_0}\n\\tag{7.4}\\] Under the null hypothesis, \\(\\chisqH \\approxsim \\chi^2_1\\) (i.e., the chi-squared distribution with one degree of freedom). The p-value is \\(1 - F(\\chisqH)\\) where \\(F\\) is the cumulative distribution function (CDF) of the \\(\\chi^2_1\\) distribution. We reject the null hypothesis at significance level \\(\\alpha\\) when \\(\\chisqH\\) is sufficiently large that the p-value is less than \\(\\alpha\\). We get exactly the same hypothesis test using \\(\\B\\), \\(\\C\\), or \\(\\D\\) instead of \\(\\A\\).\n\n\n7.2.3 Pearson’s chi-squared test\nA more general approach to testing independence of the rows and columns in a contingency table is Pearson’s chi-squared test (Pearson 1900, 1922).2 Like the hypergeometric test, Pearson’s chi-squared test conditions on the margins of the table. In a contingency table with \\(I\\) rows and \\(J\\) columns, let \\(O_{ij}\\) be the observed cell count in row \\(i\\) and column \\(j\\). Let \\(r_i\\) be the total for row \\(i\\) and \\(k_j\\) be the total for column \\(j\\). Under independence, the expected cell count is \\[\n  E_{ij} = \\frac{r_i k_j}{n}.\n\\] Pearson’s chi-squared statistic is \\[\n  \\chisqP\n  = \\sum_{i = 1}^I \\sum_{j = 1}^J \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\tag{7.5}\\] Under the null hypothesis that the variables defining the rows and the colums are independent, \\(\\chisqP\\) has a chi-squared distribution with \\((I - 1) (J - 1)\\) degrees of freedom (Fisher 1922; Boos and Stefanski 2013). In any contingency table, Pearson’s chi-squared test is the score test of the null hypothesis that the rows and columns are independent based on a multinomial model (see Section 7.1.2) for the joint distribution of the cell counts (Boos and Stefanski 2013).\nIn Table 7.2, we have \\(I = J = 2\\) with \\(O_{11} = a\\), \\(O_{12} = b\\), \\(O_{21} = c\\), and \\(O_{22} = d\\). Using the multivariate hypergeometric distribution or its multinomial approximation, we have the following estimated expected cell counts under the null hypothesis that exposure and disease are independent: \\[\n  \\begin{aligned}\n    \\E_{11} = \\E(\\A \\given \\margins)\n      &= n \\hat{\\pi} \\hat{p}        = \\frac{r_1 k_1}{n} \\\\\n    \\E_{12} = \\E(\\B \\given \\margins)\n      &= n \\hat{\\pi} (1 - \\hat{p})  = \\frac{r_1 k_0}{n} \\\\\n    \\E_{21} = \\E(\\C \\given \\margins)\n      &= n (1 - \\hat{\\pi}) \\hat{p}  = \\frac{r_0 k_1}{n} \\\\\n    \\E_{22} = \\E(\\D \\given \\margins)\n      &= n (1 - \\hat{\\pi}) (1 - \\hat{p}) = \\frac{r_0 k_0}{n}.\n  \\end{aligned}\n\\tag{7.6}\\] As in the hypergeometric chi-squared test, we are conditioning on the margins of the table because we are using the maximum likelihood estimates of \\(\\pi\\) (the prevalence of exposure) and \\(p\\) (the risk of disease). When the dust settles in Equation 7.5, we get \\[\n  \\chisqP = \\frac{n (ad - bc)^2}{r_1 r_0 k_1 k_0} = \\frac{n}{n - 1} \\chisqH.\n\\tag{7.7}\\] When exposure and disease are independent, \\(\\chisqP\\) has a chi-squared distribution with \\((2 - 1) (2 - 1) = 1\\) degrees of freedom. The p-value is \\(1 - F(\\chisqP)\\) where \\(F\\) is the CDF of the \\(\\chi^2_1\\) distribution, and we reject the null hypothesis at significance level \\(\\alpha\\) when \\(\\chisqP\\) is sufficiently large that the p-value is less than \\(\\alpha\\).\nThe chi-squared approximation to the distribution of \\(\\chisqP\\) is generally considered acceptable if the minimum expected cell count is greater than or equal to five, and it is likely to be accurate whenever the average expected cell count is greater than or equal to \\(7.5\\) (Roscoe and Byars 1971), which is equivalent to \\(n \\geq 30\\) for a 2x2 table.3 Because \\(\\chisqH &lt; \\chisqP\\), the hypergeometric chi-squared test is slightly more conservative than Pearson’s chi-squared test in the sense that it is less likely to reject the null hypothesis of independence. For large \\(n\\), there is no practical difference.\n\n\n7.2.4 Small samples and exact tests*\nIn small samples, the hypergeometric distribution can be used to calculate “exact” p-values. For two-sided alternative hypotheses, this leads to Fisher’s exact test (Fisher 1935; Irwin et al. 1935) or Blaker’s exact test (Blaker 2000). These use the hypergeometric PMF to calculate a p-value for the null hypothesis of independent rows and columns. These tests differ slightly in the way that they define the tails of the distribution of \\(\\A\\), and there are two versions of Fisher’s exact test.\nThe minimum likelihood Fisher’s exact test defines the p-value as the sum of the probabilities of all possible \\(\\mathcal{a}\\) such that \\(\\Pr(A = \\mathcal{a} \\given{} \\margins) \\leq \\Pr(\\A = a \\given{} \\margins)\\). The simpler but slightly less powerful central Fisher’s exact test defines the p-value as twice the twice the minimum of the tail probabilities \\(\\Pr(\\A \\leq a \\given{} \\margins)\\) and \\(\\Pr(A \\geq a \\given{} \\margins)\\).4\nBlaker’s exact test defines the p-value as the minumum tail probability plus the probability of an opposite tail defined so that its probability is less than or equal to that of the smaller tail. For example: If the smaller tail is \\(\\A \\leq a\\), then the p-value is \\[\n  \\Pr(A \\leq a \\given{} \\margins) + \\sum_{a' = a_\\text{opp}}^{a_\\text{max}} \\Pr(A = a' \\given{} \\margins)\n\\] where \\(a_\\text{opp}\\) is chosen so that the sum in the second term is less than or equal to \\(\\Pr(A \\leq a \\given{} \\margins)\\). Blaker’s test is sometimes more powerful and never less powerful than both versions of Fisher’s exact test (Blaker 2000; Fay 2010).\nThese tests are ``exact’’ in the sense that they reject a true null hypothesis with probability less than or equal to the nominal significance level \\(\\alpha\\). However, they are often overly conservative in that the true significance level (i.e., the actual probability of rejecting the null hypothesis when it is true) can be substantially less than \\(\\alpha\\). Using mid-p values mitigates this problem, ensuring that the true significance level stays closer to \\(\\alpha\\). The price of this is that the true significance level of the test can be slightly greater than \\(\\alpha\\), so the mid-p tests are no longer “exact” (Lancaster 1961; Routledge 1992; Agresti 2013).",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cohort and Case-Control Studies</span>"
    ]
  },
  {
    "objectID": "studydesign.html#cohort-studies",
    "href": "studydesign.html#cohort-studies",
    "title": "7  Cohort and Case-Control Studies",
    "section": "7.3 Cohort studies",
    "text": "7.3 Cohort studies\nRandom sampling from the population is not the most efficient way to detect a departure from independence of exposure and disease. By rearranging the Pearson chi-squared statistic \\(\\chisqP\\) from equation Equation 7.7, we can identify two strategies for generating a more powerful test. One is to select participants by exposure, which leads to the cohort study design. The other is to select participants by disease, which leads to the case-control study design. In both cases, a balanced study design is optimal (or near-optimal) and Pearson’s chi-squared test is the score test of the null hypothesis that exposure and disease are independent. If participation in the study involves any cost, risk, or inconvenience, then maximizing the power of the study for a given number of participants is an important ethical consideration because an inefficient study will place an unnecessary burden on some participants.\n\n7.3.1 Selection by exposure\nThe Pearson chi-squared statistic \\(\\chisqP\\) from Equation 7.5 can be rewritten in terms of the risks of disease in exposed and unexposed individuals. As above, let \\(p_1\\) be the risk of disease in the exposed and \\(p_0\\) be the risk of disease in the unexposed. In Table 7.2, their maximum likelihood estimates are \\(\\hat{p}_1 = a / r_1\\) and \\(\\hat{p}_0 = c / r_0\\). The maximum likelihood estimate of \\(p_1 - p_0\\) is \\[\n  \\hat{p}_1 - \\hat{p}_0\n  = \\frac{a}{a + b} - \\frac{c}{c + d}\n  = \\frac{a d - b c}{(a + b) (c + d)}\n  = \\frac{a d - b c}{r_1 r_0}.\n\\tag{7.8}\\] Section 7.2.1 showed that the null hypothesis that exposure and disease are independent is equivalent to \\(H_0: p_1 = p_0 = p\\) where \\(p\\) is the marginal risk of disease.\nWhen \\(n \\ll N\\) and the null hypothesis is true, \\(\\A\\) has an approximate binomial(\\(r_1\\), \\(p\\)) conditional distribution, \\(\\C\\) has an approximate binomial(\\(r_0\\), \\(p\\)) conditional distribution, and they are conditionally independent given the row sums \\(r_1\\) and \\(r_0\\). Thus, the large-sample variance of \\(\\hat{p}_1 - \\hat{p}_0\\) under the null is approximately \\[\n  \\Var_0(\\hat{p}_1 - \\hat{p}_0)\n  = p (1 - p) \\bigg(\\frac{1}{r_1} + \\frac{1}{r_0}\\bigg)\n  = p (1 - p) \\frac{n}{r_1 r_0}\n\\tag{7.9}\\] where we used \\(n = r_1 + r_0\\). Replacing the unknown \\(p\\) with its maximum likelihood estimate \\(\\hat{p} = k_1 / n\\), we get the estimated null variance \\[\n  \\hat{\\Var}_0(\\hat{p}_1 - \\hat{p}_0)\n  = \\hat{p} (1 - \\hat{p}) \\frac{n}{r_1 r_0}\n  = \\frac{k_1 k_0}{r_1 r_0 n}\n\\tag{7.10}\\] where we used \\(1 - \\hat{p} = k_0 / n\\). Combining Equation 7.8 and Equation 7.9, we get \\[\n  \\frac{(\\hat{p}_1 - \\hat{p}_0)^2}{\\hat{p} (1 - \\hat{p}) \\Big(\\frac{1}{r_1} + \\frac{1}{r_0}\\Big)}\n  = \\frac{n (a d - b c)^2}{r_1 r_0 k_1 k_0}\n  = \\chisqP\n\\] (see Equation 7.7). Let \\(\\varphi\\) be the proportion of our sample that is exposed, so \\(r_1 = \\varphi n\\) and \\(r_0 = (1 - \\varphi) n\\). As \\(n \\rightarrow \\infty\\), we have \\(r_1 \\rightarrow \\infty\\) and \\(r_0 \\rightarrow \\infty\\). The law of large numbers (LLN) guarantees that \\(\\hat{p}_1 \\rightarrow p_1\\), \\(\\hat{p}_0 \\rightarrow p_0\\), and \\[\n    \\hat{p} \\rightarrow p_\\varphi = \\varphi p_1 + (1 - \\varphi) p_0.\n\\] In large samples, \\[\n  \\chisqP\n  \\approx \\frac{(p_1 - p_0)^2}{p_\\varphi (1 - p_\\varphi) \\Big(\\frac{1}{r_1} + \\frac{1}{r_0}\\Big)}\n  = \\frac{(p_1 - p_0)^2}{p_\\varphi (1 - p_\\varphi) \\frac{1}{\\phi (1 - \\phi) n}}\n\\tag{7.11}\\] The numerator of Equation 7.11 is fixed, but the denominator depends on \\(r_1\\), \\(r_0\\), and \\(\\varphi = r_1 / n\\). By sampling according to exposure, we can choose \\(r_1\\) and \\(r_0\\) to increase the power of the Pearson chi-squared test for a fixed total number of participants.\n\n\n7.3.2 Score test for independence in a cohort study*\nWe need to make sure that sampling by exposure does not change the score test of the null hypothesis that exposure and disease are independent. Using a binomial(\\(r_1\\), \\(p_1\\)) distribution for the number of individuals with disease the exposed group and a binomial(\\(r_0\\), \\(p_0\\)) distribution for the number of individuals with disease in the unexposed group, the log likelihood is \\[\n  \\ell(p_1, p_0) = \\A \\ln p_1 + \\B \\ln(1 - p_1) + \\C \\ln p_0 + \\D \\ln(1 - p_0),\n\\] where we have dropped terms that do not depend on \\(p_1\\) or \\(p_0\\). In order to calculate the expected information for the score test, we view the log likelihood as a random variable whose value will be determined by the realized values \\(a\\), \\(b\\), \\(c\\), and \\(d\\) of the random variables \\(\\A\\), \\(\\B\\), \\(\\C\\), and \\(\\D\\). The score function and the information function will also be treated as random variables.\nBecause \\(\\ell(p_1, p_0)\\) depends on two parameters, the score function is a column vector of length two: \\[\n  U(p_1, p_0)\n  = \\begin{pmatrix}\n      \\frac{\\partial}{\\partial p_1} \\ell(p_1, p_0) \\\\[5pt]\n      \\frac{\\partial}{\\partial p_0} \\ell(p_1, p_0)\n    \\end{pmatrix}\n  = \\begin{pmatrix}\n      \\frac{\\A}{p_1} - \\frac{\\B}{1 - p_1} \\\\[5pt]\n      \\frac{\\C}{p_0} - \\frac{\\D}{1 - p_0}\n    \\end{pmatrix}.\n\\] The information \\(I(p_1, p_0)\\) is a 2x2 matrix \\[\n  \\begin{bmatrix}\n    \\frac{\\partial^2}{\\partial p_1^2} \\ell(p_1, p_0)\n      & \\frac{\\partial^2}{\\partial p_1 \\partial p_0} \\ell(p_1, p_0) \\\\[5pt]\n    \\frac{\\partial^2}{\\partial p_0 \\partial p_1} \\ell(p_1, p_0)\n      & \\frac{\\partial^2}{\\partial p_0^2} \\ell(p_1, p_0)\n  \\end{bmatrix} \\\\\n  = \\begin{bmatrix}\n      \\frac{\\A}{p_1^2} + \\frac{\\B}{(1 - p_1)^2} & 0 \\\\\n      0 & \\frac{\\C}{p_0^2} + \\frac{\\D}{(1 - p_0)^2}\n    \\end{bmatrix}.\n\\] The realized value of \\(U(p_1, p_0)\\) and the observed information \\(I(p_1, p_0)\\) are obtained by replacing the random variables \\(\\A\\), \\(\\B\\), \\(\\C\\), and \\(\\D\\) with their realized values \\(a\\), \\(b\\), \\(c\\), and \\(d\\).\nThe score statistic is calculated under the null hypothesis \\(H_0: p_1 = p_0 = p\\), and we use the expected information (Freedman 2007). Let \\(\\E_0(Y)\\) be the expected value of a random variable \\(Y\\) calculated under \\(H_0\\). Then \\(\\E_0(\\A) = n_1 p\\), \\(\\E_0(\\B) = n_1 (1 - p)\\), \\(\\E_0(\\C) = n_0 p\\), and \\(\\E_0(\\D) = n_0 (1 - p)\\), so the expected information under \\(H_0\\) is \\[\n    \\mathcal{I}(p, p)\n    = \\E_0[I(p, p)]\n    = \\begin{bmatrix}\n        \\frac{n_1}{p} + \\frac{n_1}{1 - p} & 0 \\\\\n        0 & \\frac{n_0}{p} + \\frac{n_0}{1 - p}\n    \\end{bmatrix}.\n\\] Both \\(U(p, p)\\) and \\(\\mathcal{I}(p, p)\\) depend on the unknown \\(p\\), which we replace with its maximum likelihood estimate \\(\\hat{p} = k_1 / n\\). This gives us the score \\[\n  U(\\hat{p}, \\hat{p})\n  = \\begin{pmatrix}\n    \\frac{a}{\\hat{p}} - \\frac{b}{1 - \\hat{p}} \\\\[5pt]\n    \\frac{c}{\\hat{p}} - \\frac{d}{1 - \\hat{p}}\n  \\end{pmatrix}\n  = \\begin{pmatrix}\n    \\frac{n a}{k_1} - \\frac{n b}{k_0} \\\\[5pt]\n    \\frac{n c}{k_1} - \\frac{n d}{k_0}\n  \\end{pmatrix}\n  = \\begin{pmatrix}\n      \\frac{n (a d - b c)}{k_1 k_0} \\\\[5pt]\n      - \\frac{n (a d - b c)}{k_1 k_0}\n    \\end{pmatrix}.\n\\] where we used \\(k_1 = a + c\\) and \\(k_0 = b + d\\). The expected information at \\(p = \\hat{p}\\) is \\[\n  \\mathcal{I}(\\hat{p}, \\hat{p})\n  = \\begin{bmatrix}\n      \\frac{r_1 n^2}{k_1 k_0} & 0 \\\\\n      0                       & \\frac{r_0 n^2}{k_1 k_0}\n    \\end{bmatrix}\n  \\;\\Rightarrow\\;\n  \\mathcal{I}^{\\,-1}(\\hat{p}, \\hat{p})\n  = \\begin{bmatrix}\n      \\frac{k_1 k_0}{r_1 n^2} & 0 \\\\\n      0                       & \\frac{k_1 k_0}{r_0 n^2}\n    \\end{bmatrix}\n\\] where we used \\(n_1 = r_1\\) and \\(n_0 = r_0\\). The score statistic is \\[\n  U(\\hat{p}, \\hat{p})^\\transpose \\mathcal{I}(\\hat{p}, \\hat{p})^{-1} U(\\hat{p}, \\hat{p})\n  = \\frac{n (a d - b c)^2}{r_1 r_0 k_1 k_0}\n  = \\chisqP,\n\\] from Equation 7.7. Because \\(H_0\\) reduces the degrees of freedom from two (\\(p_1\\) and \\(p_0\\)) to one (\\(p_1 = p_0 = p\\)), \\(\\chisqP\\) has an asymptotic \\(\\chi^2\\) distribution with \\(2 - 1 = 1\\) degree of freedom under the null. Therefore, Pearson’s chi-squared test is the score test of independence of exposure and disease in a cohort study. The row sums \\(r_1\\) and \\(r_0\\) are fixed by design, and we condition on the column sums \\(k_1\\) because we use the maximum likelihood estimate \\(\\hat{p} = k_1 / n\\) of the risk of disease under \\(H_0\\).\nWhen it uses the expected information, the score test does not depend on the parameterization of the model for \\(p_1\\) and \\(p_0\\) (Boos and Stefanski 2013). We get the same score statistic \\(\\chisqP\\) and the same \\(\\chi^2_1\\) distribution under the null even if the model uses transformations of \\(p_1\\) and \\(p_0\\) (e.g., log or logit) or if it is parameterized in terms of the risk difference \\(\\RD = p_1 - p_0\\), the risk ratio \\(\\RR = p_1 / p_0\\), or the odds ratio \\(\\OR = \\odds(p_1) / \\odds(p_0)\\) where \\(\\odds(p) = p / (1 - p)\\). All roads lead to the same score test of the null hypothesis that exposure and disease are independent, which corresponds to \\(\\RD = 0\\) and \\(\\RR = \\OR = 1\\).\n\n\n7.3.3 Optimal sampling by exposure\nHaving established that \\(\\chisqP\\) is the score statistic for testing the independence of exposure and disease in a cohort study, we can choose \\(r_1\\) and \\(r_0\\) to maximize the power of the test for a given number of participants \\(n = r_1 + r_0\\). The value of the chi-squared statistic in Equation 7.11 depends on \\(r_1\\) and \\(r_0\\) only in the denominator, so we can maximize the statistic by minimizing its denominator. Writing the denominator of Equation 7.10 in terms of \\(p_1\\), \\(p_0\\), and \\(\\varphi = r_1 / n\\) of the sample that is exposed and simplifying gives us \\[\n  \\frac{n p (1 - p)}{r_1 r_0}\n  = \\frac{\\varphi}{1 - \\varphi} p_1 (1 - p_1) + \\frac{1 - \\varphi}{\\varphi} p_0 (1 - p_0) + C(p_1, p_0)\n\\tag{7.12}\\] where \\(C(p_1, p_0) = p_1 (1 - p_0) + p_0 (1 - p_1)\\) does not depend on \\(\\varphi\\). The derivative of this with respect to \\(\\varphi\\) is \\[\n  \\frac{\\dif}{\\dif \\varphi} \\frac{n p (1 - p)}{r_1 r_0}\n  = \\frac{p_1 (1 - p_1)}{(1 - \\varphi)^2} - \\frac{p_0 (1 - p_0)}{\\varphi^2},\n\\tag{7.13}\\] which equals zero when \\[\n  \\frac{\\varphi}{1 - \\varphi} = \\sqrt{\\frac{p_0 (1 - p_0)}{p_1 (1 - p_1)}}.\n\\tag{7.14}\\] To see that this is a minimum and not a maximum, notice that the function in equation Equation 7.12 takes large values for \\(\\varphi\\) near one when \\(p_1 (1 - p_1) &gt; 0\\) and for \\(\\varphi\\) near zero when \\(p_0 (1 - p_0) &gt; 0\\). It also has a positive second derivative with respect to \\(\\varphi\\).\nSolving for \\(\\varphi\\) in Equation 7.14 shows that a proportion exposed of \\[\n  \\varphi^*\n  = \\frac{1}{1 + \\sqrt{\\frac{p_1 (1 - p_1)}{p_0 (1 - p_0)}}}.\n\\tag{7.15}\\] maximizes the expected value of the Pearson chi-squared statistic \\(\\chisqP\\) for a given \\(n\\) (Walter 1977). The expression inside the square root is the variance of a Bernoulli(\\(p_1\\)) random variable divided by the variance of a Bernoulli(\\(p_0\\)) random variable. Figure 7.1 shows how \\(\\varphi^*\\) depends on this variance ratio. When \\(p_1 \\approx p_0\\), the Bernoulli variance ratio is approximately one and \\(\\varphi^* \\approx 0.5\\).\n\n\n\nCode\n\noptim-phi.R\n\n## Optimal proportion exposed in a cohort study\n\n# plot of optimal phi as a function of the Bernoulli variance ratio\nlogvratio &lt;- seq(-3, 3, by = 0.01)\nphi &lt;- function(v) 1 / (1 + sqrt(v))\nplot(logvratio, phi(exp(logvratio)), type = \"l\", xaxt = \"n\", ylim = c(0, 1),\n     xlab = \"Bernoulli variance ratio (log scale)\",\n     ylab = expression(paste(\"Optimal proportion exposed or cases (\", phi, \"*)\")))\naxis(1, at = log(c(1 / c(16, 8, 4, 2), 1, c(2, 4, 8, 16))),\n     labels = c(\"1/16\", \"1/8\", \"1/4\", \"1/2\", 1, 2, 4, 8, 16))\ngrid()\nabline(h = 0.5, col = \"darkgray\")\n\n\n\n\n\n\n\n\n\nFigure 7.1: The optimal proportion exposed \\(\\varphi^*\\) in a cohort study as a function of the Bernoulli variance ratio \\(p_1 (1 - p_1) / (p_0 (1 - p_0))\\). In a case-control study, \\(\\varphi^*\\) represents the optimal proportion of the sample who are cases and the Bernoulli variance ratio is \\(\\pi_1 (1 - \\pi_1) / (\\pi_0 (1 - \\pi_0))\\). There is a dark gray horizontal line at \\(\\varphi = 0.5\\), which represents a balanced study.\n\n\n\n\n\nThe “optimal” proportion exposed \\(\\varphi^*\\) from Equation 7.15 is based on maximizing the value of \\(\\chisqP\\) in large samples. For a given sample size, the power of the test is actually determined by the distribution of possible values of \\(\\chisqP\\), so the maximum power can occur at a value of \\(\\varphi\\) slightly different from \\(\\varphi^*\\). Figure 7.2 shows the power achieved by Pearson’s chi-squared test at several combinations of \\(p_1\\), \\(p_0\\), and \\(n\\). In all cases, the power at \\(\\varphi = 0.5\\) is close to that at \\(\\varphi^*\\). In several cases, the power at \\(\\varphi = 0.5\\) exceeds that at \\(\\varphi^*\\). If we have strong enough prior information about \\(p_1\\) and \\(p_0\\) to justify an imbalanced study design, the value of testing the null hypothesis that \\(p_1 = p_0\\) is questionable. Without such prior information, a balanced study is a safe bet to be optimal or near-optimal in terms of the power to detect an association between exposure and disease (Walter 1977).\n\n\n\n\n\n\nFigure 7.2: The power of the Pearson chi-squared test from a cohort study as a function of the proportion of the sample exposed (\\(\\varphi\\)) at several combinations of \\(p_1\\) and \\(p_0\\) for \\(n = 400\\) (solid), \\(n = 200\\) (dashed), and \\(n = 100\\) (dotted). There is a dark gray solid line at \\(\\varphi = 0.5\\), representing a balanced study, and a dark gray dashed line at \\(\\varphi^*\\) from Equation 7.15. The same power is achieved by a case control study where \\(\\pi_1\\) replaces \\(p_1\\), \\(\\pi_0\\) replaces \\(p_0\\), and \\(\\varphi\\) is the proportion of the sample who are cases.\n\n\n\n\nR\n\n\n\n\nchisq-power.R\n\n## Actual power of a Pearson chi-squared test\n\n# calculate Pearson chi-squared test power\n# This can take a few minutes to run with large n.\npowers &lt;- function(p1, p0, n, level = 0.95) {\n  chisq_alpha &lt;- qchisq(level, df = 1)\n  htest &lt;- function(r1) {\n    r0 &lt;- n - r1\n    joint_dbinom &lt;- outer(0:r1, 0:r0,\n                          function(a, c) dbinom(a, r1, p1) * dbinom(c, r0, p0))\n    joint_include &lt;- outer(0:r1, 0:r0,\n                           function(a, c) max(a, c) &gt; 0 & a + c &lt; n)\n    acpower &lt;- Vectorize(function(a, c) {\n      if (max(a, c) &gt; 0 & a + c &lt; n) {\n        b &lt;- r1 - a\n        d &lt;- r0 - c\n        k1 &lt;- a + c\n        k0 &lt;- b + d\n        chisqP &lt;- n * (a * d - b * c)^2 / (r1 * r0 * k1 * k0)\n        return(chisqP &gt; chisq_alpha)\n      } else {\n        return(0)\n      }\n    })\n    joint_power &lt;- outer(0:r1, 0:r0, acpower)\n    return(sum(joint_dbinom * joint_power) / sum(joint_dbinom * joint_include))\n  }\n  r1s &lt;- 1:(n - 1)\n  powers &lt;- sapply(r1s, htest)\n  return(data.frame(r1 = r1s, power = powers, n = n))\n}\n\n# optimal value proportion exposed (or proportion cases)\noptimphi &lt;- function(p1, p0) 1 / (1 + sqrt(p1 * (1 - p1) / (p0 * (1 - p0))))\n\n# Pearson chi-squared test power for p1 = 0.1 and p0 = 0.02\npower_10_02_400 &lt;- powers(0.10, 0.02, 400)\npower_10_02_200 &lt;- powers(0.10, 0.02, 200)\npower_10_02_100 &lt;- powers(0.10, 0.02, 100)\n\n# Pearson chi-squared test power for p1 = 0.10 and p0 = 0.05\npower_10_05_400 &lt;- powers(0.10, 0.05, 400)\npower_10_05_200 &lt;- powers(0.10, 0.05, 200)\npower_10_05_100 &lt;- powers(0.10, 0.05, 100)\n\n# Pearson chi-squared test power for p1 = 0.2 and p0 = 0.02\npower_20_02_400 &lt;- powers(0.20, 0.02, 400)\npower_20_02_200 &lt;- powers(0.20, 0.02, 200)\npower_20_02_100 &lt;- powers(0.20, 0.02, 100)\n\n# Pearson chi-squared test power for p1 = 0.2 and p0 = 0.05\npower_20_05_400 &lt;- powers(0.20, 0.05, 400)\npower_20_05_200 &lt;- powers(0.20, 0.05, 200)\npower_20_05_100 &lt;- powers(0.20, 0.05, 100)\n\n# save values of graphical parameter \"mar\" before changing them\norig_mar &lt;- par(\"mar\")\norig_mfrow &lt;- par(\"mfrow\")\n\n# png(filename = \"chisq-power.png\")\npar(mar = c(4, 4, 3, 2))\npar(mfrow = c(2, 2))\n\n# Pearson chi-squared test power for p1 = 0.1 and p0 = 0.02\nplot(power_10_02_400$r1 / 400, power_10_02_400$power,\n     type = \"l\", ylim = c(0, 1),\n     main = expression(paste(p[1], \" = 0.10\", \" and \", p[0], \" = 0.02\")),\n     xlab = \"\",\n     ylab = \"Power (Pearson chi-squared test)\")\nlines(power_10_02_200$r1 / 200, power_10_02_200$power, lty = \"dashed\")\nlines(power_10_02_100$r1 / 100, power_10_02_100$power, lty = \"dotted\")\nabline(v = 0.5, col = \"darkgray\")\nabline(v = optimphi(0.10, 0.02), lty = \"dashed\", col = \"darkgray\")\ngrid()\n\n# Pearson chi-squared test power for p1 = 0.10 and p0 = 0.05\nplot(power_10_05_400$r1 / 400, power_10_05_400$power,\n     type = \"l\", ylim = c(0, 1),\n     main = expression(paste(p[1], \" = 0.10\", \" and \", p[0], \" = 0.05\")),\n     xlab = \"\", ylab = \"\")\nlines(power_10_05_200$r1 / 200, power_10_05_200$power, lty = \"dashed\")\nlines(power_10_05_100$r1 / 100, power_10_05_100$power, lty = \"dotted\")\nabline(v = 0.5, col = \"darkgray\")\nabline(v = optimphi(0.10, 0.05), lty = \"dashed\", col = \"darkgray\")\ngrid()\nlegend(\"topright\", bg = \"white\", lty = c(\"solid\", \"dashed\", \"dotted\"),\n       legend = c(\"n = 400\", \"n = 200\", \"n = 100\"))\n\n# Pearson chi-squared test power for p1 = 0.2 and p0 = 0.02\nplot(power_20_02_400$r1 / 400, power_20_02_400$power,\n     type = \"l\", ylim = c(0, 1),\n     main = expression(paste(p[1], \" = 0.20\", \" and \", p[0], \" = 0.02\")),\n     xlab = expression(paste(\"Proportion exposed (\", phi, \")\")),\n     ylab = \"Power (Pearson chi-squared test)\")\nlines(power_20_02_200$r1 / 200, power_20_02_200$power, lty = \"dashed\")\nlines(power_20_02_100$r1 / 100, power_20_02_100$power, lty = \"dotted\")\nabline(v = 0.5, col = \"darkgray\")\nabline(v = optimphi(0.20, 0.02), lty = \"dashed\", col = \"darkgray\")\ngrid()\n\n# Pearson chi-squared test power for p1 = 0.2 and p0 = 0.05\nplot(power_20_05_400$r1 / 400, power_20_05_400$power,\n     type = \"l\", ylim = c(0, 1),\n     main = expression(paste(p[1], \" = 0.20\", \" and \", p[0], \" = 0.05\")),\n     xlab = expression(paste(\"Proportion exposed (\", phi, \")\")),\n     ylab = \"\"\n    #  ylab = \"Power (Pearson chi-squared test)\"\n     )\nlines(power_20_05_200$r1 / 200, power_20_05_200$power, lty = \"dashed\")\nlines(power_20_05_100$r1 / 100, power_20_05_100$power, lty = \"dotted\")\nabline(v = 0.5, col = \"darkgray\")\nabline(v = optimphi(0.20, 0.05), lty = \"dashed\", col = \"darkgray\")\ngrid()\n# dev.off()\n\n# reset graphical parameters \"mar\" and \"mfrow\"\npar(mar = orig_mar)\npar(mfrow = orig_mfrow)",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cohort and Case-Control Studies</span>"
    ]
  },
  {
    "objectID": "studydesign.html#case-control-studies",
    "href": "studydesign.html#case-control-studies",
    "title": "7  Cohort and Case-Control Studies",
    "section": "7.4 Case-control studies",
    "text": "7.4 Case-control studies\nThe Pearson chi-squared statistic \\(\\chisqP\\) from Equation 7.7 can also be rewritten in terms of the prevalence of exposure among cases (participants who have disease or disease onset) and controls (participants who do not have disease or disease onset). This leads to the case-control study design.\n\n7.4.1 Selection by disease\nAs above, let \\(\\pi_1\\) be the exposure prevalence in cases and \\(\\pi_0\\) be the exposure prevalence in controls. Their maximum likelihood estimates are \\(\\hat{\\pi}_1 = a / k_1\\) and \\(\\hat{\\pi}_0 = c / k_0\\), so the maximum likelihood estimate of \\(\\pi_1 - \\pi_0\\) is \\[\n  \\hat{\\pi}_1 - \\hat{\\pi}_0\n  = \\frac{a}{a + c} - \\frac{b}{b + d}\n  = \\frac{a d - b c}{(a + c) (b + d)}\n  = \\frac{a d - b c}{k_1 k_0}.\n\\tag{7.16}\\] Section 7.2.1 showed that null hypothesis that exposure and disease are independent is equivalent to \\(H_0: \\pi_1 = \\pi_0 = \\pi\\) where \\(\\pi\\) is the marginal prevalence of exposure.\nIn large samples under the null, \\(\\A\\) has a binomial(\\(k_1\\), \\(\\pi\\)) conditional distribution, \\(\\B\\) has a binomial(\\(k_0\\), \\(\\pi\\)) conditional distribution, and they are conditionally independent given the column sums \\(k_1\\) and \\(k_0\\). Thus, the large-sample variance of \\(\\hat{\\pi}_1 - \\hat{\\pi}_0\\) under the null is \\[\n  \\Var_0(\\hat{\\pi}_1 - \\hat{\\pi}_0)\n  = \\pi (1 - \\pi) \\bigg(\\frac{1}{k_1} + \\frac{1}{k_0}\\bigg)\n  = \\pi (1 - \\pi) \\frac{n}{k_1 k_0}\n\\tag{7.17}\\] where we used \\(k_1 + k_0 = n\\). Replacing the unknown \\(\\pi\\) with its maximum likelihood estimate \\(\\hat{\\pi} = r_1 / n\\), we get the estimated null variance \\[\n  \\hat{\\Var}_0(\\hat{\\pi}_1 - \\hat{\\pi}_0)\n  = \\hat{\\pi} (1 - \\hat{\\pi}) \\frac{n}{k_1 k_0}\n  = \\frac{r_1 r_0}{k_1 k_0 n}\n\\tag{7.18}\\] where we used \\(1 - \\hat{\\pi} = r_0 / n\\). Combining the results in Equation 7.16} and Equation 7.18, we get \\[\n  \\frac{(\\hat{\\pi}_1 - \\hat{\\pi}_0)^2}{\\hat{\\Var}_0(\\hat{\\pi}_1 - \\hat{\\pi}_0)}\n  = \\frac{n (a d - b c)^2}{r_1 r_0 k_1 k_0}\n  = \\chisqP\n\\] (see Equation 7.7). The LLN guarantees that \\(\\hat{\\pi}_1 \\rightarrow \\pi_1\\) as \\(k_1 \\rightarrow \\infty\\) and that \\(\\hat{\\pi}_0 \\rightarrow \\pi_0\\) as \\(k_0 \\rightarrow \\infty\\). In large samples, \\[\n  \\chisqP \\approx \\frac{(\\pi_1 - \\pi_0)^2}{\\pi (1 - \\pi) \\Big(\\frac{1}{k_1} + \\frac{1}{k_0}\\Big)}\n\\tag{7.19}\\] because the sample average \\[\n  \\frac{k_1 \\pi_1 + k_0 \\pi_0}{n} \\rightarrow \\pi\n\\] as \\(n \\rightarrow \\infty\\) by the LLN. The numerator of Equation 7.19 is fixed, but the denominator depends on \\(k_1\\) and \\(k_0\\). By sampling according to disease status, we can choose \\(k_1\\) and \\(k_0\\) to increase the power of the Pearson chi-squared test for a fixed total number of participants.\n\n\n7.4.2 Score test for independence in a case-control study*\nAs with sampling by exposure in a cohort study, sampling by disease in a case-control study does not affect the score test of the null hypothesis that exposure and disease are independent. Using a binomial(\\(k_1\\), \\(\\pi_1\\)) distribution for the number of exposed cases and a binomial(\\(k_0\\), \\(\\pi_0\\)) distribution for the number of exposed controls, we get the log likelihood \\[\n  \\ell(\\pi_1, \\pi_0) = \\A \\ln \\pi_1 + \\C \\ln(1 - \\pi_1) + \\B \\ln \\pi_0 + \\D \\ln(1 - \\pi_0)\n\\] as a random variable whose value will be determined by the data. Calculating the score \\(U(\\pi, \\pi)\\) and the expected information \\(\\mathcal{I}(\\pi, \\pi)\\) under the null hypothesis \\(H_0: \\pi_1 = \\pi_0 = \\pi\\) and evaluating them at \\(\\hat{\\pi} = r_1 / n\\), we get \\[\n  U(\\hat{\\pi}, \\hat{\\pi})\n  = \\begin{pmatrix}\n    \\frac{a}{\\hat{\\pi}} + \\frac{c}{1 - \\hat{\\pi}} \\\\[5pt]\n    \\frac{b}{\\hat{\\pi}} + \\frac{d}{1 - \\hat{\\pi}}\n  \\end{pmatrix}\n  = \\begin{pmatrix}\n      \\frac{n (a d - b c)}{r_1 r_0} \\\\[5pt]\n      - \\frac{n (a d - b c)}{r_1 r_0}\n    \\end{pmatrix}\n\\] and \\[\n  \\mathcal{I}(\\hat{\\pi}, \\hat{\\pi})\n  = \\begin{bmatrix}\n      \\frac{k_1 n^2}{r_1 r_0} & 0 \\\\\n      0                       & \\frac{k_0 n^2}{r_1 r_0}\n    \\end{bmatrix}\n  \\;\\Rightarrow\\;\n  \\mathcal{I}^{\\,-1}(\\hat{\\pi}, \\hat{\\pi})\n  = \\begin{bmatrix}\n      \\frac{r_1 r_0}{k_1 n^2} & 0 \\\\\n      0                       & \\frac{r_1 r_0}{k_0 n^2}\n    \\end{bmatrix}\n\\] The score statistic is \\[\n  U(\\hat{\\pi}, \\hat{\\pi})^\\transpose \\mathcal{I}(\\hat{\\pi}, \\hat{\\pi})^{-1} U(\\hat{\\pi}, \\hat{\\pi})\n  = \\frac{n (a d - b c)^2}{r_1 r_0 k_1 k_0}\n  = \\chisqP,\n\\] which is the Pearson chi-squared statistic from Equation 7.7. The null hypothesis reduces the degrees of freedom from two (\\(\\pi_1\\) and \\(\\pi_0\\)) to one (\\(\\pi_1 = \\pi_0 = \\pi\\)), so the score statistic has a \\(\\chi^2_1\\) distribution under \\(H_0\\). Therefore, Pearson’s chi-squared test is the score test of the null hypothesis \\(H_0: \\pi_0 = \\pi_1\\) in a case-control study. The column sums \\(k_1\\) and \\(k_0\\) are fixed by design, and we condition on the row sums \\(r_1\\) and \\(r_0\\) because we use the maximum likelihood estimate \\(\\hat{\\pi} = r_1 / n\\) for the prevalence of exposure under \\(H_0\\). Because of the invariance of the score test when it uses the expected information, any parameterization of the model for the exposure prevalences \\(\\pi_1\\) and \\(\\pi_0\\) leads to the same test of the null hypothesis that exposure and disease are independent.\n\n\n7.4.3 Optimal sampling by disease\nHaving established that \\(\\chisqP\\) is the score statistic for testing the independence of exposure and disease in a case-control study, we can choose \\(k_1\\) and \\(k_0\\) to maximize the power of the test for a given number of participants \\(n = k_1 + k_0\\). Let \\(\\varphi\\) be the proportion of the sample who are cases. Then \\[\n  \\begin{aligned}\n    k_1   &= \\varphi n \\\\\n    k_0   &= (1 - \\varphi) n \\\\\n    \\pi   &= \\varphi \\pi_1 + (1 - \\varphi) \\pi_0.\n  \\end{aligned}\n\\] Substituting these into equation Equation 7.17 and simplifying gives us the denominator as a function of \\(\\varphi\\): \\[\n  \\frac{n \\pi (1 - \\pi)}{k_1 k_0}\n  = \\frac{\\varphi}{1 - \\varphi} \\pi_1 (1 - \\pi_1) + \\frac{1 - \\varphi}{\\varphi} \\pi_0 (1 - \\pi_0) + C(\\pi_1, \\pi_0)\n\\] where \\(C(\\pi_1, \\pi_0) = \\pi_1 (1 - \\pi_0) + \\pi_0 (1 - \\pi_1)\\) does not depend on \\(\\varphi\\). This is identical to The derivative with respect to \\(\\varphi\\) is \\[\n  \\frac{\\dif}{\\dif \\varphi} \\frac{n \\pi (1 - \\pi)}{r_1 r_0}\n  = \\frac{\\pi_1 (1 - \\pi_1)}{(1 - \\varphi)^2} - \\frac{\\pi_0 (1 - \\pi_0)}{\\varphi^2}.\n\\] This is identical to Equation 7.13 if we replace \\(p_1\\) with \\(\\pi_1\\) and \\(p_0\\) with \\(\\pi_0\\), so the same argument used in Section 7.3.3 tells us that the Pearson chi-squared statistic \\(\\chisqP\\) from a case-control study is maximized when the proportion of the sample comprised of cases is \\[\n  \\varphi^*\n  = \\frac{1}{1 + \\sqrt{\\frac{\\pi_1 (1 - \\pi_1)}{\\pi_0 (1 - \\pi_0)}}}.\n\\tag{7.20}\\] Here, the expression inside the square root is the variance of a Bernoulli(\\(\\pi_1\\)) random variable divided by the variance of a Bernoulli(\\(\\pi_0\\)) random variable. Figure 7.1 shows how \\(\\phi^*\\) depends on this variance ratio. When \\(\\pi_1 \\approx \\pi_0\\), the Bernoulli variance ratio is approximately one \\(\\phi^* \\approx 0.5\\).\nThe power functions shown in Figure 7.2 apply to a case-control study if we replace \\(p_1\\) with \\(\\pi_1\\) and \\(p_0\\) with \\(\\pi_0\\). The justification for recruiting equal numbers of cases and controls in a case-control study is exactly the same as that for recruiting equal numbers of exposed and unexposed in a cohort study: When testing the null hypothesis can be justified, a balanced study is almost always optimal or near-optimal in terms of its power to detect an association between exposure and disease (Walter 1977).",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cohort and Case-Control Studies</span>"
    ]
  },
  {
    "objectID": "studydesign.html#sec-design-choice",
    "href": "studydesign.html#sec-design-choice",
    "title": "7  Cohort and Case-Control Studies",
    "section": "7.5 Choice of study design",
    "text": "7.5 Choice of study design\nWe have shown that the power of the Pearson and hypergeometric chi-squared tests can be increased by sampling participants according to exposure (in a cohort study) or disease (in a case-control study) instead of taking a random sample from the population. It remains to see how to choose between a cohort study and a case-control study.\n\n7.5.1 Odds ratio\nTo choose between the cohort and case-control study designs, it is extremely helpful that the estimated odds ratio is the same for all three study designs. In Table 7.2, the estimated odds ratio comparing the risks of disease in the exposed (numerator) and the unexposed (denominator) is \\[\n  \\frac{\\odds(\\hat{p}_1)}{\\odds(\\hat{p}_0)}\n  = \\frac{a / b}{c / d}\n  = \\frac{a d}{b c}\n\\] where \\(r_1\\) canceled out of the numerator and \\(r_0\\) canceled out of the denominator in the middle expression. The estimated odds ratio comparing the prevalence exposure in cases (numerator) and controls (denominator) is \\[\n  \\frac{\\odds(\\hat{\\pi}_1)}{\\odds(\\hat{\\pi}_0)}\n  = \\frac{a / b}{c / d}\n  = \\frac{a d}{b c}\n\\] where \\(k_1\\) canceled out of the numerator and \\(k_0\\) canceled out of the denominator in the middle expression. The Pearson chi-squared statistic can be rewritten in terms of the odds ratio: \\[\n  \\chisqP\n  = \\frac{n \\big(\\frac{a d}{b c} - 1\\big)^2 b^2 c^2}{r_1 r_0 k_1 k_0}\n  = \\frac{n (\\hat{\\OR} - 1)^2 b^2 c^2}{r_1 r_0 k_1 k_0}.\n\\] Let \\[\n  \\Delta_n\n  = n (\\hat{\\OR} - 1),\n\\] which does not depend on which study design we use.\nA random sample from the population has \\[\n  \\chisqP\n  = \\Delta_n \\hat{p}_0 (1 - \\hat{p}_1) \\hat{\\pi}_0 (1 - \\hat{\\pi}_1).\n\\] because \\(b = r_1 (1 - \\hat{p}_1) = k_0 \\hat{\\pi}_0\\) and \\(c = r_0 \\hat{p}_0 = k_1 (1 - \\hat{\\pi}_1)\\). Close to the null hypothesis, \\(\\hat{p}_1 \\approx \\hat{p}_0 \\approx \\hat{p}\\) and \\(\\hat{\\pi}_0 \\approx \\hat{\\pi}_1 \\approx \\hat{\\pi}\\). In large samples close to the null hypothesis, \\[\n  \\chisqP \\approx \\Delta_n p (1 - p) \\pi (1 - \\pi).\n\\] because \\(\\hat{p} \\rightarrow p\\) and \\(\\hat{\\pi} \\rightarrow \\pi\\) as \\(n \\rightarrow \\infty\\) by the LLN. A balanced cohort study has \\(r_0 = r_1 = n / 2\\) and \\[\n  \\chisqP\n  = \\frac{\\Delta_n (1 - \\hat{p}_1)^2 \\hat{p}_0^2}{4 \\hat{p} (1 - \\hat{p})}.\n\\] because \\(b c = n^2 (1 - \\hat{p}_1) \\hat{p}_0 / 4\\) and \\(k_1 k_0 = n^2 \\hat{p} (1 - \\hat{p})\\). In a large sample close to (but not under) the null hypothesis, \\[\n  \\chisqP\n  \\approx \\frac{\\Delta_n p^2 (1 - p)^2}{4 p (1 - p)}\n  = \\frac{\\Delta_n}{4} p (1 - p).\n\\] Following similar logic for a case-control study, we get \\[\n  \\chisqP\n  \\approx \\frac{\\Delta_n}{4} \\pi (1 - \\pi)\n\\] in large samples near the null hypothesis. Because \\(v (1 - v) \\leq 1 / 4\\) for \\(v \\in [0, 1]\\), the \\(\\chisqP\\) statistics from the cohort and case-control studies are both upper bounds for the \\(\\chisqP\\) statistic from a random sample of the population.\nClose to the null, a cohort study will be more powerful than a case-control study when \\[\n  p (1 - p) &gt; \\pi (1 - \\pi)\n\\] and a case-control study will be more powerful than a cohort study when \\[\n  p (1 - p) &lt; \\pi (1 - \\pi).\n\\] The advantage of a cohort study will be greatest for a rare exposure and a risk of disease close to \\(1 / 2\\), and the advantage of a case-control study will be greatest for rare disease and a prevalence of exposure close to \\(1 / 2\\). Both study designs are always more powerful than a random sample from the population.\n\n\n7.5.2 Imbalance and efficiency on a fixed budget\nEven when testing the null hypothesis is defensible, an imbalanced study design can be justified when one exposure or disease group is substantially more difficult or expensive to recruit than the other. In a cohort study with a rare exposure, exposed individuals might be harder to recruit than unexposed individuals. In a case-control study with a rare disease, cases might be harder to recruit than controls. Even when the greatest power for a given number of participants is achieved with a balanced study, the greatest power for a given study’s resources may occur with imbalanced groups.\nDeliberately imbalanced designs are used most often in case-control studies, but the principle is the same in cohort studies. Let \\(C\\) be the ratio of the cost of recruiting a case to that of recruiting a control, and \\(B\\) be the budget of the study (expressed as the total number of controls that could be enrolled if no cases were enrolled). As in Table 7.2, \\(k_1\\) is the number of cases and \\(k_0\\) is the number of controls. We need to minimize the variance of \\(\\hat{\\pi}_1 - \\hat{\\pi}_0\\) from Equation 7.17 given that \\[\n  k_1 C + k_0 = B.\n\\] For simplicity, we will assume that the prevalences of exposure \\(\\pi_1\\) (in cases) and \\(\\pi_0\\) (in controls) are approximately equal, so we can ignore the fact that \\(\\hat{\\pi}\\) depends on \\(\\varphi = k_1 / n\\).5 To maximize the value of \\(\\chisqP\\) close to (but not under) the null, we need to minimize \\[\n  \\frac{1}{k_1} + \\frac{1}{k_0} = \\frac{1}{k_1} + \\frac{1}{B - k_1 C}\n\\] over \\(k_1\\) The derivative with respect to \\(k_1\\) is \\[\n  \\frac{\\dif}{\\dif k_1} \\bigg(\\frac{1}{k_1} + \\frac{1}{B - k_1 C}\\bigg)\n  = -\\frac{1}{k_1^2} + \\frac{C}{(B - k_1 C)^2},\n\\] which equals zero when \\[\n  k_0^2 = k_1^2 C.\n\\] This corresponds to \\(k_0 = k_1 \\sqrt{C}\\) or recruiting \\(\\sqrt{C}\\) controls per case (Miettinen 1969; Nam 1973; Gail et al. 1976). The optimal proportion of the sample who are cases is \\[\n  \\varphi^*_C = \\frac{1}{1 + \\sqrt{C}}.\n\\] A nearly identical argument based on Equation 7.9 shows that this \\(\\varphi^*_C\\) is also the optimal proportion exposed in a cohort study where the cost of recruiting an exposed individual is \\(C\\) times that of recruiting an unexposed individual. This \\(\\sqrt{C}\\) rule is a good approximation to more accurate and complicated optimal sampling rules (Meydrech and Kupper 1978; Pike and Casagrande 1979; Morgenstern and Winn 1983).\nWith a total sampling budget of \\(B\\), the optimal numbers of cases is \\[\n    k_1^* = \\frac{B}{\\sqrt{C} + C}\n\\] and the optimal number of controls is \\[\n    k_0^* = k_1^* \\sqrt{C} = \\frac{B}{1 + \\sqrt{C}}.\n\\] The minimum variance of the risk difference that we can achieve near the null is proportional to \\[\n  V^* = \\frac{1}{k_1^*} + \\frac{1}{k_0^*}\n  = \\frac{\\left(1 + \\sqrt{C}\\right)^2}{B}.\n\\] If we use a balanced study design, \\(k_1 = k_0 = B / (1 + C)\\) and the variance of the risk difference is proportional to \\[\n  V^\\text{bal} = \\frac{2}{k_1} = \\frac{2 (1 + C)}{B}.\n\\] For any given budget \\(B\\), the asymptotic relative efficiency of the optimal study compared to a balanced study is \\[\n  \\frac{V^\\text{bal}}{V^*} = \\frac{2 (1 + C)}{\\big(1 + \\sqrt{C}\\big)^2}.\n\\] It is plotted as a function of \\(C\\) in Figure 7.3. The difference is small for moderate values of \\(C\\), with relative efficiencies of approximately \\(1.029\\) for \\(C = 2\\) and \\(1.146\\) for \\(C = 5\\). In extreme scenarios (i.e., as \\(C \\rightarrow 0\\) or \\(C \\rightarrow \\infty\\)), the optimal study is twice as efficient as a balanced study with the same budget (Nam 1973).\n\n\n\nCode\n\noptimal-budget.R\n\n## Relative efficiency of imbalanced study design on fixed budget\n\n# variance ratio comparing balanced study to budget-optimal study\nlogC &lt;- seq(-3, 3, by = 0.01)\nreleff &lt;- function(C) 2 * (1 + C) / (1 + sqrt(C))^2\nplot(logC, releff(exp(logC)), type = \"l\", ylim = c(0, 2), xaxt = \"n\",\n     xlab = expression(paste(\"Cost ratio \", italic(\"C\"), \" (log scale)\")),\n     ylab = \"Asymptotic relative efficiency of budget-optimal study\")\naxis(1, at = log(c(1 / c(20, 10, 5, 2), 1, c(2, 5, 10, 20))),\n     labels = c(\"1/20\", \"1/10\", \"1/5\", \"1/2\", 1, 2, 5, 10, 20))\ngrid()\nabline(h = 1, col = \"darkgray\")\n\n\n\n\n\n\n\n\n\nFigure 7.3: The asymptotic relative efficiency of an optimal case-control study compared to a balanced study with the same budget when recruiting a case costs \\(C\\) times as much as recruiting a control. There is a dark gray line at a relative efficiency of one. The same relative efficiency applies to cohort studies when recruiting an exposed individual costs \\(C\\) times as much as recruiting an unexposed individual.\n\n\n\n\n\nThe relative efficiency can thought of as the ratio of the sampling budgets of a balanced study and an optimal study that achieve the same power (Nam 1973; Gail et al. 1976). Thus, a balanced study requires at most twice the budget of an optimal study to achieve the same power. Brittain, Schlesselman, and Stadel (1981) found that sampling costs were approximately 33-66% of total costs in five case-control studies funded by the National Institute of Child Health and Human Development in the 1970s. Compared to a balanced study design, they found that optimal sampling of cases and controls would reduce total study costs by at most 8.5% for \\(C \\leq 5\\) and at most 4.5% for \\(C \\leq 3\\). As usual, balanced study designs are close to optimal.\n\n\n\n\nAgresti, Alan. 2013. Categorical Data Analysis. Third. Vol. 792. John Wiley & Sons.\n\n\nBlaker, Helge. 2000. “Confidence Curves and Improved Exact Confidence Intervals for Discrete Distributions.” Canadian Journal of Statistics 28 (4): 783–98.\n\n\nBoos, Dennis D, and Leonard A Stefanski. 2013. Essential Statistical Inference. Springer.\n\n\nBrittain, Erica, James J Schlesselman, and Bruce V Stadel. 1981. “Cost of Case-Control Studies.” American Journal of Epidemiology 114 (2): 234–43.\n\n\nFay, Michael P. 2010. “Confidence Intervals That Match Fisher’s Exact or Blaker’s Exact Tests.” Biostatistics 11 (2): 373–74.\n\n\nFisher, Ronald A. 1922. “On the Interpretation of \\(\\chi\\) 2 from Contingency Tables, and the Calculation of p.” Journal of the Royal Statistical Society 85 (1): 87–94.\n\n\n———. 1935. “The Logic of Inductive Inference.” Journal of the Royal Statistical Society 98 (1): 39–82.\n\n\nFreedman, David A. 2007. “How Can the Score Test Be Inconsistent?” The American Statistician 61 (4): 291–95.\n\n\nGail, Mitchell, Roger Williams, David P Byar, Charles Brown, et al. 1976. “How Many Controls?” Journal of Chronic Diseases 29 (11): 723–31.\n\n\nHill, Sir Austin Bradford. 1965. “The Environment and Disease: Association or Causation?” Proceedings of the Royal Society of Medicine 58: 295–300.\n\n\nIrwin, JO et al. 1935. “Tests of Significance for Differences Between Percentages Based on Small Numbers.” Metron 12 (2): 84–94.\n\n\nLancaster, H Oliver. 1961. “Significance Tests in Discrete Distributions.” Journal of the American Statistical Association 56 (294): 223–34.\n\n\nMeydrech, Edward F, and Lawrence L Kupper. 1978. “Cost Considerations and Sample Size Requirements in Cohort and Case-Control Studies.” American Journal of Epidemiology 107 (3): 201–5.\n\n\nMiettinen, Olli S. 1969. “Individual Matching with Multiple Controls in the Case of All-or-None Responses.” Biometrics 25 (2): 339–55.\n\n\nMorgenstern, Hal, and Deborah M Winn. 1983. “A Method for Determining the Sampling Ratio in Epidemiologic Studies.” Statistics in Medicine 2 (3): 387–96.\n\n\nNam, Jun-Mo. 1973. “Optimum Sample Sizes for the Comparison of the Control and Treatment.” Biometrics 29: 101–8.\n\n\nPearson, Karl. 1900. “On the Criterion That a Given System of Deviations from the Probable in the Case of a Correlated System of Variables Is Such That It Can Be Reasonably Supposed to Have Arisen from Random Sampling.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 50 (302): 157–75.\n\n\n———. 1922. “On the \\(\\chi\\) 2 Test of Goodness of Fit.” Biometrika 14 (1/2): 186–91.\n\n\nPike, MC, and JT Casagrande. 1979. “Re:‘cost Considerations and Sample Size Requirements in Cohort and Case-Control Studies’.” American Journal of Epidemiology 110 (1): 100–102.\n\n\nRao, C Radhakrishna. 1948. “Large Sample Tests of Statistical Hypotheses Concerning Several Parameters with Applications to Problems of Estimation.” In Mathematical Proceedings of the Cambridge Philosophical Society, 44:50–57. Cambridge University Press.\n\n\nRoscoe, John T, and Jackson A Byars. 1971. “An Investigation of the Restraints with Respect to Sample Size Commonly Imposed on the Use of the Chi-Square Statistic.” Journal of the American Statistical Association 66 (336): 755–59.\n\n\nRoutledge, RD. 1992. “Resolving the Conflict over Fisher’s Exact Test.” Canadian Journal of Statistics 20 (2): 201–9.\n\n\nWalter, Samuel D. 1977. “Determination of Significant Relative Risks and Optimal Sampling Procedures in Prospective and Retrospective Comparative Studies of Various Sizes.” American Journal of Epidemiology 105 (4): 387–97.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cohort and Case-Control Studies</span>"
    ]
  },
  {
    "objectID": "studydesign.html#footnotes",
    "href": "studydesign.html#footnotes",
    "title": "7  Cohort and Case-Control Studies",
    "section": "",
    "text": "The distribution of the cell counts in Table 7.1 is exactly multinomial (and each cell count and row or column total is exactly binomial) if we sample with replacement.↩︎\n Named after Karl Pearson (1857–1936), an English statistician who appeared in the context of the Pearson correlation coefficient in Section 1.4.2. ↩︎\n These rules of thumb are for chi-squared tests with one degree of freedom and significance level \\(\\alpha = 0.05\\). Smaller \\(\\alpha\\) require larger average cell counts to estimate smaller p-values accurately, and chi-squared tests with more than one degree of freedom are more robust to small expected cell counts (Roscoe and Byars 1971).↩︎\n Inversion of the minimum likelihood Fisher’s or Blaker’s exact tests can produce confidence regions for the odds ratio that consist of two disjoint intervals, but inversion of the central Fisher’s exact test always produces a confidence region that consists of a single interval (Fay 2010).↩︎\n Without this assumption, it is difficult or impossible to derive an explicit expression for the optimal ratio \\(\\varphi^*\\) because the total sample size \\(n\\) depends on \\(\\varphi\\), which complicates the derivatives. An optimal ratio can be calculated numerically.↩︎",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cohort and Case-Control Studies</span>"
    ]
  },
  {
    "objectID": "validity.html",
    "href": "validity.html",
    "title": "8  Internal and External Validity",
    "section": "",
    "text": "8.1 Misclassification\nIn statistics, we make inferences about a population based on a sample. A study is said to have internal validity if it makes accurate measurements or inferences within the sample itself, and it is said to have external validity if these inferences accurately describe the population up to random sampling error (Campbell 1957). Both internal and external validity are best thought of as continuous, not binary. High internal validity is a prerequisite for high external validity, but there is often a tradeoff between them in practice. For simplicity, we focus on internal and external validity for descriptive epidemiology (i.e., for association and not necessarily causation).\nSo far, our discussion of 2x2 tables has assumed that the classification of exposure and disease is completely accurate and that the participants are a random sample from the population. Table 8.1 shows our 2x2 table based on true exposure and disease classifications. In reality, misclassification and selection bias threaten the validity of almost all epidemiologic studies. It is critical to understand where they come from and what they do.\nMisclassification of exposure and disease threatens both the internal and external validity of an epidemiologic study. In a cohort study, we compare the exposed and unexposed groups and we have to classify disease outcomes in each group. In a case-control study, we compare cases and controls and we have to classify exposure in each group.\nNondifferential misclassification occurs when the same classification errors affect both populations being compared. Under nondifferental misclassification, a test of the null hypothesis is still has the correct significance level but the power of the test is reduced—much like a reduction in the effective sample size (Bross 1954; Rubin, Rosenbaum, and Cobb 1956). Nondifferential classification almost always causes bias toward the null, making the expected value of a given measure of association closer to the null than its true value. However, a measure of association under nondifferential misclassification can be farther away from the null than its true value due to random variation (Gullen, Bearman, and Johnson 1968; Sorahan and Gilthorpe 1994; Wacholder et al. 1995; Yland et al. 2022).\nDifferential misclassification occurs when classification errors differ between the two populations being compared. Differental misclassification can distort both the significance level and power of a hypothesis test, and it can cause bias toward the null, away from the null, or across the null. The unpredictability of the size and direction of the bias makes differential misclassification fundamentally more dangerous than nondifferential misclassification.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Internal and External Validity</span>"
    ]
  },
  {
    "objectID": "validity.html#misclassification",
    "href": "validity.html#misclassification",
    "title": "8  Internal and External Validity",
    "section": "",
    "text": "8.1.1 Nondifferential misclassification of disease\nIn our discussion of diagnostic tests, we let \\(\\Dplus\\) indicate \\(D = 1\\), \\(\\Dminus\\) indicate \\(D = 0\\), \\(\\Tplus\\) indicate testing positive for disease, and \\(\\Tminus\\) indicate testing negative. Let \\(D^\\obs\\) be the measured disease outcome of individuals in a cohort study where disease is detected using a diagnostic test with sensitivity \\[\n  \\sens_D\n  = \\Pr(\\Tplus \\given{} \\Dplus)\n  = \\Pr\\bigl(D^\\obs = 1 \\,\\big|\\, D = 1\\bigr)\n\\] and specificity \\[\n  \\spec_D\n  = \\Pr(\\Tminus \\given{} \\Dminus)\n  = \\Pr\\bigl(D^\\obs = 0 \\,\\big|\\, D = 0\\bigr).\n\\] We assume that \\[\n  \\sens_D\n  = \\Pr(\\Tplus \\given{} \\Dplus)\n  &gt; \\Pr(\\Tplus \\,\\big|\\, \\Dminus)\n  = 1 - \\spec_D,\n\\] so individuals with disease are more likely to test positive than individuals without disease.1 This is equivalent to assuming that \\(\\sens_D + \\spec_D &gt; 1\\). We also assume that the misclassification of each participant is independent of the misclassification of all other participants. Table 8.2 shows a 2x2 table with misclassification of disease. The row sums \\(r_1\\) and \\(r_0\\) are the same as in Table 8.1 because there is no misclassification of exposure.\n\n\n\nTable 8.2: 2x2 table with misclassified disease status\n\n\n\n\n\n\n\\(D^\\obs = 1\\)\n\\(D^\\obs = 0\\)\nTotal\n\n\n\n\n\\(X = 1\\)\n\\(a^\\obs\\)\n\\(b^\\obs\\)\n\\(r_1\\)\n\n\n\\(X = 0\\)\n\\(c^\\obs\\)\n\\(d^\\obs\\)\n\\(r_0\\)\n\n\nTotal\n\\(k_1^\\obs\\)\n\\(k_0^\\obs\\)\n\\(n\\)\n\n\n\n\n\n\nMisclassification of disease is nondifferential when the sensitivity and specificity of the test are the same in all exposure groups. In other words, we have nondifferential misclassification of disease if and only if \\[\n  \\Pr\\bigl(\\Tplus \\,\\big|\\, \\Dplus, X = x\\bigr)\n  = \\Pr(\\Tplus \\given{} \\Dplus)\n  = \\sens_D\n\\] and \\[\n  \\Pr\\bigl(\\Tminus \\,\\big|\\, \\Dminus, X = x\\bigr)\n  = \\Pr(\\Tminus \\given{} \\Dminus)\n  = \\spec_D\n\\] for all possible values \\(x\\) of exposure \\(X\\). It is critical that nondifferential misclassification is defined in terms of the sensitivity and specificity of the test, not its positive predictive value (PPV) or negative predictive value (NPV). When there is an association between exposure and disease, nondifferential misclassification of disease may produce different PPVs and NPVs in the exposed and unexposed because these predictive values depend on the prevalence of disease in addition to the sensitivity and specificity of the test (D. J. Newell 1962; Buell and Dunn Jr 1964).\nUnder nondifferential misclassification, the probability that an exposed person tests positive for disease is \\[\n  \\begin{aligned}\n    p_1^\\obs\n    &= p_1 \\sens_D + (1 - p_1) (1 - \\spec_D) \\\\\n    &= (1 - \\spec_D) + (\\sens_D + \\spec_D - 1) p_1\n  \\end{aligned}\n\\] where \\(p_1\\) is the true risk of disease in the exposed. Similarly, the probability that an unexposed person tests positive for disease is \\[\n  p_0^\\obs\n  = (1 - \\spec_D) + (\\sens_D + \\spec_D - 1) p_0\n\\] where \\(p_0\\) is the true risk of disease in the unexposed. The misclassified risk difference is \\[\n  \\RD^\\obs\n  = p_1^\\obs - p_0^\\obs\n  = (\\sens_D + \\spec_D - 1) (p_1 - p_0).\n\\tag{8.1}\\] Given the margins of Table 8.2, the number \\(\\A^\\obs\\) of exposed individuals who test positive for disease has a hypergeometric distribution with mean \\(r_1 p_1^\\obs\\) and the number \\(\\C^\\obs\\) of unexposed people who test positive for disease has a hypergeometric distribution with mean \\(r_0 p_0^\\obs\\). The estimated risk difference based on the misclassified data is \\[\n  \\hat{\\RD}^\\obs = \\hat{p}_1^\\obs - \\hat{p}_0^\\obs,\n\\] where \\(\\hat{p}_1^\\obs = a^\\obs / r_1\\) and \\(\\hat{p}_0^\\obs = c^\\obs / r_0\\). It is an unbiased estimate of \\(\\RD^\\obs\\).\nWhen \\(\\sens_D &lt; 1\\) or \\(\\spec_D &lt; 1\\), the misclassified risk difference \\(\\RD^\\obs\\) in Equation 8.1 is closer to zero than the true risk difference (Bross 1954; Rubin, Rosenbaum, and Cobb 1956; D. Newell 1963). The risk ratio and odds ratio are also biased toward the null under nondifferential misclassification (Goldberg 1975; Copeland et al. 1977). This bias operates on average, not for every single estimate based on misclassified data. Even under nondifferential misclassification of disease, random variation can produce an estimate of the risk difference, risk ratio, or odds ratio that is farther from the null than the true value (Gullen, Bearman, and Johnson 1968; Sorahan and Gilthorpe 1994; Wacholder et al. 1995; Yland et al. 2022).\nWhen \\(\\sens_D + \\spec_D &gt; 1\\) (as is true of any useful diagnostic or screening test), Equation 8.1 implies that the null hypothesis that \\(p_1^\\obs = p_0^\\obs\\) is equivalent to the null hypothesis that \\(p_1 = p_0\\). Both null hypotheses are equivalent to the null hypothesis that exposure and disease are independent (see Section 7.2.1). Therefore, a test of the null hypothesis that \\(X\\) and \\(D^\\obs\\) are independent is also a valid test of the independence of \\(X\\) and \\(D\\) (Bross 1954; Rubin, Rosenbaum, and Cobb 1956). The Pearson chi-squared statistic for Table 8.2 is \\[\n  \\chisqPobs\n  = \\frac{n \\bigl(a^\\obs d^\\obs - b^\\obs c^\\obs\\bigr)^2}{r_1 r_0 k_1^\\obs k_0^\\obs},\n\\tag{8.2}\\] and it has a \\(\\chi^2_1\\) distribution under the null hypothesis that \\(p_1 = p_0\\). If we set the critical value at the \\(1 - \\alpha\\) quantile of the \\(\\chi^2_1\\) distribution, the test will reject the null with probability \\(\\alpha\\) when \\(p_1 = p_0\\) even under nondifferential misclassification of disease. A similar result holds for the hypergeometric chi-squared test, Fisher’s exact test, and other tests of independence for 2x2 tables from Section 7.2.\nAlthough nondifferential misclassification does not affect the significance level of a hypothesis test of the null hypothesis that \\(p_1 = p_0\\), it reduces the power of the test away from the null (Bross 1954; Rubin, Rosenbaum, and Cobb 1956; Rogot 1961). The Pearson chi-squared statistic based on the misclassified data in Table 8.2 can be rewritten \\[\n  \\chisqPobs\n  = \\frac{(\\hat{p}_1^\\obs - \\hat{p}_0^\\obs)^2}\n    {\\hat{p}^\\obs (1 - \\hat{p}^\\obs) \\Bigl(\\frac{1}{r_1} + \\frac{1}{r_0}\\Bigr)}.\n\\] where \\(\\hat{p}^\\obs = k_1^\\obs / n\\) is the misclassified estimate of the marginal risk of disease among the study participants. Let \\(\\varphi \\in (0, 1)\\) be the proportion of the sample that is exposed, which we assume to be (approximately) constant as \\(n \\rightarrow \\infty\\). Let \\[\n  K_D = \\sens_D + \\spec_D - 1,\n\\] so \\(K_D \\in (0, 1)\\) whenever we have a useful but imperfect test for disease. When both \\(r_1 = \\varphi n\\) and \\(r_0 = (1 - \\varphi) n\\) are large, the numerator on the right-hand side of Equation 8.2 is approximately \\[\n  \\E(\\hat{p}_1^\\obs - \\hat{p}_0^\\obs)^2 = K_D^2 (p_1 - p_0)^2\n\\] by the law of large numbers (LLN) and the continuous mapping theorem.2 Similarly, \\(\\hat{p}^\\obs\\) is approximately \\[\n  \\begin{aligned}\n    p^\\obs\n    &= p\\, \\sens_D + (1 - p) (1 - \\spec_D) \\\\\n    &= (1 - \\spec_D) + K_D p,\n  \\end{aligned}\n\\] where \\[\n  p = \\varphi p_1 + (1 - \\varphi) p_0\n\\] is the marginal risk of disease among the study participants. Thus, \\[\n  \\hat{p}^\\obs (1 - \\hat{p}^\\obs)\n  \\approx \\big(1 - \\spec_D + K_D p\\big) \\big(1 - \\sens_D + K_D (1 - p)\\big).\n\\] in large samples. It follows that \\[\n  \\chisqPobs\n  \\approx \\frac{K_D^2 p (1 - p)}{\\big(1 - \\spec_D + K_D p\\big) \\big(1 - \\sens_D + K_D (1 - p)\\big)} \\chisqP\n\\] where \\(\\chisqP\\) is the Pearson chi-squared statistic based on the correctly classified data in Table 8.1. Therefore, nondifferential misclassification of disease has approximately the same effect as multiplying the sample size \\(n\\) by the effective sample size ratio \\[\n  \\ESSR_D\n  = \\frac{K_D^2 p (1 - p)}{\\big(1 - \\spec_D + K_D p\\big) \\big(1 - \\sens_D + K_D (1 - p)\\big)}\n  \\leq 1\n\\] with equality if and only if \\(\\sens_D = \\spec_D = 1\\). Thinking about nondifferential misclassification as a reduction in the effective sample size is a good way to remember both that it preserves the correct significance level under the null and that it reduces power away from the null (Bross 1954; Rubin, Rosenbaum, and Cobb 1956).\nWhen the prevalence or risk of disease is low, the effective sample size depends much more on the specificity of the test than on the sensitivity of the test (Rubin, Rosenbaum, and Cobb 1956). Figure 8.1 shows the effective sample size ratio \\(\\ESSR_D\\) for three values of the marginal risk of disease in the sample (\\(p\\)) under two different scenarios: one where \\(\\spec_D = 1\\) while sensitivity varies from zero to one and one where \\(\\sens_D = 1\\) while specificity varies from zero to one. For all three values of \\(p\\), \\(\\ESSR_D\\) is substantially lower in the scenario with varying specificity. This difference is greatest for \\(p = 0.02\\) and smallest for \\(p = 0.40\\). If \\(p = 0.5\\), the two scenarios produce identical curves. If \\(p &gt; 0.5\\), then \\(\\ESSR_D\\) depends more on the sensitivity than the specificity of the test for disease.\n\n\n\nCode\n\nESSratio.R\n\n## Effective sample size under nondifferential misclassification\n\n# function that returns the effective sample size ratio\ness_ratio &lt;- function(sens, spec, p) {\n  # returns the multiplier of the sample size to get the effective sample size\n  # under nondifferential misclassification with marginal risk (or prevalence) \n  # p in the sample\n  K &lt;- sens + spec - 1\n  Kden &lt;- (1 - spec + K * p) * (1 - sens + K * (1 - p))\n  return(K^2 * p * (1 - p) / Kden)\n}\n\n# data frame for ESSR with specificity = 1 and varying sensitivity\ns &lt;- seq(0.01, 1, by = 0.005)\np &lt;- c(0.02, 0.05, 0.1, 0.2, 0.4)\npnames &lt;- c(\"p02\", \"p05\", \"p10\", \"p20\", \"p40\")\nessr_sens &lt;- outer(s, p, function(s, p) ess_ratio(sens = s, spec = 1, p = p))\ncolnames(essr_sens) &lt;- pnames\nessr_sens &lt;- as.data.frame(essr_sens)\n\n# data frame for ESSR with sensitivity = 1 and varying specificity\nessr_spec &lt;- outer(s, p, function(s, p) ess_ratio(sens = 1, spec = s, p = p))\ncolnames(essr_spec) &lt;- pnames\nessr_spec &lt;- as.data.frame(essr_spec)\n\n# plot\nplot(s, essr_spec$p02, type = \"l\", asp = 1, xlim = c(0, 1), ylim = c(0, 1),\n     xlab = \"Sensitivity (gray) or specificity (black)\",\n     ylab = \"Effective sample size ratio\")\nlines(s, essr_spec$p10, lty = \"dashed\")\nlines(s, essr_spec$p40, lty = \"dotdash\")\nlines(s, essr_sens$p02, col = \"darkgray\")\nlines(s, essr_sens$p10, col = \"darkgray\", lty = \"dashed\")\nlines(s, essr_sens$p40, col = \"darkgray\", lty = \"dotdash\")\ngrid()\nlegend(\"topleft\", bg = \"white\",\n       lty = c(\"solid\", \"dashed\", \"dotdash\"),\n       legend = c(\"p = 0.02\", \"p = 0.10\", \"p = 0.40\"))\ntext(0.48, 0.52, srt = 45, col = \"darkgray\",\n     \"Specificity = 1 with varying sensitivity\")\ntext(0.68, 0.32, srt = 45, \"Sensitivity = 1 with varying specificity\")\n\n\n\n\n\n\n\n\n\nFigure 8.1: The effective sample size ratio \\(\\ESSR_D\\) as a function of \\(\\sens_D\\) when \\(\\spec_D = 1\\) (gray lines) and as a function of \\(\\spec_D\\) when \\(\\sens_D = 1\\) (black lines).\n\n\n\n\n\n\n\n8.1.2 Nondifferential misclassification of exposure\nNondifferential misclassification of exposure has effects similar to those of nondifferential misclassification of disease. Although we usually discuss sensitivity and specificity in the context of a test for disease, the same ideas can be applied to a test or measurement used to determine exposure status. For simplicity, we will focus on a binary exposure \\(X\\) with \\(\\Xplus\\) indicating \\(X = 1\\) and \\(\\Xminus\\) indicating \\(X = 0\\).\nLet \\(\\Xobs\\) be the measured disease outcome of individuals in a case-control study when we classify exposure using a test \\(T_X\\) that has sensitivity \\[\n  \\sens_X\n  = \\Pr\\bigl(\\Tplus_X \\,\\big|\\,\\Xplus\\bigr)\n  = \\Pr\\bigl(\\Xobs = 1 \\,\\big|\\, X = 1\\bigr)\n\\] and specificity \\[\n  \\spec_X\n  = \\Pr\\bigl(\\Tminus_X \\,\\big|\\, \\Xminus\\bigr)\n  = \\Pr\\bigl(\\Xobs = 0 \\,\\big|\\, X = 0\\bigr).\n\\] We assume that \\(\\sens_X &gt; 1 - \\spec_X\\), so exposed individuals are more likely to test positive for exposure than unexposed individuals. This is equivalent to assuming \\(\\sens_X + \\spec_X &gt; 1\\).3 We also assume that the misclassification of each participant is independent of the misclassification of all other participants. Table 8.3 shows a 2x2 table with exposure misclassification. The column sums \\(k_1\\) and \\(k_0\\) are the same in both tables because there is no misclassification of disease status.\n\n\n\nTable 8.3: 2x2 table with misclassified exposure\n\n\n\n\n\n\n\\(D = 1\\)\n\\(D = 0\\)\nTotal\n\n\n\n\n\\(X^\\obs = 1\\)\n\\(a^\\obs\\)\n\\(b^\\obs\\)\n\\(r_1^\\obs\\)\n\n\n\\(X^\\obs = 0\\)\n\\(c^\\obs\\)\n\\(d^\\obs\\)\n\\(r_0^\\obs\\)\n\n\nTotal\n\\(k_1\\)\n\\(k_0\\)\n\\(n\\)\n\n\n\n\n\n\nThe misclassification of exposure is nondifferential when the sensitivity and specificity of the exposure test are the same in cases and controls. In other words, we need \\[\n  \\Pr\\bigl(\\Tplus_X \\,\\big|\\, \\Xplus, D = d\\bigr)\n  = \\Pr\\bigl(\\Tplus_X \\,\\big|\\, \\Xplus\\bigr)\n\\] and \\[\n  \\Pr\\bigl(\\Tminus_X \\,\\big|\\, \\Xminus, D = d\\bigr)\n  = \\Pr\\bigl(\\Tminus_X \\,\\big|\\, \\Xminus\\bigr)\n\\] for all possible values \\(d\\) of disease status \\(D\\). As with disease, it is critical that nondifferential misclassification of exposure is defined through the sensitivity and specificity of the test for exposure. When there is an association between exposure and disease, nondifferential misclassification can produce a PPV and NPV that differ between cases and controls because these predictive values depend on the prevalence of exposure in addition to the sensitivity and specificity of the test (D. J. Newell 1962; Buell and Dunn Jr 1964).\nUnder nondifferential misclassification, the probability that a case tests positive for exposure is \\[\n    \\pi_1^\\obs\n    = (1 - \\spec_X) + (\\sens_X + \\spec_X - 1) \\pi_1\n\\] where \\(\\pi_1\\) is the true prevalence of exposure among cases. Similarly, the probability that a control tests positive for exposure is \\[\n  \\pi_0^\\obs\n  = (1 - \\spec_X) + (\\sens_X + \\spec_X - 1) p_0\n\\] where \\(\\pi_0\\) is the true prevalence of exposure in controls. The misclassified difference in exposure prevalences is \\[\n  \\pi_1^\\obs - \\pi_0^\\obs\n  = (\\sens_X + \\spec_X - 1) (\\pi_1 - \\pi_0).\n\\] When \\(\\sens_X + \\spec_X &gt; 1\\) (as is true of any useful test for exposure), the null hypothesis that \\(\\pi_1^\\obs = \\pi_0^\\obs\\) is equivalent to the null hypothesis that \\(\\pi_1 = \\pi_0\\), which is equivalent to the null hypothesis that exposure and disease are independent (see Section 7.2.1). Therefore, any test of the independence of \\(X^\\obs\\) and \\(D\\) has the correct significance level under the null hypothesis that \\(X\\) and \\(D\\) are independent (Bross 1954).4\nLike nondifferential misclassification of disease, nondifferential misclassification of exposure reduces the power of the hypothesis test that exposure and disease are independent (Bross 1954; Rubin, Rosenbaum, and Cobb 1956; Rogot 1961). The maximum likelihood estimates \\(\\hat{\\pi}_1^\\obs = a^\\obs / k_1\\) and \\(\\hat{\\pi}_0^\\obs = b^\\obs / k_0\\) are unbiased. The Pearson chi-squared statistic based on the misclassified data in Table~[tab:2by2obsX] can be rewritten \\[\n  \\chisqPobs\n  = \\frac{(\\hat{\\pi}_1^\\obs - \\hat{\\pi}_0^\\obs)^2}\n    {\\hat{\\pi}^\\obs (1 - \\hat{\\pi}^\\obs) \\Bigl(\\frac{1}{k_1} + \\frac{1}{k_0}\\Bigr)}.\n\\tag{8.3}\\] where \\(\\hat{\\pi}^\\obs = k_1^\\obs / n\\) is the misclassified estimate of the marginal prevalence of exposure among the study participants. Let \\(\\varphi_X\\) be the proportion of the sample that consists of cases, and let \\(K_X = \\sens_X + \\spec_X - 1\\), so \\(K_X \\in (0, 1)\\) whenever we have a useful but imperfect test for exposure. When both \\(k_1 = \\varphi_X n\\) and \\(k_0 = (1 - \\varphi_X) n\\) are large, the numerator on the right-hand side of Equation 8.3 is approximately \\[\n  \\E(\\hat{\\pi}_1^\\obs - \\hat{\\pi}_0^\\obs)^2\n  = K_X^2 (\\pi_1 - \\pi_0)^2,\n\\] and \\[\n    \\hat{\\pi}^\\obs\n    \\approx \\E(\\hat{\\pi}^\\obs)\n    = (1 - \\spec_X) + K_X \\pi\n\\] where \\[\n  \\pi = \\varphi_X \\pi_1 + (1 - \\varphi_X) \\pi_0\n\\] is the marginal prevalence of exposure among the study participants, In large samples, \\[\n    \\hat{\\pi}^\\obs (1 - \\hat{\\pi}^\\obs)\n    \\approx \\big(1 - \\spec_X + K_X \\pi\\big) \\big(1 - \\sens_X + K_X (1 - \\pi)\\big).\n\\] For hypothesis testing, nondifferential misclassification of exposure has approximately the same effect as multiplying the sample size \\(n\\) by the effective sample size ratio \\[\n  \\ESSR_X\n  = \\frac{K_X^2 \\pi (1 - \\pi)}{\\big(1 - \\spec_X + K_X \\pi\\big) \\big(1 - \\sens_X + K_X (1 - \\pi)\\big)}\n  \\leq 1\n\\] with equality if and only if \\(\\sens_X = \\spec_X = 1\\). Just like nondifferential misclassification of disease, nondifferential misclassification of exposure acts like reduction in the effective sample size. It preserves the significance level under the null, but it reduces the power of the test away from the null. A similar reduction in power occurs when more complex exposures (such as dietary intakes) are measured with error, requiring larger sample sizes to achieve a given power (Freedman, Schatzkin, and Wax 1990).\nThe curves in Figure 8.1 are the same if we replace the marginal risk of disease \\(p\\) with the marginal prevalence of exposure \\(\\pi\\). The effective sample size ratio \\(\\ESSR_X\\) depends on the specificity more than the sensitivity when \\(\\pi &lt; 0.5\\), and it depends on the sensitivity more than the specificity when \\(\\pi &gt; 0.5\\). While diseases typically (and fortunately) have low risks, exposures can have both low and high prevalences.\nWhen there are more than two levels of exposure, nondifferential misclassification does not always bias a measure of association toward the null for all exposure categories (Walker, Velema, and Robins 1988; Dosemeci, Wacholder, and Lubin 1990; Verkerk and Buitendijk 1992; Correa-Villaseñor et al. 1995). Misclassification causes the risks of disease in different exposure categories to get closer to each other on average. Without loss of generality, suppose that higher exposure is associated with a higher risk of disease. Misclassification of high-exposure individuals to lower-exposure categories can increase the apparent risk of disease in these categories, and misclassification of low-exposure individuals into higher-exposure categories can decrease the apparent risk of disease in these categories. The risk in the highest-exposure category can only go down on average due to misclassification, and the risk in the lowest-exposure category can only go up on average. In both cases, this results in bias toward the null—and these are the only possible cases for a binary exposure. The risk in an intermediate-exposure category can go up or down on average, so some measures of association can be biased away from the null. Despite this exception, bias toward the null remains the most likely outcome of nondifferential misclassification of exposure (Dosemeci, Wacholder, and Lubin 1990; Correa-Villaseñor et al. 1995). However, random variation can produce point estimates closer to, farther from, or across the null compared to the true value of a measure of association.\n\n\n8.1.3 Simultaneous nondifferential misclassification\nAlthough we discussed nondifferential misclassification of disease in the context of a cohort study and nondifferential misclassification of exposure in the context of a case-control study, simultaneously misclassification of \\(X\\) and \\(D\\) can occur in any epidemiologic study. Table 8.4 shows a 2x2 table with \\(X\\) and \\(D\\) both misclassified. The row totals are affected by misclassification of \\(X\\), and the column totals are affected by misclassification of \\(D\\). Only the total sample size \\(n\\) is unaffected.\n\n\n\nTable 8.4: 2x2 table for a study with misclassified exposure and disease\n\n\n\n\n\n\n\\(D^\\obs = 1\\)\n\\(D^\\obs = 0\\)\nTotal\n\n\n\n\n\\(X^\\obs = 1\\)\n\\(a^\\obs\\)\n\\(b^\\obs\\)\n\\(r_1^\\obs\\)\n\n\n\\(X^\\obs = 0\\)\n\\(c^\\obs\\)\n\\(d^\\obs\\)\n\\(r_0^\\obs\\)\n\n\nTotal\n\\(k_1^\\obs\\)\n\\(k_0^\\obs\\)\n\\(n\\)\n\n\n\n\n\n\nThe effects of nondifferential misclassification of both \\(X\\) and \\(D\\) can be derived by imagining that we misclassify one first and then the other. Here, we will consider misclassifying \\(X\\) and then \\(D\\). When we misclassify \\(X\\), we have the equivalent null hypotheses \\[\n  X \\indep D \\iff X^\\obs \\indep D.\n\\] where the symbol \\(\\indep\\) indicates independence (Dawid 1979). When we misclassify \\(D\\) in addition to \\(X\\), we have the equivalent null hypotheses \\[\n  X^\\obs \\indep D \\iff X^\\obs \\indep D^\\obs.\n\\] Therefore, \\[\n  X \\indep D \\iff X^\\obs \\indep D^\\obs\n\\] so a test of the null hypothesis that \\(X\\) and \\(D\\) are independent based on the misclassified data in Table 8.4 has the correct significance level (as long as \\(\\sens_X + \\spec_X &gt; 1\\) and \\(\\sens_D + \\spec_D &gt; 1\\)). However, the power of the test is reduced when we misclassify \\(X\\) and reduced again when we misclassify \\(D\\). Simultaneous misclassification of \\(X\\) and \\(D\\) has approximately the same effect as multiplying the sample size by \\[\n  \\ESSR_X \\ESSR_D \\leq 1\n\\] with equality if and only if \\(\\sens_X = \\spec_X = 1\\) and \\(\\sens_D = \\spec_D = 1\\).",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Internal and External Validity</span>"
    ]
  },
  {
    "objectID": "validity.html#selection-bias",
    "href": "validity.html#selection-bias",
    "title": "8  Internal and External Validity",
    "section": "8.2 Selection bias",
    "text": "8.2 Selection bias\nParticipants in an epidemiologic study can be selected according to exposure (in a cohort study) or according to disease (in a case-control study), but they cannot be selected according to both. In a cohort study, selection must be conditionally independent of disease given exposure so that risks of disease are estimated accurately in all exposure groups. In a case-control study, selection must be conditionally independent of exposure given disease, so the prevalence of exposure is measured accurately among both cases and controls.. Selection according to exposure and disease simultaneously leads to selection bias, which is a threat to the external validity of a study.5 A study with uncontrolled selection bias is neither generalizable nor transportable.\n\n8.2.1 Selection bias in cohort studies\nIn a cohort study, selection bias leads to biased estimates of the risks of disease in exposure groups. Let \\(S\\) indicate selection into the study, so \\(S_i = 1\\) if individual \\(i\\) is selected into the study and \\(S_i = 0\\) otherwise. When only \\(X\\) and \\(D\\) are measured, there is no selection bias in a cohort study if and only if the conditional probability of disease given exposure in sampled individuals equals that in the population: \\[\n  \\Pr(D = 1 \\given{} X = x, S = 1)\n  = \\Pr(D = 1 \\given{} X = x)\n\\tag{8.4}\\] for both \\(x = 1\\) and \\(x = 0\\). By the definition of conditional probability, \\[\n  \\Pr(D = 1 \\given{} X = x, S = 1)\n  = \\frac{\\Pr(D = 1, S = 1 \\given{} X = x)}{\\Pr(S = 1 \\given{} X = x)}.\n\\] Multiplying both sides of Equation 8.4 by \\(\\Pr(S = 1 \\given{} X = x)\\) show that there is no selection bias in a cohort study if and only if \\[\n  \\Pr(D = 1, S = 1 \\given{} X = x)\n  = \\Pr(D = 1 \\given{} X = x) \\Pr(S = 1 \\given{} X = x),\n\\tag{8.5}\\] which means that \\(D\\) and \\(S\\) are conditionally independent given \\(X\\). This condition can be relaxed somewhat if additional covariates are measured. In that case, we only need \\(D\\) and \\(S\\) to be conditionally independent given the measured covariates.\n\n\n8.2.2 Selection bias in case-control studies\nIn a case-control study, selection bias leads to biased estimates of exposure prevalences in cases and controls. When only \\(X\\) and \\(D\\) are measured, there is no selection bias in a case-control study if and only if the conditional probability of exposure given disease in sampled individuals equals that in the underlying population: \\[\n  \\Pr(X = 1 \\given{} D = d)\n  = \\Pr(X = 1 \\given{} D = d, S = 1)\n\\tag{8.6}\\] for \\(d = 1\\) and \\(d = 0\\). By the same argument used for the cohort study, there is no selection bias in a case-control study if and only if \\[\n  \\Pr(X = 1, S = 1 \\given{} D = d)\n  = \\Pr(X = 1 \\given{} D = d) \\Pr(S = 1 \\given{} D = d),\n\\] which means that \\(X\\) and \\(S\\) are conditionally independent given \\(D\\). As with selection bias in a cohort study, this condition can be relaxed if additional covariates are measured.\n\n\n8.2.3 Prospective and retrospective studies\nA prospective study is one in which exposure information is collected and recorded prior to disease onset. A retrospective study is one in which exposure information is collected after the onset of disease. Traditionally, cohort studies were called “prospective studies” and case-control studies were called “retrospective studies”. While this classification is often accurate, it is possible for either design to be prospective or retrospective (Rothman, Greenland, and Lash 2008).\nBecause selection into a retrospective study occurs after disease onset and relevant exposures occur prior to disease onset, retrospective studies are more susceptible to selection bias than prospective studies. Because knowledge of disease occurrence can affect recall or measurement of exposure, retrospective studies are also more susceptible to differential misclassification.\n\n\n8.2.4 Generalizability and transportability\nTo discuss sources of bias in epidemiologic studies, we will use terminology adapted from Dahabreh and Hernán (2019) and illustrated in Figure 8.2. These terms are meant to be consistent with the Consolidated Standards of Reporting Trials (CONSORT) statement (Moher et al. 2001; Altman et al. 2001) as well as the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) statement (Elm et al. 2007; Vandenbroucke et al. 2007). The eligible population is the population of individuals who meet the eligibility criteria for a study—whether or not they are invited to participate or willing to participate. Individuals within the eligible population who are invited to participate are the invited population, and those within the invited population who enroll in the study are the study sample.6 Members of the study sample are called participants. Inferences based on data from the study sample are applied to a target population that could be the eligible population or a population that includes individuals outside the eligible population.\nA study that makes valid inferences for the eligible population has generalizability, and a study that makes valid inferences for a larger or different target population has transportability to that population. Generalizability and transportability live along a spectrum of external validity. Generalizability is typically a prerequisite for transportability, and a generalizable study can be transportable to some target populations but not to others. Qualitative insights (e.g., smoking causes lung cancer) might be generalizable or transportable even when the estimated risks of disease or prevalences of exposure are not. The best way to ensure that the results of a study are widely applicable is to make the eligible population as inclusive as possible within ethical and logistical constraints (Bibbins-Domingo and Helman 2022).\n\n\n\n\n\n\nFigure 8.2: Schematic illustration of the relationship between the study sample, the eligible population, and a target population. A target population can contain all, part, or none of the eligible population.\n\n\n\n\n\n8.2.5 Example: Berkson’s bias\nBerkson (1946) discussed a hypothetical case-control study to test whether cholecystitis (i.e., gallbladder inflammation) is associated with diabetes. The cases are patients who come to the clinic for diabetes treatment. The controls are nondiabetic patients who come to the clinic to get eyeglasses to correct refractive errors, a diagnosis considered to be independent of cholecystitis based on existing knowledge of human biology. The overall population has 10,000,000 individuals. The prevalence of diabetes is 1%, the prevalence of refractive errors is 10%, and the prevalence of cholecystitis is 3%. All three conditions are assumed to occur independently.\nTable 8.5 shows the distribution of cholecystitis among individuals with diabetes and individuals with refractive errors. Eligible cases are individuals with diabetes, and eligible controls are individuals with refractive errors but no diabetes. The prevalence of cholecystitis is \\(3\\%\\) in both groups, so \\(\\pi_1 = \\pi_0 = 0.03\\). It follows that the Pearson chi-squared statistic \\(\\chisqP = 0\\) in the 2x2 table for the eligible population.\n\n\n\nTable 8.5: Cholecystitis among eligible cases and controls in Berkson (1946)\n\n\n\n\n\n\nEligible cases\nEligible controls\nTotal\n\n\n\n\nCholecystitis\n3,000\n29,700\n32,700\n\n\nNo cholecystitis\n97,000\n960,300\n1,057,300\n\n\nTotal\n100,000\n990,000\n1,090,000\n\n\n\n\n\n\nThe study sample consists of patients who visit the clinic for diabetes or refractive errors. Each diagnosis comes with a probability of visiting the clinic in the relevant time period. In one version of the example, diabetes patients visit the clinic with probability \\(0.05\\), refractive error patients visit with probability \\(0.20\\), and cholecystitis patients visit with probability \\(0.15\\). For patients with multiple conditions, “we shall say that these selective probabilities operate independently, as though a person who had two diseases were like [conjoined] twins, each one of whom had one disease, so that the probability of the twins’ coming to the hospital is the probability of either one getting there, but the presence of one disease does not affect the other in any way” (Berkson 1946). For example, a patient with both diabetes and refractive error visits the clinic with probability \\(1 - (1 - 0.05) (1 - 0.20) = 0.24\\). Table 8.6 shows the numbers of individuals in the population and among clinic visitors at each combination of cholecystitis, diabetes, and refractive errors.\n\n\n\nTable 8.6: Combinations of cholecystitis (C), diabetes (D), and refractive errors (R) in Berkson (1946).\n\n\n\n\n\nCondition\nPopulation\nSelection probability\nClinic visitors\n\n\n\n\nNone\n8,642,700\n0\n0\n\n\nC only\n267,300\n0.15\n40,095\n\n\nD only\n87,300\n0.05\n4,365\n\n\nR only\n960,300\n0.20\n192,060\n\n\nC and D\n2,700\n0.1925\n520\n\n\nC and R\n29,700\n0.32\n9,504\n\n\nD and R\n9,700\n0.24\n2,328\n\n\nC, D, and R\n300\n0.354\n106\n\n\nTotal\n10,000,000\n0.0249\n248,978\n\n\n\n\n\n\nTable 8.7 shows the 2x2 table for the study sample. The cases with cholecystitis include the 520 individuals with diabetes and cholecystitis only and the 106 individuals with all three conditions. The cases without cholecystitis include the 4,365 individuals with diabetes only and the 2,328 individuals with diabetes and refractive errors only. In this table, we have \\[\n  \\chisqP = \\frac{208{,}883 (626 \\times 192{,}060 - 6{,}693 \\times 192{,}060)^2}\n    {10{,}130 \\times 198{,}753 \\times 7{,}319 \\times 201{,}564}\n  \\approx 225.447.\n\\] The p-value is \\(5.9 \\times 10^{-51}\\).\n\n\n\nTable 8.7: Cholecystitis among cases and controls in Berkson (1946)\n\n\n\n\n\n\nCases\nControls\nTotal\n\n\n\n\nCholecystitis\n520 + 106 = 626\n9,504\n10,130\n\n\nNo cholecystitis\n4,365 + 2,328 = 6,693\n192,060\n198,753\n\n\nTotal\n7,319\n201,564\n208,883\n\n\n\n\n\n\nIf we ignored selection bias, we would conclude that there is almost certainly an association between cholecystitis and diabetes in the eligible population. The example is constructed with no such association. Because the study sample included only clinic visitors and individuals with multiple conditions (including cholecystitis) were more likely to visit the clinic, selection and exposure (cholecystitis) are not conditionally independent given disease in this example.\n\nR\n\n\n\n\nBerkson.R\n\n## Berkson (1946) example of selection bias\n\n# Pearson chi-squared test for the eligible population (X and D independent)\npoptab &lt;- matrix(c(3000, 97000, 29700, 960300), nrow = 2)\nchisq.test(poptab, correct = FALSE)\n# Fisher's exact test (with confidence limits for odds ratio)\nfisher.test(poptab)\n\n# Pearson chi-squared test for the study sample (X and D not independent)\nsampletab &lt;- matrix(c(626, 6693, 9504, 192060), nrow = 2)\nchisq.test(sampletab, correct = FALSE)\n# Fisher's exact test (with confidence limits for odds ratio)\nfisher.test(sampletab)\n\n\n\n\n\n\n\n\nAltman, Douglas G, Kenneth F Schulz, David Moher, Matthias Egger, Frank Davidoff, Diana Elbourne, Peter C Gøtzsche, Thomas Lang, and CONSORT Group. 2001. “The Revised CONSORT Statement for Reporting Randomized Trials: Explanation and Elaboration.” Annals of Internal Medicine 134 (8): 663–94.\n\n\nBerkson, Joseph. 1946. “Limitations of the Application of Fourfold Table Analysis to Hospital Data.” Biometrics Bulletin 2 (3): 47–53.\n\n\nBibbins-Domingo, Kirsten, and Alex Helman, eds. 2022. Improving Representation in Clinical Trials and Research: Building Research Equity for Women and Underrepresented Groups. National Academies of Sciences, Engineering,; Medicine (National Academies Press).\n\n\nBross, Irwin. 1954. “Misclassification in 2 x 2 Tables.” Biometrics 10 (4): 478–86.\n\n\nBuell, Philip, and John E Dunn Jr. 1964. “The Dilution Effect of Misclassification.” American Journal of Public Health 54 (4): 598–602.\n\n\nCampbell, Donald T. 1957. “Factors Relevant to the Validity of Experiments in Social Settings.” Psychological Bulletin 54 (4): 297.\n\n\nChung, Kai Lai. 2000. A Course in Probability Theory. Third. Elsevier.\n\n\nCopeland, Karen T, Harvey Checkoway, Anthony J McMichael, and Robert H Holbrook. 1977. “Bias Due to Misclassification in the Estimation of Relative Risk.” American Journal of Epidemiology 105 (5): 488–95.\n\n\nCorrea-Villaseñor, Adolfo, Walter F Stewart, Francisco Franco-Marina, and Hui Seacat. 1995. “Bias from Nondifferential Misclassification in Case-Control Studies with Three Exposure Levels.” Epidemiology 6 (3): 276–81.\n\n\nDahabreh, Issa J, and Miguel A Hernán. 2019. “Extending Inferences from a Randomized Trial to a Target Population.” European Journal of Epidemiology 34 (8): 719–22.\n\n\nDawid, A Philip. 1979. “Conditional Independence in Statistical Theory.” Journal of the Royal Statistical Society: Series B (Methodological) 41 (1): 1–15.\n\n\nDosemeci, Mustafa, Sholom Wacholder, and Jay H Lubin. 1990. “Does Nondifferential Misclassification of Exposure Always Bias a True Effect Toward the Null Value?” American Journal of Epidemiology 132 (4): 746–48.\n\n\nElm, Erik von, Douglas G Altman, Matthias Egger, Stuart J Pocock, Peter C Gøtzsche, and Jan P Vandenbroucke. 2007. “The Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) Statement: Guidelines for Reporting Observational Studies.” The Lancet 370 (9596): 1453–57.\n\n\nFreedman, Laurence S, Arthur Schatzkin, and Yohanan Wax. 1990. “The Impact of Dietary Measurement Error on Planning Sample Size Required in a Cohort Study.” American Journal of Epidemiology 132 (6): 1185–95.\n\n\nGoldberg, Judith D. 1975. “The Effects of Misclassification on the Bias in the Difference Between Two Proportions and the Relative Odds in the Fourfold Table.” Journal of the American Statistical Association 70 (351): 561–67.\n\n\nGullen, Warren H, Jacob E Bearman, and Eugene A Johnson. 1968. “Effects of Misclassification in Epidemiologic Studies.” Public Health Reports 83 (11): 914–18.\n\n\nHernán, Miguel Á, Sonia Hernández-Dı́az, and James M Robins. 2004. “A Structural Approach to Selection Bias.” Epidemiology 15 (5): 615–25.\n\n\nMoher, David, Kenneth F Schulz, Douglas G Altman, Matthias Egger, Frank Davidoff, Diana Elbourne, Peter C. Gøtzsche, Thomas Lang, and CONSORT Group. 2001. “The CONSORT Statement: Revised Recommendations for Improving the Quality of Reports of Parallel-Group Randomized Trials.” Annals of Internal Medicine 134 (8): 657–62.\n\n\nNewell, David J. 1962. “Errors in the Interpretation of Errors in Epidemiology.” American Journal of Public Health 52 (11): 1925–28.\n\n\nNewell, DJ. 1963. “Note: Misclassification in 2 x 2 Tables.” Biometrics 19 (1): 187–88.\n\n\nRogot, Eugene. 1961. “A Note on Measurement Errors and Detecting Real Differences.” Journal of the American Statistical Association 56 (294): 314–19.\n\n\nRothman, Kenneth J, Sander Greenland, and Timothy L Lash. 2008. Modern Epidemiology. Lippincott Williams & Wilkins.\n\n\nRubin, Theodore, Joseph Rosenbaum, and Sidney Cobb. 1956. “The Use of Interview Data for the Detection of Associations in Field Studies.” Journal of Chronic Diseases 4 (3): 253–66.\n\n\nSorahan, Tom, and Mark S Gilthorpe. 1994. “Non-Differential Misclassification of Exposure Always Leads to an Underestimate of Risk: An Incorrect Conclusion.” Occupational and Environmental Medicine 51 (12): 839–40.\n\n\nVandenbroucke, Jan P, Erik von Elm, Douglas G Altman, Peter C Gøtzsche, Cynthia D Mulrow, Stuart J Pocock, Charles Poole, James J Schlesselman, Matthias Egger, and Strobe Initiative. 2007. “Strengthening the Reporting of Observational Studies in Epidemiology (STROBE): Explanation and Elaboration.” Annals of Internal Medicine 147 (8): W–163.\n\n\nVerkerk, PH, and SE Buitendijk. 1992. “Non-Differential Underestimation May Cause a Threshold Effect of Exposure to Appear as a Dose-Response Relationship.” Journal of Clinical Epidemiology 45 (5): 543–45.\n\n\nWacholder, Sholom, Patricia Hartge, Jay H Lubin, and Mustafa Dosemeci. 1995. “Non-Differential Misclassification and Bias Towards the Null: A Clarification.” Occupational and Environmental Medicine 52 (8): 557.\n\n\nWalker, Alexander M, Johan P Velema, and James M Robins. 1988. “Analysis of Case-Control Data Derived in Part from Proxy Respondents.” American Journal of Epidemiology 127 (5): 905–14.\n\n\nYland, Jennifer J, Amelia K Wesselink, Timothy L Lash, and Matthew P Fox. 2022. “Misconceptions about the Direction of Bias from Nondifferential Misclassification.” American Journal of Epidemiology 191 (8): 1485–95.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Internal and External Validity</span>"
    ]
  },
  {
    "objectID": "validity.html#footnotes",
    "href": "validity.html#footnotes",
    "title": "8  Internal and External Validity",
    "section": "",
    "text": "These tests are in the top left half of a receiver operating characteristic (ROC) plot from Section 2.3.2, which has \\(1 - \\spec_D\\) is the horizontal axis and \\(\\sens_D\\) is the vertical axis. A test with \\(\\sens_D = 1 - \\spec_D\\) (on the diagonal of the ROC plot) is a useless test. A test with \\(\\sens_D &lt; 1 - \\spec_D\\) (in the bottom right half of an ROC plot) is worse than useless, but it can be redeemed by swapping the definitions of \\(\\Tplus\\) and \\(\\Tminus\\).↩︎\n The continuous mapping theorem says that if a statistic \\(\\hat{\\theta}_n \\rightarrow \\theta\\) in probability and \\(f\\) is a continuous function in a neighborhood of \\(\\theta\\), then \\(f(\\hat{\\theta}_n) \\rightarrow f(\\theta)\\) in probability. Convergence in probability can be replaced with convergence in distribution or convergence almost surely (Chung 2000). Here, the statistic is \\(\\hat{p}_1 - \\hat{p}_0\\), which converges to \\(p_1 - p_0\\), and the function is \\(f(v) = v^2\\).↩︎\n As with a test for disease, a test for exposure with \\(\\sens_X + \\spec_X = 1\\) would be a useless test (on the diagonal of an ROC curve) and a test with \\(\\sens_X + \\spec_X &lt; 1\\) would need to have the definitions of \\(\\Tplus_X\\) and \\(\\Tminus_X\\) reversed.↩︎\n Case control studies typically use the odds ratio \\(\\odds(\\pi_1) / \\odds(\\pi_0)\\), not the difference \\(\\pi_1 - \\pi_0\\), to compare the exposure prevalences in cases and controls. The difference between the prevalences is being used here only to establish that a hypothesis test of the independence of \\(X\\) and \\(D\\) based on misclassified data has the correct significance level.↩︎\n In causal inference, selection bias can threaten both the internal and external validity of a study. It can cause an apparent association between \\(X\\) and \\(D\\) within the study sample that does not represent a causal effect (Hernán, Hernández-Dı́az, and Robins 2004).↩︎\n We avoid use of the term “study population” because it sometimes refers to the study sample and sometimes to the eligible population.↩︎",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Internal and External Validity</span>"
    ]
  },
  {
    "objectID": "cohort.html",
    "href": "cohort.html",
    "title": "9  Measures of Association in Cohort Studies",
    "section": "",
    "text": "9.1 Measures of association\nA measure of association indicates both the direction and magnitude of the difference between two groups, so a point and interval estimate of a measure of association conveys much more information than a hypothesis test (Rothman 1978). We will first consider measures of association in cohort studies, where participants are selected based on exposure and then followed up (prospectively or retrospectively) to measure disease occurrence. As before, let \\(X\\) be a binary treatment or exposure, and let \\(D\\) be a binary disease outcome. We assume that the exposure \\(X_i\\) of person \\(i\\) is fixed by his or her time origin, that our study sample consists of individuals who were at risk of the outcome, and that there was no left truncation (i.e., delayed entry) or right censoring due to loss to follow-up. Our results can be summarized in a 2x2 table as in Table 2.1.\nIdeally, the groups in a cohort study are identical except for exposure so that any difference in the outcomes can be attributed to a causal effect of the difference in exposure. In a clinical trial, this similarity can be achieved by randomizing participants to different levels of exposure. If the number of study participants is large enough, the different groups will be nearly identical in both measured and unmeasured covariates. In an observational study, exposure groups can have differences other than exposure that completely or partially explain the observed differences in outcomes, which is called confounding (Simpson 1951; O. S. Miettinen and Cook 1981). For now, we will not worry about whether the differences between groups are causal effects of treatment or exposure, which is why we call these measures of association.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Measures of Association in Cohort Studies</span>"
    ]
  },
  {
    "objectID": "cohort.html#sec-massociation",
    "href": "cohort.html#sec-massociation",
    "title": "9  Measures of Association in Cohort Studies",
    "section": "",
    "text": "9.1.1 Risk difference\nIf the risk of disease onset in \\((0, t]\\) is \\(p_1\\) in the exposed and \\(p_0\\) in the unexposed, then the risk difference is \\[\n  \\RD = p_1 - p_0,\n\\] which can take any value in \\([-1, 1]\\). When exposure and disease are independent, \\(\\RD = 0\\) because \\(p_1 = p_0\\) (see Section 7.2.1).\nWhen there is no delayed entry or loss to follow-up, the risk (cumulative incidence) in any given time interval \\((0, t]\\) can be estimated using methods for a binomial proportion. Based on the results from Section 3.1.3, the maximum likelihood estimate (MLE) of the risk difference is \\[\n  \\hat{\\RD}\n  = \\hat{p}_1 - \\hat{p}_0\n  = \\frac{a}{r_1} - \\frac{c}{r_0},\n\\tag{9.1}\\] which is defined as long as \\(r_1 &gt; 0\\) and \\(r_0 &gt; 0\\). It is an unbiased estimate of the true \\(\\RD\\).\nBecause the outcomes in the two exposure groups are independent, the estimated variance of \\(\\hat{\\RD}\\) is \\[\n  \\hat{\\Var}(\\hat{\\RD})\n  = \\frac{\\hat{p_1}(1 - \\hat{p_1})}{r_1} + \\frac{\\hat{p_0} (1 - \\hat{p_0})}{r_0}.\n\\tag{9.2}\\] The \\(r_1\\) and \\(r_0\\) in the denominators are sometimes replaced with \\(r_1 - 1\\) and \\(r_0 - 1\\) by analogy with the \\(n - 1\\) in the denominator of the sample variance (Rothman, Greenland, and Lash 2008). The Wald test statistic for the null hypothesis \\(H_0: \\RD = \\RD_0\\) is \\[\n  Z_{\\RD} = \\frac{\\hat{\\RD} - \\RD_0}{\\sqrt{\\hat{\\Var}(\\hat{\\RD})}}.\n\\tag{9.3}\\] In large samples under the null, \\(Z_{\\RD}\\) has a standard normal distribution and \\(Z_{\\RD}^2\\) has a chi-squared distribution with one degree of freedom. The corresponding \\(1 - \\alpha\\) Wald confidence limits for the \\(\\RD\\) are \\[\n  \\hat{\\RD} \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\hat{\\Var}(\\hat{\\RD})}.\n\\tag{9.4}\\] As in Section 3.4, this confidence interval includes all values of \\(\\RD_0\\) such that the Wald test in Equation 9.3 would fail to reject the null hypothesis at significance level \\(\\alpha\\). For a 95% confidence interval, \\(z_{1 - \\frac{\\alpha}{2}} \\approx 1.96\\). When the expected numbers of events are small, this confidence interval can have limits outside \\([-1, 1]\\). Confidence intervals with better performance can be obtained by inverting score or likelihood ratio tests or by using Bayesian methods (Agresti and Min 2005).\n\nR\n\n\n\n\nriskdiff.R\n\n## Risk difference point and interval estimates\n\n# rats data is in the survival package\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)\n?rats               # type q to exit\ndim(rats)\nnames(rats)\n\n# numbers of events and row sums\n# Because follow-up of the rats was complete, risk calculations are valid.\na &lt;- sum(rats$status[rats$rx == 1])\nr1 &lt;- sum(rats$rx == 1)\nc &lt;- with(subset(rats, rx == 0), sum(status))\nr0 &lt;- sum(rats$rx == 0)\n\n# estimated risks of tumor onset in 104 weeks\np1hat &lt;- a / r1\np0hat &lt;- c / r0\n\n# estimated risk difference and its variance\nRDhat &lt;- p1hat - p0hat\nRDvar &lt;- p1hat * (1 - p1hat) / r1 + p0hat * (1 - p0hat) / r0\n\n# Wald hypothesis test that RD = 0 in normal and chi-squared versions\nRDz &lt;- RDhat / sqrt(RDvar)\nRDz\n2 * pnorm(-abs(RDz))\nRDchisq &lt;- RDz^2\nRDchisq\n1 - pchisq(RDchisq, df = 1)\n\n# point estimate and 95% confidence interval for the risk difference\nRDhat\nRDhat + c(-1, 1) * qnorm(0.975) * sqrt(RDvar)\n\n\n\n\n\n\n9.1.2 Risk ratio\nThe risk ratio comparing the exposed to the unexposed is \\[\n  \\RR = \\frac{p_1}{p_0}\n\\] where \\(p_1\\) is the risk in the exposed and \\(p_0\\) is the risk in the unexposed. It is defined whenever \\(p_1 \\geq 0\\) and \\(p_0 &gt; 0\\), and it can take any value in \\([0, \\infty)\\). When exposure and disease are independent, \\(\\RR = 1\\) because \\(p_1 = p_0\\) (see Section 7.2.1). The MLE of the risk ratio is \\[\n  \\hat{\\RR}\n  = \\frac{\\hat{p}_1}{\\hat{p}_0}\n  = \\frac{a / r_1}{c / r_0},\n\\tag{9.5}\\] which is defined as long as \\(c &gt; 0\\) (which implies \\(r_0 &gt; 0\\)) and \\(r_1 &gt; 0\\).\nIt is difficult to estimate the variance of a ratio, so we use a log transformation to get a sum: \\[\n    \\ln \\hat{\\RR} = \\ln \\hat{p}_1 - \\ln \\hat{p_0}.\n\\] Because outcomes in the two groups are independent, we have \\[\n    \\Var(\\ln \\hat{\\RR}) = \\Var(\\ln \\hat{p}_0) + \\Var(\\ln \\hat{p}_1)\n\\] by Equation 1.12. Using the delta method from Section 3.4.1 and the fact that \\(\\frac{\\dif}{\\dif x} \\ln x = \\frac{1}{x}\\), we get the estimated variance \\[\n  \\hat{\\Var}(\\ln \\hat{p}_i)\n  = \\frac{1}{\\hat{p}_i^2} \\Var(\\hat{p}_i)\n  = \\frac{1}{\\hat{p}_i^2} \\times \\frac{\\hat{p}_i (1 - \\hat{p}_i)}{n_i}\n  = \\frac{1}{m_i} - \\frac{1}{r_i}\n\\] where \\(m_1 = a\\) and \\(m_0 = c\\). The Wald test statistic for the null hypothesis \\(H_0: \\RR = \\RR_0\\) is calculated on the log scale: \\[\n  Z_{\\RR}\n  = \\frac{\\ln\\hat{\\RR} - \\ln\\RR_0}{\\sqrt{\\hat{\\Var}(\\ln\\hat{\\RR})}}\n  = \\frac{\\ln\\hat{\\RR} - \\ln\\RR_0}{\\sqrt{\\frac{1}{a} - \\frac{1}{r_1} + \\frac{1}{c} - \\frac{1}{r_0}}}.\n\\tag{9.6}\\] Because \\(a \\leq r_1\\), we have \\(1 / a \\geq 1 / r_1\\) with equality only if \\(a = r_1\\). Similarly, \\(1 / c \\geq 1 / r_0\\) with equality only if \\(c = r_0\\). In large samples under the null, \\(Z_{\\RR}\\) has a standard normal distribution and \\(Z_{\\RR}^2\\) has a chi-squared distribution with one degree of freedom (Katz et al. 1978).\nThe \\(1 - \\alpha\\) Wald confidence limits for \\(\\ln \\RR\\) are \\[\n    \\ln \\hat{\\RR} \\pm\n    z_{1 - \\frac{\\alpha}{2}}\n    \\sqrt{\\frac{1}{a} - \\frac{1}{r_1} + \\frac{1}{c} - \\frac{1}{r_0}}.\n\\tag{9.7}\\] The corresponding log-transformed \\(1 - \\alpha\\) Wald confidence limits for the \\(\\RR\\) are \\[\n    \\hat{\\RR}\n    \\exp\\Biggl(\\pm z_{1 - \\frac{\\alpha}{2}}\n      \\sqrt{\\frac{1}{a} - \\frac{1}{r_1} + \\frac{1}{c} - \\frac{1}{r_0}}\\Biggr)\n\\tag{9.8}\\] As in Section 3.4, this confidence interval includes all possible values \\(\\RR_0\\) such that the Wald test in Equation 9.6 would fail to reject the null hypothesis at significance level \\(\\alpha\\). The log transformation ensures that the confidence interval has appropriate bounds because \\(\\ln\\RR \\in (-\\infty, \\infty)\\) if and only if \\(\\RR \\in (0, \\infty)\\). Confidence intervals with better performance can be obtained by inverting score or likelihood ratio tests or by using Bayesian methods (Agresti and Min 2005).\n\nR\n\n\n\n\nriskratio.R\n\n## Risk ratio point and interval estimates\n\n# rats data is in the survival package\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)\n?rats               # type q to exit\ndim(rats)\nnames(rats)\n\n# numbers of events and row sums\n# Because follow-up of the rats was complete, risk calculations are valid.\na &lt;- sum(rats$status[rats$rx == 1])\nr1 &lt;- sum(rats$rx == 1)\nc &lt;- with(subset(rats, rx == 0), sum(status))\nr0 &lt;- sum(rats$rx == 0)\n\n# estimated risks of tumor onset in 104 weeks\np1hat &lt;- a / r1\np0hat &lt;- c / r0\n\n# estimated risk ratio, log risk ratio, and variance of ln(RRhat)\nRRhat &lt;- p1hat / p0hat\nlnRRhat &lt;- log(RRhat)\nlnRRvar &lt;- 1 / a - 1 / r1 + 1 / c - 1 / r0\n\n# Wald hypothesis test that ln(RR) = 0 in normal and chi-squared versions\nlnRRz &lt;- lnRRhat / sqrt(lnRRvar)\nlnRRz\n2 * pnorm(-abs(lnRRz))\nlnRRchisq &lt;- lnRRz^2\nlnRRchisq\n1 - pchisq(lnRRchisq, df = 1)\n\n# point estimate and 95% confidence interval for log risk ratio\nlnRRci &lt;- lnRRhat + c(-1, 1) * qnorm(0.975) * sqrt(lnRRvar)\nlnRRhat\nlnRRci\n\n# point estimate and 95% confidence interval for risk ratio\nRRhat\nexp(lnRRci)\n\n\n\n\n\n\n9.1.3 Odds ratio\nThe odds corresponding to a probability \\(p\\) is \\[\n  \\odds(p) = \\frac{p}{1 - p}.\n\\] For \\(p \\in [0, 1)\\), the odds can take any value in \\([0, \\infty)\\). The odds function is one-to-one because \\[\n  \\frac{\\odds(p)}{1 + \\odds(p)} = p\n\\] for any \\(p \\in [0, 1)\\). Thus, any probability can be converted to an odds and vice versa.\nThe odds ratio comparing the exposed to the unexposed is \\[\n  \\OR = \\frac{\\odds(p_1)}{\\odds(p_0)}\n  = \\frac{p_1 / (1 - p_1)}{p_0 / (1 - p_0)},\n\\] which is defined when \\(p_1 \\geq 0\\) and \\(p_0 &gt; 0\\). It can take any value in \\([0, \\infty)\\). When exposure and disease are independent, \\(\\OR = 1\\) because \\(\\odds(p_1) = \\odds(p_0)\\). The MLE of the odds ratio is \\[\n  \\hat{\\OR}\n  = \\frac{\\odds(\\hat{p}_1)}{\\odds(\\hat{p}_0)}\n  = \\frac{a / (r_1 - a)}{c / (r_0 - c)}\n  = \\frac{a d}{b c},\n\\tag{9.9}\\] which is defined as long as \\(b &gt; 0\\) (which implies \\(r_1 &gt; 0\\)) and \\(c &gt; 0\\) (which implies \\(r_0 &gt; 0\\)).\nAs with the risk ratio, we use a log transformation to turn the product into a sum for variance calculations: \\[\n  \\ln \\hat{\\OR}\n  = \\logit(\\hat{p}_1) - \\logit(\\hat{p}_0)\n\\] where \\[\n  \\logit(p) = \\ln \\frac{p}{1 - p}\n\\] is the logit transformation from Section 3.4.1. Because outcomes in the two groups are independent, we have \\[\n  \\Var\\big(\\ln \\hat{\\OR}\\big) =\n  \\Var\\bigl(\\logit(\\hat{p}_1)\\bigr) + \\Var\\bigl(\\logit(\\hat{p}_0)\\bigr)\n\\tag{9.10}\\] by Equation 1.12. Because \\[\n    \\logit'(p) = \\frac{1}{p} + \\frac{1}{1 - p} = \\frac{1}{p (1 - p)},\n\\] the delta method gives us the estimated variance \\[\n  \\hat{\\Var}\\bigl(\\logit(\\hat{p}_i)\\bigr)\n  = \\frac{1}{\\hat{p}^2_i (1 - \\hat{p}_i)^2} \\Var(\\hat{p}_i)\n  = \\frac{1}{m_i} + \\frac{1}{r_i - m_i}\n\\] where \\(m_1 = a\\) and \\(m_0 = c\\). The Wald test statistic for the null hypothesis \\(H_0: \\OR = \\OR_0\\) is calculated on the log scale: \\[\n  Z_{\\OR} = \\frac{\\ln\\hat{\\OR} - \\ln\\OR_0}{\\sqrt{\\hat{\\Var}(\\ln\\hat{\\OR})}}.\n\\tag{9.11}\\] In large samples under the null, \\(Z_{\\OR}\\) has a standard normal distribution and \\(Z_{\\OR}^2\\) has a chi-squared distribution with one degree of freedom (Woolf 1955).\nThe \\(1 - \\alpha\\) Wald confidence limits for \\(\\ln \\OR\\) are \\[\n  \\ln \\hat{\\OR}\n  \\pm z_{1 - \\frac{\\alpha}{2}}\n    \\sqrt{\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}}.\n\\tag{9.12}\\] The corresponding log-transformed \\(1 - \\alpha\\) Wald confidence limits for the \\(\\OR\\) are \\[\n  \\hat{\\OR}\n  \\exp\\Biggl(\\pm z_{1 - \\frac{\\alpha}{2}}\n            \\sqrt{\\frac{1}{a} + \\frac{1}{b}\n                  + \\frac{1}{c} + \\frac{1}{d}}\\Biggr)\n\\tag{9.13}\\] As in Section 3.4, this confidence interval includes all possible values \\(\\OR_0\\) such that the Wald test in Equation 9.11 would fail to reject the null hypothesis at significance level \\(\\alpha\\). The log transformation ensures that the confidence interval has appropriate bounds because \\(\\ln\\OR \\in (-\\infty, \\infty)\\) if and only if \\(\\OR \\in (0, \\infty)\\). Confidence intervals with better performance can be obtained by inverting score or likelihood ratio tests or by using Bayesian methods (Agresti and Min 2005).\n\nR\n\n\n\n\noddsratio.R\n\n## Odds ratio point and interval estimates\n\n# rats data is in the survival package\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)\n?rats               # type q to exit\ndim(rats)\nnames(rats)\n\n# numbers of events and row sums\n# Because follow-up of the rats was complete, risk calculations are valid.\na &lt;- sum(rats$status[rats$rx == 1])\nr1 &lt;- sum(rats$rx == 1)\nc &lt;- with(subset(rats, rx == 0), sum(status))\nr0 &lt;- sum(rats$rx == 0)\n\n# estimated risks of tumor onset in 104 weeks\np1hat &lt;- a / r1\np0hat &lt;- c / r0\n\n# estimated odds ratio, log odds ratio, and variance of ln(ORhat)\nb &lt;- r1 - a\nd &lt;- r0 - c\nodds &lt;- function(p) p / (1 - p)\nORhat &lt;- odds(p1hat) / odds(p0hat)\nlnORhat &lt;- log(ORhat)\nlnORvar &lt;- 1 / a + 1 / b + 1 / c + 1 / d\n\n# Wald hypothesis test that ln(OR) = 0 in normal and chi-squared versions\nlnORz &lt;- lnORhat / sqrt(lnORvar)\nlnORz\n2 * pnorm(-abs(lnORz))    # two-tailed test\nlnORchisq &lt;- lnORz^2\nlnORchisq\n1 - pchisq(lnORchisq, df = 1)\n\n# 95% confidence interval for log odds ratio\nlnORci &lt;- lnORhat + c(-1, 1) * qnorm(0.975) * sqrt(lnORvar)\nlnORhat\nlnORci\n\n# point estimate and 95% confidence interval for the odds ratio\nORhat\nexp(lnORci)\n\n\n\n\n\n\n9.1.4 Incidence rate ratio\nIf there is delayed entry or loss to follow-up, we cannot calculate a risk ratio or odds ratio directly but we can still calculate the incidence rate in each group. Recall that the incidence rate is a maximum likelihood for an exponential rate parameter \\(\\lambda\\). If times to disease onset in the each exposure groups have an exponential distribution, then the incidence rate ratio is \\[\n  \\IRR = \\frac{\\lambda_1}{\\lambda_0}\n\\] where \\(\\lambda_1\\) is the rate in the exposed and \\(\\lambda_0\\) is the rate in the unexposed. It is defined whenever \\(\\lambda_1 \\geq 0\\) and \\(\\lambda_0 &gt; 0\\), and it can take any value in \\([0, \\infty)\\). When exposure and disease are independent, \\(\\IRR = 1\\) because \\(\\lambda_1 = \\lambda_0\\). The incidence rate ratio is sometimes called the incidence density ratio (O. Miettinen 1976), and it equals the hazard ratio when the times to events in both exposure groups have exponential distributions.\nLet \\(T_1\\) be the total person-time in the exposed group and \\(T_0\\) be the total person-time in the unexposed group. Based on the results from Section 5.3.2, the MLE of the incidence rate ratio is \\[\n  \\hat{\\IRR}\n  = \\frac{\\hat{\\lambda}_1}{\\hat{\\lambda}_0}\n  = \\frac{a / T_1}{c / T_0}.\n\\tag{9.14}\\] As with the risk ratio and odds ratio, we use a log transformation to get a sum instead of a ratio for variance calculations: \\[\n  \\ln \\hat{\\IRR} = \\ln \\frac{a}{T_1} - \\ln \\frac{c}{T_0}.\n\\] Because outcomes in the two groups are independent, \\[\n  \\Var\\big(\\ln \\hat{\\IRR}\\big) =\n  \\Var\\bigg(\\ln \\frac{a}{T_1}\\bigg) + \\Var\\bigg(\\ln \\frac{c}{T_0}\\bigg).\n\\] By the delta method, the estimated variance of \\(\\ln \\hat{IRR}\\) is \\[\n  \\hat{\\Var}\\bigg(\\ln \\frac{m_i}{T_i}\\bigg) =\n  \\bigg(\\frac{T_i}{m_i}\\bigg)^2 \\Var\\bigg(\\frac{m_i}{T_i}\\bigg)\n  = \\frac{T_i^2}{m_i^2} \\times \\frac{m_i}{T_i^2}\n  = \\frac{1}{m_i}\n\\tag{9.15}\\] where \\(m_1 = a\\) and \\(m_0 = c\\). The Wald test statistic for the null hypothesis \\(H_0: \\IRR = \\IRR_0\\) is calculated on the log scale: \\[\n  Z_{\\IRR}\n  = \\frac{\\ln\\hat{\\IRR} - \\ln\\IRR_0}{\\sqrt{\\hat{\\Var}(\\ln\\hat{\\IRR})}}\n  = \\frac{\\ln\\hat{\\IRR} - \\ln\\IRR_0}{\\sqrt{\\frac{1}{a} + \\frac{1}{c}}}.\n\\tag{9.16}\\] In large samples under the null, \\(Z_{\\IRR}\\) has a standard normal distribution and \\(Z_{\\IRR}^2\\) has a chi-squared distribution with one degree of freedom.\nThe \\(1 - \\alpha\\) Wald confidence limits for the \\(\\ln \\IRR\\) are \\[\n  \\ln \\hat{\\IRR}\n  \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\frac{1}{a} + \\frac{1}{c}}.\n\\] The corresponding log-transformed \\(1 - \\alpha\\) Wald confidence limits for the \\(\\IRR\\) are \\[\n  \\hat{\\IRR}\n  \\exp\\Bigg(\\pm z_{1 - \\frac{\\alpha}{2}}\n            \\sqrt{\\frac{1}{a} + \\frac{1}{c}}\\Bigg).\n\\tag{9.17}\\] This confidence interval includes all possible values \\(\\IRR_0\\) such that the Wald test in Equation 9.16 would fail to reject the null hypothesis at significance level \\(\\alpha\\). The log transformation ensures that the confidence interval has appropriate bounds because \\(\\ln\\IRR \\in (-\\infty, \\infty)\\) if and only if \\(\\IRR \\in (0, \\infty)\\). Confidence intervals with better performance can be obtained by inverting score or likelihood ratio tests or by using Bayesian methods (Agresti 2013).\n\nR\n\n\n\n\nirratio.R\n\n## Incidence rate ratio point and interval estimates\n\n# rats data is in the survival package\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)\n?rats               # type q to exit\ndim(rats)\nnames(rats)\n\n# numbers of events and total rat-weeks\n# The subset() function can be used as an alternative to vector indexing.\na &lt;- sum(rats$status[rats$rx == 1])\nT1 &lt;- sum(rats$time[rats$rx == 1])\nc &lt;- with(subset(rats, rx == 0), sum(status))\nT0 &lt;- with(subset(rats, rx == 0), sum(time))\n\n# estimated incidence rates\n# The units are tumor onsets per rat-week.\nir1hat &lt;- a / T1\nir0hat &lt;- c / T0\n\n# estimated incidence rate ratio, log IRR, and variance of ln(IRRhat)\nIRRhat &lt;- ir1hat / ir0hat\nlnIRRhat &lt;- log(IRRhat)\nlnIRRvar &lt;- 1 / a + 1 / c\n\n# Wald hypothesis test that ln(IRR) = 0 in normal and chi-squared versions\nlnIRRz &lt;- lnIRRhat / sqrt(lnIRRvar)\nlnIRRz\n2 * pnorm(-abs(lnIRRz))    # two-tailed test\nlnIRRchisq &lt;- lnIRRz^2\nlnIRRchisq\n1 - pchisq(lnIRRchisq, df = 1)\n\n# 95% confidence interval for log incidence rate ratio\nlnIRRci &lt;- lnIRRhat + c(-1, 1) * qnorm(0.975) * sqrt(lnIRRvar)\nlnIRRhat\nlnIRRci\n\n# point estimate and 95% confidence interval for the incidence rate ratio\nIRRhat\nexp(lnIRRci)",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Measures of Association in Cohort Studies</span>"
    ]
  },
  {
    "objectID": "cohort.html#generalized-linear-models",
    "href": "cohort.html#generalized-linear-models",
    "title": "9  Measures of Association in Cohort Studies",
    "section": "9.2 Generalized linear models",
    "text": "9.2 Generalized linear models\nThe foundation for all regression models is linear regression. Let \\(Y\\) denote an outcome and let \\(X_1, \\ldots, X_k\\) be a set of predictors or covariates that we wish to use to predict \\(Y\\). Then a linear regression model has the form \\[\n  \\E[Y \\given{} X] = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k,\n\\] or equivalently \\[\n  Y = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k + \\varepsilon,\n\\] where \\(\\varepsilon\\) is an error term with mean zero. The point estimates of the coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\) are obtained by ordinary least squares, which is equivalent to assuming a normal distribution for \\(\\varepsilon\\) and using maximum likelihood estimation. The coefficient \\(\\beta_0\\) is usually called the intercept, and it represents the mean outcome when all predictors equal zero. Each \\(\\beta_j\\) can be interpreted as the change in the expected value of \\(\\E(Y \\given{} X)\\) associated with a one-unit increase in \\(X_j\\) when all other predictors are held constant.\nBecause probabilities are means of indicator variables (see Section 1.3.1), a linear regression model with binary disease outcome \\(D\\) with a binary exposure or treatment \\(X\\) can be used to estimate risks: \\[\n\\Pr(D = 1 \\given{} X = x)\n= \\beta_0 + \\beta_1 x.\n\\] The intercept \\(\\beta_0\\) is the risk of disease among the unexposed (\\(X = 0\\)), and the coefficient \\(\\beta_1\\) is the risk difference \\(\\RD\\): \\[\n  \\RD\n  = (\\beta_0 + \\beta_1) - \\beta_0\n  = \\beta_1.\n\\] However, the likelihood of this model is not quite correct for binary outcomes. The conditional distribution of \\(D\\) given \\(X\\) is a Bernoulli (see Section 1.3.5) distribution, not a normal distribution. Although the point estimates of \\(\\beta_0\\) and \\(\\beta_1\\) are unbiased, hypothesis tests and confidence intervals based on this model are distorted.\nIn a generalized linear model (GLM), we have \\[\n  g\\big(\\E[Y \\given{} X]\\big)\n  = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k\n\\tag{9.18}\\] where \\(g\\) is called the link function and the likelihood is determined by the family of the model, which specifies the conditional distribution of \\(Y\\) given \\(X\\) (Nelder and Wedderburn 1972; McCullagh and Nelder 1989). The left-hand side of Equation 9.18 is called the linear predictor. Linear regression is a GLM from the Gaussian family with the identity link \\(g(y) = y\\). In practice, the family is usually determined by the outcome type (e.g., binomial family for binary outcomes, Gaussian family for continuous outcomes, and Poisson family for count outcomes) and the link is usually determined by the measure of association that we are trying to estimate.\nGLMs from the binomial family can be used to estimate risk differences, risk ratios, odds ratios, and incidence rate ratios using binary outcome data. For some of these measures of association, there are alternative GLMs that can be used if a binomial GLM does not converge. For simplicity, we will focus on two-sample inference, so our models will contain a single binary predictor \\(X\\). The measure of association estimated by the model is determined by the link function. In models with a single binary covariate, the Wald confidence intervals for the measure of association match the traditional confidence intervals from Section 9.1 calculated using the delta method and the binomial variance. However, binomial GLMs allow the calculation of score and likelihood ratio confidence intervals, and they can be used in Bayesian inference. The resulting interval estimates often perform better than Wald confidence intervals in terms of coverage probability and width (Agresti and Min 2005; Agresti 2013).\n\n9.2.1 Risk differences and the identity link\nTo estimate risk differences, a GLM from the binomial family with an identity link \\(g(p) = p\\) can be used. In this model \\[\n  \\Pr(D = 1 \\given{} X = x)\n  = \\beta_0 + \\beta_1 x,\n\\] and a binomial likelihood is used to estimate the coefficients. The risk of disease in the unexposed is \\[\n  p_0 = \\Pr(D = 1 \\given{} X = 0)\n  = \\beta_0,\n\\] and the risk of disease in the exposed is \\[\n  p_1 = \\Pr(D = 1 \\given{} X = 1)\n  = \\beta_0 + \\beta_1.\n\\] The risk difference is \\[\n  \\RD = p_1 - p_0\n  = (\\beta_1 + \\beta_0) - \\beta_0\n  = \\beta_1.\n\\] Thus, \\(\\hat{\\beta}_0\\) is the estimated risk of disease in the unexposed and \\(\\hat{\\beta}_1\\) is the estimated risk difference. Their \\(1 - \\alpha\\) confidence intervals can be interpreted as confidence intervals for the risk in the exposed and the risk difference, respectively. When the confidence interval for \\(\\beta_1\\) is a Wald confidence interval, it will match Equation 9.4. However, the Gaussian GLM can be used to calculate score or likelihood ratio confidence limits or calculate a Bayesian posterior distribution.\nThe estimated risk of disease in the exposed is \\(\\hat{\\beta}_0 + \\hat{\\beta}_1\\), and its estimated variance is \\[\n  \\hat{\\Var}\\bigl(\\hat{\\beta}_0 + \\hat{\\beta}_1\\bigr)\n  = \\hat{\\Var}\\bigl(\\hat{\\beta}_0\\bigr) + \\hat{\\Var}\\bigl(\\hat{\\beta}_1\\bigr)\n    + 2 \\Cov\\bigl(\\hat{\\beta}_0, \\hat{\\beta}_1\\bigr)\n\\tag{9.19}\\] by equation Equation 1.12. Each of the variances and covariances on the right can be obtained from the covariance matrix for the regression coefficients. The corresponding \\(1 - \\alpha\\) Wald confidence interval is \\[\n  \\big(\\hat{\\beta}_0 + \\hat{\\beta}_1\\big)\n  \\pm z_{1 - \\frac{\\alpha}{2}} \\sqrt{\\hat{\\Var}\\bigl({\\hat{\\beta}_0 + \\hat{\\beta}_1}}\\bigr),\n\\tag{9.20}\\] which is accurate in large samples.\nBecause probabilities must live in the interval \\([0, 1]\\), the set of possible combinations of \\(\\beta_0\\) and \\(\\beta_1\\) is constrained. We need both \\(\\beta_0 \\in [0, 1]\\) and \\(\\beta_0 + \\beta_1 X \\in [0, 1]\\) for all values of \\(X\\) in our data. Figure 9.1 shows the possible combinations of \\(\\beta_0\\) and \\(\\beta_1\\) in a binomial GLM with the identity link and a binary \\(X\\) that is an indicator variable for exposure or treatment as in Table 2.1. In this case, the model will fail to converge if \\(a = 0\\) (which implies \\(\\beta_0 + \\beta_1 = 0\\)), \\(b = 0\\) (which implies \\(\\beta_0 + \\beta_1 = 1)\\), \\(c = 0\\) (which implies \\(\\beta_0 = 0\\)), or \\(d = 0\\) (which implies \\(\\beta_1 = 1\\)).\n\n\n\nCode\n\nglm-RD-bounds.R\n\n## Bounds on beta0 and beta1 in a binomial GLM for the risk difference\n# This uses the identity link. We assume a single binary covariate coded 0/1.\n\n# plot\nplot(c(0, 0, 1, 1), c(0, 1, 0, -1), type = \"n\",\n     xlim = c(-0.75, 1.75), ylim = c(-1.25, 1.25),\n     xlab = expression(paste(\"Intercept \", (beta[0]))),\n     ylab = expression(paste(\"Slope \", (beta[1]))))\ngrid()\npolygon(c(0, 0, 1, 1), c(0, 1, 0, -1), border = \"darkgray\", col = \"gray\")\ngrid()\nabline(h = 0)  # x-axis (y = 0)\nabline(v = 0)  # y-axis (x = 0)\n\n# labels\ntext(-0.1, 0.5, expression(beta[0] == 0), srt = 90)\ntext(1.1, -0.5, expression(beta[0] == 1), srt = 90)\ntext(0.4, -0.5, expression(beta[0] + beta[1] == 0), srt = -45)\ntext(0.6, 0.5, expression(beta[0] + beta[1] == 1), srt = -45)\n\n\n\n\n\n\n\n\n\nFigure 9.1: The gray triangle shows possible combinations of the intercept \\(\\beta_0\\) and the slope \\(\\beta_1\\) in a binomial GLM with the identity link and a single binary predictor coded 0/1.\n\n\n\n\n\n\n9.2.1.1 Gaussian GLM with robust variance for risk differences\nWhen a linear binomial GLM fails to converge, a common workaround is to use a Gaussian GLM with an identity link and a robust (sandwich) variance estimator. A normal(\\(p\\), \\(\\sigma^2\\)) distribution has mean \\(p\\), so the regression produces unbiased point estimates of the coefficients. However, it estimates a single variance \\(\\sigma^2\\). The Bernoulli variance changes with \\(p\\), so groups with different risks should have different variances. The \\(\\sigma^2\\) from the linear regression typically produces Wald confidence intervals that are slightly too narrow. The robust variance estimator corrects for this, producing valid Wald confidence intervals. Score and likelihood ratio confidence intervals are not available because the likelihood is not quite correct.\n\nR\n\n\n\n\nriskdiff-GLM.R\n\n## Risk difference estimation with GLMs\n\n# to get help on GLMs in R (type q to exit)\n?glm\n?family\n\n# load packages\n# The rats data is from:\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)     # rats data\nlibrary(sandwich)     # sandwich() for robust variance\nlibrary(lmtest)       # coefci() for Wald confidence intervals\n\n# traditional RD point estimate and Wald 95% confidence interval\n# Because follow-up of the rats was complete, risk calculations are valid.\nr1 &lt;- sum(rats$rx == 1)\na &lt;- with(rats, sum(status[rx == 1]))\nr0 &lt;- sum(rats$rx == 0)\nc &lt;- with(rats, sum(status[rx == 0]))\nrisk1 &lt;- a / r1\nrisk0 &lt;- c / r0\nRDhat &lt;- risk1 - risk0\nRDvar &lt;- risk1 * (1 - risk1) / r1 + risk0 * (1 - risk0) / r0\n\n# point and interval estimates of the RD\nRDhat\nRDhat + c(-1, 1) * qnorm(0.975) * sqrt(RDvar)\n\n# Binomial GLM with identity link for the RD\n# Default binomial link = \"logit\", so we must specify link = \"identity\".\nRDglm &lt;- glm(status ~ rx, family = binomial(link = \"identity\"), data = rats)\nsummary(RDglm)                  # point estimates, p-values, CIs, global tests\nnames(RDglm)                    # parts of the fitted model\ncoef(RDglm)                     # estimated risk (x = 0) and RD\nconfint(RDglm)                  # likelihood ratio CIs (better)\ncoefci(RDglm)                   # Wald CI for RD matches calculation above\n\n# Gaussian (normal) GLM with identity link and robust variance for the RD\nRDglm2 &lt;- glm(status ~ rx, data = rats)\ncoef(RDglm2)                    # point estimates match binomial GLM above\nconfint(RDglm2)                 # likelihood ratio CI too narrow\ncoefci(RDglm2)                  # Wald CI also too narrow\ncoefci(RDglm2, vcov = sandwich) # robust Wald CIs match binomial GLM above\n\n# point and interval estimates for risk in the exposed\n# The vector c(1, 1) represents 1 * beta0 + 1 * beta1,\n# and as.numeric() is used to return a number instead of a matrix.\np1hat &lt;- sum(coef(RDglm))\np1var &lt;- as.numeric(c(1, 1) %*% vcov(RDglm) %*% c(1, 1))\np1hat\np1hat + c(-1, 1) * qnorm(0.975) * sqrt(p1var)\n\n# Gaussian GLM with robust variance\nsum(coef(RDglm2))                         # matches p1hat above\nc(1, 1) %*% sandwich(RDglm2) %*% c(1, 1)  # matches p1var above\n\n\n\n\n\n\n\n9.2.2 Risk ratios and the log link\nA binomial GLM with a log link \\(g(p) = \\ln p\\) is called a log-binomial regression model. In this model \\[\n  \\ln\\bigl(\\Pr(D = 1 \\given{} X = x)\\bigr) = \\beta_0 + \\beta_1 x.\n\\] The risk of disease in the unexposed is \\[\n  p_0 = \\exp(\\beta_0),\n\\] and the risk of diseased in the exposed is \\[\n  p_1 = \\exp(\\beta_0 + \\beta_1).\n\\] The risk ratio comparing the exposed to the unexposed is \\[\n  \\RR\n  = \\frac{e^{\\beta_0 + \\beta_1}}{e^{\\beta_0}}\n  = e^{\\beta_1}.\n\\] Thus, \\(\\exp(\\hat{\\beta}_0)\\) is the estimated risk of disease in the unexposed and \\(\\exp(\\hat{\\beta}_1)\\) is the estimated risk ratio comparing the exposed to the unexposed. If the \\(1 - \\alpha\\) confidence interval for \\(\\beta_i\\) is \\((\\ell_i, u_i)\\), then \\((e^{\\ell_i}, e^{u_i})\\) is the corresponding confidence interval for the risk of disease in the unexposed (\\(i = 0\\)) or the risk ratio (\\(i = 1\\)). When the confidence interval for \\(\\beta_1\\) is a Wald confidence interval, it will match the confidence interval for \\(\\ln \\RR\\) in Equation 9.7 and the corresponding confidence interval for the \\(\\RR\\) in Equation 9.8. However, the log-binomial GLM can be used to calculate score or likelihood ratio confidence limits or a Bayesian posterior distribution.\nThe estimated risk of disease in the exposed is \\(\\exp\\bigl(\\hat{\\beta}_0 + \\hat{\\beta}\\bigr)\\). The estimated variance of \\(\\hat{\\beta}_0 + \\hat{\\beta}_1\\) is given in Equation 9.19, and \\(1 - \\alpha\\) confidence limits for the sum are given in Equation 9.20. The corresponding confidence limits for the risk in the exposed are \\[\n  e^{\\big(\\hat{\\beta}_0 + \\hat{\\beta}_1\\big)\n    \\pm z_{1 - \\frac{\\alpha}{2}}\n      \\sqrt{\\hat{\\Var}\\bigl({\\hat{\\beta}_0 + \\hat{\\beta}_1}}\\bigr)}\n\\tag{9.21}\\] which is accurate in large samples. It is not possible to get score or likelihood ratio confidence intervals for the risk in the exposed without reparameterizing the model, but Bayesian methods can be used to calculate credible intervals.\nAs in the binomial GLM for risk differences, not all combinations of \\(\\beta_0\\) and \\(\\beta_1\\) produce probabilities in \\([0, 1]\\). the parameter space for log-binomial regression is bounded in the sense that not all combinations of \\(\\beta_0\\) and \\(\\beta_1\\) are possible. We get a probability greater than one whenever \\(\\beta_0 + \\beta_1 X &gt; 0\\). When the maximum likelihood estimate \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) is close to this boundary for some values of \\(X\\) in the data, the model can fail to converge. In some cases, this problem can be solved by reparameterizing the model to put the maximum farther away from the boundary of the new parameter space or by using more sophisticated numerical methods to maximize the likelihood (Williamson, Eliasziw, and Fick 2013). Figure 9.2 shows the possible combinations of \\(\\beta_0\\) and \\(\\beta_1\\) in a binomial GLM with the identity link and a binary predictor \\(X\\) that is an indicator variable for exposure or treatment as in Table 2.1. In this case, the model will not converge if \\(a = 0\\) (which implies \\(\\beta_0 + \\beta_1 = -\\infty\\)), \\(b = 0\\) (which implies \\(\\beta_0 + \\beta_1 = 0\\)), \\(c = 0\\) (which implies \\(\\beta_0 = -\\infty\\)) or \\(d = 0\\) (which implies \\(\\beta_0 = 0\\)).\n\n\n\nCode\n\nglm-RR-bounds.R\n\n## Bounds on beta0 and beta1 in a binomial GLM for the risk ratio\n# This uses the log link, and we assume a single binary covariate coded 0/1.\n\n# plot\nplot(c(0, 0, -2, -2), c(0, -2, -2, 2), type = \"n\",\n     xlim = c(-1.5, 1), ylim = c(-1.5, 1),\n     xlab = expression(paste(\"Intercept \", (beta[0]))),\n     ylab = expression(paste(\"Slope \", (beta[1]))))\ngrid()\npolygon(c(0, 0, -2, -2), c(0, -2, -2, 2),\n        border = \"darkgray\", col = \"gray\")\ngrid()\nabline(h = 0)  # x-axis (y = 0)\nabline(v = 0)  # y-axis (x = 0)\n\n# labels\ntext(0.1, -0.75, expression(beta[0] == 0), srt = 90)\ntext(-0.5, 0.6, expression(beta[0] + beta[1] == 0), srt = -45)\narrows(-1.2, 0.2, -1.5, 0.2, code = 2, length = 0.1)\ntext(-0.95, 0.2, expression(beta[0] %-&gt;% -infinity))\narrows(-1.3, -1.3, -1.5, -1.5, code = 2, length = 0.1)\ntext(-1.1, -1.2, expression(beta[0] + beta[1] %-&gt;% -infinity))\narrows(-0.2, -1.2, -0.2, -1.5, code = 2, length = 0.1)\ntext(-0.2, -1.1, expression(beta[1] %-&gt;% -infinity))\n\n\n\n\n\n\n\n\n\nFigure 9.2: The gray area shows the possible combinations of the intercept \\(\\beta_0\\) and the slope \\(\\beta_1\\) in a binomial GLM with the log link and a single binary predictor coded 0/1.\n\n\n\n\n\n\n9.2.2.1 Poisson GLM with robust variance for risk ratios\nWhen a log-binomial GLM fails to converge, a common workaround is to use a Poisson GLM with a log link and a robust (sandwich) variance (Zou 2004; Naimi and Whitcomb 2020). The Poisson GLM with a log link has no constraints on \\(\\beta_0\\) and \\(\\beta_1\\) because a Poisson mean can be any positive number. Because the mean of both the Poisson(\\(p\\)) and Bernoulli(\\(p\\)) distributions equal \\(p\\), the Poisson GLM produces unbiased point estimates of the coefficients. However, the variance of a Poisson(\\(p\\)) distribution is \\(p\\), which is larger than the Bernoulli(\\(p\\)) variance of \\(p (1 - p)\\). Zou (2004) showed that, in a Poisson GLM with a log link and a single binary covariate, the robust (sandwich) variance equals the delta method variance of \\(\\ln \\RR\\). Point estimates and Wald confidence limits for the coefficients, the risk in the unexposed and the exposed, and the risk ratio can be obtained in the same way as for the log-binomial model.\n\nR\n\n\n\n\nriskratio-GLM.R\n\n## Risk ratio estimation with GLMs\n\n# to get help on GLMs in R (type q to exit)\n?glm\n?family\n\n# rats data is in the survival package\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)     # rats data\nlibrary(sandwich)     # sandwich() for robust variance\nlibrary(lmtest)       # coefci() for Wald confidence intervals\n\n# traditional RR point estimate and Wald 95% confidence interval\n# Because follow-up of the rats was complete, risk calculations are valid.\nr1 &lt;- sum(rats$rx == 1)\na &lt;- with(rats, sum(status[rx == 1]))\nr0 &lt;- sum(rats$rx == 0)\nc &lt;- with(rats, sum(status[rx == 0]))\nrisk1 &lt;- a / r1\nrisk0 &lt;- c / r0\nRRhat &lt;- (a / r1) / (c / r0)\nlnRRvar &lt;- 1 / a - 1 / r1 + 1 / c - 1 / r0\nlnRRci &lt;- log(RRhat) + c(-1, 1) * qnorm(.975) * sqrt(lnRRvar)\n\n# point and interval estimates of the RR\nRRhat\nexp(lnRRci)\n\n# binomial GLM with log link (log-binomial model) for the RR\n# Default binomial link = \"logit\", so we must specify link = \"log\".\nRRglm &lt;- glm(status ~ rx, family = binomial(link = \"log\"), data = rats)\nsummary(RRglm)                  # point estimates, p-values, CIs, global tests\nnames(RRglm)                    # parts of the fitted model\nexp(coef(RRglm))                # estimated risk (x = 0) and RR\nexp(confint(RRglm))             # likelihood ratio CIs (better)\nexp(coefci(RRglm))              # log-transformed Wald CI for RD matches above\n\n# Poisson GLM with log link and sandwich variance for the RR\n# The log link is the default for the Poisson family.\nRRglm2 &lt;- glm(status ~ rx, family = poisson(link = \"log\"), data = rats)\nexp(coef(RRglm2))               # point estimates match binomial GLM above\nexp(confint(RRglm2))            # likelihood ratio CI too wide\nexp(coefci(RRglm2))             # Wald CI also too wide\nexp(coefci(RRglm2, vcov = sandwich))  # robust Wald CI matches log binomial\n\n# point and interval estimates for risk in the exposed\n# The vector c(1, 1) represents 1 * beta0 + 1 * beta1,\n# and as.numeric() is used to return a number instead of a matrix.\np1hat &lt;- exp(sum(coef(RRglm)))\nlnp1var &lt;- as.numeric(c(1, 1) %*% vcov(RRglm) %*% c(1, 1))\np1hat\np1hat * exp(c(-1, 1) * qnorm(0.975) * sqrt(lnp1var))\n\n# Poisson GLM with robust variance\nexp(sum(coef(RRglm2)))                    # matches p1hat above\nc(1, 1) %*% sandwich(RRglm2) %*% c(1, 1)  # matches lnp1var above\n\n\n\n\n\n\n\n9.2.3 Odds ratios and the logit link\nA binomial GLM with a logit link \\(g(p) = \\logit(p)\\) is called a logistic regression model, which was originally developed by Berkson (1944) and later refined by Cox (1958). In this model, \\[\n  \\logit\\bigl(\\Pr(D = 1 \\given{} X = x)\\bigr) = \\beta_0 + \\beta_1 x.\n\\tag{9.22}\\] The odds of disease in the unexposed is \\[\n  \\odds(p_0) = e^{\\beta_0},\n\\] and the odds of diseased in the exposed is \\[\n  \\odds(p_1) = e^{\\beta_0 + \\beta_1}.\n\\] The odds ratio comparing the exposed to the unexposed is \\[\n  \\OR = \\frac{e^{\\beta_0 + \\beta_1}}{e^{\\beta_0}} = e^{\\beta_1}.\n\\] Thus, \\(\\exp(\\hat{\\beta}_0)\\) is the estimated odds of disease in the unexposed and \\(\\exp(\\hat{\\beta}_1)\\) is the estimated odds ratio comparing the exposed to the unexposed. If the \\(1 - \\alpha\\) confidence interval for \\(\\beta_i\\) is \\((\\ell_i, u_i)\\), then \\((e^{\\ell_i}, e^{u_i})\\) is the corresponding confidence interval for the odds of disease in the unexposed (\\(i = 0\\)) or the odds ratio (\\(i = 1\\)). When the confidence interval for \\(\\beta_1\\) is a Wald confidence interval, it will match the confidence interval for \\(\\ln \\OR\\) in Equation 9.12 and the corresponding confidence interval for the \\(\\OR\\) in Equation 9.13. However, the logistic GLM can be used to calculate score or likelihood ratio confidence limits or a Bayesian posterior distribution.\nThe estimated odds of disease in the exposed is \\(\\exp\\bigl(\\hat{\\beta}_0 + \\hat{\\beta}\\bigr)\\). The estimated variance of \\(\\hat{\\beta}_0 + \\hat{\\beta}_1\\) is given in Equation 9.19, and \\(1 - \\alpha\\) confidence limits for the sum are given in Equation 9.20. The corresponding confidence limits for the odds ratio in the exposed are given in Equation 9.21, which is accurate in large samples. It is not possible to get score or likelihood ratio confidence intervals for the odds in the exposed without reparameterizing the model, but Bayesian methods can be used to calculate credible intervals.\nUnlike the GLMs for the risk difference and risk ratio, there are no restrictions on \\(\\beta_0\\) and \\(\\beta_1\\) in a logistic regression model. Thus, it is the most numerically stable of the three regression models. However, with a single predictor \\(X\\) that is an indicator of treatment or exposure as in Table 2.1, it will fail to converge if \\(a = 0\\) (which implies \\(\\beta_0 + \\beta_1 = -\\infty\\)), \\(b = 0\\) (which implies \\(\\beta_0 + \\beta_1 = \\infty\\)), \\(c = 0\\) (which implies \\(\\beta_0 = \\infty\\)), or \\(d = 0\\) (which implies \\(\\beta_0 = -\\infty\\)).\nAny estimate of the odds of disease can be used to obtain an estimate of the risk of disease using the inverse function for the logit transformation, which is the expit or logistic function from Equation 3.19. The name logistic regression comes from the fact that the logistic function is used to get a probability from the corresponding odds(Berkson 1944). In the model from Equation 9.22, the risk of disease in the unexposed is \\[\n  \\expit\\bigl(e^{\\beta_0}\\bigr)\n  = \\frac{1}{1 + e^{-(e^\\beta_0)}},\n\\] and the risk of disease in the exposed is \\[\n  \\expit\\bigl(e^{\\beta_0 + \\beta_1}\\bigr)\n  = \\frac{1}{1 + e^{-(e^{\\beta_0 + \\beta_1})}}.\n\\] Point estimates of these risks can be obtained by replacing the unknown \\(\\beta_0\\) and \\(\\beta_1\\) with their maximum likelihood estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), and these estimated risks can be used to obtain point estimates of the risk ratio and risk difference. Confidence limits for the risks in the exposed and unexposed can be obtained by applying the logistic function to the confidence limits for the corresponding odds. Because the same odds ratio can occur with many different values of the risk difference or risk ratio, confidence limits for the \\(\\RD\\) or \\(\\RR\\) cannot be obtained directly from the confidence limits for the \\(\\OR\\).\nBecause the risks of disease in the exposed and unexposed can be calculated using the logistic regression model, we can also get point estimates of the risk difference and risk ratio. Logit-transformed Wald confidence limits for the risk difference or risk ratio can be obtained using a multivariable version of the delta method from Section 3.4.1 and the covariance matrix from the fitted logistic regression model. Similarly, the binomial GLM for the risk difference in Section 9.2.1 could be used to obtain point and interval estimates of the risk ratio or odds ratio, and the log-binomial for the risk ratio in Section 9.2.2 could be used to obtain point and interval estimates of the risk difference or odds ratio. Although it is more convenient to use a regression model that expresses your preferred measure of association in terms of a single coefficient, the measure of association does not dictate the choice of regression model or vice versa.\n\nR\n\n\n\n\noddsratio-GLM.R\n\n## Odds ratio estimation with GLMs\n\n# to get help on GLMs in R (type q to exit)\n?glm\n?family\n\n# rats data is in the survival package\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)     # rats data\nlibrary(sandwich)     # sandwich() for robust variance\nlibrary(lmtest)       # coefci() for Wald confidence intervals\n\n# traditional OR point estimate and Wald 95% confidence interval\n# Because follow-up of the rats was complete, risk calculations are valid.\nr1 &lt;- sum(rats$rx == 1)\na &lt;- with(rats, sum(status[rx == 1]))\nb &lt;- r1 - a\nr0 &lt;- sum(rats$rx == 0)\nc &lt;- with(rats, sum(status[rx == 0]))\nd &lt;- r0 - c\nodds1 &lt;- a / b\nodds0 &lt;- c / d\nORhat &lt;- odds1 / odds0\nlnORvar &lt;- 1 / a + 1 / b + 1 / c + 1 / d\nlnORci &lt;- log(ORhat) + c(-1, 1) * qnorm(.975) * sqrt(lnORvar)\n\n# point and interval estimates of the OR\nORhat\nexp(lnORci)\n\n# binomial GLM with logit link for the OR\n# Default binomial link is \"logit\", so it does not need to be specified.\nORglm &lt;- glm(status ~ rx, family =  binomial(), data = rats)\nsummary(ORglm)                  # point estimates, p-values, CIs, global tests\nnames(ORglm)                    # parts of the fitted model\nexp(coef(ORglm))                # estimated odds (x = 0) and OR\nexp(confint(ORglm))             # likelihood ratio CIs (better)\nexp(coefci(ORglm))              # log-transformed Wald CI matches above\n\n# point and interval estimates for risk in the exposed\n# The vector c(1, 1) represents 1 * beta0 + 1 * beta1,\n# and as.numeric() is used to return a number instead of a matrix.\nodds1hat &lt;- exp(sum(coef(ORglm)))\nlnodds1var &lt;- as.numeric(c(1, 1) %*% vcov(ORglm) %*% c(1, 1))\nodds1hat\nodds1hat * exp(c(-1, 1) * qnorm(0.975) * sqrt(lnodds1var))\n\n# estimates of risks, risk difference, and risk ratio based on odds\nexpit &lt;- function(v) 1 / (1 + exp(-v))    # logistic function\nbeta0 &lt;- coef(ORglm)[\"(Intercept)\"]\nbeta1 &lt;- coef(ORglm)[\"rx\"]\n# risk in exposed\nrisk1 &lt;- a / r1\nrisk1\nexpit(beta0 + beta1)\n# risk in unexposed\nrisk0 &lt;- c / r0\nrisk0\nexpit(beta0)\n# risk difference\nRDhat &lt;- risk1 - risk0\nRDhat\nexpit(beta0 + beta1) - expit(beta0)\n# risk ratio\nRRhat &lt;- risk1 / risk0\nRRhat\nexpit(beta0 + beta1) / expit(beta0)\n\n\n\n\n\n\n9.2.4 Cumulative hazard ratio and the complementary log-log link*\nIf \\(H_1(t)\\) is the cumulative hazard function in the exposed and \\(H_0(t)\\) is the cumulative hazard function in the unexposed (see Section 5.2.3), then \\[\n  H_1(t) = -\\ln(1 - p_1)\n\\] and \\[\n  H_0(t) = -\\ln(1 - p_0).\n\\] The cumulative hazard ratio is \\[\n  \\cHR(t)\n  = \\frac{H_1(t)}{H_0(t)}\n  = \\frac{-\\ln(1 - p_1)}{-\\ln(1 - p_0)}.\n\\tag{9.23}\\] If the instantaneous hazard ratio \\(h_1(t) / h_0(t)\\) is constant in time, then this is equal to the cumulative hazard ratio. If the unexposed have exponential times to events, then \\(h_0(t) = \\lambda_0\\) and the cumulative hazard ratio equals \\[\n  \\frac{H_1(t)}{H_0(t)}\n  = \\frac{\\int_0^t h_1(u) \\,\\dif u}{\\lambda_0 t}\n  = \\frac{1}{t} \\int_0^t \\frac{h_1(u)}{\\lambda_0} \\,\\dif u,\n\\] which is the average instantaneous hazard ratio over \\((0, t]\\).\nIf we take logarithms on both sides of Equation 9.23, we get \\[\n  \\ln\\bigl(-\\ln(1 - p_1)\\bigr)\n  = \\ln(\\cHR(t)) + \\ln\\bigl(-\\ln(1 - p_0)\\bigr).\n\\] The function of \\(p_1\\) and \\(p_0\\) in this equation is the complementary log-log function \\[\n  \\cloglog(p) = \\ln(-\\ln (1 - p)),\n\\] which has the inverse function \\[\n  \\cloglog^{-1}(v) = 1 - e^{-(e^v)}.\n\\] If we have data with no delayed entry or loss to follow-up, the complementary log-log link function allows us to estimate the cumulative hazard ratio using a binomial GLM.\nIn a binomial GLM with the complementary log-log link, \\[\n  \\cloglog\\bigl(\\Pr(D = 1 \\given{} X = x)\\bigr) = \\beta_0 + \\beta_1 x.\n\\tag{9.24}\\] The cumulative hazard of disease in the unexposed (\\(x = 0\\)) is \\(e^{\\beta_0}\\), so their risk of disease is \\[\n  \\cloglog^{-1}(\\beta_0)\n  = 1 - e^{-(e^{\\beta_0})}.\n\\] The cumulative hazard in the exposed (\\(x = 1\\)) is \\(e^{\\beta_0 + \\beta_1}\\), so their risk of disease is \\[\n  \\cloglog^{-1}(\\beta_0 + \\beta_1)\n  = 1 - e^{-(e^{\\beta_0 + \\beta_1})}.\n\\] The cumulative hazard ratio comparing the exposed to the unexposed is \\[\n  \\frac{e^{\\beta_0 + \\beta_1}}{e^{\\beta_0}}\n  = e^{\\beta_1}\n\\] Point estimates of these quantities can be obtained by replacing the unknown \\(\\beta_0\\) and \\(\\beta_1\\) with their maximum likelihood estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). The \\(1 - \\alpha\\) confidence limits for the cumulative hazard in the unexposed can be obtained by exponentiating the confidence limits for \\(\\beta_0\\), and confidence limits for their risk of disease can be obtained by applying \\(\\cloglog^{-1}\\) to the confidence limits for \\(\\beta_0\\). Confidence limits for the cumulative hazard ratio can be obtained by applying \\(\\cloglog^{-1}\\) to the \\(1 - \\alpha\\) confidence limits for \\(\\beta_1\\).\nBecause risks can be estimated for both exposure groups, we can also calculate point estimates of the risk difference, risk ratio, and odds ratio. Complementary log-log-transformed Wald confidence limits for each of these measures of association can be calculated using the multivariable version of the delta method from Section 3.4.1 and the covariance matrix from the fitted model.\nThe binomial GLM for the cumulative hazard ratio in Equation 9.24 assumes complete follow-up of all participants in the interval \\((0, t]\\) over which the risks \\(p_1\\) and \\(p_0\\) are defined, so it cannot be used if there is delayed entry or loss to follow-up. When there is delayed entry or loss to follow-up, methods from survival analysis are needed to estimate the hazard ratio.\n\nR\n\n\n\n\ncumhazratio-GLM.R\n\n## Cumulative hazard ratio estimation with a GLM\n\n# rats data is in the survival package\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)     # rats data\nlibrary(lmtest)       # coefci() for Wald confidence intervals\n\n# numbers of events and total rat-weeks\n# The subset() function can be used as an alternative to vector indexing.\na &lt;- sum(rats$status[rats$rx == 1])\nT1 &lt;- sum(rats$time[rats$rx == 1])\nc &lt;- with(subset(rats, rx == 0), sum(status))\nT0 &lt;- with(subset(rats, rx == 0), sum(time))\nir1hat &lt;- a / T1\nir0hat &lt;- c / T0\nIRRhat &lt;- ir1hat / ir0hat\nlnIRRvar &lt;- 1 / a + 1 / c\nlnIRRci &lt;- log(IRRhat) + c(-1, 1) * qnorm(0.975) * sqrt(lnIRRvar)\n\n# point and interval estimate of the IRR\nIRRhat\nexp(lnIRRci)\n\n# binomial GLM with complementary log-log link\n# The cumulative hazard ratio equals the hazard ratio if the HR is constant.\nHRglm &lt;- glm(status ~ rx, family = binomial(link = \"cloglog\"), data = rats)\nsummary(HRglm)                  # point estimates, p-values, CIs, global tests\nnames(HRglm)                    # parts of the fitted model\nexp(coef(HRglm))                # cumulative hazard (x = 0) and HR\nexp(confint(HRglm))             # likelihood ratio CIs (better)\nexp(coefci(HRglm))              # log-transformed Wald CI for RD matches above\n\n# estimates of risks, RR, and RD based on log cumulative hazard\ninvcloglog &lt;- function(v) 1 - exp(-exp(v))  # inverse of complementary log-log\nbeta0 &lt;- coef(HRglm)[\"(Intercept)\"]\nbeta1 &lt;- coef(HRglm)[\"rx\"]\n# risk in exposed\nrisk1 &lt;- a / sum(rats$rx == 1)\nrisk1\ninvcloglog(beta0 + beta1)\n# risk in unexposed\nrisk0 &lt;- c / sum(rats$rx == 0)\nrisk0\ninvcloglog(beta0)\n# risk difference\nrisk1 - risk0\ninvcloglog(beta0 + beta1) - invcloglog(beta0)\n# risk ratio\nrisk1 / risk0\ninvcloglog(beta0 + beta1) / invcloglog(beta0)\n# odds ratio\nodds &lt;- function(p) p / (1 - p)\nodds(risk1) / odds(risk0)\nodds(invcloglog(beta0 + beta1)) / odds(invcloglog(beta0))\n\n\n\n\n\n\n9.2.5 Poisson GLMs for incidence rate ratios\nWhen times to events have an exponential(\\(\\lambda\\)) distribution, then the number of events in a total person-time of \\(T\\) has a Poisson(\\(\\lambda T\\)) distribution (see Section 5.4). In a Poisson GLM with a log link for incidence data, we have \\[\n  \\ln\\big(\\E[\\text{count} \\given{} X, T_X]\\big) = \\ln \\lambda(X) + \\ln T_X\n\\] where \\(T_X\\) is the total person time in the group with exposure \\(X\\). Let \\(\\mu(x) = \\lambda(x) T_x\\) be expected count in the group with \\(X = x\\). The Poisson GLM is \\[\n  \\ln \\mu(x)\n  = \\ln \\lambda(x) + \\ln T_x\n  = \\beta_0 + \\beta_1 x + \\ln T_x.\n\\] The term \\(\\ln T_x\\) is an offset because it has a fixed coefficient, which equals one in this case. Thus, \\[\n  \\ln \\lambda(x) = \\beta_0 + \\beta_1 x.\n\\] The incidence rate in the unexposed is \\[\n  \\lambda_0 = e^{\\beta_0},\n\\] the incidence rate in the exposed is \\[\n  \\lambda_1 = e^{\\beta_0 + \\beta_1},\n\\] and the incidence rate ratio is \\[\n  \\IRR = \\frac{e^{\\beta_0 + \\beta_1}}{e^{\\beta_0}} = e^{\\beta_1}.\n\\] The point estimates and \\(1 - \\alpha\\) confidence limits for \\(\\beta_0\\) and \\(\\beta_1\\), for the incidence rate in the unexposed and the IRR, and for the risk in the exposed can be obtained in the same way as in a log-binomial GLM. Because the sum of Poisson random variables is also a Poisson random variable, this models can be fit with either individual or grouped person-time data. This model does not directly assume rare events, and a robust variance is not necessary. However, if an event can occur only once per participant, the rare event assumption ensures that the count of events has a low probability of being limited by the numbers of exposed and unexposed participants in the study.\n\nR\n\n\n\n\nirratio-GLM.R\n\n## Incidence rate ratio estimation with a Poisson GLM\n\n# to get help on GLMs in R (type q to exit)\n?glm\n?family\n\n# rats data is in the survival package\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)     # rats data\nlibrary(lmtest)       # coefci() for Wald confidence intervals\n\n# numbers of events and total rat-weeks\n# The subset() function can be used as an alternative to vector indexing.\na &lt;- sum(rats$status[rats$rx == 1])\nT1 &lt;- sum(rats$time[rats$rx == 1])\nc &lt;- with(subset(rats, rx == 0), sum(status))\nT0 &lt;- with(subset(rats, rx == 0), sum(time))\nir1hat &lt;- a / T1\nir0hat &lt;- c / T0\nIRRhat &lt;- ir1hat / ir0hat\nlnIRRvar &lt;- 1 / a + 1 / c\nlnIRRci &lt;- log(IRRhat) + c(-1, 1) * qnorm(0.975) * sqrt(lnIRRvar)\n\n# point and interval estimate of the IRR\nIRRhat\nexp(lnIRRci)\n\n# Poisson GLM with log(rat-weeks) offset to get incidence rate ratio\n# The default link = \"log\".\nIRRglm &lt;- glm(status ~ rx  + offset(log(time)), family = poisson(), data = rats)\nsummary(IRRglm)                 # point estimates, p-values, CIs, global tests\nnames(IRRglm)                   # parts of the fitted model\nexp(coef(IRRglm))               # estimated incidence rate (x = 0) and IRR\nexp(confint(IRRglm))            # likelihood ratio CIs (better)\nexp(coefci(IRRglm))             # log-transformed Wald CI for IRR matches above\n\n# point and interval estimates for incidence rate in the exposed\n# The vector c(1, 1) represents 1 * beta0 + 1 * beta1,\n# and as.numeric() is used to return a number instead of a matrix.\nrate1hat &lt;- exp(sum(coef(IRRglm)))\nlnrate1var &lt;- as.numeric(c(1, 1) %*% vcov(IRRglm) %*% c(1, 1))\nrate1hat\nrate1hat * exp(c(-1, 1) * qnorm(0.975) * sqrt(lnrate1var))\n\n# Poisson regression using counts in each exposure group\n# Make data frame with counts and total rat-time in each treatment group.\ncountdat &lt;- data.frame(rx = c(0, 1),\n                       count = as.vector(by(rats$status, rats$rx, sum)),\n                       time = as.vector(by(rats$time, rats$rx, sum)))\nIRRcount &lt;- glm(count ~ rx + offset(log(time)),\n                family = poisson(), data = countdat)\nexp(coef(IRRcount))       # point estimates identical to IRRglm\nexp(confint(IRRcount))    # likelihood ratio CIs identical to IRRglm\nexp(coefci(IRRcount))     # Wald CIs also identical to IRRglm\n\n\n\n\n\n\n9.2.6 Poisson GLMs for risks\nFor rare events (i.e., events with probabilities close to zero), the Poisson likelihood is almost the same as the binomial likelihood. If \\(X \\sim \\text{Poisson}(p)\\) and \\(p\\) is close to zero, then \\[\n  \\begin{aligned}\n    \\Pr(X = 0)  &= e^{-p} \\approx 1 - p, \\\\\n    \\Pr(X = 1)  &= p e^{-p} \\approx p (1 - p) \\approx p.\n  \\end{aligned}\n\\] Because \\((1 + p) (1 - p) = 1 - p^2\\), we have \\[\n  \\Pr(X &gt; 1)\n  = 1 - (1 + p) e^{-p}\n  \\approx 1 - (1 + p) (1 - p)\n  = p^2,\n\\] which is much smaller than both \\(p\\) and \\(1 - p\\). Because \\(1 - p \\approx 1\\), the Poisson variance \\(p\\) and the Bernoulli variance \\(p (1 - p)\\) are nearly identical for rare events.\nA Poisson GLM with a log link can also be used to estimate risk ratios using counts of rare events in populations with known sizes. The expected count in each exposure group \\(X = x\\) is \\[\n  \\ln\\big(\\E[\\text{count} \\,\\big|\\, X = x, N_x]\\big) = \\ln p(x) + \\ln N_x\n\\] where \\(p(x)\\) is the risk of the event and \\(N_X\\) is the total population with exposure \\(X\\). Let \\(\\mu(x) = p(x) N_x\\) be the expected count in the group with \\(X = x\\). The Poisson GLM for the risk is \\[\n  \\ln \\mu(x)\n  = \\ln p(x) + \\ln N_x\n  = \\beta_0 + \\beta_1 x + \\ln N_x.\n\\] The term \\(\\ln N_x\\) on the right-hand side is an offset because it has a fixed coefficient equal to one. Thus, \\[\n  \\ln p(x) = \\beta_0 + \\beta_1 x.\n\\] The risk in the unexposed is \\[\n  p_0 = e^{\\beta_0},\n\\] the risk in the exposed is \\[\n  p_1 = e^{\\beta_0 + \\beta_1},\n\\] and the risk ratio is \\[\n  \\RR = \\frac{e^{\\beta_0 + \\beta_1}}{e^{\\beta_0}} = e^{\\beta_1}.\n\\] The point estimates and \\(1 - \\alpha\\) confidence limits for \\(\\beta_0\\) and \\(\\beta_1\\), the risk in the unexposed, the risk ratio, and the risk in the exposed can be obtained in the same way as in a log-binomial GLM. When the event is not rare, a robust variance should be used. However, this adjustment will underestimate the variance when the number of groups is small.\nThe primary drawback of using Poisson GLMs to estimate risks and risk ratios is that the likelihood is not quite correct—especially for events that are not rare. This limits their use in Bayesian methods and forces the use of Wald instead of score or likelihood ratio confidence intervals. However, a Poisson GLM with a log link and a robust variance is an alternative to a log-binomial GLM when the latter fails to converge.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Measures of Association in Cohort Studies</span>"
    ]
  },
  {
    "objectID": "cohort.html#rothman-diagrams",
    "href": "cohort.html#rothman-diagrams",
    "title": "9  Measures of Association in Cohort Studies",
    "section": "9.3 Rothman diagrams",
    "text": "9.3 Rothman diagrams\nThe unit square is the set of all ordered pairs \\((x, y) \\in \\mathbb{R}^2\\) where both \\(x\\) and \\(y\\) are in the interval \\([0, 1]\\). To represent a point \\((x, y)\\) in the unit square, we can plot \\(x\\) on a horizontal axis (the x-axis) and \\(y\\) on a vertical axis (the y-axis). Rothman (1975) introduced a very useful geometric perspective where we let \\(x\\) represent the risk of disease in the unexposed or untreated and \\(y\\) represent the risk of disease in the exposed or treated. We call these Rothman diagrams (Richardson, Robins, and Wang 2017; Kenah 2024), and they are similar to the L’Abbé plots used in meta-analysis (L’Abbé, Detsky, and O’Rourke 1987).\n\n9.3.1 Contour lines\nOn a Rothman diagram, every point in the unit square represents a risk in the exposed and a risk in the unexposed. We can use these risks to calculate measures of association such as the risk difference, risk ratio, odds ratio, or (cumulative) hazard ratio. For a function \\(f(x, y)\\), the set of points with \\(f(x, y) = m\\) for any given \\(m\\) is called a contour line or contour of \\(f\\). If we think of the risk difference as a function \\[\n  \\RD(p_0, p_1) = p_1 - p_0,\n\\] then the contour lines of \\(\\RD\\) link the points \\((p_0, p_1)\\) in the unit square that have the same risk difference. Similarly, we can consider the contour lines of the risk ratio \\[\n  \\RR(p_0, p_1) = \\frac{p_1}{p_0},\n\\] the odds ratio \\[\n  \\OR(p_0, p_1) = \\frac{\\odds(p_1)}{\\odds(p_0)},\n\\] and the (cumulative) hazard ratio \\[\n  \\HR(p_0, p_1) = \\frac{-\\ln(1 - p_1)}{-\\ln(1 - p_0)}.\n\\] Figure 9.3 shows the contour lines for the risk difference, risk ratio, odds ratio, and hazard ratio. The null line \\(p_1 = p_0\\) is a contour line for all four measures of association. All other contours of the risk difference and risk ratio are straight, and all other contours of the odds ratio and hazard ratio are curved.\n\n\n\nCode\n\ncontours.R\n\n## Contour lines for measures of association\nrequire(plotrix, quietly = TRUE)\n\n# odds, logistic, and cumulative hazard functions\nodds &lt;- function(p) p / (1 - p)\ninvodds &lt;- function(odds) odds / (1 + odds)\ncumhaz &lt;- function(p) -log(1 - p)\n\n# risk difference contours\nriskdiff_contours &lt;- function() {\n  plot(x, x, type = \"l\", main = \"Risk difference contours\",\n       xlab = \"\", ylab = \"Risk in the exposed\")\n  grid()\n  boxed.labels(.5, .5, \"0\", xpad = 1.5, ypad = 1.5, border = FALSE)\n  for (riskdiff in c(-0.5, -0.2, 0, 0.2, 0.5)) {\n    y &lt;- x + riskdiff\n    yrange &lt;- y &gt;= 0 & y &lt;= 1\n    lines(x[yrange], y[yrange], lty = \"dashed\")\n    boxed.labels(0.5 - riskdiff / 2, 0.5 + riskdiff / 2,\n                 as.character(riskdiff),\n                 xpad = 1.5, ypad = 1.5, border = FALSE)\n  }\n}\n\n# risk ratio contours\nriskratio_contours &lt;- function() {\n  plot(x, x, type = \"l\", main = \"Risk ratio contours\",\n       xlab = \"\", ylab = \"\")\n  grid()\n  boxed.labels(.5, .5, \"1\", xpad = 1.5, ypad = 1.5, border = FALSE)\n  for (riskratio in c(0.2, 0.5, 2, 5)) {\n    y &lt;- riskratio * x\n    yrange &lt;- (y &lt;= 1)\n    lines(x[yrange], y[yrange], lty = \"dashed\")\n    boxed.labels(1 / (riskratio + 1), riskratio / (riskratio + 1),\n                 as.character(riskratio),\n                 xpad = 1.5, ypad = 1.5, border = FALSE)\n  }\n}\n\n# odds ratio contours\noddsratio_contours &lt;- function() {\n  plot(x, x, type = \"l\", main = \"Odds ratio contours\",\n       xlab = \"Risk in the unexposed\",\n       ylab = \"Risk in the exposed\")\n  grid()\n  boxed.labels(.5, .5, \"1\", xpad = 1.5, ypad = 1.5, border = FALSE)\n  for (oddsratio in c(0.2, 0.5, 2, 5)) {\n    lines(x, invodds(oddsratio * odds(x)), lty = \"dashed\")\n    boxed.labels((sqrt(oddsratio) - 1) / (oddsratio - 1),\n                 (oddsratio - sqrt(oddsratio)) / (oddsratio - 1),\n                 as.character(oddsratio),\n                 xpad = 1.5, ypad = 1.5, border = FALSE)\n  }\n}\n\n# hazard ratios\nhazratio_contours &lt;- function() {\n  plot(x, x, type = \"l\", main = \"Hazard ratio contours\",\n       xlab = \"Risk in the unexposed\", ylab = \"\")\n  grid()\n  boxed.labels(.5, .5, \"1\", xpad = 1.5, ypad = 1.5, border = FALSE)\n  for (hazratio in c(0.2, 0.5, 2, 5)) {\n    lines(c(x, 1), c(1 - (1 - x)^hazratio, 1), lty = \"dashed\")\n    labelx &lt;- uniroot(function(x) x - (1 - x)^hazratio, c(0, 1))$root\n    labely &lt;- 1 - labelx\n    boxed.labels(labelx, labely, as.character(hazratio),\n                 xpad = 1.5, ypad = 1.5, border = FALSE)\n  }\n}\n\n# save old graphical parameters and set new ones\noldmar &lt;- par(\"mar\")\noldmgp &lt;- par(\"mgp\")\npar(mfrow = c(2, 2), mar = 0.6 * (c(5, 5, 4, 1) + 0.1), mgp = c(2, 1, 0))\n\n# grid of plots\nx &lt;- seq(0.001, 0.999, by = 0.001)\nriskdiff_contours()\nriskratio_contours()\noddsratio_contours()\nhazratio_contours()\n# dev.copy2pdf(file = \"contours.pdf\")\n\n# reset graphical parameters\npar(mfrow = c(1, 1), mar = oldmar, mgp = oldmgp)\n\n\n\n\n\n\n\n\n\nFigure 9.3: Contour lines for the odds ratio, risk ratio, risk difference, and hazard ratio. The null line is solid. All other contours lines are dashed. Each contour line is labeled with the corresponding value of the measure of association.\n\n\n\n\n\n\n\n\n9.3.2 Relationships among the RR, OR, and HR\nThe odds ratio is always farther from the null than the risk ratio because \\[\n  \\OR = \\RR \\frac{1 - p_0}{1 - p_1}\n  \\implies\n  \\begin{cases}\n    \\OR &gt; \\RR &\\text{if } p_1 &gt; p_0, \\\\\n    \\OR = \\RR &\\text{if } p_1 = p_0, \\\\\n    \\OR &lt; \\RR &\\text{if } p_1 &lt; p_0.\n  \\end{cases}\n\\] Because \\(\\Var(\\ln \\hat{\\OR}) &gt; \\Var(\\ln \\hat{\\RR})\\), this does not imply that a hypothesis test based on the odds ratio is more powerful than a test based on the risk ratio.\n\n\n9.3.3 Relationships between changes in the RD, RR, and OR*\nIt is surprising but true that the odds ratio, risk ratio, and risk difference can change in different directions relative to ther null values when the risks of disease in the exposed or the unexposed change. Even if we accept this possiblity, it can seem pathological and the algebra can be complicated (Brumback and Berg 2008). The problem becomes simpler if we focus on local changes where we start at a point on a Rothman diagram and keep track of the changes in the risk difference, risk ratio, and odds ratio as we move away from it.\nThere are no surprises if we start on the null line where \\(p_1 = p_0\\), so \\(\\RD = 0\\), \\(\\RR = 1\\), and \\(\\OR = 1\\). If we move along the null line, all measures stay the same. If we move to a point where \\(p_1 &gt; p_0\\), then all three measures increase so \\(\\RD &gt; 0\\), \\(\\RR &gt; 1\\), and \\(\\OR &gt; 1\\). If we move to a point where \\(p_1 &lt; p_0\\), then all three measures decrease so \\(\\RD &lt; 0\\), \\(\\RR &lt; 1\\), and \\(\\OR &lt; 1\\). Thus, all three measures agree about which direction we are moving relative to the null.\nIf we start at any point off the null line, these measures of association can change in different directions—some toward the null and others away from the null. If we allow the risk difference, risk ratio, and odds ratio to move independently toward or away from their null values, there are \\(2^3 = 8\\) possible combinations. It turns out that six of these combinations are possible around any point off the null line. This is hard to see algebraically, but it is easy to see on a Rothman diagram. Figure 9.4 shows the contours of the risk difference, risk ratio, and odds ratio passing through two points: (0.2, 0.6) above the null line and (0.8, 0.4) below the null line. Figure 9.5 shows the gray square around (0.2, 0.6), where the contour line for the \\(\\OR\\) is between the contours for the \\(\\RD\\) and \\(\\RR\\). If the \\(\\RD\\) and \\(\\RR\\) both move in the same direction, the \\(\\OR\\) must follow them—precluding two of the eight possible combinations of changes toward and away from the null. Figure 9.6 shows the gray square around (0.8, 0.4), where the contour line for the \\(\\RD\\) is between the contours for the \\(\\RR\\) and \\(\\OR\\). If the \\(\\RR\\) and \\(\\OR\\) both move in the same direction, the \\(\\RD\\) must follow them—which prevents two of the eight possible combinations of changes toward and away from the null. At both points, six of the eight possible combations of movements toward or away from the null are possible, one in each of the six angles defined by the contour lines through the point. However, the regions where all three measures of associaton agree about whether we are moving toward or away from the null occupy much larger angles than the regions where they disagree.\n\n\n\nCode\n\nchanges.R\n\n## Contours for the RD, RR, and OR at (0.2, 0.6) and (0.8, 0.4)\n\n# required packages\nrequire(ggplot2, quietly = TRUE)\nrequire(geomtextpath, quietly = TRUE)\n\n# odds and logistic functions\nodds &lt;- function(p) p / (1 - p)\ninvodds &lt;- function(odds) odds / (1 + odds)\n\n# Point A at (0.2, 0.6) with OR = 6, RR = 3, and RD = 0.4\n# and point B at (0.6, 0.2) with OR = 1/6, RR = 1/3, and RD = -0.4\nx &lt;- seq(0.001, 0.999, by = 0.001)\ndat &lt;- data.frame(x = x, yORa = invodds(6 * odds(x)),\n                  yRRa = 3 * x, yRDa = x + 0.4,\n                  yORb = invodds(odds(x) / 6), yRRb = x / 2,\n                  yRDb = x - 0.4)\n\n# with points 1A and 1B\n(ggplot(dat, aes(x, yORa))\n  + theme_bw()\n  + scale_x_continuous(limits = c(0, 1), expand = expansion(mult = 0.02),\n                       breaks = seq(0, 1, by = 0.2))\n  + scale_y_continuous(limits = c(0, 1), expand = expansion(mult = 0.02),\n                       breaks = seq(0, 1, by = 0.2))\n  + xlab(\"Risk in the unexposed\")\n  + ylab(\"Risk in the exposed\")\n  # point A\n  + annotate(\"point\", x = 0.2, y = 0.6, size = 3)\n  + annotate(\"text\", x = 0.2, y = 0.58, hjust = 0, vjust = 1,\n             label = \"(0.2, 0.6)\")\n  + geom_textline(label = \"OR = 6\", hjust = 0.8)\n  + geom_textline(aes(x, yRRa), label = \"RR = 3\", hjust = 0.9,\n                  linetype = \"dashed\")\n  + geom_textline(aes(x, yRDa), label = \"RD = 0.4\", hjust = 0.9,\n                  linetype = \"dotted\")\n  + geom_textline(aes(x, x), col = \"darkgray\", label = \"Null line\")\n  + annotate(\"rect\", xmin = 0.09, xmax = 0.31, ymin = 0.49, ymax = 0.71,\n             alpha = 0.2)\n  + annotate(\"text\", x = 0.11, y = 0.69, hjust = 0, vjust = 1,\n             label = \"Fig. 9.5\")\n  # point B\n  + annotate(\"point\", x = 0.8, y = 0.4, size = 3)\n  + annotate(\"text\", x = 0.8, y = 0.38, hjust = 0, vjust = 1,\n             label = \"(0.8, 0.4)\")\n  + geom_textline(aes(x, yORb), hjust = 0.8, label = \"OR = 1/6\")\n  + geom_textline(aes(x, yRRb), hjust = 0.5, linetype = \"dashed\",\n                  label = \"RR = 1/2\")\n  + geom_textline(aes(x, yRDb), hjust = 0.05, linetype = \"dotted\",\n                  label = \"RD = -0.4\")\n  + annotate(\"rect\", xmin = 0.69, xmax = 0.91, ymin = 0.29, ymax = 0.51,\n             alpha = 0.2)\n  + annotate(\"text\", x = 0.71, y = 0.49, hjust = 0, vjust = 1,\n             label = \"Fig. 9.6\")\n)\n\n\n\n\n\n\n\n\n\nFigure 9.4: Rothman diagram showing the contour lines for the odds ratio (solid), risk ratio (dashed), and risk difference (dotted) that pass through the point \\((0.2, 0.6)\\) above the null line and the point \\((0.8, 0.4)\\) below the null line. The gray squares are the areas shown in Figure 9.5 and Figure 9.6.\n\n\n\n\n\n\n\n\nCode\n\nchangesA.R\n\n## Local changes in the RD, RR, and OR near (0.2, 0.6)\n\n# required packages\nrequire(ggplot2, quietly = TRUE)\nrequire(geomtextpath, quietly = TRUE)\n\n# odds and logistic functions, conversion from degrees to radians\nodds &lt;- function(p) p / (1 - p)\ninvodds &lt;- function(odds) odds / (1 + odds)\ndegtorad &lt;- function(degrees) pi * (degrees / 180)\n\n# Point A at (0.2, 0.6) with OR = 6, RR = 3, and RD = 0.4\n# and point B at (0.6, 0.2) with OR = 1/6, RR = 1/3, and RD = -0.4\nx &lt;- seq(0.001, 0.999, by = 0.001)\ndatA &lt;- data.frame(x = x, yORa = invodds(6 * odds(x)),\n                   yRRa = 3 * x, yRDa = x + 0.4)\n\n# plot of contours near point A\n(ggplot(datA, aes(x, yORa))\n  + coord_cartesian(xlim = c(0.1, 0.3), ylim = c(0.5, 0.7))\n  + xlab(\"Risk in the unexposed\")\n  + ylab(\"Risk in the exposed\")\n  + annotate(\"point\", x = 0.2, y = 0.6, size = 3)\n  + annotate(\"text\", 0.202, 0.598, hjust = 0, vjust = 1, label = \"(0.2, 0.6)\")\n  + geom_textline(label = \"OR = 6\", hjust = 0.385)\n  + geom_textline(aes(x, yRDa), label = \"RD = 0.40\", hjust = 0.22,\n                  linetype = \"dotted\")\n  + geom_textline(aes(x, yRRa), label = \"RR = 3\", hjust = 0.208,\n                  linetype = \"dashed\")\n  + annotate(\"text\", 0.25, 0.55,\n             label = \"All toward the null\\n OR &lt; 6, RR &lt; 3, RD &lt; 0.4\")\n  + annotate(\"text\", 0.15, 0.65,\n             label = \"All away from the null\\n OR &gt; 6, RR &gt; 3, RD &gt; 0.4\")\n  + annotate(\"text\", angle = 49,\n             x = 0.2 + 0.09 * cos(degtorad(49)),\n             y = 0.6 + 0.09 * sin(degtorad(49)),\n             label = \"OR &lt; 6, RR &lt; 3, RD &gt; 0.4\")\n  + annotate(\"text\", angle = 63,\n             x = 0.2 + 0.09 * cos(degtorad(63)),\n             y = 0.6 + 0.09 * sin(degtorad(63)),\n             label = \"OR &gt; 6, RR &lt; 3, RD &gt; 0.4\")\n  + annotate(\"text\", angle = 52,\n             x = 0.2 + 0.09 * cos(degtorad(180 + 52)),\n             y = 0.6 + 0.09 * sin(degtorad(180 + 52)),\n             label = \"OR &gt; 6, RR &gt; 3, RD &lt; 0.4\")\n  + annotate(\"text\", angle = 66,\n             x = 0.2 + 0.087 * cos(degtorad(180 + 66)),\n             y = 0.6 + 0.087 * sin(degtorad(180 + 66)),\n             label = \"OR &lt; 6, RR &gt; 3, RD &lt; 0.4\")\n)\n\n\n\n\n\n\n\n\n\nFigure 9.5: The odds ratio, risk ratio, and risk difference near the point \\((0.2, 0.6)\\). The contour line for the odds ratio (solid) is between the contours for the risk difference (dotted) and risk ratio (dashed). Six combinations of changes toward and away from the null are possible when starting from this point, but the odds ratio must follow the risk difference and risk ratio if they move in the same direction.\n\n\n\n\n\n\n\n\nCode\n\nchangesB.R\n\n## Local changes in the RD, RR, and OR near (0.8, 0.4)\n\n# required packages\nrequire(ggplot2, quietly = TRUE)\nrequire(geomtextpath, quietly = TRUE)\n\n# odds and logistic functions, conversion from degrees to radians\nodds &lt;- function(p) p / (1 - p)\ninvodds &lt;- function(odds) odds / (1 + odds)\ndegtorad &lt;- function(degrees) pi * (degrees / 180)\n\n# Point A at (0.2, 0.6) with OR = 6, RR = 3, and RD = 0.4\n# and point B at (0.6, 0.2) with OR = 1/6, RR = 1/3, and RD = -0.4\nx &lt;- seq(0.001, 0.999, by = 0.001)\ndatB &lt;- data.frame(x = x, yORb = invodds(odds(x) / 6), yRRb = x / 2,\n                   yRDb = x - 0.4)\n\n(ggplot(datB, aes(x, yORb))\n  + coord_cartesian(xlim = c(0.7, 0.9), ylim = c(0.3, 0.5))\n  + xlab(\"Risk in the unexposed\")\n  + ylab(\"Risk in the exposed\")\n  + annotate(\"point\", x = 0.8, y = 0.4, size = 3)\n  + annotate(\"text\", 0.802, 0.398, hjust = 0, vjust = 1, label = \"(0.8, 0.4)\")\n  + geom_textline(label = \"OR = 1/6\", hjust = 0.615)\n  + geom_textline(aes(x, yRDb), label = \"RD = -0.4\", hjust = 0.78,\n                  linetype = \"dotted\")\n  + geom_textline(aes(x, yRRb), label = \"RR = 1/2\", hjust = 0.84,\n                  linetype = \"dashed\")\n  + annotate(\"text\", 0.75, 0.45,\n             label = \"All toward the null\\n OR &gt; 1/6, RR &gt; 1/2, RD &gt; -0.4\")\n  + annotate(\"text\", 0.85, 0.35,\n             label = \"All away from the null\\n OR &lt; 1/6, RR &lt; 1/2, RD &lt; -0.4\")\n  + annotate(\"text\", angle = 36,\n             x = 0.8 + 0.09 * cos(degtorad(36)),\n             y = 0.4 + 0.09 * sin(degtorad(36)),\n             label = \"OR &lt; 1/6, RR &gt; 1/2, RD &lt; -0.4\")\n  + annotate(\"text\", angle = 52,\n             x = 0.8 + 0.09 * cos(degtorad(52)),\n             y = 0.4 + 0.09 * sin(degtorad(52)),\n             label = \"OR &lt; 1/6, RR &gt; 1/2, RD &gt; -0.4\")\n  + annotate(\"text\", angle = 36,\n             x = 0.8 + 0.09 * cos(degtorad(180 + 36)),\n             y = 0.4 + 0.09 * sin(degtorad(180 + 36)),\n             label = \"OR &gt; 1/6, RR &lt; 1/2, RD &gt; -0.4\")\n  + annotate(\"text\", angle = 47,\n             x = 0.8 + 0.095 * cos(degtorad(180 + 49)),\n             y = 0.4 + 0.095 * sin(degtorad(180 + 49)),\n             label = \"OR &gt; 1/6, RR &lt; 1/2, RD &lt; -0.4\")\n)\n\n\n\n\n\n\n\n\n\nFigure 9.6: The odds ratio, risk ratio, and risk difference near the point \\((0.8, 0.4)\\). The contour line for the risk difference (dotted) is between the contours for the risk ratio (dashed) and the odds ratio (solid). Six combinations of changes toward and away from the null are possible in any neighborhood of the point, but the risk difference must follow the risk ratio and odds ratio if they move in the same direction.\n\n\n\n\n\n\n\n\n\nAgresti, Alan. 2013. Categorical Data Analysis. Third. Vol. 792. John Wiley & Sons.\n\n\nAgresti, Alan, and Yongyi Min. 2005. “Frequentist Performance of Bayesian Confidence Intervals for Comparing Proportions in 2\\(\\times\\) 2 Contingency Tables.” Biometrics 61 (2): 515–23.\n\n\nBerkson, Joseph. 1944. “Application of the Logistic Function to Bio-Assay.” Journal of the American Statistical Association 39 (227): 357–65.\n\n\nBrumback, Babette, and Arthur Berg. 2008. “On Effect-Measure Modification: Relationships Among Changes in the Relative Risk, Odds Ratio, and Risk Difference.” Statistics in Medicine 27 (18): 3453–65.\n\n\nCox, David R. 1958. “The Regression Analysis of Binary Sequences.” Journal of the Royal Statistical Society Series B: Statistical Methodology 20 (2): 215–32.\n\n\nKatz, DJSM, J Baptista, SP Azen, and MC Pike. 1978. “Obtaining Confidence Intervals for the Risk Ratio in Cohort Studies.” Biometrics, 469–74.\n\n\nKenah, Eben. 2024. “Rothman Diagrams: The Geometry of Confounding and Standardization.” International Journal of Epidemiology 53 (6): dyae139.\n\n\nL’Abbé, Kristan A, Allan S Detsky, and Keith O’Rourke. 1987. “Meta-Analysis in Clinical Research.” Annals of Internal Medicine 107 (2): 224–33.\n\n\nMcCullagh, Peter, and J A Nelder. 1989. Generalized Linear Models. 2nd ed. Chapman & Hall.\n\n\nMiettinen, Olli. 1976. “Estimability and Estimation in Case-Referent Studies.” American Journal of Epidemiology 103 (2): 226–35.\n\n\nMiettinen, Olli S, and E Francis Cook. 1981. “Confounding: Essence and Detection.” American Journal of Epidemiology 114 (4): 593–603.\n\n\nNaimi, Ashley I, and Brian W Whitcomb. 2020. “Estimating Risk Ratios and Risk Differences Using Regression.” American Journal of Epidemiology 189 (6): 508–10.\n\n\nNelder, John Ashworth, and Robert W M Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society Series A: Statistics in Society 135 (3): 370–84.\n\n\nRichardson, Thomas S, James M Robins, and Linbo Wang. 2017. “On Modeling and Estimation for the Relative Risk and Risk Difference.” Journal of the American Statistical Association 112 (519): 1121–30.\n\n\nRothman, Kenneth J. 1975. “A Pictorial Representation of Confounding in Epidemiologic Studies.” Journal of Chronic Diseases 28 (2): 101–8.\n\n\n———. 1978. “A Show of Confidence.” New England Journal of Medicine 299 (24): 1362–63.\n\n\nRothman, Kenneth J, Sander Greenland, and Timothy L Lash. 2008. Modern Epidemiology. Lippincott Williams & Wilkins.\n\n\nSimpson, Edward H. 1951. “The Interpretation of Interaction in Contingency Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 13 (2): 238–41.\n\n\nWilliamson, Tyler, Misha Eliasziw, and Gordon Hilton Fick. 2013. “Log-Binomial Models: Exploring Failed Convergence.” Emerging Themes in Epidemiology 10 (1): 1–10.\n\n\nWoolf, Barnet. 1955. “On Estimating the Relation Between Blood Group and Disease.” Annals of Human Genetics 19 (4): 251–53.\n\n\nZou, Guangyong. 2004. “A Modified Poisson Regression Approach to Prospective Studies with Binary Data.” American Journal of Epidemiology 159 (7): 702–6.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Measures of Association in Cohort Studies</span>"
    ]
  },
  {
    "objectID": "survival2.html",
    "href": "survival2.html",
    "title": "10  Two-Sample Survival Analysis and the Cox Model",
    "section": "",
    "text": "10.1 Measures of association in survival analysis\nSimple measures of association such as the risk ratio or odds ratio require the calculation of risks of disease over a specified time interval. Often, it is more efficient to compare times to events throughout an interval rather than at a single time point. Under the assumption of exponential times to events, the incidence rate allows us to calculate a rate that applies througout the observed time period. This is a step in the right direction, but calculations of incidence rates assume exponential times to events.\nFor one-sample inference, survival analysis allowed us to make more relaxed parametric assumptions (e.g., Weibull or log-logistic times to events) or to avoid any parametric assumption at all (e.g., the Kaplan-Meier and Nelson-Aalen estimators). Survival analysis plays a similar role in two-sample inference, allowing us to relax assumptions and compare survival time distributions in their entirety. The resulting hypothesis tests and measures of association can be more powerful and accurate than comparisons based on risks or incidence rates. All of the measures of association we discussed in Chapter 9 can be calculated using survival analysis. We focus on a single binary exposure as before, and we assume independent left truncation (i.e., delayed entry) and right censoring as described in Section 5.1.\nLet \\(\\lambda_1\\) be the rate parameter for the exposed and \\(\\lambda_0\\) be the rate parameter for the unexposed, and assume (where needed) that both groups have the same shape parameter \\(\\alpha\\). The rate ratio is just \\(\\lambda_1 / \\lambda_0\\). The hazard ratio at time \\(t\\) is \\[\n  \\HR(t) = \\frac{h(t, \\alpha, \\lambda_1)}{h(t, \\alpha, \\lambda_0)}.\n\\] The risk of disease onset in any interval \\((0, t]\\) can be calculated using the survival, cumulative hazard, or hazard functions. This allows us to calculate risk differences, risk ratios, or odds ratios similar to those in Chapter 9. In many cases, these measures of association depend on \\(t\\).",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Two-Sample Survival Analysis and the Cox Model</span>"
    ]
  },
  {
    "objectID": "survival2.html#sec-surv-meas",
    "href": "survival2.html#sec-surv-meas",
    "title": "10  Two-Sample Survival Analysis and the Cox Model",
    "section": "",
    "text": "10.1.1 Rate ratios and hazard ratios\nSuppose times to events have a Weibull(\\(\\alpha\\), \\(\\lambda_1\\)) distribution in the exposed and a Weibull(\\(\\alpha\\), \\(\\lambda_0\\)) in the unexposed. The rate ratio is \\(\\lambda_1 / \\lambda_0\\) at all times. However, the hazard ratio at time \\(t\\) is \\[\n  \\HR_\\text{W}\n  = \\frac{\\alpha \\lambda_1^\\alpha t^{\\alpha - 1}}{\\alpha \\lambda_0^\\alpha t^{\\alpha - 1}}\n  = \\bigg(\\frac{\\lambda_1}{\\lambda_0}\\bigg)^\\alpha.\n\\] Therefore, the hazard ratio is constant and equal to \\((\\text{rate ratio})^\\alpha\\). Because the exponential distribution is a special case of the Weibull distribution with \\(\\gamma = 1\\), the hazard ratio equals the rate ratio when both groups have exponential times to events.\nIf times to events have a log-logistic(\\(\\alpha\\), \\(\\lambda_1\\)) distribution in the exposed and a log-logistic(\\(\\alpha\\), \\(\\lambda_0\\)) in the unexposed, the rate ratio is still \\(\\lambda_1 / \\lambda_0\\) at all times. However, the hazard ratio is \\[\n  \\HR_\\text{LL}\n  = \\frac{\\alpha \\lambda_1^\\alpha t^{\\alpha - 1}}\n         {\\alpha \\lambda_0^\\alpha t^{\\alpha - 1}}\n    \\times \\frac{1 + (\\lambda_0 t)^\\alpha}{1 + (\\lambda_1 t)^\\alpha}\n  = \\bigg(\\frac{\\lambda_1}{\\lambda_0}\\bigg)^\\alpha\n    \\frac{1 + (\\lambda_0 t)^\\alpha}{1 + (\\lambda_1 t)^\\alpha},\n\\tag{10.1}\\] which is not constant in time for any shape parameter \\(\\alpha &gt; 0\\). The hazard ratio is approximately \\((\\lambda_1 / \\lambda_0)^\\alpha\\) just after \\(t = 0\\), and it approaches one as \\(t \\rightarrow \\infty\\). Figure 10.1 shows an example of this with shape \\(\\alpha = 2\\) and rates \\(\\lambda_1 = 2\\) and \\(\\lambda_1 = 1\\).\n\n\n\nCode\n\nloglogistic-HR.R\n\n## Rate ratios and hazard ratios for the log-logistic distribution\n\n# log-logistic hazard function\nhllog &lt;- function(t, shape=1, rate=1) {\n  shape * rate^shape * t^(shape - 1) / (1 + (rate * t)^shape)\n}\n\n# save old graphics parameters to restore them\nmfrow_old &lt;- par(\"mfrow\")\nmar_old &lt;- par(\"mar\")\n\n# set two rows and one column and adjust margins between plots\npar(mar = c(4, 4 ,1, 1), mfrow = c(2, 1))\n\n# hazards with shape = 2 and rates = 2 and 1\nt &lt;- seq(0.01, 3, by = 0.01)\nplot(t, hllog(t, shape = 2, rate = 2), type = \"l\", lty = \"dashed\",\n     xlab = \"\", ylab = \"Hazard\")\nlines(t, hllog(t, shape = 2, rate = 1))\ngrid()\ntext(2.5, 1.75, labels = \"both shape = 2\")\ntext(1.0, 0.75, labels = \"rate = 1\")\ntext(1.25, 1.75, labels = \"rate = 2\")\n\n# hazard ratio\nplot(t, hllog(t, 2, 2) / hllog(t, 2, 1), type = \"l\", ylim = c(0, 4),\n     xlab = \"Time\", ylab = \"Hazard ratio\")\ngrid()\ntext(2.5, 3.5, labels = \"rate ratio = 2\")\n\n# restore old graphics parameters\npar(mar = mar_old, mfrow = mfrow_old)\n\n\n\n\n\n\n\n\n\nFigure 10.1: Hazards (top) and hazard ratio (bottom) for log-logistic distributions with shape \\(\\alpha = 2\\) and rates \\(\\lambda_1 = 2\\) in the exposed and \\(\\lambda_0 = 1\\) in the unexposed.\n\n\n\n\n\n\n\n10.1.2 Risk differences, risk ratios, and odds ratios\nAll of the measures of association from Chapter 9 can be calculating using survival analysis because the estimated survival function can be used to calculate risks. For example: If the times to events have Weibull distributions in both groups, the risk of disease onset in \\((0, t]\\) given \\(X = x\\) is \\[\n  1 - S(t, \\alpha, \\lambda_x)\n  = 1 - e^{-(\\lambda_x t)^\\alpha}\n\\] where \\(x = 1\\) for the exposed and \\(x = 0\\) for the unexposed. Using these risks, we can calculate the risk difference, risk ratio, and odds ratio comparing the exposed to the unexposed over any time interval \\((0, t]\\). The odds of disease onset in \\((0, t]\\) given \\(X = x\\) is \\[\n  \\frac{1 - e^{-(\\lambda_x t)^\\alpha}}{e^{-(\\lambda_x t)^\\alpha}}\n  = e^{(\\lambda_x t)^\\alpha} - 1,\n\\] so the odds ratio comparing the exposed to the unexposed at time \\(t\\) is \\[\n  \\OR(t) = \\frac{e^{(\\lambda_1 t)^\\alpha} - 1}{e^{(\\lambda_0 t)^\\alpha} - 1}.\n\\] When \\(\\lambda_1 = \\lambda_0\\), this equals one at all \\(t\\). When \\(\\lambda_1 \\neq \\lambda_0\\), it varies with \\(t\\). Figure 10.2 shows an example of this with shape \\(\\alpha = 2\\) and rates \\(\\lambda_1 = 2\\) and \\(\\lambda_1 = 1\\). Similarly, the risk difference equals zero and the risk ratio equals one when \\(\\lambda_1 = \\lambda_0\\) but both vary with \\(t\\) otherwise.\n\n\n\nCode\n\nWeibull-OR.R\n\n## Rate ratios and odds ratios for the Weibull distribution\n\n# log-logistic odds function\nodds_weib &lt;- function(t, shape=1, rate=1) {\n  p &lt;- 1 - exp(-(rate * t)^shape)\n  p / (1 - p)\n}\n\n# save old graphics parameters to restore them\nmfrow_old &lt;- par(\"mfrow\")\nmar_old &lt;- par(\"mar\")\n\n# set two rows and one column and adjust margins between plots\npar(mar = c(4, 4 ,1, 1), mfrow = c(2, 1))\n\n# odds with shape = 2 and rates = 1 and 2\nt &lt;- seq(0.01, 0.6, by = 0.01)\nplot(t, odds_weib(t, shape = 2, rate = 2), type = \"l\", lty = \"dashed\",\n     xlab = \"\", ylab = \"Odds\")\nlines(t, odds_weib(t, shape = 2, rate = 1))\ngrid()\ntext(0.1, 2.75, labels = \"both shape = 2\")\ntext(0.4, 0.4, labels = \"rate = 1\")\ntext(0.4, 1.3, labels = \"rate = 2\")\n\n# odds ratio\nplot(t, odds_weib(t, 2, 2) / odds_weib(t, 2, 1), type = \"l\", ylim = c(0, 8),\n     xlab = \"Time\", ylab = \"Odds ratio\")\ngrid()\ntext(0.1, 7, labels = \"rate ratio = 2\")\n\n# restore old graphics parameters\npar(mar = mar_old, mfrow = mfrow_old)\n\n\n\n\n\n\n\n\n\nFigure 10.2: Odds (top) and odds ratio (bottom) for Weibull distributions with shape \\(\\alpha = 2\\) and rates \\(\\lambda_1 = 2\\) in the exposed and \\(\\lambda_0 = 1\\) in the unexposed.\n\n\n\n\n\nIf the failure time distributions are log-logistic with the same shape parameter \\(\\alpha\\), the risk of disease onset in \\((0, t]\\) given \\(X = x\\) is \\[\n  1 - S(t, \\alpha, \\lambda_x)\n  = 1 - \\frac{1}{1 + (\\lambda_1 t)^\\alpha}\n  = \\frac{(\\lambda_1 t)^\\alpha}{1 + (\\lambda_1 t)^\\alpha},\n\\] and the corresponding odds of disease is \\[\n  \\frac{(\\lambda_x t)^\\alpha}{x + (\\lambda_x t)^\\alpha} \\times \\frac{x + (\\lambda t)^\\alpha}{1}\n  = (\\lambda_x t)^\\alpha.\n\\] When \\(\\lambda_1 \\neq \\lambda_0\\), the risk difference and risk ratio are not constant in time. However, the odds ratio is \\[\n  \\frac{(\\lambda_1 t)^\\alpha}{(\\lambda_0 t)^\\alpha}\n  = \\bigg(\\frac{\\lambda_1}{\\lambda_0}\\bigg)^\\alpha,\n\\] which is constant in time and equal to \\((\\text{rate ratio})^\\alpha\\). Thus, the odds ratio for log-logistic times to events has the same form as the hazard ratio for Weibull times to events. The risk difference and risk ratio both vary with \\(t\\) when \\(\\lambda_1 \\neq \\lambda_0\\).",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Two-Sample Survival Analysis and the Cox Model</span>"
    ]
  },
  {
    "objectID": "survival2.html#accelerated-failure-time-models",
    "href": "survival2.html#accelerated-failure-time-models",
    "title": "10  Two-Sample Survival Analysis and the Cox Model",
    "section": "10.2 Accelerated failure time models",
    "text": "10.2 Accelerated failure time models\nParametric regression models in survival analysis are based on rate parameters, which measure how quickly time passes relative to a baseline time-to-event distribution. In an accelerated failure time (AFT) regression model, predictors act multiplicatively on the random time to event \\(T\\): \\[\n  T = e^{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k} \\times \\tau,\n\\] where \\(\\tau\\) is a random sample from the baseline survival time distribution, which defines \\(\\lambda = 1\\). The multiplier of \\(\\tau\\) is the scale parameter \\[\n  \\sigma(\\beta, x) = e^{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k},\n\\] which measures how much the baseline failure time distribution is compressed (\\(\\sigma &lt; 1\\)) or stretched out (\\(\\sigma &gt; 1\\)). A one-unit increase in \\(x_j\\) with all other covariates held constant is associated with a survival time that is multiplied by \\(\\exp(\\beta_j)\\), which is called the acceleration factor. The rate parameter is \\[\n  \\lambda(\\beta, x) =\n  e^{-\\big(\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k\\big)},\n\\tag{10.2}\\] which measures how quickly time passes relative to the baseline failure-time distribution. A one-unit increase in \\(x_j\\) with all other covariates held constant is associated with a rate ratio of \\(\\exp(-\\beta_j)\\). The null hypothesis of no association with \\(X_j\\) corresponds to \\(\\beta_j = 0\\).\nWe will discuss AFT models that use exponential, Weibull, and log-logistic distributions with rate \\(\\lambda = 1\\) has their baseline distributions. The gamma and log-normal distributions are also widely used, but they do not have simple closed forms for the hazard, cumulative hazard, or survival functions. All of these models can be fit using data with left truncation and right censoring, and the construction of the likelihood is the same. AFT models are a useful alternative to analyses based on incidence rates when the times to events are not exponential.\nFor simplicity, we will focus on AFT models with a single binary predictor \\(X\\). In these models, the rate parameter is \\[\n  \\lambda(x, \\beta)\n  = e^{-(\\beta_0 + \\beta_1 x)} =\n  \\begin{cases}\n    e^{-\\beta_0}              & \\text{in the unexposed,}\\\\\n    e^{-(\\beta_0 + \\beta_1)}  & \\text{in the exposed.}\n  \\end{cases}\n\\tag{10.3}\\] The rate ratio is \\[\n  \\frac{\\lambda_1}{\\lambda_0}\n  = \\frac{e^{-(\\beta_0 + \\beta_1)}}{e^{-\\beta_0}}\n  = e^{-\\beta_1}.\n\\] The relationship between this rate ratio and other measures of association depends on the underlying failure time distribution.\n\n10.2.1 Exponential AFT model\nAn exponential distribution has a rate parameter but no shape parameter, so an exponential AFT model only has to estimate the coefficient vector \\(\\beta\\) from Equation 10.3. In an exponential AFT model, the hazard function is \\[\n  h(t, x, \\beta) = \\lambda(x, \\beta),\n\\] which is the exponential hazard function with \\(\\lambda = \\lambda(x, \\beta)\\) from Equation 10.3. The corresponding cumulative hazard is \\[\n  H(t, x, \\beta) = \\lambda(X, \\beta) t.\n\\] With left-truncated and right-censored data \\[\n  (\\tentry_i, \\texit_i, \\delta_i, x_i) \\text{ for } i = 1, \\ldots, n,\n\\tag{10.4}\\] the log likelihood for \\(\\beta\\) is \\[\n  \\ell(\\beta)\n  = \\sum_{i = 1}^n\n    \\Big(\\delta_i \\ln h(\\texit_i, x_i, \\beta)\n      - H(\\texit_i, x_i, \\beta) + H(\\tentry_i, x_i, \\beta)\\Big).\n\\] Point and interval estimates of \\(\\beta\\) can be obtained using maximum likelihood estimation (as in Chapter 3) or Bayesian methods (see ?sec-bayes).\n\nR\n\n\n\n\nAFT-exponential.R\n\n## Exponential accelerated failure time (AFT) model\n\n# The rats data is from:\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)               # rats data and survreg()\nlibrary(lmtest)                 # coefci() for Wald confidence intervals\n\n# exponential AFT model\n# Default distribution is Weibull, so we must specify dist = \"exponential\".\n# Coefficients are log scale parameters = -log rate parameters.\naft_exp &lt;- survreg(Surv(time, status) ~ rx, data = rats, dist = \"exponential\")\nexp(-coef(aft_exp))             # estimated rate (x = 0) and rate ratio\nexp(-confint(aft_exp))          # log-transformed 95% Wald confidence intervals\n\n# estimated rate in the exposed\nlnrate1hat &lt;- -sum(coef(aft_exp))\nlnrate1var &lt;- as.numeric(c(1, 1) %*% vcov(aft_exp) %*% c(1, 1))\nlnrate1ci &lt;- lnrate1hat + c(-1, 1) * qnorm(0.975) * sqrt(lnrate1var)\nexp(lnrate1hat)                 # point estimate\nexp(lnrate1ci)                  # log-transformed 95% Wald confidence interval\n\n# exponential AFT results match Poisson GLM with log(rat-weeks) offset\nIRRglm &lt;- glm(status ~ rx  + offset(log(time)), family = poisson(), data = rats)\nexp(coef(IRRglm))               # estimated incidence rate (x = 0) and IRR\nexp(coefci(IRRglm))             # log-transformed Wald CIs matches above\n\n# compare with nonparametric estimates\n# Nelson-Aalen estimates (with Fleming-Harrington correction for ties)\n# Formula ~ rx produces a separate estimate for each treatment group.\n# Arguments stype = 2 and ctype = 2 produces NA estimate with FH correction.\nratsNA &lt;- survfit(Surv(time, status) ~ rx, data = rats,\n                  stype = 2, ctype = 2, conf.type =  \"log-log\")\n\n# plot of Nelson-Aalen cumulative hazard curves in treated and untreated rats\nplot(ratsNA, fun = \"cumhaz\", col = c(\"darkgray\", \"black\"),\n     xlab = \"Weeks after treatment\", ylab = \"Cumulative hazard\")\ngrid()\nlegend(\"topleft\", bg = \"white\", lty = rep(c(\"solid\", \"dashed\"), times = 2),\n       col = rep(c(\"black\", \"darkgray\"), each = 2),\n       legend = c(\"Control (Nelson-Aalen)\", \"Control (Exponential AFT model)\",\n                  \"Treated (Nelson-Aalen)\", \"Treated (Exponential AFT model)\"))\n\n# estimated cumulative hazards from exponential AFT model\naft_exp &lt;- survreg(Surv(time, status) ~ rx, data = rats, dist = \"exponential\")\nrate1_exp &lt;- as.numeric(exp(-c(1, 1) %*% coef(aft_exp)))\nrate0_exp &lt;- exp(-coef(aft_exp)[\"(Intercept)\"])\nH_exp &lt;- function(t, rate) rate * t\n\n# add lines to plot\nt &lt;- seq(0, max(ratsNA$time), by = 0.1)\nlines(t, H_exp(t, rate1_exp), lty = \"dashed\", col = \"black\")\nlines(t, H_exp(t, rate0_exp), lty = \"dashed\", col = \"darkgray\")\n\n\n\n\n\n\n10.2.2 Weibull AFT model\nThe Weibull AFT model generalizes the exponential model in the same way that the Weibull distribution generalizes the exponential distribution. As in the exponential AFT model, the rate parameter is determined by \\(x\\) and \\(\\beta\\) as in Equation 10.3. The hazard function in a Weibull AFT model is \\[\n  h(t, \\alpha, \\beta, x) =\n  \\alpha \\lambda(\\beta, x)^\\alpha t^{\\alpha - 1},\n\\] which is the hazard function of the Weibull(\\(\\alpha\\), \\(\\lambda\\)) distribution where \\(\\lambda = \\lambda(\\beta, X)\\). The corresponding cumulative hazard function is \\[\n    H(t, \\alpha, \\beta, x) =\n    \\big(\\lambda(\\beta, x) t\\big)^\\alpha.\n\\] With left-truncated and right-censored data as in Equation 10.4, the log likelihood for \\(\\alpha\\) and \\(\\beta\\) is \\[\n    \\ell(\\alpha, \\beta) =\n    \\sum_{i = 1}^n \\Big(\\delta_i \\ln h(\\texit_i, x_i, \\alpha, \\beta)\n      - H(t_i, x_i, \\alpha, \\beta) + H(\\tentry_i, x_i, \\alpha, \\beta)\\Big).\n\\tag{10.5}\\] Point and interval estimates of \\(\\alpha\\) and \\(\\beta\\) can be obtained using maximum likelihood estimation or Bayesian methods.\n\nR\n\n\n\n\nAFT-Weibull.R\n\n## Weibull accelerated failure time (AFT) model\n\n# The rats data is from:\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)               # rats data and survreg()\n\n# Weibull accelerated failure time model\n# The Weibull distribution is the default for survival::survreg().\n# Coefficients are log scale parameters = -log rate parameters.\naft_weib &lt;- survreg(Surv(time, status) ~ rx, data = rats)\nexp(-coef(aft_weib)[\"rx\"])      # estimated rate (x = 0) and rate ratio\nexp(-confint(aft_weib)[\"rx\", ]) # log-transformed 95% Wald confidence intervals\nshapehat &lt;- 1 / aft_weib$scale  # shape parameter point estimate\nshapehat\n\n# shape parameter Z-score and p-value\n# The null hypothesis shape = 0 corresponds to an exponential distribution.\nlnshapevar &lt;- vcov(aft_weib)[\"Log(scale)\", \"Log(scale)\"]\nshapeZ &lt;- log(shapehat) / sqrt(lnshapevar)\nshapeZ\n2 * pnorm(-abs(shapeZ))\n\n# shape parameter 95% CI\nexp(-log(aft_weib$scale) + c(-1, 1) * qnorm(.975) *\n    sqrt(vcov(aft_weib)[\"Log(scale)\", \"Log(scale)\"]))\n\n# estimated rate in the exposed\nlnrate1hat &lt;- exp(-sum(coef(aft_weib)))\nlnrate1var &lt;- as.numeric(c(1, 1, 0) %*% vcov(aft_weib) %*% c(1, 1, 0))\nlnrate1ci &lt;- lnrate1hat + c(-1, 1) * qnorm(0.975) * sqrt(lnrate1var)\nexp(lnrate1hat)                 # point estimate\nexp(lnrate1ci)                  # log-transformed 95% Wald confidence interval\n\n# compare with nonparametric estimates\n# Nelson-Aalen estimates (with Fleming-Harrington correction for ties)\n# Formula ~ rx produces a separate estimate for each treatment group.\n# Arguments stype = 2 and ctype = 2 produces NA estimate with FH correction.\nratsNA &lt;- survfit(Surv(time, status) ~ rx, data = rats,\n                  stype = 2, ctype = 2, conf.type =  \"log-log\")\n\n# plot of Nelson-Aalen cumulative hazard curves in treated and untreated rats\nplot(ratsNA, fun = \"cumhaz\", col = c(\"darkgray\", \"black\"),\n     xlab = \"Weeks after treatment\", ylab = \"Cumulative hazard\")\ngrid()\nlegend(\"topleft\", bg = \"white\", lty = rep(c(\"solid\", \"dashed\"), times = 2),\n       col = rep(c(\"black\", \"darkgray\"), each = 2),\n       legend = c(\"Control (Nelson-Aalen)\", \"Control (Weibull AFT model)\",\n                  \"Treated (Nelson-Aalen)\", \"Treated (Weibull AFT model)\"))\n\n# estimated cumulative hazards from Weibull AFT model\naft_weib &lt;- survreg(Surv(time, status) ~ rx, data = rats)\nrate1_weib &lt;- as.numeric(exp(-c(1, 1) %*% coef(aft_weib)))\nrate0_weib &lt;- exp(-coef(aft_weib)[\"(Intercept)\"])\nshape_weib &lt;- 1 / aft_weib$scale\nH_weib &lt;- function(t, shape, rate) (rate * t)^shape\n\n# add lines to plot\nt &lt;- seq(0, max(ratsNA$time), by = 0.1)\nlines(t, H_weib(t, shape_weib, rate1_weib), lty = \"dashed\")\nlines(t, H_weib(t, shape_weib, rate0_weib), lty = \"dashed\", col = \"darkgray\")\n\n\n\n\n\n\n10.2.3 Log-logistic AFT model\nThe log-logistic AFT model has the same likelihood as the Weibull AFT model except that we replace the Weibull hazard and cumulative hazard functions with the log-logistic hazard and cumulative hazard functions. As in the exponential and Weibull AFT models, the rate parameter is determined by \\(x\\) and \\(\\beta\\) as in Equation 10.3. The log-logistic hazard function is \\[\n  h(t, x, \\alpha, \\beta) =\n  \\frac{\\alpha \\lambda(x, \\beta)^\\alpha t^{\\alpha - 1}}\n    {1 + \\big(\\lambda(x, \\beta) t\\big)^\\alpha},\n\\] and the cumulative hazard function is \\[\n  H(t, X, \\beta, \\alpha) =\n  \\ln\\Bigl(1 + t^\\alpha e^{-\\alpha (\\beta_0 + \\beta_1 X)}\\Bigr).\n\\] With left-truncated and right-censored data as in Equation 10.4, the likelihood is given by Equation 10.5. Point and interval estimates of \\(\\alpha\\) and \\(\\beta\\) can be obtained using maximum likelihood estimation or Bayesian methods.\n\nR\n\n\n\n\nAFT-loglogistic.R\n\n## Log-logistic accelerated failure time (AFT) model\n\n# The rats data is from:\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)               # rats data and survreg()\n\n# Log-logistic accelerated failure time model\n# Default distribution is Weibull, so we must specify dist = \"loglogistic\".\n# Coefficients are log scale parameters = -log rate parameters.\naft_llog &lt;- survreg(Surv(time, status) ~ rx, data = rats, dist = \"loglogistic\")\nexp(-coef(aft_llog)[\"rx\"])      # estimated rate (x = 0) and rate ratio\nexp(-confint(aft_llog)[\"rx\", ]) # log-transformed 95% Wald confidence intervals\n1 / aft_llog$scale              # shape parameter point estimate\n\n# shape parameter 95% CI\nexp(-log(aft_llog$scale) + c(-1, 1) * qnorm(.975) *\n    sqrt(vcov(aft_llog)[\"Log(scale)\", \"Log(scale)\"]))\n\n# estimated rate in the exposed\nlnrate1hat &lt;- -sum(coef(aft_llog))\nlnrate1var &lt;- as.numeric(c(1, 1, 0) %*% vcov(aft_llog) %*% c(1, 1, 0))\nlnrate1ci &lt;- lnrate1hat + c(-1, 1) * qnorm(0.975) * sqrt(lnrate1var)\nexp(lnrate1hat)                 # point estimate\nexp(lnrate1ci)                  # log-transformed 95% Wald confidence interval\n\n# compare with nonparametric estimates\n# Nelson-Aalen estimates (with Fleming-Harrington correction for ties)\n# Formula ~ rx produces a separate estimate for each treatment group.\n# Arguments stype = 2 and ctype = 2 produces NA estimate with FH correction.\nratsNA &lt;- survfit(Surv(time, status) ~ rx, data = rats,\n                  stype = 2, ctype = 2, conf.type =  \"log-log\")\n\n# plot of Nelson-Aalen cumulative hazard curves in treated and untreated rats\nplot(ratsNA, fun = \"cumhaz\", col = c(\"darkgray\", \"black\"),\n     xlab = \"Weeks after treatment\", ylab = \"Cumulative hazard\")\ngrid()\nlegend(\"topleft\", bg = \"white\", lty = rep(c(\"solid\", \"dashed\"), times = 2),\n       col = rep(c(\"black\", \"darkgray\"), each = 2),\n       legend = c(\"Control (Nelson-Aalen)\", \"Control (Log-logistic AFT model)\",\n                  \"Treated (Nelson-Aalen)\", \"Treated (Log-logistic AFT model)\"))\n\n# estimated cumulative hazards from log-logistic AFT model\naft_llog &lt;- survreg(Surv(time, status) ~ rx, data = rats, dist = \"loglogistic\")\nrate1_llog &lt;- as.numeric(exp(-c(1, 1) %*% coef(aft_llog)))\nrate0_llog &lt;- exp(-coef(aft_llog)[\"(Intercept)\"])\nshape_llog &lt;- 1 / aft_llog$scale\nH_llog &lt;- function(t, shape, rate) log(1 + (rate * t)^shape)\n\n# add lines to plot\nt &lt;- seq(0, max(ratsNA$time), by = 0.1)\nlines(t, H_llog(t, shape_llog, rate1_llog), lty = \"dashed\")\nlines(t, H_llog(t, shape_llog, rate0_llog), lty = \"dashed\", col = \"darkgray\")",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Two-Sample Survival Analysis and the Cox Model</span>"
    ]
  },
  {
    "objectID": "survival2.html#sec-logrank",
    "href": "survival2.html#sec-logrank",
    "title": "10  Two-Sample Survival Analysis and the Cox Model",
    "section": "10.3 Log-rank test",
    "text": "10.3 Log-rank test\nThe log rank test (Mantel 1966) is a nonparametric score test for the null hypothesis that two groups have the same failure time distribution. Let \\(t_1 &lt; t_2 &lt; \\cdots &lt; t_m\\) be the distinct times where failures occur. Let \\(d_i\\) denote the number of failures at time \\(t_i\\), and let \\(n_i\\) denote the number of people in the risk set \\(\\mathcal{R}_i\\) (i.e., the set of people at risk of failure at time \\(t_i\\)). Let \\(X\\) be a binary covariate. Let \\(n_{1i}\\) denote the number of people in \\(R_i\\) with \\(X = 1\\), and let \\(d_{1i}\\) denote the number of of these that fail at time \\(t_i\\). For now, we assume no tied failure times, so \\(d_{1i} = 0\\) or \\(d_{1i} = 1\\) for each \\(i\\).\n\n10.3.1 Observed and expected failures among the exposed\nThe log-rank test is a score test, so it uses a test statistic calculated assuming the null hypothesis is true. Here, the null hypothesis is that the survival time distribution is the same among the exposed (\\(X = 1\\)) and the unexposed (\\(X = 0\\)). Under the null hypothesis, the \\(d_i\\) failures at time \\(t_i\\) occur in individuals randomly chosen from \\(n_i\\) individuals in the risk set \\(\\mathcal{R}(t_i)\\). The number of exposed individuals \\(D_{1i}\\) among the randomly chosen \\(d_i\\) individuals has a hypergeometric distribution (see Section 7.1.1) with mean \\[\n    \\E(D_{1i}) = d_{i} p_{1i}\n\\] where \\[\n    p_{1i} = \\frac{n_{1i}}{n_i}\n\\] is the proportion of the risk set that is exposed. Its variance is \\[\n  \\Var(D_{1i})\n  = p_{1i} (1 - p_{1i}) \\frac{d_i (n_i - d_i)}{n_i - 1}.\n\\] In the special case where \\(d_i = 1\\), this hypergeometric distribution is a Bernoulli(\\(p_{1i}\\)) distribution with mean \\(p_{1i}\\) and variance \\(p_{1i} (1 - p_{1i})\\).\n\n\n10.3.2 Chi-squared statistic\nThe numerator of the log-rank chi-squared statistic is total number of observed failures among the exposed minus the total number of expected failures among the exposed under the null hypothesis: \\[\n  U = \\sum_{i = 1}^m \\Big(d_{1i} - d_i p_{1i}\\Big).\n\\tag{10.6}\\] The denominator is the variance of \\(U_\\text{logrank}\\), which is \\[\n  \\Var(U)\n  = \\sum_{i = 1}^m p_{1i} (1 - p_{1i}) \\frac{d_i (n_i - d_i)}{n_i - 1}\n\\] because the \\(D_{1i}\\) are independent. The log-rank test statistic is \\[\n  \\chi^2_\\text{logrank} = \\frac{U^2}{\\Var(U)}.\n\\tag{10.7}\\] When the number of observed failures is large, this has a chi-squared distribution with one degree of freedom under the null. We reject the null hypothesis for large values of \\(\\chi^2_\\text{logrank}\\). We get exactly the same chi-squared statistic and p-value if we use the observed and expected number of failures among the unexposed.\n\n\n\n10.3.3 Weighted log-rank tests\nMany nonparametric tests of differences between survival curves are weighted versions of the log-rank test. The weighted sum of the differences between the observed and expected numbers of events among the exposed is \\[\n    U = \\sum_{i = 1}^m w_i (d_{1i} - d_i p_{1i}),\n\\] and the corresponding variance is \\[\n  V = \\sum_{i = 1}^m w_i^2 \\bigg(p_{1i}(1 - p_{1i}) \\frac{d_i (n_i - d_i)}{n_i - 1}\\bigg).\n\\] Table 10.1 shows the weights used by various tests. In the weights, \\(\\hat{S}(t)\\) is the Kaplan-Meier estimate of the survival function, \\(\\hat{S}_0(t)\\) is the Kaplan-Meier estimate among the unexposed, and \\(\\hat{S}_1(t)\\) is the Kaplan-Meier estimate among the exposed. The Harrington-Fleming test is equivalent to the log-rank test when \\(\\rho = 0\\), and it approximates the Peto-Prentice test when \\(\\rho = 1\\). The weighted tests often give larger weights to earlier survival times, where there is usually more data.\n\n\n\nTable 10.1: Weighted log-rank tests (Aalen, Borgan, and Gjessing 2008).\n\n\n\n\n\nTest\nWeight (proportional to \\(w_i\\))\nReference\n\n\n\n\nLog-rank\n\\(1\\)\n(Mantel 1966)\n\n\nGehan-Breslow\n\\(n_i\\)\n(Gehan 1965; N. Breslow 1970)\n\n\nTarone-Ware\n\\(\\sqrt{n_i}\\)\n(Tarone and Ware 1977)\n\n\nHarrington-Fleming\n\\(\\hat{S}(t_{i - 1})^\\rho\\)\n(Harrington and Fleming 1982)\n\n\nEfron\n\\(\\hat{S}_0(t_{i - 1}) \\hat{S}_1(t_{i - 1})\\)\n(Efron 1967)\n\n\nPeto-Prentice\n\\(\\prod_{i: t_i &lt; t} \\Big(1 - \\frac{d_i}{n_i + 1}\\Big)\\)\n(Peto and Peto 1972; Prentice 1978)\n\n\n\n\n\n\n\nR\n\n\n\n\nlogrank.R\n\n## Log-rank test\n\n# The log-rank test is done using survdiff() is in the survival package.\n# The rats data is from:\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)               # rats data and survdiff\nlibrary(lmtest)                 # coeftest() for Wald test of coefficients\n\n# log-rank test (rho = 0 by default)\n# More tumor incidence observed than expected under the null in treated rats.\nsurvdiff(Surv(time, status) ~ rx, data = rats)\n\n# Peto-Prentice test (rho = 1)\n# More tumor incidence observed than expected under the null in treated rats.\nsurvdiff(Surv(time, status) ~ rx, data = rats, rho = 1)\n\n# comparison with parametric AFT models\n# exponential\naft_exp &lt;- survreg(Surv(time, status) ~ rx, data = rats, dist = \"exponential\")\ncoeftest(aft_exp)[\"rx\", \"Pr(&gt;|z|)\"]\n# Weibull\naft_weib &lt;- survreg(Surv(time, status) ~ rx, data = rats, dist = \"weibull\")\ncoeftest(aft_weib)[\"rx\", \"Pr(&gt;|z|)\"]\n# log-logistic\naft_llog &lt;- survreg(Surv(time, status) ~ rx, data = rats, dist = \"loglogistic\")\ncoeftest(aft_llog)[\"rx\", \"Pr(&gt;|z|)\"]",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Two-Sample Survival Analysis and the Cox Model</span>"
    ]
  },
  {
    "objectID": "survival2.html#sec-Cox-model",
    "href": "survival2.html#sec-Cox-model",
    "title": "10  Two-Sample Survival Analysis and the Cox Model",
    "section": "10.4 Cox model",
    "text": "10.4 Cox model\nThe Cox proportional hazards model [Cox (1972)} is a regression model that can estimate hazard ratios without making any other assumption about the underlying failure time distributions.1 It is a semiparametric model, which means that it has a parametric component and a nonparametric component. In the Cox model, the hazard function is \\[\n  h(t, x, \\beta) = e^{\\beta_1 x_1 + \\ldots + \\beta_k x_k} h_0(t),\n\\tag{10.8}\\] where the first term on the right-hand side is the relative hazard function and \\(h_0(t)\\) is an unspecified baseline hazard function. The parametric component of the model is the relative hazard, where each predictor has a multiplicative effect on the hazard function. The nonparametric component of the model is the unspecified baseline hazard. There is no intercept because \\(h_0(t)\\) represents the hazard for an individual with all predictors equal to zero.\nFor simplicity, we will focus on Cox models with a single binary predictor \\(X\\). In these models, the relative hazard is \\[\n  e^{\\beta_1 x} =\n  \\begin{cases}\n    1           & \\text{in the unexposed,}\\\\\n    e^{\\beta_1} & \\text{in the exposed.}\n  \\end{cases}\n\\] Thus, the hazard ratio comparing the exposed to the unexposed is \\(e^{\\beta_1}\\). The Cox model gives us a way of estimating the hazard ratio without making any assumption about the distributions of times to events in the exposed and unexposed except for proportional hazards. In a Cox model with a single binary covariate, the log-rank test from Section 10.3 is a score test of the null hypothesis that \\(\\beta = 0\\).\n\n10.4.1 Proportional hazards assumption\nThe proportional hazards assumption is much weaker than any assumption that the times to events in the exposed and unexposed come from a specific family, such as exponential, Weibull, or log-logistic distributions. However, it is still an assumption. The exponential and Weibull AFT models are special cases of the Cox model because the hazard ratio comparing the exposed to the unexposed is always \\((\\lambda_1 / \\lambda_0)^\\alpha\\) where \\(\\lambda_1\\) is the rate parameter for the exposed, \\(\\lambda_0\\) is the rate parameter for the unexposed, and \\(\\alpha\\) is the shape parameter. When the underlying failure times are truly exponential or Weibull, the corresponding AFT models will have slightly more power than the Cox model. The log-logistic AFT model is not a proportional hazards model because the hazard ratio changes over time, as shown in Equation 10.1.\n\n\n10.4.2 Partial likelihood\nThe Cox model is fit using a partial likelihood, which leaves out parts of the full likelihood but behaves like a likelihood for the purposes of maximum likelihood estimation. The Cox partial likelihood retains critical information about the hazard ratios without placing constraints on the baseline hazard. To do this, it relies on a fundamental relationship between conditional probabilities and hazard functions.\nSuppose \\(T_1\\) and \\(T_2\\) are independent failure times with survival functions \\(S_1(t)\\) and \\(S_2(t)\\), respectively. Let \\(T = \\min(T_1, T_2)\\). Then the survival function of \\(T\\) is \\[\n  S(t) = S_1(t)S_2(t)\n\\] because \\(T &gt; t\\) if and only if \\(T_1 &gt; t\\) and \\(T_2 &gt; t\\). The cumulative hazard function of \\(T\\) is \\[\n  H(t) = -\\ln S(t)\n  = -\\ln S_1(t) - \\ln S_2(t)\n  = H_1(t) + H_2(t),\n\\] where \\(H_1(t)\\) an \\(H_2(t)\\) are the cumulative hazard functions of \\(T_1\\) and \\(T_2\\). Taking derivatives, we get the hazard function \\[\n  h(t) = H'(t)\n  = H_1'(t) + H_2'(t)\n  = h_1(t) + h_2(t),\n\\] where \\(h_1(t)\\) and \\(h_2(t)\\) are the hazard functions of \\(T_1\\) and \\(T_2\\).\nThis logic extends to the minimum of any finite set of survival times \\(T_1, \\ldots, T_n\\). The survival function of \\(T = \\min(T_1, \\ldots, T_n)\\) is \\[\n  S(t) = \\prod_{i = 1}^n S_i(t),\n\\tag{10.9}\\] and the hazard function is \\[\n  h(t) = \\sum_{i = 1}^n h_i(t),\n\\tag{10.10}\\] where \\(S_i(t)\\) is the survival function and \\(h_i(t)\\) is the hazard function of \\(T_i\\).\nThe probability density function is the product of the hazard in Equation 10.10 and the survival in Equation 10.9. The probability density for a failure in any individual at time \\(t\\) is \\[\n  \\big(h_1(t) + \\ldots + h_n(t)\\big) \\prod_{i = 1}^n S_i(t).\n\\tag{10.11}\\] If the first failure occurred in person \\(k\\), then the likelihood contribution would be \\[\n  h_k(t) \\prod_{i = 1}^n S_i(t).\n\\tag{10.12}\\] because the person who has an event contributes a hazard term and everyone (including the person who failed) contributes a survival term. To calculate the conditional probability that the failure occurred in person \\(k\\) given that there was a failure at time \\(t\\), we divide the likelihood in Equation 10.12 by the likelihood in Equation 10.11: \\[\n  \\frac{h_k(t) \\prod_{i = 1}^n S_i(t)}\n       {\\big(h_1(t) + \\ldots + h_n(t)\\big) \\prod_{i = 1}^n S_i(t)}\n  = \\frac{h_k(t)}{h_1(t) + \\ldots + h_n(t)}.\n\\] The survival functions cancel out, leaving only the hazards. Given that there is a failure at time \\(t\\), the probability that it occurred in person \\(k\\) is proportional to their hazard \\(h_k(t)\\).\nThe same logic applies to failures other than the first. As before, let \\(\\mathcal{R}(t)\\) denote the risk set (i.e., the set of individuals at risk of an observed event) at time \\(t\\). Given that a failure occurs at time \\(t\\), the probability that it occurred in person \\(k\\) is \\[\n  \\frac{h_k(t)}{\\sum_{j \\in \\mathcal{R}(t)} h_j(t)}.\n\\] At any failure time \\(t\\), these probabilities add up to one. If we substitute the hazard function for the Cox model from Equation 10.8, we get \\[\n  \\frac{e^{\\beta x_k} h_0(t)}{\\sum_{j \\in \\mathcal{R}(t) e^{\\beta x_j} h_0(t)}}\n  = \\frac{e^{\\beta x_k}}{\\sum_{j \\in \\mathcal{R}(t) e^{\\beta x_j}}}.\n\\] because the baseline hazard \\(h_0(t)\\) cancels out. When there are no tied failure times, let \\(i\\) be the index of the individual who has an event at time \\(t_i\\). Then the Cox partial likelihood is \\[\n  L_\\text{Cox}(\\beta)\n  = \\prod_{i = 1}^m \\frac{e^{\\beta x_i}}{\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta x_j}}.\n\\] The risk sets are determined in exactly the same way as the Kaplan-Meier and Nelson-Aalen estimators in Chapter 6, so the Cox model can handle left truncation (i.e., delayed entry) and right censoring. It uses the same data from Equation 10.4 as the AFT models. For estimation of \\(\\beta\\), the partial likelihood can be used just like a normal likelihood. This is a consequence of the fact that it can be derived as a profile likelihood where the likelihood for \\(\\beta\\) is the full likelihood maximized over all possible baseline hazard functions (Johansen 1983).\n\n\n10.4.3 Correction for ties*\nThere are three common methods for dealing with ties in a Cox model (in order of complexity): Breslow, Efron, and exact. The Breslow approximation (Peto 1972; N. Breslow 1974) is the simplest but least accurate. The exact method is accurate but computationally complex (Kalbfleisch and Prentice 2011). The Efron approximation (Efron 1977) is both accurate and computationally efficient. All of these methods assume that failures happen in continuous time and that ties are caused by rounding off of survival times (i.e., times measured to the day or week rather than hour, minute, and second).\n\n10.4.3.1 Exact method\nIf there are tied survival times at time \\(t_i\\), then the exact method calculates the mean partial likelihood contribution at time \\(t_i\\) over all \\(d_i!\\) possible ways of breaking ties among the people who failed at time \\(t_i\\) (Kalbfleisch and Prentice 2011). These calculations get complex quickly—there are \\(5! = 120\\) ways to break ties among 5 events, \\(10! = 3,628,800\\) ways among 10 events, \\(15! \\approx 1.3\\) trillion ways among 15 events, and so on. For simplicity, we illustrate the calculation for two events.\nLet \\(A\\) and \\(B\\) denote the indices of the individuals who fail at time \\(t_i\\), with \\(d_i = 2\\). Let \\(x_A\\) and \\(x_B\\) denote their covariates. There are two ways to break the tie: \\(A\\) fails first or \\(B\\) fails first. If \\(A\\) fails first, then the partial likelihood contribution from the two failures is \\[\n  \\frac{e^{\\beta x_A}}{\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta x_j}}\n  \\times \\frac{e^{\\beta x_B}}{\\Big(\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta x_j}\\Big) - e^{\\beta x_A}},\n\\] where the denominator in the second term accounts for the fact that \\(A\\) is no longer in the risk set when \\(B\\) fails. If \\(B\\) fails first, the likelihood contribution from the two failures is \\[\n  \\frac{e^{\\beta x_B}}{\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta x_j}}\n  \\times \\frac{e^{\\beta x_A}}{\\Big(\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta x_j}\\Big) - e^{\\beta x_B}},\n\\] where the denominator in the second term accounts for the fact that \\(B\\) is no longer in the risk set when \\(A\\) fails. In both cases, the numerator is \\(e^{\\beta x_A + x_B}\\), but the denominator is slightly different. If both possibilities are equally likely (which is true if \\(\\beta = 0\\) or \\(x_A = x_B\\)), the average likelihood contribution is \\[\n  \\begin{aligned}\n    &\\frac{1}{2} \\frac{e^{\\beta (x_A + x_B)}}\n      {\\Big(\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta x_j}\\Big) \\Big[\\Big(\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta X_j}\\Big) - e^{\\beta x_A}\\Big]}  \\\\\n    &\\qquad + \\frac{1}{2} \\frac{e^{\\beta (x_A + x_B)}}\n      {\\Big(\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta x_j}\\Big) \\Big[\\Big(\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta X_j}\\Big) - e^{\\beta x_B}\\Big]}\n  \\end{aligned}\n\\tag{10.13}\\] Extending this to larger numbers of ties is straightforward but tedious. When there are failure times with a large number of ties, this approach becomes computationally intractable.\n\n\n10.4.3.2 Efron approximation\nLet \\(\\mathcal{D}(t_i)\\) denote the set of individuals who fail at time \\(t_i\\). The Efron approximation (Efron 1977) approximates the denominator of the exact estimate using the mean relative hazard among the \\(d_i\\) individuals in \\(\\mathcal{D}(t_i)\\), which is \\[\n  \\frac{1}{d_i} \\sum_{j \\in \\mathcal{D}(t_i)} e^{\\beta x_j}.\n\\] This is also the average amount taken out of the sum in the denominator for each of the first \\(d_i - 1\\) failures at time \\(t_i\\). The Efron approximation to the exact mean likelihood contribution is \\[\n  \\frac{\\prod_{j \\in \\mathcal{D}(t_i)} e^{\\beta x_j}}\n    {\\prod_{k = 1}^{d_i} \\Big(\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta x_j} - \\frac{k - 1}{d_i} \\sum_{j \\in \\mathcal{D}(t_i)} e^{\\beta x_j}\\Big)}\n\\] This is easy to calculate and is a good approximation to the exact mean likelihood contribution even when there are many ties.\nIn our example with two tied failures, the Efron approximation to the likelihood contribution in equation~[eq:exact] is \\[\\frac{e^{\\beta (x_A + x_B)}}{\\Big(\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta x_j}\\Big) \\Big(\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta x_j} - \\frac{1}{2} \\big(e^{\\beta x_A} + e^{\\beta x_B}\\big)\\Big)}.\\] In the second term in the denominator, we have taken out the average of individuals \\(A\\) and \\(B\\).\n\n\n10.4.3.3 Breslow approximation\nThe Breslow approximation (Peto 1972; N. E. Breslow 1972) simply ignores the fact that failures are removed from the risk set, giving the following approximation to the exact likelihood contribution: \\[\n  \\frac{\\prod_{j \\in D_i} e^{\\beta X_j}}{\\Big(\\sum_{j \\in R_i} e^{\\beta X_j}\\Big)^{d_i}}.\n\\] In our example with tied failure times in \\(y\\) and \\(z\\), the Breslow approximation to the likelihood contribution in Equation 10.13 is \\[\n  \\frac{e^{\\beta (x_A + x_B)}}{\\Big(\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta x_j}\\Big)^2}.\n\\] This leaves the first person to fail (\\(A\\) or \\(B\\)) in the risk set when the second person fails. When the size of the risk set is large compared to the number of failures, this is not a terrible approximation. However, the Efron approximation stays accurate when ties become more severe.\n\n\n\n10.4.4 Estimation of baseline survival and cumulative hazard*\nGiven an estimate of \\(\\hat{\\beta}\\) of \\(\\beta_\\true\\), we can estimate the baseline cumulative hazard \\[\n  H_0(t) = \\int_0^t h(u) \\,\\dif u\n\\] and the baseline survival \\[\n  S_0(t) = e^{-H_0(t)}.\n\\] This allows us to estimate the cumulative hazard or survival for any given combination of covariates \\(x\\):We can also estimate the cumulative hazard function for any given covariate \\(x\\): \\[\n  \\hat{H}(t, x) = e^{\\hat{\\beta} x} \\hat{H}_0(t)\n\\] and \\[\n  \\hat{S}(t, x) = e^{-\\hat{H}(t, x)}.\n\\] Thus, a Cox model can be used to estimate risks, odds, and measures of association such as the risk difference, risk ratio, or odds ratio. As in Section 10.1, these measures of association will often depend on \\(t\\).\nOne way to estimate the baseline cumulative hazard is to calculate a Nelson-Aalen estimate in the subset with \\(X = 0\\). However, it is much more efficient to use all of the data. Here, we look at three methods that do this. Two of them are generalizations of the Nelson-Aalen estimator, and one is a generalization of the Kaplan-Meier estimator. Variance estimation for these baseline hazard estimators accounts for two sources of uncertainty: the uncertainty in our estimate of \\(\\beta_\\true\\) and the uncertainty that we would have in the baseline cumulative hazard or survival even if we knew \\(\\beta_\\true\\) (similar to the variance of the Nelson-Aalen estimator in Section 6.3 or Kaplan-Meier estimator in Section 6.2). For simplicity, we will focus only on the point estimates.\n\n10.4.4.1 Breslow estimate of the baseline cumulative hazard\nThe Breslow estimator of \\(H_0(t)\\) is \\[\n  \\hat{H}_0(t)\n  = \\sum_{i: t_i \\leq t} \\frac{d_i}{\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\hat{\\beta} x_j}},\n\\] where \\(\\hat{\\beta}\\) is the maximum partial likelihood estimate of \\(\\beta\\) (N. E. Breslow 1972). When \\(\\hat{\\beta} = 0\\), this reduces to the Nelson-Aalen estimator from Section 6.3.\n\n\n10.4.4.2 Efron estimate of the baseline cumulative hazard\nThe Efron estimate of \\(H_0(t)\\) handles ties in the same way as the Efron approximation for the partial likelihood. Whenever there are \\(d_i &gt; 1\\) failures at time \\(t_i\\) in risk set \\(R_i\\), the contribution to the cumulative hazard estimate is \\[\n  \\sum_{k = 1}^{d_i}\n    \\frac{1}{\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\hat{\\beta} x_j}\n      - \\frac{k - 1}{d_i} \\sum_{j \\in \\mathcal{D}(t_i)} e^{\\hat{\\beta} X_j}},\n\\] where \\(\\mathcal{D}(t_i)\\) represents the set of \\(d_i\\) individuals who failed. The Efron estimate of \\(H_0(t)\\) is \\[\n  \\hat{H}_0(t)\n  = \\sum_{i : t_i \\leq t} \\sum_{k = 1}^{d_i}\n    \\frac{1}{\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\hat{\\beta} x_j}\n      - \\frac{k - 1}{d_i} \\sum_{j \\in \\mathcal{D}(t_i)} e^{\\hat{\\beta} x_j}}.\n\\] When \\(\\hat{\\beta} = 0\\), this reduces to the Nelson-Aalen estimator with the Fleming-Harrington correction for ties from Section 6.3.1.\n\n\n10.4.4.3 Kalbfleisch-Prentice estimate of the baseline survival\nThe Kalbfleisch-Prentice estimate of the survival function is based on treating the observed failure times as discrete time points where the probability of failure at time \\(t_i\\) is \\(1 - s_i\\) when \\(X = 0\\). Let \\(\\mathbf{s} = (s_1, \\ldots, s_m)\\). Then the likelihood for the data is \\[\n  L(\\mathbf{s}) = \\prod_{i = 1}^m \\bigg(\\prod_{j \\in \\mathcal{D}(t_i)} \\Big(1 - s_i^{\\exp(\\hat{\\beta} x_j)}\\Big) \\prod_{j \\in \\mathcal{R}(t_i) \\setminus \\mathcal{D}(t_i)} s_i^{\\exp(\\hat{\\beta} X_j)}\\bigg),\n\\] where \\(\\mathcal{R}(t_i) \\setminus \\mathcal{D}(t_i)\\) is the set of people in the risk set at time \\(t_i\\) who did not fail. The corresponding log likelihood is \\[\n  \\ell(\\mathbf{s})\n  = \\sum_{i = 1}^m \\bigg(\\sum_{j \\in \\mathcal{D}(t_i)} \\ln\\Big(1 - s_i^{\\exp(\\hat{\\beta} x_j)}\\Big)\n    + \\sum_{j \\in \\mathcal{R}(t_i) \\setminus \\mathcal{D}(t_i)} e^{\\hat{\\beta} x_j} \\ln s_i\\bigg).\n\\] Differentiating \\(\\ell(\\mathbf{s})\\) with respect to \\(s_i\\) and rearranging, we find that \\(s_i\\) solves the equation \\[\n  \\sum_{j \\in \\mathcal{D}(t_i)} \\frac{e^{\\hat{\\beta} x_j}}{1 - s_i^{\\exp(\\hat{\\beta} x_j)}}\n  = \\sum_{j \\in \\mathcal{R}(t_i)} e^{\\hat{\\beta} X_j}.\n\\tag{10.14}\\] When \\(d_i = 1\\) and person \\(i\\) fails at time \\(t_i\\), this is solved by \\[\n  \\hat{s}_i\n  = \\bigg(1 - \\frac{e^{\\hat{\\beta} x_i}}{\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\hat{\\beta} x_j}}\\bigg)^{\\exp(-\\beta x_i)}.\n\\] When \\(d_i &gt; 1\\), Equation 10.14 must be solved numerically. The Kalbfleisch-Prentice estimate of the survival function is \\[\n  \\hat{S}_0(t) = \\prod_{i : t_i \\leq t} \\hat{s}_i.\n\\] When \\(\\hat{\\beta} = 0\\), this reduces to the Kaplan-Meier survival estimate from Section 6.2.\n\nR\n\n\n\n\nCox-model.R\n\n## Cox proportional hazards regression model\n\n# The rats data is from:\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)               # rats data and survreg()\n\n# Cox model with Efron correction for ties (the default)\n# With a single binary predictor, the global score test is the log-rank test.\ncox_efron &lt;- coxph(Surv(time, status) ~ rx, data = rats)\nnames(cox_efron)\nsummary(cox_efron)\n\n# coefficient point estimates, confidence intervals, and covariance matrix\ncoef(cox_efron)\nconfint(cox_efron)\nvcov(cox_efron)\n\n# hazard ratio and confidence intervals\nexp(coef(cox_efron))\nexp(confint(cox_efron))\n\n# Breslow correction for ties\ntable(rats$time[rats$status == 1])  # table of number of tumor onsets\ncox_breslow &lt;- coxph(Surv(time, status) ~ rx, data = rats,\n                     ties = \"breslow\")\nsummary(cox_breslow)\ncox_breslow$method\n\n# exact correction for ties\ncox_exact &lt;- coxph(Surv(time, status) ~ rx, data = rats,\n                   ties = \"exact\")\nsummary(cox_exact)\ncox_exact$method\n\n# predicted survival in both treatment groups\n# Method for estimating survival mirrors method used to correct for ties.\nS_pred &lt;- survfit(cox_efron, newdata = data.frame(rx = c(0, 1)),\n                  conf.type = \"log-log\")\nS_summary &lt;- summary(S.efron, times = c(20, 40, 60, 80))\nS_summary\nnames(S_summary)\nS_summary$newdata     # order of predicted survival probabilities, etc.\nS_summary$conf.int    # confidence level\nS_summary$lower       # lower 95% confidence limits\nS_summary$upper       # upper 95% confidence limits\n\n# predicted cumulative hazard and confidence limits\n# The lower bound for H(t) comes from the upper bound for S(t) and vice versa.\nS_summary$cumhaz\n-log(S_summary$surv)\n-log(S_summary$upper)\n-log(S_summary$lower)\n\n# plot of predicted cumulative hazards\n# Uses data from both groups for each predicted cumulative hazard function.\nplot(S_pred, fun = \"cumhaz\", lty = c(\"dashed\", \"solid\"),\n     xlab = \"Weeks since treatment\", ylab = \"Cumulative hazard\")\ngrid()\nlegend(\"topleft\", bg = \"white\", lty = rep(c(\"solid\", \"dashed\"), each = 2),\n       col = rep(c(\"black\", \"darkgray\"), 2),\n       legend = c(\"Treated rats (Cox model)\", \"Treated rats (Nelson-Aalen)\",\n                  \"Untreated rats (Cox model)\", \"Unreated rats (Nelson-Aalen)\"))\n# add Nelson-Aalen curves\nNAest &lt;- survfit(Surv(time, status) ~ rx, data = rats, stype = 2, ctype = 2,\n                 conf.type = \"log-log\")\nlines(NAest, fun = \"cumhaz\", col = \"darkgray\", lty = c(\"dashed\", \"solid\"))\n\n# predicted survival in the untreated rats\nS1_pred &lt;- survfit(cox_efron, newdata = data.frame(rx = 1),\n                   conf.type = \"log-log\")\nsummary(S1_pred, times = c(20, 40, 60, 80))\n\n# predicted survival in the treated rats\nS0_pred &lt;- survfit(cox_efron, newdata = data.frame(rx = 0),\n                   conf.type = \"log-log\")\nsummary(S0_pred, times = c(20, 40, 60, 80))\n\n# plot of risk ratio, hazard ratio, and odds ratio over time\n# Risks are zero until time[3], which is 3.\n# Negative indices remove elements from vectors.\nodds &lt;- function(p) p / (1 - p)\nt_indices &lt;- -(1:2)\ntimes &lt;- S0_pred$time[t_indices]\nR1_pred &lt;- 1 - S1_pred$surv[t_indices]\nR0_pred &lt;- 1 - S0_pred$surv[t_indices]\nplot(times, R1_pred / R0_pred, type = \"l\", lty = \"dashed\",\n     ylim = c(1.7, 2.3), xlab = \"Weeks since treatment\",\n     ylab = \"Estimated tumor incidence risk ratio\")\nlines(times, odds(R1_pred) / odds(R0_pred), lty = \"dotted\")\nabline(h = exp(coef(cox_efron)[\"rx\"]))\ngrid()\nlegend(\"bottomleft\", lty = c(\"dotted\", \"solid\", \"dashed\"), bg = \"white\",\n       legend = c(\"Odds ratio\", \"Hazard ratio\", \"Risk ratio\"))\n\n\n\n\n\n\n\n\nAalen, Odd, Ørnulf Borgan, and Håkon Gjessing. 2008. Survival and Event History Analysis: A Process Point of View. Springer Science & Business Media.\n\n\nBreslow, Norman. 1970. “A Generalized Kruskal-Wallis Test for Comparing k Samples Subject to Unequal Patterns of Censorship.” Biometrika 57 (3): 579–94.\n\n\n———. 1974. “Covariance Analysis of Censored Survival Data.” Biometrics 30 (1): 89–99.\n\n\nBreslow, Norman E. 1972. “Contribution to Discussion of Paper by DR Cox.” Journal of the Royal Statistical Society: Series B (Methodological) 34: 216–17.\n\n\nCox, David R. 1958. “The Regression Analysis of Binary Sequences.” Journal of the Royal Statistical Society Series B: Statistical Methodology 20 (2): 215–32.\n\n\n———. 1972. “Regression Models and Life-Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 34 (2): 187–202.\n\n\nEfron, Bradley. 1967. “The Two Sample Problem with Censored Data.” In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, 4:831–53. University of California Press.\n\n\n———. 1977. “The Efficiency of Cox’s Likelihood Function for Censored Data.” Journal of the American Statistical Association 72 (359): 557–65.\n\n\nGehan, Edmund A. 1965. “A Generalized Wilcoxon Test for Comparing Arbitrarily Singly-Censored Samples.” Biometrika 52 (1-2): 203–24.\n\n\nHarrington, David P, and Thomas R Fleming. 1982. “A Class of Rank Test Procedures for Censored Survival Data.” Biometrika 69 (3): 553–66.\n\n\nJohansen, Søren. 1983. “An Extension of Cox’s Regression Model.” International Statistical Review 51 (2): 165–74.\n\n\nKalbfleisch, John D, and Ross L Prentice. 2011. The Statistical Analysis of Failure Time Data. John Wiley & Sons.\n\n\nMantel, Nathan. 1966. “Evaluation of Survival Data and Two New Rank Order Statistics Arising in Its Consideration.” Cancer Chemotherapy Reports 50 (3): 163–70.\n\n\nPeto, Richard. 1972. “Contribution to Discussion of Paper by DR Cox.” Journal of the Royal Statistical Society: Series B (Methodological) 34: 202–7.\n\n\nPeto, Richard, and Julian Peto. 1972. “Asymptotically Efficient Rank Invariant Test Procedures.” Journal of the Royal Statistical Society: Series A (General) 135 (2): 185–98.\n\n\nPrentice, Ross L. 1978. “Linear Rank Tests with Right Censored Data.” Biometrika 65 (1): 167–79.\n\n\nTarone, Robert E, and James Ware. 1977. “On Distribution-Free Tests for Equality of Survival Distributions.” Biometrika 64 (1): 156–60.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Two-Sample Survival Analysis and the Cox Model</span>"
    ]
  },
  {
    "objectID": "survival2.html#footnotes",
    "href": "survival2.html#footnotes",
    "title": "10  Two-Sample Survival Analysis and the Cox Model",
    "section": "",
    "text": "Sir David Roxbee Cox (1924–2022) is a British statistician who helped develop logistic regression (Cox 1958) and proportional hazards regression. He worked at Imperial College London and Oxford University. As of 6 February 2025, Cox (1972) has 63,224 citations on Google Scholar.↩︎",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Two-Sample Survival Analysis and the Cox Model</span>"
    ]
  },
  {
    "objectID": "casecontrol.html",
    "href": "casecontrol.html",
    "title": "11  Design and Analysis of Case-Control and Case-Cohort studies",
    "section": "",
    "text": "11.1 Case-control studies for rare diseases\nIn a cohort study, we select participants based on exposure \\(X\\) and follow them over time to ascertain the occurrence of disease \\(D\\). When the disease is rare or when the time to disease onset is long, estimating a measure of association might take a large cohort or a long follow-up period. Because we have selected based on exposure, the study is powered to estimate the association between \\(X\\) and any disease outcome. However, it may not be better than a random sample from the population for looking the association between a different exposure \\(X'\\) and disease outcomes. When there is no clear hypothesis about the cause of a disease, it can be difficult to select a useful cohort based on exposure.\nThe case-control study was one of the great innovations in epidemiology in the 20th century (Nigel Paneth, Susser, and Susser 2002; Nigl Paneth, Susser, and Susser 2002). In a case-control study, we select participants based on disease outcome \\(D\\) and then measure their exposure \\(X\\). This study design is powered to estimate the association between \\(D\\) and any exposure, which makes it useful for rare diseases and common exposures or for a disease for which there is no clear hypothesis about its causes. It was the study design that was used to establish that smoking causes lung cancer (Doll and Hill 1950), aspirin causes Reyes syndrome (Hurwitz et al. 1987), and that prenatal exposure to diethylstilbestrol causes vaginal clear-cell adenocarcinoma (Herbst, Ulfelder, and Poskanzer 1971). In each case, there was a rare or slowly-developing disease and no clear hypothesis about its causes.\nWhile case control studies are more efficient than cohort studies with the same total number of participants for rare diseases and common exposures (see Section 7.5), they have less flexibility in the choice of a measure of association. The odds ratio in a case-control study approximates the risk ratio or hazard ratio from a hypothetical cohort study in the same population (Pearce 1993) However, data from a case-control study can be used to estimate absolute risks or rates if there is additional information such as the sampling fraction of cases, the total size of the population giving rise to the cases and controls, or the overall incidence of disease (Miettinen 1976). Case-control studies are often retrospective, and retrospective studies can be more susceptible to differential misclassification (e.g., recall bias) and selection bias than prospective studies (see Section 8.2.3).\nIn case-control studies, the odds raito for exposure comparing cases to controls approximates the risk ratio or the incidence rate ratio that would be calculated from a large cohort study in the population from which the cases and control are sampled.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Design and Analysis of Case-Control and Case-Cohort studies</span>"
    ]
  },
  {
    "objectID": "casecontrol.html#case-control-studies-for-rare-diseases",
    "href": "casecontrol.html#case-control-studies-for-rare-diseases",
    "title": "11  Design and Analysis of Case-Control and Case-Cohort studies",
    "section": "",
    "text": "11.1.1 Cumulative case-control studies and the odds ratio\nIn a cumulative case-control study, controls are sampled from the people in the population who do not have disease. Using the notation from Table 2.1, the risk ratio comparing the exposed to the unexposed is \\[\n  \\RR\n  = \\frac{a / r_1}{c / r_0}\n  = \\frac{a / c}{r_1 / r_0}.\n\\tag{11.1}\\] The final expression is an odds ratio because the numerator is the odds of exposure among the cases and the denominator is the odds of exposure in the entire cohort. This use of the odds ratio to approximate the risk ratio was first proposed by Cornfield (1951).1 When the disease is rare in both exposure groups in a cohort study, \\(a \\ll r_1\\) and \\(c \\ll r_0\\) so \\[\n  \\frac{r_1}{r_0}\n  \\approx \\frac{r_1 - a}{r_0 - c}\n  = \\frac{b}{d}.\n\\] Therefore, \\[\n  \\RR\n  = \\frac{a / r_1}{c / r_0}\n  \\approx \\frac{a / c}{b / d}\n  = \\OR_X\n\\] so the odds ratio for exposure comparing those with \\(D = 1\\) (cases) to those with \\(D = 0\\) (controls) approximates the risk ratio for disease comparing the exposed to the unexposed.\nThe odds ratio comparing cases to controls can be calculated in a case-control study, where we select participants based on disease status. From Section 9.1.2, the variance of the log risk ratio is \\[\n  \\Var\\bigl(\\ln \\RR\\bigr)\n  = \\frac{1}{a} - \\frac{1}{r_1} + \\frac{1}{c} - \\frac{1}{r_0}\n  \\approx \\frac{1}{a} + \\frac{1}{c}\n\\tag{11.2}\\] when the risks of disease \\(p_1\\) and \\(p_0\\) are both small. Thus, the precision of the risk ratio estimate in a large cohort study with a rare disease is determined almost entirely by the number of cases in each exposure group. Instead of recruiting a large population and waiting for cases to occur, it is more efficient to recruit cases directly, measure their exposure, and compare their exposure odds to those of a sample of controls.\nThe variance of the log odds ratio is \\[\n  \\Var\\bigl(\\ln \\OR\\bigr)\n  = \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}.\n\\tag{11.3}\\] If we recruit \\(k\\) controls per case, then \\[\n  \\Var\\bigl(\\ln \\OR\\bigr)\n  = \\frac{1}{a} + \\frac{1}{k a} + \\frac{1}{c} + \\frac{1}{k c}\n  = \\bigg(1 + \\frac{1}{k}\\bigg) \\bigg(\\frac{1}{a} + \\frac{1}{c}\\bigg).\n\\tag{11.4}\\] Let \\(\\sigma_{\\text{cc}}\\) be the standard error the \\(\\ln \\RR\\) estimated via the \\(\\ln \\OR\\) from a case-control study with \\(k\\) controls per case, and let \\(\\sigma_{\\text{cohort}}\\) be the standard error of the estimated \\(\\ln \\RR\\) from a large cohort study with a rare disease. Comparing Equation 11.2 and Equation 11.4, we get \\[\n  \\sigma_\\text{cc}\n  \\approx \\sigma_\\text{cohort} \\sqrt{1 + \\frac{1}{k}}.\n\\tag{11.5}\\] Figure 11.1 plots this function. A case-control study with \\(k = 5\\) has about 10% higher standard error than a full cohort study, and a case-control study with \\(k = 10\\) has about 5% higher standard error. In practice, most case-control studies recruit 3–5 controls per case.\n\n\n\nCode\n\ncasecontrol-SE.R\n\n## Ratio of case-control and cohort standard errors\n\n# plot\ncpc &lt;- 1:10    # controls per case\nplot(cpc, sqrt(1 + 1 / cpc), ylim = c(0, 2),\n     xlab = \"Controls per case\",\n     ylab = expression(sigma[plain(cc)] / sigma[plain(cohort)]))\ngrid()\nabline(h = 1, col = \"darkgray\")\n\n\n\n\n\n\n\n\n\nFigure 11.1: The standard error of the estimated \\(\\ln \\RR\\) from a cumulative case-control study or the estimated \\(\\ln \\IRR\\) from a density case-control study divided by the standard error of corresponding estimator from a large cohort study with the same number of cases.\n\n\n\n\n\n\nR\n\n\n\n\ncasecontrol-cumulative.R\n\n## Cumulative case-control study\n\n# The rats data is from:\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)               # rats data\n\n# estimate rsk ratio from cohort study with log-binomial regression\n# Risk calculations are valid because follow-up is complete over 104 weeks.\ncohort_RR &lt;- glm(status ~ rx, data = rats, family = binomial(link = \"log\"))\nexp(coef(cohort_RR))               # estimated risk ratio = 2\nexp(confint(cohort_RR))            # 95% confidence interval (1.14, 3.51)\n\n# sample controls from rats who did not have incidence tumors\n# We will use 4 controls per case and sample without replacement.\nncase &lt;- sum(rats$status)\nrat_id &lt;- 1:nrow(rats)\ncontrols &lt;- sample(rat_id[rats$status == 0], 4 * ncase)\n\n# combine case and control data\ncasedat &lt;- subset(rats, status == 1)\ncontroldat &lt;- rats[controls, ]\nccrats &lt;- rbind(casedat, controldat)\n\n# logistic regression model for exposure in cases and controls\n# The exposure odds ratio comparing cases and controls approximates the RR.\n# In the case-controls data, status = 1 for cases and status = 0 for controls.\nccx &lt;- glm(rx ~ status, data = ccrats, family = binomial())\nexp(coef(ccx))\nexp(confint(ccx))\n\n# logistic regression model for case/control status in exposed and unexposed\n# By symmetry of the odds ratio, we can switch exposure and outcome.\n# The estimated odds ratio is the same, but the intercepts are different.\nccd &lt;- glm(status ~ rx, data = ccrats, family = binomial())\nexp(coef(ccd))\nexp(confint(ccd))\n\n# case-control / cohort coefficient standard error ratio\n# With 4 controls per case, it is approximately sqrt(1 + 1 / 4) = 1.12.\nsqrt(vcov(cohort_RR)[2, 2])     # log risk ratio SE in cohort study\nsqrt(vcov(ccd)[2, 2])           # log odds ratio SE in case-control study\nsqrt(vcov(ccd)[2, 2] / vcov(cohort_RR)[2, 2])\n\n\n\n\n\n\n11.1.2 Density case-control studies and risk-set sampling\nThe cumulative case-control design implicitly assumes complete follow-up (so that it is possible to sample from the entire non-diseased population at the end of follow-up). In a density case-control study, the odds ratio for exposure is meant to approximate the incidence rate ratio \\(\\IRR\\). The name comes from the use of incidence density to refer to an incidence rate. If we have \\(a\\) cases from \\(T_1\\) units of person-time among the exposed and \\(c\\) cases from \\(T_0\\) units of person-time among the unexposed, the incidence rate ratio is \\[\n  \\IRR\n  = \\frac{a / T_1}{c / T_0}\n  = \\frac{a / c}{T_1 / T_0}.\n\\tag{11.6}\\] The numerator is the odds ratio for exposure among the cases, and the denominator is the odds ratio for exposure among units of person-time.\nIn risk set sampling (sometimes called incidence density sampling), controls are sampled from the risk set at each failure time. As described in Section 6.2.1, the risk set \\(\\mathcal{R}(t)\\) consists of all participants at risk of an observed event at time \\(t\\). This produces a set of controls with an exposure odds \\(b / d\\) approximately equal to \\(T_1 / T_0\\) when each group has exponential times to events (as assumed by the use of incidence rates—see Section 5.3.2) and the prevalence of exposure does not change over time (Miettinen 1976). When these assumptions are approximately correct, \\[\n  \\IRR\n  \\approx \\frac{a / c}{b / d}\n  = \\OR_X\n\\] so the odds ratio for exposure comparing cases to controls approximates the incidence rate ratio for disease comparing the exposed to the unexposed. From Section 9.1.4, \\[\n  \\Var\\bigl(\\ln \\IRR\\bigr)\n  = \\frac{1}{a} + \\frac{1}{c}.\n\\] In a density case-control study, the same control can be selected for multiple cases, and a person who is selected as a control can later become a case. If the disease is rare, there will be little or no overlap between cases and controls, so the variance of the estimated \\(\\ln \\OR\\) when we recruit \\(k\\) controls per case is given by Equation 11.4. Then relationship in Equation 11.5 and Figure 11.1 holds if \\(\\sigma_{\\text{cc}}\\) is the standard error the estimated \\(\\ln \\IRR\\) from a density case-control study and \\(\\sigma_{\\text{cohort}}\\) is the standard error of the estimated \\(\\ln \\RR\\) from a large cohort study.\nIn a closed cohort, the prevalence of exposure will stay approximately constant only under the null hypothesis of no exposure-disease association or when the risk of disease over the follow-up period is small in both exposure groups. For rare diseases with exponential times to events, \\[\n  \\RR = \\frac{1 - e^{-\\lambda_1 t}}{1 - e^{-\\lambda_0 t}}\n  \\approx \\frac{\\lambda_1}{\\lambda_0}\n  = \\IRR\n\\] where \\(\\lambda_1\\) is the incidence rate in the exposed and \\(\\lambda_0\\) is the incidence rate in the unexposed. Thus, the incidence rate ratio estimated by a density case-control study of a rare disease is also an approximation to the risk ratio from a large cohort study. The approximation to the risk ratio from a density case-control study is generally more accurate than that of a cumulative case-control study (Greenland and Thomas 1982). The reason for this can be seen in Figure 11.2, where the contours for the incidence rate ratio (which is a special case of the hazard ratio) are closer to the risk ratio contours than the odds ratio contours are. These are the same contour lines shown in Figure 9.3, but they are plotted together and we have zoomed in to the bottom left corner.\n\n\n\nCode\n\nRRapprox.R\n\n## Approximation to the risk ratio in case-control studies\n\n# contour labels\nrequire(plotrix, quietly = TRUE)\n\n# odds, logistic, and cumulative hazard functions\nodds &lt;- function(p) p / (1 - p)\ninvodds &lt;- function(odds) odds / (1 + odds)\ncumhaz &lt;- function(p) -log(1 - p)\n\n# Rothman diagram with RR, OR, and HR contours\nriskratios &lt;- c(0.2, 0.5, 2, 5)\np0 &lt;- seq(0.0001, 0.11, by = 0.001)\nplot(p0, p0, type = \"l\", col = \"darkgray\", xlim = c(0, 0.1), ylim = c(0, 0.1),\n     xlab = \"Risk in the unexposed\", ylab = \"Risk in the exposed\")\ngrid()\nfor (riskratio in riskratios) {\n  p1 &lt;- riskratio * p0\n\n  # risk ratio contours\n  lines(p0, riskratio * p0, lty = \"solid\")\n\n  # odds ratio contours\n  lines(p0, invodds(riskratio * odds(p0)), lty = \"dashed\")\n\n  # cumulative hazard ratio contours\n  lines(p0, 1 - (1 - p0)^riskratio, lty = \"dotted\")\n\n  # labels\n  boxed.labels(0.1 / (riskratio + 1), 0.1 * riskratio / (riskratio + 1),\n               as.character(riskratio),\n               xpad = 1.62, ypad = 1.62, border = FALSE)\n}\nlegend(\"topright\", bg = \"white\", lty = c(\"solid\", \"dotted\", \"dashed\"),\n       legend = c(\"Risk ratio\", \"Hazard ratio\", \"Odds ratio\"))\n\n\n\n\n\n\n\n\n\nFigure 11.2: Rothman diagram (Rothman 1975; Kenah 2024) with contour lines for the risk ratio, incidence rate ratio, and odds ratio for rare diseases. Each group of contours is labeled with their common value.\n\n\n\n\n\n\nR\n\n\n\n\ncasecontrol-density.R\n\n## Density case-control study\n\n# The rats data is from:\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)               # rats data and survreg()\n\n# estimate incidence rate ratio from cohort study with exponential AFT model\n# The estimated incidence rate ratio is exp(-beta_rx).\ncohort_IRR &lt;- survreg(Surv(time, status) ~ rx, data = rats,\n                      dist = \"exponential\")\nexp(-coef(cohort_IRR))\nexp(-confint(cohort_IRR))\n\n# function to sample controls for each case\ncontrol_sample &lt;- function(id, k) {\n  # id should be a rat who has an event\n  if (rats$status[id] == 0) stop(\"Rat \", id, \" did not have a tumor.\")\n  t &lt;- rats$time[id]\n\n  # possible controls are riskset minus individuals with events at time t\n  rat_id &lt;- 1:nrow(rats)\n  controlset &lt;- rat_id[with(rats, (time &gt; t) | (time == t & status == 0))]\n\n  # sample up to k controls\n  if (length(controlset) &gt; 1) {\n    ksamp &lt;- min(length(controlset), k)\n    controls &lt;- sample(controlset, ksamp)\n  } else {\n    controls &lt;- NULL\n  }\n  controls\n}\ncontrol_sample(1, 4)            # no tumor\ncontrol_sample(2, 4)            # returns sampled control ids\n\n# generate case-control data for one case\ncc_set &lt;- function(id, k) {\n  controls &lt;- rats[control_sample(id, k), ]\n  controls$status &lt;- 0\n  ccdat &lt;- rbind(rats[id, ], controls)\n\n  # use case id to identify case-control set\n  ccdat$case &lt;- id\n\n  # return data for case-control set\n  ccdat\n}\ncc_set(1, 4)\ncc_set(2, 4)\n\n# generate density case-control data\n# We are recruiting k = 4 controls per case in each risk set.\n# The same control could be chosen more than once, and\n# a control could later become a case.\ncases &lt;- rat_id[rats$status == 1]   # rats with incident tumors\nccrats &lt;- do.call(rbind, lapply(cases, cc_set, k = 4))\n\n# estimate the incidence rate ratio with logistic regression\n# The exposure odds ratio comparing cases and controls approximates the IRR.\nccx &lt;- glm(rx ~ status, data = ccrats, family = binomial())\nexp(coef(ccx))\nexp(confint(ccx))\n\n# logistic regression model for case/control status in exposed and unexposed\n# By symmetry of the odds ratio, we can switch exposure and outcome.\n# The estimated odds ratio is the same, but the intercepts are different.\nccd &lt;- glm(status ~ rx, data = ccrats, family = binomial())\nexp(coef(ccd))\nexp(confint(ccd))\n\n# case-control / cohort coefficient standard error ratio\n# With 4 controls per case, it is approximately sqrt(1 + 1 / 4) = 1.12.\nsqrt(vcov(cohort_IRR)[2, 2])    # log incidence rate ratio SE in cohort study\nsqrt(vcov(ccd)[2, 2])           # log odds ratio SE in case-control study\nsqrt(vcov(ccd)[2, 2] / vcov(cohort_IRR)[2, 2])",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Design and Analysis of Case-Control and Case-Cohort studies</span>"
    ]
  },
  {
    "objectID": "casecontrol.html#case-cohort-studies",
    "href": "casecontrol.html#case-cohort-studies",
    "title": "11  Design and Analysis of Case-Control and Case-Cohort studies",
    "section": "11.2 Case-cohort studies",
    "text": "11.2 Case-cohort studies\nFrom Equation 11.1, the risk ratio for disease comparing the exposed to the unexposed is equal to the odds ratio for exposure comparing the cases to the entire cohort. In a case-cohort study, the odds ratio for exposure in the cohort is approximated using the odds ratio for exposure in a random sample of the cohort (Kupper, McMichael, and Spirtas 1975). This random sample is called the subcohort. Members of the subcohort are allowed to become cases, but all cases must come from within the original cohort.\nLet \\(a_\\text{s}\\) denote the number of exposed cases that come from the subcohort, and let \\(a_\\text{x}\\) denote the number of exposed cases that come from outside the subcohort. Similarly define \\(c_\\text{s}\\) and \\(c_\\text{x}\\) be the number of unexposed cases from inside and outside the subcohort, respectively. Then \\(a = a_\\text{s} + a_\\text{x}\\) is the total number of exposed cases and \\(c = c_\\text{s} + c_\\text{x}\\) is the total number of unexposed cases. The total number of cases from the subcohort is \\[\n  k_{1 \\text{s}} = a_\\text{s} + c_\\text{s},\n\\] and the total number of cases outside the subcohort is \\[\n  k_{1 \\text{x}} = a_\\text{x} + c_\\text{x}.\n\\] The total number of cases is \\[\n  k_1 = a + c = k_{1 \\text{s}} + k_{1 \\text{x}}.\n\\] The \\(b\\) exposed controls and \\(d\\) unexposed controls are all within the subcohort, and the total number of controls is still \\(k_0 = b + d\\). Table 11.1 shows a 2x2 table adapted to the case-cohort design.\n\n\n\nTable 11.1: “2x2 table” of exposure, cases, noncases, subcohort, and study sample in a case-cohort study.\n\n\n\n\n\n\nCases\nNoncases\nSubcohort\nStudy Sample\n\n\n\n\n\\(X = 1\\)\n\\(a = a_\\text{x} + a_\\text{s}\\)\n\\(b\\)\n\\(r_{1\\text{s}} = a_\\text{s} + b\\)\n\\(r_1 = a + b\\)\n\n\n\\(X = 0\\)\n\\(c = c_\\text{x} + c_\\text{s}\\)\n\\(d\\)\n\\(r_{0\\text{s}} = c_\\text{s} + d\\)\n\\(r_0 = c + d\\)\n\n\nTotal\n\\(k_1 = k_{1\\text{x}} + k_{1\\text{s}}\\)\n\\(k_0\\)\n\\(n_\\text{s}\\)\n\\(n\\)\n\n\n\n\n\n\nIn the notation of Table 11.1, \\[\n  \\RR_D\n  \\approx \\frac{a / c}{r_{1\\text{s}} / r_{0\\text{s}}}\n\\tag{11.7}\\] where the last expression is the odds ratio for exposure comparing the cases to the entire subcohort. The variance calculation for the \\(\\ln \\OR\\) from a case-cohort study must account for overlap between the cases and the subcohort. When there is no overlap, \\(r_{1\\text{s}} = b\\) and \\(r_{0\\text{s}} = d\\) so the variance of the \\(\\ln \\OR\\) in Equation 11.3 is correct.\n\n11.2.1 Rare diseases with rare exposures\nSection 7.5 showed that a cohort study is the most powerful design for detecting a lack of independence between a rare exposure and a common disease, where \\(\\pi (1 - \\pi) &lt; p (1 - p)\\) where \\(\\pi\\) is the marginal prevalence of exposure and \\(p\\) is the marginal prevalence of disease. For a common exposure and a rare disease, where \\(\\pi (1 - \\pi) &gt; p (1 - p)\\), a case-control study is the most powerful design for detecting a lack of independence. The case-cohort design can be efficient when you have a rare exposure and a rare disease: You can sample a cohort by exposure to ensure adequate sample sizes in both exposure groups, and you can sample by disease within the cohort to ensure there are enough cases without following the entire cohort. Sampling by both exposure and disease seems to invite selection bias, but this is prevented by taking cases only within the cohort and by using the odds ratio to approximate the risk ratio as in Equation 11.7.\n\n\n11.2.2 Estimation without the rare disease assumption*\nWhen there is overlap between the cases and the subcohort, it is possible to get a more efficient estimator of the risk ratio than the estimator in Equation 11.7. To avoid selection bias, the selection of cases within the cohort must be independent of exposure status, so \\[\n  \\Pr\\bigl(\\text{in subcohort} \\,\\big|\\, \\text{case}, X = x\\bigr)\n  = \\Pr(\\text{in subcohort} \\given{} \\text{case}).\n\\] The maximum likelihood estimate of the common probability of being in the subcohort given becoming a case is \\[\n  \\hat{\\Pr}\\bigl(\\text{in subcohort} \\,\\big|\\, \\text{case}\\bigr)\n  = \\frac{a_\\text{s} + c_\\text{s}}{a + c}.\n\\] Using this common probability gives us \\[\n  \\tilde{a}_\\text{s}\n  = a \\times \\hat{\\Pr}\\bigl(\\text{in subcohort} \\,\\big|\\, \\text{case}\\bigr)\n  = \\frac{a (a_\\text{s} + c_\\text{s})}{a + c}\n\\] expected exposed cases in the subcohort and \\[\n  \\tilde{a}_\\text{x}\n  = a \\times \\hat{\\Pr}\\bigl(\\text{outside subcohort} \\,\\big|\\, \\text{case}\\bigr)\n  = \\frac{a (a_\\text{x} + c_\\text{x})}{a + c}\n\\] expected exposed cases outside the subcohort. The total number of exposed cases inside and outside of the subcohort is unchanged because \\[\n  \\tilde{a}_\\text{s} + \\tilde{a}_\\text{x}\n  = \\frac{a \\big((a_\\text{s} + c_\\text{s}) + (a_\\text{x} + c_\\text{x})\\big)}{a + c}\n  = a,\n\\] but the corresponding number of exposed controls is \\[\n  \\tilde{r}_1 = \\tilde{a}_\\text{s} + b.\n\\] The expected number of unexposed cases inside and outside the subcohort is calculated in the same way, leaving the total number of unexposed cases unchanged but altering the number of unexposed members of the subcohort. Table 11.2 shows the adjusted case-cohort 2x2 table.\n\n\n\nTable 11.2: Adjusted “2x2 table” of exposure, noncases, subcohort, and study sample in a case-cohort study.\n\n\n\n\n\n\nCases\nNoncases\nSubcohort\nStudy Sample\n\n\n\n\n\\(X = 1\\)\n\\(a = \\tilde{a}_\\text{x} + \\tilde{a}_\\text{s}\\)\n\\(b\\)\n\\(\\tilde{r}_{1\\text{s}} = \\tilde{a}_\\text{s} + b\\)\n\\(r_1 = a + b\\)\n\n\n\\(X = 0\\)\n\\(c = \\tilde{c}_\\text{x} + \\tilde{c}_\\text{s}\\)\n\\(d\\)\n\\(\\tilde{r}_{0\\text{s}} = \\tilde{c}_\\text{s} + d\\)\n\\(r_0 = c + d\\)\n\n\nTotal\n\\(k_1 = \\tilde{k}_{1\\text{x}} + \\tilde{k}_{1\\text{s}}\\)\n\\(k_0\\)\n\\(\\tilde{n}_\\text{s}\\)\n\\(n\\)\n\n\n\n\n\n\nThe estimated risk ratio comparing the exposed to the unexposed is approximated by the odds ratio for exposure comparing cases to the adjusted subcohort: \\[\n  \\hat{\\RR}_\\text{Sato}\n  = \\frac{a / c}{\\tilde{r}_{1\\text{s}} / \\tilde{r}_{0\\text{s}}}.\n\\tag{11.8}\\] The variance for Wald hypothesis tests and confidence intervals is calculated on the log scale: \\[\n  \\Var\\bigl(\\ln \\hat{\\RR}_\\text{Sato}\\bigr)\n  = \\frac{1}{a} + \\frac{1}{c}\n    + \\frac{k_{1\\text{x}} - k_{1\\text{s}}}{k_1}\n        \\bigg(\\frac{1}{\\tilde{r}_{1\\text{s}}} + \\frac{1}{\\tilde{r}_{0\\text{s}}}\\bigg)\n    - \\frac{n^2 k_{1\\text{x}} k_{1\\text{s}} a c}{k_1^3 \\tilde{r}_{1\\text{s}}^2 \\tilde{r}_{0\\text{s}}^2}\n\\tag{11.9}\\] This analysis of a case-cohort study spans the range from a cumulative case-control study (when all cases occur outside the subcohort) to a cohort study (when the subcohort is the entire cohort).\n\n11.2.2.1 All cases outside the subcohort: cumulative case-control study\nWhen the disease is sufficiently rare that all cases occur outside the subcohort, we have \\(k_{1\\text{s}} = 0\\) and \\(k_{1\\text{x}} = k_1\\). Thus, \\(\\tilde{a}_s = \\tilde{c}_\\text{s}  = 0\\). Plugging these into Equation 11.8, we get the estimated risk ratio \\[\n  \\hat{\\RR}_\\text{Sato}\n  = \\frac{a / c}{\\tilde{r}_{1\\text{s}} / \\tilde{r}_{0\\text{s}}}\n  = \\frac{a / c}{b / d},\n\\] which is the odds ratio from a cumulative case-control study. Plugging these into Equation 11.9, we get \\[\n  \\Var\\bigl(\\ln \\hat{\\RR}_\\text{Sato}\\bigr)\n  = \\frac{1}{a} + \\frac{1}{c} + \\frac{1}{\\tilde{r}_{1\\text{s}}} + \\frac{1}{\\tilde{r}_{0\\text{s}}}\n  = \\frac{1}{a} + \\frac{1}{c} + \\frac{1}{b} + \\frac{1}{d},\n\\] which the large-sample variance of the log odds ratio from Equation 11.3. Thus, the case-cohort point and interval estimates of the risk ratio match those of the cumulative case-control study in Section 11.1.1.\n\n\n11.2.2.2 All cases in the subcohort: cohort study\nWhen the subcohort includes the entire cohort, we have \\(k_{1 \\text{x}} = 0\\) and \\(k_{1 \\text{s}} = k_1\\), so \\(\\tilde{a}_\\text{s} = a\\) and \\(\\tilde{c}_\\text{s} = c\\). It follows that \\(\\tilde{r}_{1\\text{s}} = r_1\\) and \\(\\tilde{r}_{0\\text{s}} = r_0\\). Plugging these into Equation 11.8, we get the estimated risk ratio \\[\n  \\hat{\\RR}_\\text{Sato}\n  = \\frac{a / c}{\\tilde{r}_{1\\text{s}} / \\tilde{r}_{0\\text{s}}}\n  = \\frac{a / r_1}{c / r_0},\n\\] which is the estimated risk ratio from a cohort study. Plugging these into Equation 11.9, we get \\[\n  \\Var\\bigl(\\ln \\hat{\\RR}_\\text{Sato}\\bigr)\n  = \\frac{1}{a} + \\frac{1}{c} - \\frac{1}{\\tilde{r}_{1\\text{s}}} - \\frac{1}{\\tilde{r}_{0\\text{s}}}\n  = \\frac{1}{a} - \\frac{1}{r_1} + \\frac{1}{c} - \\frac{1}{r_0},\n\\] which the large-sample variance of the log risk ratio from Section 9.1.2. Thus, the case-cohort point and interval estimates of the risk ratio match those of a cohort study.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Design and Analysis of Case-Control and Case-Cohort studies</span>"
    ]
  },
  {
    "objectID": "casecontrol.html#nested-case-control-studies",
    "href": "casecontrol.html#nested-case-control-studies",
    "title": "11  Design and Analysis of Case-Control and Case-Cohort studies",
    "section": "11.3 Nested case-control studies",
    "text": "11.3 Nested case-control studies\nThe nested case-control study is a generalization of the density case-control study that relaxes the assumption of constant hazards of failure. It was first used to study lung cancer among asbestos miners in Quebec (Liddell, McDonald, and Thomas 1977; Thomas 1977). Let \\(X_j\\) denote the exposure status of person \\(j\\). The Cox partial likelihood for the log hazard ratio \\(\\beta\\) is \\[\n  L(\\beta) = \\prod_{i = 1}^m \\frac{e^{\\beta X_i})}{\\sum_{j \\in \\mathcal{R}(t_i)} e^{\\beta X_j}},\n\\] where \\(t_1 &lt; t_2 &lt; \\ldots &lt; t_m\\) are the distinct failure times, person \\(i\\) is the person who fails at time \\(t_i\\), and \\(\\mathcal{R}(t)\\) is the risk set at time \\(t\\) as in Section 10.4. In a nested case-control study, each risk set \\(\\mathcal{R}(t_i)\\) is replaced by a sample \\(R^*(t_i)\\) that includes the individual who had an event and a random sample of other members of \\(R(t_i)\\). The Cox partial likelihood becomes \\[\n  L(\\beta)\n  = \\prod_{i = 1}^m \\frac{e^{\\beta x_i}}{\\sum_{j \\in \\mathcal{R}^*(t_i)} e^{\\beta x_j}}.\n\\tag{11.10}\\] By the same logic that led to the Cox partial likelihood for a cohort study, the likelihood contribution at each event time \\(t_i\\) is the conditional probability that an event occurred in individual \\(i\\) given that an event occurred in the matched case-control set \\(\\mathcal{R}^*(t_i)\\) (Oakes 1981). Maximum likelihood estimation yields consistent and asymptotically normal point and interval estimates for \\(\\hat{\\beta}\\) without any rare disease assumption (Borgan, Goldstein, and Langholz 1995).\nLet \\(\\hat{\\beta}_\\text{cc}\\) be the maximum partial likelihood estimate from a nested case-control study and \\(\\hat{\\beta}_\\text{cohort}\\) be the maximum partial likelihood estimate from the full cohort. When \\(\\beta = 0\\) and there are \\(k\\) controls sampled per case, \\[\n  \\Var(\\hat{\\beta}_\\text{cc})\n  \\approx \\bigg(1 + \\frac{1}{k}\\bigg) \\Var(\\hat{\\beta}_\\text{cohort})\n\\] which is the same relationship that held for the cumulative case-control study versus a full cohort study in Figure 11.1. When \\(\\beta \\neq 0\\), the relative efficiency of a nested case-control study can be somewhat lower than this. However, this efficiency can be improved using more sophisticated strategies of sampling from the risk sets.\n\n11.3.1 Conditional logistic regression\nA nested case-control study can be analyzed using conditional logistic regression, which produces the partial likelihood in Equation 11.10 using the matched case-control sets (Breslow et al. 1978). To see the connection with the logistic regression model (i.e., a binomial generalized linear model with a logit link), consider a matched set with one case and \\(k\\) controls and let \\(p_j\\) the risk of disease in person \\(j\\). Then the probability that there is one failure and no other failures is \\[\n  \\sum_{i = 1}^{k + 1} \\bigg(p_i \\prod_{j \\neq i} (1 - p_j) \\bigg)\n  = \\sum_{i = 1}^{k + 1} \\frac{p_i}{1 - p_i} \\prod_{j = 1}^{k + 1} (1 - p_j).\n\\] Without loss of generality, we can number the group members so that person one is the case. The probability that person one is the case given that there is one failure in the group is \\[\n  \\frac{\\frac{p_1}{1 - p_1} \\prod_{j = 1}^{k + 1} (1 - p_j)}{\\sum_{i = 1}^k \\frac{p_i}{1 - p_i} \\prod_{j = 1}^{k + 1} (1 - p_j)}\n  = \\frac{\\odds(p_1)}{\\sum_{i = 1}^{k + 1} \\odds(p_j)}\n\\tag{11.11}\\] where \\(\\odds(p_j) = p_j / (1 - p_j)\\) is the odds corresponding to \\(p_j\\). Let \\(\\odds(p_{ij})\\) be the odds of disease for person \\(j\\) in the matched case-control set \\(\\mathcal{R}^*_i\\) where person \\(i\\) is the case. Then the conditional logistic regression model assumes \\[\n  \\logit(p_{ij})\n  = \\ln \\odds(p_{ij})\n  = \\beta_{0i} + \\beta x_j,\n\\] where \\(\\beta_{i0}\\) is an intercept specific to \\(\\mathcal{R}^*_i\\) and \\(x_j\\) is the covariate for person \\(j\\). When there are \\(m\\) distinct event times, the likelihood for \\(\\beta\\) in the conditional logistic regression model is \\[\n  L(\\beta)\n  = \\prod_{i = 1}^m \\frac{e^{\\beta_{0i} + \\beta x_i}}{\\sum_{j \\in \\mathcal{R}^*(t_i)} e^{\\beta_{0i} + \\beta x_j}}\n  = \\prod_{i = 1}^m \\frac{e^{\\beta x_i}}{\\sum_{j \\in \\mathcal{R}^*(t_i)} e^{\\beta x_j}},\n\\tag{11.12}\\] which is exactly the same as the Cox partial likelihood for the nested case-control study in Equation 11.10. The intercept cancels out just like the unspecified baseline hazard. Point and interval estimates of \\(\\beta_\\true\\) can be calculated using maximum likelihood estimation or Bayesian methods. The estimated hazard ratio comparing the exposed to the unexposed is \\(e^{\\hat{\\beta}}\\), and confidence limits or credible limits for the hazard ratio can be obtained by exponentiating the confidence limits or credible limits for \\(\\beta_\\true\\).\n\n\nR\n\n\n\n\ncasecontrol-nested.R\n\n## Nested case-control study and conditional logistic regression\n\n# The rats data is from:\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(survival)               # rats data and clogit() function\n\n# estimate hazard ratio from cohort study using Cox model\ncohort_HR &lt;- coxph(Surv(time, status) ~ rx, data = rats)\nexp(coef(cohort_HR))\nexp(confint(cohort_HR))\n\n# function to sample controls for each case\ncontrol_sample &lt;- function(id, k) {\n  # id should be a rat who has an event\n  if (rats$status[id] == 0) stop(\"Rat \", id, \" did not have a tumor.\")\n  t &lt;- rats$time[id]\n\n  # possible controls are riskset minus individuals with events at time t\n  rat_id &lt;- 1:nrow(rats)\n  controlset &lt;- rat_id[with(rats, (time &gt; t) | (time == t & status == 0))]\n\n  # sample up to k controls\n  if (length(controlset) &gt; 1) {\n    ksamp &lt;- min(length(controlset), k)\n    controls &lt;- sample(controlset, ksamp)\n  } else {\n    controls &lt;- NULL\n  }\n  controls\n}\ncontrol_sample(1, 4)            # no tumor\ncontrol_sample(2, 4)            # returns sampled control ids\n\n# generate case-control data for one case\ncc_set &lt;- function(id, k) {\n  controls &lt;- rats[control_sample(id, k), ]\n  controls$status &lt;- 0\n  ccdat &lt;- rbind(rats[id, ], controls)\n\n  # use case id to identify case-control set\n  ccdat$case &lt;- id\n\n  # return data for case-control set\n  ccdat\n}\ncc_set(1, 4)\ncc_set(2, 4)\n\n# generate nested case-control data\n# We are recruiting k = 4 controls per case.\ncases &lt;- rat_id[rats$status == 1]   # rats with incident tumors\nccrats &lt;- do.call(rbind, lapply(cases, cc_set, k = 4))\n\n# estimate hazard ratio with conditional logistic regression\nnested_ccd &lt;- clogit(status ~ rx + strata(case), data = ccrats)\nexp(coef(nested_ccd))\nexp(confint(nested_ccd))\n\n# estimate hazard ratio with conditional logistic regression\n# As in standard logistic regression, exposure and disease can be switched.\nnested_ccx &lt;- clogit(rx ~ status + strata(case), data = ccrats)\nexp(coef(nested_ccx))\nexp(confint(nested_ccx))\n\n# case-control / cohort coefficient standard error ratio\n# With 4 controls per case, it is approximately sqrt(1 + 1 / 4) = 1.12.\nsqrt(vcov(cohort_HR)[2, 2])     # log hazard ratio SE in cohort study\nsqrt(vcov(nested_ccd)[2, 2])    # log odds ratio SE in case-control study\nsqrt(vcov(nested_ccd)[2, 2] / vcov(cohort_IRR)[2, 2])\n\n\n\n\n\n\n\n\nBorgan, Ørnulf, Larry Goldstein, and Bryan Langholz. 1995. “Methods for the Analysis of Sampled Cohort Data in the Cox Proportional Hazards Model.” The Annals of Statistics, 1749–78.\n\n\nBreslow, NE, NE Day, KT Halvorsen, RL Prentice, and C Sabai. 1978. “Estimation of Multiple Relative Risk Functions in Matched Case-Control Studies.” American Journal of Epidemiology 108 (4): 299–307.\n\n\nCornfield, Jerome. 1951. “A Method of Estimating Comparative Rates from Clinical Data; Applications to Cancer of the Lung, Breast, and Cervix.” Journal of the National Cancer Institute 11: 1269–75.\n\n\nDoll, Richard, and Austin Bradford Hill. 1950. “Smoking and Carcinoma of the Lung.” British Medical Journal 2 (4682): 739.\n\n\nGreenland, Sander, and Duncan C Thomas. 1982. “On the Need for the Rare Disease Assumption in Case-Control Studies.” American Journal of Epidemiology 116 (3): 547–53.\n\n\nHerbst, Arthur L, Howard Ulfelder, and David C Poskanzer. 1971. “Adenocarcinoma of the Vagina: Association of Maternal Stilbestrol Therapy with Tumor Appearance in Young Women.” New England Journal of Medicine 284 (16): 878–81.\n\n\nHurwitz, Eugene S, Michael J Barrett, Dennis Bregman, Walter J Gunn, Paul Pinsky, Lawrence B Schonberger, Joseph S Drage, et al. 1987. “Public Health Service Study of Reye’s Syndrome and Medications: Report of the Main Study.” JAMA 257 (14): 1905–11.\n\n\nKenah, Eben. 2024. “Rothman Diagrams: The Geometry of Confounding and Standardization.” International Journal of Epidemiology 53 (6): dyae139.\n\n\nKupper, LL, AJ McMichael, and R Spirtas. 1975. “A Hybrid Epidemiologic Study Design Useful in Estimating Relative Risk.” Journal of the American Statistical Association 70 (351a): 524–28.\n\n\nLiddell, FDK, JC McDonald, and DC Thomas. 1977. “Methods of Cohort Analysis: Appraisal by Application to Asbestos Mining.” Journal of the Royal Statistical Society: Series A (General) 140 (4): 469–83.\n\n\nMiettinen, Olli. 1976. “Estimability and Estimation in Case-Referent Studies.” American Journal of Epidemiology 103 (2): 226–35.\n\n\nOakes, David. 1981. “Survival Times: Aspects of Partial Likelihood.” International Statistical Review, 235–52.\n\n\nPaneth, Nigel, Ezra Susser, and Mervyn Susser. 2002. “Origins and Early Development of the Case-Control Study: Part 1, Early Evolution.” Sozial-Und Präventivmedizin 47 (5): 282–88.\n\n\nPaneth, Nigl, Ezra Susser, and Mervyn Susser. 2002. “Origins and Early Development of the Case-Control Study: Part 2, the Case-Control Study from Lane-Claypon to 1950.” Sozial-Und Präventivmedizin 47 (6): 359–65.\n\n\nPearce, Neil. 1993. “What Does the Odds Ratio Estimate in a Case-Control Study?” International Journal of Epidemiology 22 (6): 1189–92.\n\n\nRothman, Kenneth J. 1975. “A Pictorial Representation of Confounding in Epidemiologic Studies.” Journal of Chronic Diseases 28 (2): 101–8.\n\n\nThomas, DC. 1977. “Appendum to ‘Methods of Cohort Analysis: Appraisal by Application to Asbestos Mining,’ by Liddell, FDK, McDonald, JC and Thomas, DC.” Journal of the Royal Statistical Society: Series A (General) 140: 469–90.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Design and Analysis of Case-Control and Case-Cohort studies</span>"
    ]
  },
  {
    "objectID": "casecontrol.html#footnotes",
    "href": "casecontrol.html#footnotes",
    "title": "11  Design and Analysis of Case-Control and Case-Cohort studies",
    "section": "",
    "text": "Jerome Cornfield (1912–1979) was an American statistician who spent most of his career at the National Cancer Institute. He joined the US Bureau of Labor Statistics during the Great Depression and took math and statistics courses at the US Department of Agriculture. He combined the case control data of Doll and Hill (1950) and National Institutes of Health data to calculate the risk of lung cancer among smokers, and he showed that this strong association could not plausibly be explained by any known source of bias. He also helped design the Framingham Heart Study, a large cohort study of risk factors for heart disease that begain in 1948.↩︎",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Design and Analysis of Case-Control and Case-Cohort studies</span>"
    ]
  },
  {
    "objectID": "bayes2.html",
    "href": "bayes2.html",
    "title": "12  Bayesian analysis of cohort and case-control studies",
    "section": "",
    "text": "12.1 Prior distributions\nWhen we considered the Bayesian estimation of risks and rates in Chapter 4, we used the beta conjugate distribution for binomial models for risks and the gamma conjugate distribution for exponential or Poisson models for rates. When we are comparing two samples in a cohort or case-control study, there are no simple conjugate distributions for parameters. Instead, there are algorithms for sampling from the posterior distribution, and inferences are based on these samples.\nA noninformative prior is meant to “let the data speak for itself”, but this generally comes at the price of ignoring existing knowledge about the exposure, the outcome, and the population. With a large sample, Bayesian analysis with noninformative priors produces results similar to maximum likelihood. However, Bayesian methods do not rely on the central limit theorem, so they adapt more easily to small sample sizes.\nAlthough prior distributions are often viewed with suspicion, they are an important advantage of the Bayesian approach to statistical inference. The flat priors implicit in frequentist methods do not imply objectivity. Using a prior distribution that incorporates background knowledge will produce more precise and accurate parameter estimates and predictions than an analysis using noninformative priors.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian analysis of cohort and case-control studies</span>"
    ]
  },
  {
    "objectID": "bayes2.html#posterior-distributions",
    "href": "bayes2.html#posterior-distributions",
    "title": "12  Bayesian analysis of cohort and case-control studies",
    "section": "12.2 Posterior distributions",
    "text": "12.2 Posterior distributions\nWithout conjugate distributions, deriving the joint posterior distribution for model parameters can be much more difficult. The most common approach is to take samples from the posterior distribution and use these samples to get point and interval estimates of parameters or functions of parameters. Sampling from the posterior is complicated by the fact that we only know the posterior up to a constant of proportionality. If \\(\\pi(\\theta)\\) is the prior distribution and \\(L(\\theta)\\) is the likelihood for our data, then \\[\n  \\pi(\\theta \\given{} \\data)\n  = \\frac{L(\\theta) \\pi(\\theta)}{\\pi(\\data)}\n  \\propto L(\\theta) \\pi(\\theta)\n\\tag{12.1}\\] where \\(\\pi(data)\\) is the marginal probability of the data, which is generally difficult or impossible to calculate.\n\n12.2.1 Markov chain Monte Carlo\nIn Markov chain Monte Carlo (MCMC), we generate a random walk across the set of possible \\(\\theta\\) so that the distribution of points we visit converges to the posterior distribution (Gelman et al. 2014). We choose a starting point \\(\\theta_0\\). At each \\(\\theta_k\\), we propose a \\(\\theta^*\\) from a proposal distribution or jumping distribution \\(J(\\theta^* \\given{} \\theta_k)\\). In the Metropolis algorithm (Metropolis et al. 1953), the proposal distribution is symmetric in the sense that \\[\n  J(\\theta^* \\given{} \\theta_k)\n  = J(\\theta_k \\given{} \\theta^*)\n\\] for any \\(\\theta^*\\) and \\(\\theta_k\\). We then choose whether to stay at the same place (i.e., \\(\\theta_{k + 1} = \\theta_k\\)) or to jump to \\(\\theta^*\\) (i.e., \\(\\theta_{k + 1} = \\theta_*\\)). The distribution of points we visit will converge the posterior distribution of \\(\\theta\\) if \\[\n  \\pi(\\theta_k \\given{} \\data) \\Pr(\\text{jump from } \\theta_k \\text{ to } \\theta^*)\n  = \\pi(\\theta^* \\given{} \\data) \\Pr(\\text{jump from } \\theta^* \\text{ to } \\theta^k)\n\\] so that jumps from \\(\\theta_k\\) to \\(\\theta^*\\) happen as often as the reverse jumps from \\(\\theta^*\\) to \\(\\theta_k\\). This is called the detailed balance condition, and the jump probabilities can be replaced with probability densities when the space of possible \\(\\theta\\) is continuous.\nRearranging the detailed balance condition, we get \\[\n  \\Pr(\\text{jump from } \\theta_k \\text{ to } \\theta^*)\n  = \\frac{\\pi(\\theta^* \\given{} \\data)}{\\pi(\\theta_k \\given{} \\data)}\n    \\Pr(\\text{jump from } \\theta^* \\text{ to } \\theta_k).\n\\] If we always jump from \\(\\theta^*\\) to \\(\\theta_k\\) when \\(\\pi(\\theta^* \\given{} \\data) &lt; \\pi(\\theta_k \\given{} \\data)\\), then the probability of jumping from \\(\\theta_k\\) to \\(\\theta^*\\) must be \\[\n  r = \\frac{\\pi(\\theta^* \\given{} \\data)}{\\pi(\\theta_k \\given{} \\data)}\n  = \\frac{L(\\theta^*) \\pi(\\theta^*)}{L(\\theta_k) \\pi(\\theta_k)},\n\\] where the unknown \\(\\pi(\\data)\\) from Equation 12.1 has canceled out. Therefore, we will meet the detailed balance condition if \\[\n  \\theta_{k + 1} =\n  \\begin{cases}\n    \\theta^*  &\\text{ with probability} \\min(r, 1), \\\\\n    \\theta_k  &\\text{otherwise}.\n  \\end{cases}\n\\] This is not the only rule that will lead to the detailed balance condition, but it is the one used by the Metropolis algorithm.\nThe Metropolis-Hastings algorithm (Hastings 1970) allows the proposal distribution to be asymmetric. In that case, the procedure is the same except that \\[\n  r = \\frac{\\pi(\\theta^* \\given{} \\data) / J(\\theta^* \\given{} \\theta_k)}\n    {\\pi(\\theta_k \\given{} \\data) / J(\\theta_k \\given{} \\theta^*)}.\n\\] This allows much greater flexibility in proposal distributions. A good proposal distribution should be easy to sample from, make it easy to calculate \\(r\\), propose jumps that are not too big and not too small (Gelman et al. 2014). If the proposed jumps are too small, the space of possible \\(\\theta\\) will not be explored effectively by the MCMC random walk even if we accept most jumps. If the proposed jumps are too large, the proposed \\(\\theta^*\\) will be rejected too often and the MCMC random walk will get stuck.\nThe initial samples in an MCMC have a distribution that has not yet converged to the posterior distribution of \\(\\theta\\). This period is called burn-in or warm-up, and early samples are generally discarded. Even after burn-in, the samples produced by an MCMC can be highly correlated, so the effective sample size is much less than the actual number of samples. This correlation can be reduced by thinning the chain by keeping, for example, every 10th sample. To monitor convergence, it is common to start several chains in different places and calculate the variation within chains and between chains for each component of \\(\\theta\\). When convergence is reached, the variation within chains is similar to the variation between chains.\n\n\n12.2.2 Hamiltonian Monte Carlo\nHamiltonian Monte Carlo (HMC) adapts methods from physics to tune the proposed jumps in an MCMC (Duane et al. 1987; Neal 1996), allowing the random walk to explore the posterior distribution more effectively. HMC treats \\(\\theta\\) as a position and adds a momentum variable \\(\\phi\\) that has the same dimension (i.e., number of components) as \\(\\theta\\). Starting from \\((\\theta_k, \\phi_k)\\), both \\(\\theta\\) and \\(\\phi\\) are updated multiple times in leapfrog steps to generate a proposal \\((\\theta^*, \\phi^*)\\). At the end of the leapfrog steps, a Metropolis-Hastings step decides whether to stay at \\((\\theta_k, \\phi_k)\\) or jump to \\((\\theta^*, \\phi^*)\\). Usually, these steps are rejected with much lower probability than the proposed jumps in an MCMC.\nThe momentum is used to nudge \\(\\theta\\) toward values of \\(\\theta\\) with higher posterior density. If \\(\\theta\\) takes a path with decreasing posterior density, its momentum \\(\\phi\\) starts to decrease and eventually point in the opposite direction, pulling \\(\\theta\\) back toward a region with higher posterior density. If \\(\\theta\\) takes a path with increasing posterior density, its momentum \\(\\phi\\) will increase in that direction until it crosses a peak and the posterior density starts to decrease again (Gelman et al. 2014). The size of the jumps acts like kinetic energy, and \\(-\\ln \\pi(\\theta \\given{} \\data)\\) acts like the potential energy. The random walk is attracted to regions of low potential energy (i.e., high posterior density) like a marble rolling around a bowl, but the conservation of energy ensures that the marble does not get stuck at the bottom.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian analysis of cohort and case-control studies</span>"
    ]
  },
  {
    "objectID": "bayes2.html#cohort-studies",
    "href": "bayes2.html#cohort-studies",
    "title": "12  Bayesian analysis of cohort and case-control studies",
    "section": "12.3 Cohort studies",
    "text": "12.3 Cohort studies\n\n12.3.1 Risk ratios\nMain messages:\n\nSpecifying an informative prior based on background knowledge can improve precision.\nSamples from the posterior can be used for inference about parameters and functions of parameters.\n\n\nR\n\n\n\n\nriskratio-Bayes.R\n\n## Bayesian estimation of risk ratios\n\n# load packages\n# The rats data is from:\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(rstanarm)               # Bayesian regression models\nlibrary(survival)               # rats data\n\n# GLMs in rstanarm are \"stanreg\" objects\n? stan_glm\n? coef.stanreg\n\n# binomial GLM with log link (log-binomial model) for the RR\nRRglm &lt;- glm(status ~ rx, family = binomial(link = \"log\"), data = rats)\nexp(coef(RRglm))                # estimated risk (x = 0) and RR\nexp(confint(RRglm))             # likelihood ratio CIs\n\n# Bayesian GLM with log link fit using a flat prior for coefficients\n# Use \"prior = NULL\" to specify a flat prior.\n# Samples four chains with 2000 iteractions each by default.\n# In each chain, the first 1000 samples are warmup and the last 1000 are kept.\nRRbayes_non &lt;- stan_glm(status ~ rx, data = rats,\n                        family = binomial(link = \"log\"), prior = NULL)\nprior_summary(RRbayes_non)      # summary of prior distributions\nsummary(RRbayes_non)            # summary of model fit (look for Rhat = 1)\ncoef(RRbayes_non)               # posterior medians (linear predictor scale)\nposterior_interval(RRbayes_non) # 90% credible intervals (default)\nposterior_interval(RRbayes_non, prob = 0.95)      # 95% credible intervals\nexp(coef(RRbayes_non))          # posterior medians for risk (rx = 0) and RR\nexp(posterior_interval(RRbayes_non, prob = 0.95)) # 95% credible intervals\n\n# data frame of samples from the posterior distribution\n# This allows you to calculate posterior means and functions of coefficients.\n# Use double quotes for variable names with non-letters, like (Intercept).\npost_non &lt;- as.data.frame(RRbayes_non)\ncoef(RRbayes_non)\nmedian(post_non$\"(Intercept)\")  # equals intercept from coef()\nmedian(post_non$rx)             # equals rx coefficient from coef()\nmean(post_non$rx)               # posterior mean log RR\nquantile(post_non$rx, 0.025)    # lower 95% credible interval for log RR\nquantile(post_non$rx, 0.975)    # upper 95% credible interval for log RR\n\n# point estimates and credible intervals for risk in the treated\npost_non$lnrisk1 &lt;- post_non$\"(Intercept)\" + post_non$rx\npost_non$risk1 &lt;- exp(post_non$lnrisk1)\nmedian(post_non$lnrisk1)        # posterior median log risk in treated rats\nmedian(post_non$risk1)          # posterior median risk in treated rats\nquantile(post_non$risk1, c(0.025, 0.975)) # 95% credible interval for risk\n\n# histogram of samples from posterior distribution of log(risk0)\n# Use freq = FALSE to get probability densities instead of counts\npost_non$lnrisk0 &lt;- post_non$\"(Intercept)\"      # rename for convenience\nsummary(post_non$lnrisk0)\npost_non$risk0 &lt;- exp(post_non$lnrisk0)\nhist(post_non$lnrisk0, freq = FALSE, xlab = \"Log risk in untreated rats\")\ngrid()\n# add kernel density estimate\nlines(density(post_non$lnrisk0), lty = \"dashed\")\n# add approximate normal distribution of MLE\nlnp0 &lt;- seq(-4, -1.5, by = 0.01)\nlines(lnp0, dnorm(lnp0, mean = coef(RRglm)[1], sd = sqrt(vcov(RRglm)[1, 1])))\n\n# plot of empirical CDF and normal CDF for MLE of risk in untreated rats\n# For transformations, comparing CDFs does not require the delta method.\nplot(ecdf(post_non$risk0), main = NULL,\n     xlab = \"Risk in untreated rats\",\n     ylab = \"Cumulative distribution function\")\nlines(exp(lnp0), pnorm(lnp0, mean = coef(RRglm)[1],\n                       sd = sqrt(vcov(RRglm)[1, 1])),\n      lty = \"dashed\")\ngrid()\n\n# histogram of samples from posterior distribution of log(risk ratio)\nsummary(post_non$rx)\nhist(post_non$rx, freq = FALSE, ylim = c(0, 1.4),\n     xlab = \"Log risk ratio (treated / untreated)\")\ngrid()\nlnrr &lt;- seq(-0.3, 1.7, by = 0.01)\nlines(density(post_non$rx), lty = \"dashed\")\nlines(lnrr, dnorm(lnrr, coef(RRglm)[2], sqrt(vcov(RRglm)[2, 2])))\n\n# plot of empirical CDF and normal CDF for MLE of risk ratio\n# For transformations, comparing CDFs does not require the delta method.\npost_non$rr &lt;- exp(post_non$rx)\nplot(ecdf(post_non$rr), main = NULL, lty = \"dotted\",\n     xlab = \"Risk ratio (treated / untreated)\",\n     ylab = \"Cumulative distribution function\")\nlines(exp(lnrr), pnorm(lnrr, mean = coef(RRglm)[2],\n                       sd = sqrt(vcov(RRglm)[2, 2])),\n      lty = \"dashed\")\ngrid()\n\n# using the default prior (weakly informative normal distributions)\n# You can use the chains and iter arguments to change the sampling.\n# The first half of each chain is warmup, and the last half is kept as samples.\nRRbayes_def &lt;- stan_glm(status ~ rx, data = rats, family = binomial(link = \"log\"),\n                        iter = 1500, chains = 6)\nprior_summary(RRbayes_def)\nsummary(RRbayes_def)\nexp(coef(RRbayes_def))          # posterior medians for risk (rx = 0) and RR\nexp(posterior_interval(RRbayes_def, prob = 0.95)) # 95% credible intervals\npost_def &lt;- as.data.frame(RRbayes_def)\n\n# compare distributions of RR under noninformative and default priors\npost_def$rr &lt;- exp(post_def$rx)\nplot(ecdf(post_def$rr), main = NULL,\n     xlab = \"Risk ratio (treated / untreated)\",\n     ylab = \"Cumulative distribution function\")\nlines(ecdf(post_non$rr), col = \"darkgray\")\ngrid()\nlegend(\"bottomright\", bg = \"white\", lty = \"solid\",\n       col = c(\"black\", \"darkgray\"),\n       legend = c(\"Weakly informative prior\", \"Noninformative prior\"))\n\n# specifying an informative prior\n# For the intercept, we use normal with mean -ln(0.1) and sd = ln(1.5).\n# For the coefficient on rx, we use a Laplace (double exponential).\nRRbayes &lt;- stan_glm(status ~ rx, data = rats, family = binomial(link = \"log\"),\n                    prior_intercept = normal(location = -log(0.2),\n                                             scale = log(2)),\n                    prior = normal(location = 0, scale = log(2)))\nsummary(RRbayes)\nprior_summary(RRbayes)\nexp(coef(RRbayes))              # posterior medians for risk (rx = 0) and RR\nexp(posterior_interval(RRbayes, prob = 0.95)) # 95% credible intervals\npost &lt;- as.data.frame(RRbayes)\npost$lnrisk1 &lt;- post$\"(Intercept)\" + post$rx\npost$risk1 &lt;- exp(post$lnrisk1)\nmedian(post$risk1)              # posterior median risk in treated rats\nquantile(post$risk1, c(0.025, 0.975))   # 95% credible interval for risk\nquantile(post$rx, c(0.025, 0.975))      # compare to GLM 95% CI for rx coef\n\n\n\n\n\n\n12.3.2 Odds ratios\nMain messages:\n\nThe regression model does not dictate the measure of association. Here, we get risks and risk ratios out of a logistic regression model, which is specified in terms of odds and odds ratios.\nUsing samples from the posterior distribution is easier and more accurate than using the delta method.\n\n\nR\n\n\n\n\noddsratio-Bayes.R\n\n## Bayesian estimation of odds ratios\n\n# load packages\n# The rats data is from:\n# Mantel, Bohidar, and Ciminera (1977). Cancer Research 37: 3863-3868.\nlibrary(rstanarm)               # Bayesian regression models\nlibrary(survival)               # rats data\n\n# GLMs in rstanarm are \"stanreg\" objects\n? stan_glm\n? coef.stanreg\n\n# binomial GLM with log link (log-binomial model) for the RR\nORglm &lt;- glm(status ~ rx, family = binomial(link = \"log\"), data = rats)\nexp(coef(ORglm))                # estimated odds (x = 0) and OR\nexp(confint(ORglm))             # likelihood ratio CIs\n\n# Bayesian GLM with identity link fit using a flat prior for coefficients\n# Use \"prior = NULL\" to specify a flat prior.\n# Samples four chains with 2000 iteractions each by default.\n# In each chain, the first 1000 samples are warmup and the last 1000 are kept.\nORbayes_non &lt;- stan_glm(status ~ rx, data = rats,\n                        family = binomial(link = \"logit\"), prior = NULL)\nprior_summary(ORbayes_non)      # summary of prior distributions\nsummary(ORbayes_non)            # summary of model fit (look for Rhat = 1)\ncoef(ORbayes_non)               # posterior medians (linear predictor scale)\nposterior_interval(ORbayes_non) # 90% credible intervals (default)\nposterior_interval(ORbayes_non, prob = 0.95)      # 95% credible intervals\nexp(coef(ORbayes_non))          # posterior medians for odds (rx = 0) and OR\nexp(posterior_interval(ORbayes_non, prob = 0.95)) # 95% credible intervals\n\n# data frame of samples from the posterior distribution\n# This allows you to calculate posterior means and functions of coefficients.\npost_non &lt;- as.data.frame(ORbayes_non)\n\n# point estimates and credible intervals for pdds in the treated\npost_non$lnodds1 &lt;- post_non$\"(Intercept)\" + post_non$rx\npost_non$odds1 &lt;- exp(post_non$lnodds1)\nmedian(post_non$lnodds1)        # posterior median log odds in treated rats\nmedian(post_non$odds1)          # posterior median odds in treated rats\nquantile(post_non$odds1, c(0.025, 0.975)) # 95% credible interval for odds\n\n# point estimates and credible intervals for risks and risk ratios\nexpit &lt;- function(v) 1 / (1 + exp(-v))\npost_non$risk0 &lt;- expit(post_non$\"(Intercept)\")\npost_non$risk1 &lt;- expit(post_non$lnodds1)\npost_non$oddsratio &lt;- exp(post_non$rx)\npost_non$riskratio &lt;- post_non$risk1 / post_non$risk0\nmedian(post_non$riskratio)\nquantile(post_non$riskratio, c(0.025, 0.975))\n\n\n\n\n\n\n\n12.3.3 Accelerated failure time models\n\n\n12.3.4 Proportional hazards models",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian analysis of cohort and case-control studies</span>"
    ]
  },
  {
    "objectID": "bayes2.html#case-control-studies",
    "href": "bayes2.html#case-control-studies",
    "title": "12  Bayesian analysis of cohort and case-control studies",
    "section": "12.4 Case-control studies",
    "text": "12.4 Case-control studies\n\n12.4.1 Conditional logistic regression\n\n\n\n\nDuane, Simon, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. 1987. “Hybrid Monte Carlo.” Physics Letters B 195 (2): 216–22.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2014. Bayesian Data Analysis. Third. CRC press.\n\n\nGreenland, Sander. 1998. “Probability Logic and Probabilistic Induction.” Epidemiology 9 (3): 322–32.\n\n\nHastings, W Keith. 1970. “Monte Carlo Sampling Methods Using Markov Chains and Their Applications.” Biometrika 57 (1): 97–109.\n\n\nMetropolis, Nicholas, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. 1953. “Equation of State Calculations by Fast Computing Machines.” The Journal of Chemical Physics 21 (6): 1087–92.\n\n\nNeal, Radford M. 1996. “Monte Carlo Implementation.” Bayesian Learning for Neural Networks, 55–98.",
    "crumbs": [
      "Study Design and Measures of Association",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian analysis of cohort and case-control studies</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aalen, Odd. 1978. “Nonparametric Inference for a Family of\nCounting Processes.” The Annals of Statistics 6: 701–26.\n\n\nAalen, Odd, Ørnulf Borgan, and Håkon Gjessing. 2008. Survival and\nEvent History Analysis: A Process Point of View. Springer Science\n& Business Media.\n\n\nAgresti, Alan. 2013. Categorical Data Analysis. Third. Vol.\n792. John Wiley & Sons.\n\n\nAgresti, Alan, and Brent A Coull. 1998. “Approximate Is Better\nThan ‘Exact’ for Interval Estimation of Binomial\nProportions.” The American Statistician 52 (2): 119–26.\n\n\nAgresti, Alan, and Yongyi Min. 2005. “Frequentist Performance of\nBayesian Confidence Intervals for Comparing Proportions in\n2× 2 Contingency Tables.”\nBiometrics 61 (2): 515–23.\n\n\nAitchison, John, and SD Silvey. 1958. “Maximum-Likelihood\nEstimation of Parameters Subject to Restraints.” The Annals\nof Mathematical Statistics 29: 813–28.\n\n\nAlbert, Adelin. 1982. “On the Use and Computation of Likelihood\nRatios in Clinical Chemistry.” Clinical Chemistry 28\n(5): 1113–19.\n\n\nAlho, Juha M. 1992. “On Prevalence, Incidence, and Duration in\nGeneral Stable Populations.” Biometrics 48 (2): 587–92.\n\n\nAltman, Douglas G, Kenneth F Schulz, David Moher, Matthias Egger, Frank\nDavidoff, Diana Elbourne, Peter C Gøtzsche, Thomas Lang, and CONSORT\nGroup. 2001. “The Revised CONSORT Statement for\nReporting Randomized Trials: Explanation and Elaboration.”\nAnnals of Internal Medicine 134 (8): 663–94.\n\n\nAltshuler, Bernard. 1970. “Theory for the Measurement of Competing\nRisks in Animal Experiments.” Mathematical Biosciences\n6: 1–11.\n\n\nBaduashvili, Amiran, Arthur T Evans, and Todd Cutler. 2020. “How\nto Understand and Teach p-Values: A Diagnostic Test Framework.”\nJournal of Clinical Epidemiology 122: 49–55.\n\n\nBamber, Donald. 1975. “The Area Above the Ordinal Dominance Graph\nand the Area Below the Receiver Operating Characteristic Graph.”\nJournal of Mathematical Psychology 12 (4): 387–415.\n\n\nBayes, Thomas. 1763. “LII. An Essay\nTowards Solving a Problem in the Doctrine of Chances. By the Late\nRev. Mr. Bayes, FRS\nCommunicated by Mr. Price, in a Letter to\nJohn Canton, AMFRS.”\nPhilosophical Transactions of the Royal Society of London 53:\n370–418.\n\n\nBengtsson, Ewert, and Patrik Malm. 2014. “Screening for Cervical\nCancer Using Automated Analysis of PAP-Smears.”\nComputational and Mathematical Methods in Medicine 2014:\n842037.\n\n\nBerger, James O, and Thomas Sellke. 1987. “Testing a Point Null\nHypothesis: The Irreconcilability of p Values and Evidence.”\nJournal of the American Statistical Association 82 (397):\n112–22.\n\n\nBerkson, Joseph. 1942. “Tests of Significance Considered as\nEvidence.” Journal of the American Statistical\nAssociation 37 (219): 325–35.\n\n\n———. 1944. “Application of the Logistic Function to\nBio-Assay.” Journal of the American Statistical\nAssociation 39 (227): 357–65.\n\n\n———. 1946. “Limitations of the Application of Fourfold Table\nAnalysis to Hospital Data.” Biometrics Bulletin 2 (3):\n47–53.\n\n\nBibbins-Domingo, Kirsten, and Alex Helman, eds. 2022. Improving\nRepresentation in Clinical Trials and Research: Building Research Equity\nfor Women and Underrepresented Groups. National Academies of\nSciences, Engineering,; Medicine (National Academies Press).\n\n\nBirnbaum, Allan. 1964. “Median-Unbiased Estimators.”\nBulletin of Mathematical Statistics 11: 25–34.\n\n\nBlaker, Helge. 2000. “Confidence Curves and Improved Exact\nConfidence Intervals for Discrete Distributions.” Canadian\nJournal of Statistics 28 (4): 783–98.\n\n\nBlumberg, Mark S. 1957. “Evaluating Health Screening\nProcedures.” Operations Research 5 (3): 351–60.\n\n\nBoos, Dennis D, and Leonard A Stefanski. 2013. Essential Statistical\nInference. Springer.\n\n\nBorgan, Ørnulf, Larry Goldstein, and Bryan Langholz. 1995.\n“Methods for the Analysis of Sampled Cohort Data in the Cox\nProportional Hazards Model.” The Annals of Statistics,\n1749–78.\n\n\nBostrom, RC, HS Sawyer, and WE Tolles. 1959. “Instrumentation for\nAutomatically Prescreening Cytological Smears.” Proceedings\nof the IRE 47 (11): 1895–1900.\n\n\nBreslow, NE, NE Day, KT Halvorsen, RL Prentice, and C Sabai. 1978.\n“Estimation of Multiple Relative Risk Functions in Matched\nCase-Control Studies.” American Journal of Epidemiology\n108 (4): 299–307.\n\n\nBreslow, Norman. 1970. “A Generalized Kruskal-Wallis Test for\nComparing k Samples Subject to Unequal Patterns of Censorship.”\nBiometrika 57 (3): 579–94.\n\n\n———. 1974. “Covariance Analysis of Censored Survival Data.”\nBiometrics 30 (1): 89–99.\n\n\nBreslow, Norman E. 1972. “Contribution to Discussion of Paper by\nDR Cox.” Journal of the Royal Statistical Society: Series B\n(Methodological) 34: 216–17.\n\n\nBrittain, Erica, James J Schlesselman, and Bruce V Stadel. 1981.\n“Cost of Case-Control Studies.” American Journal of\nEpidemiology 114 (2): 234–43.\n\n\nBross, Irwin. 1954. “Misclassification in 2 x 2 Tables.”\nBiometrics 10 (4): 478–86.\n\n\nBrown, Lawrence D, T Tony Cai, and Anirban DasGupta. 2001.\n“Interval Estimation for a Binomial Proportion.”\nStatistical Science 16 (2): 101–17.\n\n\n———. 2003. “Interval Estimation in Exponential Families.”\nStatistica Sinica 13: 19–49.\n\n\nBrumback, Babette, and Arthur Berg. 2008. “On Effect-Measure\nModification: Relationships Among Changes in the Relative Risk, Odds\nRatio, and Risk Difference.” Statistics in Medicine 27\n(18): 3453–65.\n\n\nBuell, Philip, and John E Dunn Jr. 1964. “The Dilution Effect of\nMisclassification.” American Journal of Public Health 54\n(4): 598–602.\n\n\nCampbell, Donald T. 1957. “Factors Relevant to the Validity of\nExperiments in Social Settings.” Psychological Bulletin\n54 (4): 297.\n\n\nChung, Kai Lai. 2000. A Course in Probability Theory. Third.\nElsevier.\n\n\nClarke, RD. 1946. “An Application of the Poisson\nDistribution.” Journal of the Institute of Actuaries 72\n(3): 481–81.\n\n\nClayton, David, and Michael Hills. 1993. Statistical Models in\nEpidemiology. Oxford University Press.\n\n\nClopper, Charles J, and Egon S Pearson. 1934. “The Use of\nConfidence or Fiducial Limits Illustrated in the Case of the\nBinomial.” Biometrika 26 (4): 404–13.\n\n\nCohen, Geoffrey R, and Shu-Ying Yang. 1994. “Mid-p Confidence\nIntervals for the Poisson Expectation.”\nStatistics in Medicine 13 (21): 2189–2203.\n\n\nCohen, I Bernard. 1984. “Florence Nightingale.”\nScientific American 250 (3): 128–37.\n\n\nCopeland, Karen T, Harvey Checkoway, Anthony J McMichael, and Robert H\nHolbrook. 1977. “Bias Due to Misclassification in the Estimation\nof Relative Risk.” American Journal of Epidemiology 105\n(5): 488–95.\n\n\nCornfield, Jerome. 1951. “A Method of Estimating Comparative Rates\nfrom Clinical Data; Applications to Cancer of the Lung, Breast, and\nCervix.” Journal of the National Cancer Institute 11:\n1269–75.\n\n\nCorrea-Villaseñor, Adolfo, Walter F Stewart, Francisco Franco-Marina,\nand Hui Seacat. 1995. “Bias from Nondifferential Misclassification\nin Case-Control Studies with Three Exposure Levels.”\nEpidemiology 6 (3): 276–81.\n\n\nCox, David R. 1958. “The Regression Analysis of Binary\nSequences.” Journal of the Royal Statistical Society Series\nB: Statistical Methodology 20 (2): 215–32.\n\n\n———. 1972. “Regression Models and Life-Tables.” Journal\nof the Royal Statistical Society: Series B (Methodological) 34 (2):\n187–202.\n\n\nCox, David R, and E Joyce Snell. 1968. “A General Definition of\nResiduals.” Journal of the Royal Statistical Society: Series\nB (Methodological) 30 (2): 248–65.\n\n\nCramér, Harald. 1946. Mathematical Methods of Statistics.\nPrinceton University Press.\n\n\nDahabreh, Issa J, and Miguel A Hernán. 2019. “Extending Inferences\nfrom a Randomized Trial to a Target Population.” European\nJournal of Epidemiology 34 (8): 719–22.\n\n\nDawid, A Philip. 1979. “Conditional Independence in Statistical\nTheory.” Journal of the Royal Statistical Society: Series B\n(Methodological) 41 (1): 1–15.\n\n\nDiamond, George A, and James S Forrester. 1983. “Clinical Trials\nand Statistical Verdicts: Probable Grounds for Appeal.”\nAnnals of Internal Medicine 98 (3): 385–94.\n\n\nDoll, Richard, and Austin Bradford Hill. 1950. “Smoking and\nCarcinoma of the Lung.” British Medical Journal 2\n(4682): 739.\n\n\nDosemeci, Mustafa, Sholom Wacholder, and Jay H Lubin. 1990. “Does\nNondifferential Misclassification of Exposure Always Bias a True Effect\nToward the Null Value?” American Journal of Epidemiology\n132 (4): 746–48.\n\n\nDuane, Simon, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth.\n1987. “Hybrid Monte Carlo.” Physics\nLetters B 195 (2): 216–22.\n\n\nDunn Jr, John E. 1962. “The Use of Incidence and Prevalence in the\nStudy of Disease Development in a Population.” American\nJournal of Public Health 52 (7): 1107–18.\n\n\nEdwards, Ward, Harold Lindman, and Leonard J Savage. 1963.\n“Bayesian Statistical Inference for Psychological\nResearch.” Psychological Review 70 (3): 193–242.\n\n\nEfron, Bradley. 1967. “The Two Sample Problem with Censored\nData.” In Proceedings of the Fifth Berkeley Symposium on\nMathematical Statistics and Probability, 4:831–53. University of\nCalifornia Press.\n\n\n———. 1977. “The Efficiency of Cox’s Likelihood Function for\nCensored Data.” Journal of the American Statistical\nAssociation 72 (359): 557–65.\n\n\nEfron, Bradley, and David V Hinkley. 1978. “Assessing the Accuracy\nof the Maximum Likelihood Estimator: Observed Versus Expected\nFisher Information.” Biometrika 65 (3):\n457–83.\n\n\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to\nthe Bootstrap. Chapman & Hall/CRC.\n\n\nElandt-Johnson, Regina C. 1975. “Definition of Rates: Some Remarks\non Their Use and Misuse.” American Journal of\nEpidemiology 102 (4): 267–71.\n\n\nElm, Erik von, Douglas G Altman, Matthias Egger, Stuart J Pocock, Peter\nC Gøtzsche, and Jan P Vandenbroucke. 2007. “The Strengthening the\nReporting of Observational Studies in Epidemiology (STROBE) Statement:\nGuidelines for Reporting Observational Studies.” The\nLancet 370 (9596): 1453–57.\n\n\nFagan, Terrence J. 1975. “Nomogram for Bayes’s\nTheorem.” New England Journal of Medicine 293 (5): 257.\n\n\nFarr, William. 1838. “On Prognosis.” British Medical\nAlmanack Supplement: 199–216.\n\n\nFay, Michael P. 2010. “Confidence Intervals That Match Fisher’s\nExact or Blaker’s Exact Tests.” Biostatistics 11 (2):\n373–74.\n\n\nFisher, Ronald A. 1922. “On the Interpretation of χ 2 from Contingency Tables, and the\nCalculation of p.” Journal of the Royal Statistical\nSociety 85 (1): 87–94.\n\n\n———. 1935. “The Logic of Inductive Inference.” Journal\nof the Royal Statistical Society 98 (1): 39–82.\n\n\nFisk, Peter R. 1961. “The Graduation of Income\nDistributions.” Econometrica: Journal of the Econometric\nSociety 29 (2): 171–85.\n\n\nFleming, Thomas R, and David P Harrington. 1984. “Nonparametric\nEstimation of the Survival Distribution in Censored Data.”\nCommunications in Statistics-Theory and Methods 13 (20):\n2469–86.\n\n\n———. 2005. Counting Processes and Survival Analysis. Vol. 625.\nJohn Wiley & Sons.\n\n\nFreedman, David A. 2007. “How Can the Score Test Be\nInconsistent?” The American Statistician 61 (4): 291–95.\n\n\nFreedman, Laurence S, Arthur Schatzkin, and Yohanan Wax. 1990.\n“The Impact of Dietary Measurement Error on Planning Sample Size\nRequired in a Cohort Study.” American Journal of\nEpidemiology 132 (6): 1185–95.\n\n\nFreeman, Jonathan, and George B Hutchison. 1980. “Prevalence,\nIncidence and Duration.” American Journal of\nEpidemiology 112 (5): 707–23.\n\n\nGail, Mitchell, Roger Williams, David P Byar, Charles Brown, et al.\n1976. “How Many Controls?” Journal of Chronic\nDiseases 29 (11): 723–31.\n\n\nGarwood, F. 1936. “Fiducial Limits for the Poisson\nDistribution.” Biometrika 28 (3/4): 437–42.\n\n\nGehan, Edmund A. 1965. “A Generalized Wilcoxon Test for Comparing\nArbitrarily Singly-Censored Samples.” Biometrika 52\n(1-2): 203–24.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari,\nand Donald B Rubin. 2014. Bayesian Data Analysis. Third. CRC\npress.\n\n\nGoldberg, Judith D. 1975. “The Effects of Misclassification on the\nBias in the Difference Between Two Proportions and the Relative Odds in\nthe Fourfold Table.” Journal of the American Statistical\nAssociation 70 (351): 561–67.\n\n\nGompertz, Benjamin. 1825. “XXIV. On the\nNature of the Function Expressive of the Law of Human Mortality, and on\na New Mode of Determining the Value of Life Contingencies.”\nPhilosophical Transactions of the Royal Society of London 115:\n513–83.\n\n\nGreenland, Sander. 1998. “Probability Logic and Probabilistic\nInduction.” Epidemiology 9 (3): 322–32.\n\n\n———. 2006. “Bayesian Perspectives for Epidemiological Research: I.\nFoundations and Basic Methods.” International Journal of\nEpidemiology 35 (3): 765–75.\n\n\nGreenland, Sander, and Charles Poole. 2013. “Living with p Values:\nResurrecting a Bayesian Perspective on Frequentisi Statistics.”\nEpidemiology 24 (1): 62–68.\n\n\nGreenland, Sander, Stephen J Senn, Kenneth J Rothman, John B Carlin,\nCharles Poole, Steven N Goodman, and Douglas G Altman. 2016.\n“Statistical Tests, p Values, Confidence Intervals, and Power: A\nGuide to Misinterpretations.” European Journal of\nEpidemiology 31 (4): 337–50.\n\n\nGreenland, Sander, and Duncan C Thomas. 1982. “On the Need for the\nRare Disease Assumption in Case-Control Studies.” American\nJournal of Epidemiology 116 (3): 547–53.\n\n\nGreenwood, Major. 1926. “The Natural Duration of Cancer.”\nReports on Public Health and Medical Subjects 33: 1–26.\n\n\nGullen, Warren H, Jacob E Bearman, and Eugene A Johnson. 1968.\n“Effects of Misclassification in Epidemiologic Studies.”\nPublic Health Reports 83 (11): 914–18.\n\n\nHanley, James A, and Barbara J McNeil. 1982. “The Meaning and Use\nof the Area Under a Receiver Operating Characteristic (ROC)\nCurve.” Radiology 143 (1): 29–36.\n\n\nHarrington, David P, and Thomas R Fleming. 1982. “A Class of Rank\nTest Procedures for Censored Survival Data.” Biometrika\n69 (3): 553–66.\n\n\nHastings, W Keith. 1970. “Monte Carlo Sampling\nMethods Using Markov Chains and Their Applications.”\nBiometrika 57 (1): 97–109.\n\n\nHerbst, Arthur L, Howard Ulfelder, and David C Poskanzer. 1971.\n“Adenocarcinoma of the Vagina: Association of Maternal Stilbestrol\nTherapy with Tumor Appearance in Young Women.” New England\nJournal of Medicine 284 (16): 878–81.\n\n\nHernán, Miguel Á, Sonia Hernández-Dı́az, and James M Robins. 2004.\n“A Structural Approach to Selection Bias.”\nEpidemiology 15 (5): 615–25.\n\n\nHill, Sir Austin Bradford. 1965. “The Environment and Disease:\nAssociation or Causation?” Proceedings of the Royal Society\nof Medicine 58: 295–300.\n\n\nHurwitz, Eugene S, Michael J Barrett, Dennis Bregman, Walter J Gunn,\nPaul Pinsky, Lawrence B Schonberger, Joseph S Drage, et al. 1987.\n“Public Health Service Study of Reye’s\nSyndrome and Medications: Report of the Main Study.”\nJAMA 257 (14): 1905–11.\n\n\nIrwin, JO et al. 1935. “Tests of Significance for Differences\nBetween Percentages Based on Small Numbers.” Metron 12\n(2): 84–94.\n\n\nJeffreys, Harold. 1946. “An Invariant Form for the Prior\nProbability in Estimation Problems.” Proceedings of the Royal\nSociety of London. Series A. Mathematical and Physical Sciences 186\n(1007): 453–61.\n\n\nJohansen, Søren. 1983. “An Extension of Cox’s Regression\nModel.” International Statistical Review 51 (2): 165–74.\n\n\nKalbfleisch, John D, and Ross L Prentice. 2011. The Statistical\nAnalysis of Failure Time Data. John Wiley & Sons.\n\n\nKaplan, Edward L, and Paul Meier. 1958. “Nonparametric Estimation\nfrom Incomplete Observations.” Journal of the American\nStatistical Association 53 (282): 457–81.\n\n\nKatz, DJSM, J Baptista, SP Azen, and MC Pike. 1978. “Obtaining\nConfidence Intervals for the Risk Ratio in Cohort Studies.”\nBiometrics, 469–74.\n\n\nKeiding, Niels. 1991. “Age-Specific Incidence and Prevalence: A\nStatistical Perspective.” Journal of the Royal Statistical\nSociety: Series A (Statistics in Society) 154 (3): 371–96.\n\n\nKenah, Eben. 2024. “Rothman Diagrams: The Geometry of Confounding\nand Standardization.” International Journal of\nEpidemiology 53 (6): dyae139.\n\n\nKenward, Michael G, and Geert Molenberghs. 1998. “Likelihood Based\nFrequentist Inference When Data Are Missing at Random.”\nStatistical Science 13 (3): 236–47.\n\n\nKessel, Elton. 1962. “Diabetes Detection: An Improved\nApproach.” Journal of Chronic Diseases 15 (12): 1109–21.\n\n\nKupper, LL, AJ McMichael, and R Spirtas. 1975. “A Hybrid\nEpidemiologic Study Design Useful in Estimating Relative Risk.”\nJournal of the American Statistical Association 70 (351a):\n524–28.\n\n\nL’Abbé, Kristan A, Allan S Detsky, and Keith O’Rourke. 1987.\n“Meta-Analysis in Clinical Research.” Annals of\nInternal Medicine 107 (2): 224–33.\n\n\nLancaster, H Oliver. 1961. “Significance Tests in Discrete\nDistributions.” Journal of the American Statistical\nAssociation 56 (294): 223–34.\n\n\nLaplace, Pierre Simon. 1820. Théorie Analytique Des\nProbabilités. Vol. 7. Courcier.\n\n\nLe Cam, Lucien. 1953. “On Some Asymptotic Properties of Maximum\nLikelihood Estimates and Related Bayes’ Estimates.”\nUniversity of California Publications in Statistics 1 (11):\n277–330.\n\n\nLiddell, FDK, JC McDonald, and DC Thomas. 1977. “Methods of Cohort\nAnalysis: Appraisal by Application to Asbestos Mining.”\nJournal of the Royal Statistical Society: Series A (General)\n140 (4): 469–83.\n\n\nLusted, Lee B. 1971a. “Decision-Making Studies in Patient\nManagement.” New England Journal of Medicine 284 (8):\n416–24.\n\n\n———. 1971b. “Signal Detectability and Medical\nDecision-Making.” Science 171 (3977): 1217–19.\n\n\n———. 1984. “ROC Recollected.” Medical\nDecision Making 4: 131–35.\n\n\nMacMahon, Brian, and William D Terry. 1958. “Application of Cohort\nAnalysis to the Study of Time Trends in Neoplastic Disease.”\nJournal of Chronic Diseases 7 (1): 24–35.\n\n\nMakeham, William Matthew. 1860. “On the Law of Mortality and the\nConstruction of Annuity Tables.” The Assurance Magazine, and\nJournal of the Institute of Actuaries 8 (6): 301–10.\n\n\nMantel, Nathan. 1966. “Evaluation of Survival Data and Two New\nRank Order Statistics Arising in Its Consideration.” Cancer\nChemotherapy Reports 50 (3): 163–70.\n\n\nMcCullagh, Peter, and J A Nelder. 1989. Generalized Linear\nModels. 2nd ed. Chapman & Hall.\n\n\nMetropolis, Nicholas, Arianna W Rosenbluth, Marshall N Rosenbluth,\nAugusta H Teller, and Edward Teller. 1953. “Equation of State\nCalculations by Fast Computing Machines.” The Journal of\nChemical Physics 21 (6): 1087–92.\n\n\nMeydrech, Edward F, and Lawrence L Kupper. 1978. “Cost\nConsiderations and Sample Size Requirements in Cohort and Case-Control\nStudies.” American Journal of Epidemiology 107 (3):\n201–5.\n\n\nMiettinen, Olli. 1976. “Estimability and Estimation in\nCase-Referent Studies.” American Journal of Epidemiology\n103 (2): 226–35.\n\n\nMiettinen, Olli S. 1969. “Individual Matching with Multiple\nControls in the Case of All-or-None Responses.”\nBiometrics 25 (2): 339–55.\n\n\nMiettinen, Olli S, and E Francis Cook. 1981. “Confounding: Essence\nand Detection.” American Journal of Epidemiology 114\n(4): 593–603.\n\n\nMoher, David, Kenneth F Schulz, Douglas G Altman, Matthias Egger, Frank\nDavidoff, Diana Elbourne, Peter C. Gøtzsche, Thomas Lang, and CONSORT\nGroup. 2001. “The CONSORT Statement: Revised\nRecommendations for Improving the Quality of Reports of Parallel-Group\nRandomized Trials.” Annals of Internal Medicine 134 (8):\n657–62.\n\n\nMorabia, Alfredo. 2004. “Epidemiology: An Epistemological\nPerspective.” In A History of Epidemiologic Methods and\nConcepts, edited by Alfredo Morabia, 3–125. Springer.\n\n\nMorgenstern, Hal, David G Kleinbaum, and Lawrence L Kupper. 1980.\n“Measures of Disease Incidence Used in Epidemiologic\nResearch.” International Journal of Epidemiology 9 (1):\n97–104.\n\n\nMorgenstern, Hal, and Deborah M Winn. 1983. “A Method for\nDetermining the Sampling Ratio in Epidemiologic Studies.”\nStatistics in Medicine 2 (3): 387–96.\n\n\nNaimi, Ashley I, and Brian W Whitcomb. 2020. “Estimating Risk\nRatios and Risk Differences Using Regression.” American\nJournal of Epidemiology 189 (6): 508–10.\n\n\nNam, Jun-Mo. 1973. “Optimum Sample Sizes for the Comparison of the\nControl and Treatment.” Biometrics 29: 101–8.\n\n\nNeal, Radford M. 1996. “Monte Carlo\nImplementation.” Bayesian Learning for Neural Networks,\n55–98.\n\n\nNelder, John Ashworth, and Robert W M Wedderburn. 1972.\n“Generalized Linear Models.” Journal of the Royal\nStatistical Society Series A: Statistics in Society 135 (3):\n370–84.\n\n\nNelson, Wayne. 1969. “Hazard Plotting for Incomplete Failure\nData.” Journal of Quality Technology 1: 27–52.\n\n\n———. 1972. “Theory and Applications of Hazard Plotting for\nCensored Failure Data.” Technometrics 14 (4): 945–66.\n\n\nNewell, David J. 1962. “Errors in the Interpretation of Errors in\nEpidemiology.” American Journal of Public Health 52\n(11): 1925–28.\n\n\nNewell, DJ. 1963. “Note: Misclassification in 2 x 2\nTables.” Biometrics 19 (1): 187–88.\n\n\nNeyman, Jerzy, and Egon Sharpe Pearson. 1933. “On the Problem of\nthe Most Efficient Tests of Statistical Hypotheses.”\nPhilosophical Transactions of the Royal Society of London. Series A,\nContaining Papers of a Mathematical or Physical Character 231\n(694-706): 289–337.\n\n\nOakes, David. 1981. “Survival Times: Aspects of Partial\nLikelihood.” International Statistical Review, 235–52.\n\n\nPaneth, Nigel, Ezra Susser, and Mervyn Susser. 2002. “Origins and\nEarly Development of the Case-Control Study: Part 1, Early\nEvolution.” Sozial-Und Präventivmedizin 47\n(5): 282–88.\n\n\nPaneth, Nigl, Ezra Susser, and Mervyn Susser. 2002. “Origins and\nEarly Development of the Case-Control Study: Part 2, the Case-Control\nStudy from Lane-Claypon to 1950.” Sozial-Und\nPräventivmedizin 47 (6): 359–65.\n\n\nPearce, Neil. 1993. “What Does the Odds Ratio Estimate in a\nCase-Control Study?” International Journal of\nEpidemiology 22 (6): 1189–92.\n\n\nPearson, Karl. 1900. “On the Criterion That a Given System of\nDeviations from the Probable in the Case of a Correlated System of\nVariables Is Such That It Can Be Reasonably Supposed to Have Arisen from\nRandom Sampling.” The London, Edinburgh, and Dublin\nPhilosophical Magazine and Journal of Science 50 (302): 157–75.\n\n\n———. 1922. “On the χ 2\nTest of Goodness of Fit.” Biometrika 14 (1/2): 186–91.\n\n\nPeto, Richard. 1972. “Contribution to Discussion of Paper by DR\nCox.” Journal of the Royal Statistical Society: Series B\n(Methodological) 34: 202–7.\n\n\nPeto, Richard, and Julian Peto. 1972. “Asymptotically Efficient\nRank Invariant Test Procedures.” Journal of the Royal\nStatistical Society: Series A (General) 135 (2): 185–98.\n\n\nPike, MC, and JT Casagrande. 1979. “Re:‘cost Considerations\nand Sample Size Requirements in Cohort and Case-Control\nStudies’.” American Journal of Epidemiology 110\n(1): 100–102.\n\n\nPratt, John W. 1965. “Bayesian Interpretation of Standard\nInference Statements.” Journal of the Royal Statistical\nSociety: Series B (Methodological) 27 (2): 169–203.\n\n\nPrentice, Ross L. 1978. “Linear Rank Tests with Right Censored\nData.” Biometrika 65 (1): 167–79.\n\n\nPreston, Samuel H. 1987. “Relations Among Standard Epidemiologic\nMeasures in a Population.” American Journal of\nEpidemiology 126 (2): 336–45.\n\n\nRao, C Radhakrishna. 1945. “Information and Accuracy Attainable in\nthe Estimation of Statistical Parameters.” Bulletin of the\nCalcutta Mathematical Society 37 (3): 81–91.\n\n\n———. 1948. “Large Sample Tests of Statistical Hypotheses\nConcerning Several Parameters with Applications to Problems of\nEstimation.” In Mathematical Proceedings of the Cambridge\nPhilosophical Society, 44:50–57. Cambridge University Press.\n\n\nReid, Nancy. 2003. “Asymptotics and the Theory of\nInference.” The Annals of Statistics 31 (6): 1695–2095.\n\n\nRemein, Quentin R, and Hugh LC Wilkerson. 1961. “The Efficiency of\nScreening Tests for Diabetes.” Journal of Chronic\nDiseases 13 (1): 6–21.\n\n\nRichardson, Thomas S, James M Robins, and Linbo Wang. 2017. “On\nModeling and Estimation for the Relative Risk and Risk\nDifference.” Journal of the American Statistical\nAssociation 112 (519): 1121–30.\n\n\nRobert, Christian P, and George Casella. 2004. Monte Carlo\nStatistical Methods. Second edition. Springer.\n\n\nRogot, Eugene. 1961. “A Note on Measurement Errors and Detecting\nReal Differences.” Journal of the American Statistical\nAssociation 56 (294): 314–19.\n\n\nRoscoe, John T, and Jackson A Byars. 1971. “An Investigation of\nthe Restraints with Respect to Sample Size Commonly Imposed on the Use\nof the Chi-Square Statistic.” Journal of the American\nStatistical Association 66 (336): 755–59.\n\n\nRothman, Kenneth J. 1975. “A Pictorial Representation of\nConfounding in Epidemiologic Studies.” Journal of Chronic\nDiseases 28 (2): 101–8.\n\n\n———. 1978. “A Show of Confidence.” New England Journal\nof Medicine 299 (24): 1362–63.\n\n\n———. 1981. “Induction and Latent Periods.” American\nJournal of Epidemiology 114 (2): 253–59.\n\n\nRothman, Kenneth J, Sander Greenland, and Timothy L Lash. 2008.\nModern Epidemiology. Lippincott Williams & Wilkins.\n\n\nRoutledge, RD. 1992. “Resolving the Conflict over Fisher’s Exact\nTest.” Canadian Journal of Statistics 20 (2): 201–9.\n\n\nRubin, Theodore, Joseph Rosenbaum, and Sidney Cobb. 1956. “The Use\nof Interview Data for the Detection of Associations in Field\nStudies.” Journal of Chronic Diseases 4 (3): 253–66.\n\n\nSimpson, Edward H. 1951. “The Interpretation of Interaction in\nContingency Tables.” Journal of the Royal Statistical\nSociety: Series B (Methodological) 13 (2): 238–41.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera.\nSecond edition. John Churchill. https://wellcomecollection.org/works/uqa27qrt.\n\n\nSorahan, Tom, and Mark S Gilthorpe. 1994. “Non-Differential\nMisclassification of Exposure Always Leads to an Underestimate of Risk:\nAn Incorrect Conclusion.” Occupational and Environmental\nMedicine 51 (12): 839–40.\n\n\nSwets, John A. 1973. “The Relative Operating Characteristic in\nPsychology: A Technique for Isolating Effects of Response Bias Finds\nWide Use in the Study of Perception and Cognition.”\nScience 182 (4116): 990–1000.\n\n\n———. 1988. “Measuring the Accuracy of Diagnostic Systems.”\nScience 240 (4857): 1285–93.\n\n\nSwift, Michael Bruce. 2009. “Comparison of Confidence Intervals\nfor a Poisson Mean–Further Considerations.”\nCommunications in Statistics–Theory and Methods 38 (5): 748–59.\n\n\nTarone, Robert E, and James Ware. 1977. “On Distribution-Free\nTests for Equality of Survival Distributions.”\nBiometrika 64 (1): 156–60.\n\n\nThomas, DC. 1977. “Appendum to ‘Methods of Cohort Analysis:\nAppraisal by Application to Asbestos Mining,’ by Liddell, FDK,\nMcDonald, JC and Thomas, DC.” Journal of the Royal\nStatistical Society: Series A (General) 140: 469–90.\n\n\nTukey, John W. 1960. “Conclusions Vs Decisions.”\nTechnometrics 2 (4): 423–33.\n\n\n———. 1962. “The Future of Data Analysis.” The Annals of\nMathematical Statistics 33 (1): 1–67.\n\n\nVan der Vaart, Aad W. 2000. Asymptotic Statistics. Cambridge\nUniversity Press.\n\n\nVandenbroucke, Jan P, Erik von Elm, Douglas G Altman, Peter C Gøtzsche,\nCynthia D Mulrow, Stuart J Pocock, Charles Poole, James J Schlesselman,\nMatthias Egger, and Strobe Initiative. 2007. “Strengthening the\nReporting of Observational Studies in Epidemiology (STROBE): Explanation\nand Elaboration.” Annals of Internal Medicine 147 (8):\nW–163.\n\n\nVecchio, Thomas J. 1966. “Predictive Value of a Single Diagnostic\nTest in Unselected Populations.” New England Journal of\nMedicine 274 (21): 1171–73.\n\n\nVerkerk, PH, and SE Buitendijk. 1992. “Non-Differential\nUnderestimation May Cause a Threshold Effect of Exposure to Appear as a\nDose-Response Relationship.” Journal of Clinical\nEpidemiology 45 (5): 543–45.\n\n\nWacholder, Sholom, Patricia Hartge, Jay H Lubin, and Mustafa Dosemeci.\n1995. “Non-Differential Misclassification and Bias Towards the\nNull: A Clarification.” Occupational and Environmental\nMedicine 52 (8): 557.\n\n\nWald, Abraham. 1943. “Tests of Statistical Hypotheses Concerning\nSeveral Parameters When the Number of Observations Is Large.”\nTransactions of the American Mathematical Society 54 (3):\n426–82.\n\n\nWalker, Alexander M, Johan P Velema, and James M Robins. 1988.\n“Analysis of Case-Control Data Derived in Part from Proxy\nRespondents.” American Journal of Epidemiology 127 (5):\n905–14.\n\n\nWalter, Samuel D. 1977. “Determination of Significant Relative\nRisks and Optimal Sampling Procedures in Prospective and Retrospective\nComparative Studies of Various Sizes.” American Journal of\nEpidemiology 105 (4): 387–97.\n\n\nWeibull, Waloddi et al. 1951. “A Statistical Distribution Function\nof Wide Applicability.” Journal of Applied Mechanics 18\n(3): 293–97.\n\n\nWilks, Samuel S. 1938. “The Large-Sample Distribution of the\nLikelihood Ratio for Testing Composite Hypotheses.” The\nAnnals of Mathematical Statistics 9 (1): 60–62.\n\n\nWilliamson, Tyler, Misha Eliasziw, and Gordon Hilton Fick. 2013.\n“Log-Binomial Models: Exploring Failed Convergence.”\nEmerging Themes in Epidemiology 10 (1): 1–10.\n\n\nWilson, Edwin B. 1927. “Probable Inference, the Law of Succession,\nand Statistical Inference.” Journal of the American\nStatistical Association 22 (158): 209–12.\n\n\nWinkelstein Jr, Warren. 2009. “Florence Nightingale: Founder of\nModern Nursing and Hospital Epidemiology.” Epidemiology\n20 (2): 311.\n\n\nWoolf, Barnet. 1955. “On Estimating the Relation Between Blood\nGroup and Disease.” Annals of Human Genetics 19 (4):\n251–53.\n\n\nYerushalmy, Jacob. 1947. “Statistical Problems in Assessing\nMethods of Medical Diagnosis, with Special Reference to\nX-Ray Techniques.” Public Health Reports\n(1896-1970) 62 (40): 1432–49.\n\n\nYland, Jennifer J, Amelia K Wesselink, Timothy L Lash, and Matthew P\nFox. 2022. “Misconceptions about the Direction of Bias from\nNondifferential Misclassification.” American Journal of\nEpidemiology 191 (8): 1485–95.\n\n\nZou, Guangyong. 2004. “A Modified Poisson Regression\nApproach to Prospective Studies with Binary Data.” American\nJournal of Epidemiology 159 (7): 702–6.\n\n\nZweig, Mark H, and Gregory Campbell. 1993. “Receiver-Operating\nCharacteristic (ROC) Plots: A Fundamental Evaluation Tool\nin Clinical Medicine.” Clinical Chemistry 39 (4):\n561–77.",
    "crumbs": [
      "References"
    ]
  }
]