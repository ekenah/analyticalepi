<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Probability, Random Variables, and Disease Occurrence – Analytical Epidemiology</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./condprob.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-27896f7bfb4f984e41754f49ae4beae8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script>
window.MathJax = {
  tex: {
    macros: {
      Bernoulli: ["\\operatorname\{Bernoulli\}"],
      comp: "\\mathsf\{C\}",
      Cov: ["\\operatorname\{Cov\}"],
      E: ["\\operatorname\{\\mathbb\{E\}\}"],
      indicator: "\\mathbb\{1\}",
      supp: ["\\operatorname\{supp\}"],
      tonset: "t^\\text\{ons\}",
      trec: "t^\\text\{rec\}",
      Var: ["\\operatorname\{Var\}"],
      RR: "{\\bf R}",
      bold: ["{\\bf #1}", 1]
    }
  }
};
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./probability.html">A. One-Sample Inference for Risks and Rates</a></li><li class="breadcrumb-item"><a href="./probability.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability, Random Variables, and Disease Occurrence</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Analytical Epidemiology</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">A. One-Sample Inference for Risks and Rates</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability, Random Variables, and Disease Occurrence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./condprob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Probability and Diagnostic Tests</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">B. Two-Sample Inference and Study Design</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">C. Principles of Causal Inference</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">D. Epidemologic and Statistical Methods for Causal Inference</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sets-experiments-and-events" id="toc-sets-experiments-and-events" class="nav-link active" data-scroll-target="#sets-experiments-and-events"><span class="header-section-number">1.1</span> Sets, experiments, and events</a>
  <ul class="collapse">
  <li><a href="#experiments-and-events" id="toc-experiments-and-events" class="nav-link" data-scroll-target="#experiments-and-events"><span class="header-section-number">1.1.1</span> Experiments and events</a></li>
  <li><a href="#set-operations-and-logic" id="toc-set-operations-and-logic" class="nav-link" data-scroll-target="#set-operations-and-logic"><span class="header-section-number">1.1.2</span> Set operations and logic</a></li>
  <li><a href="#venn-diagrams" id="toc-venn-diagrams" class="nav-link" data-scroll-target="#venn-diagrams"><span class="header-section-number">1.1.3</span> Venn diagrams</a></li>
  <li><a href="#sec-evseq" id="toc-sec-evseq" class="nav-link" data-scroll-target="#sec-evseq"><span class="header-section-number">1.1.4</span> Sequences of events*</a></li>
  <li><a href="#algebra-of-sets" id="toc-algebra-of-sets" class="nav-link" data-scroll-target="#algebra-of-sets"><span class="header-section-number">1.1.5</span> Algebra of sets<span class="math inline">\(^*\)</span></a></li>
  </ul></li>
  <li><a href="#probability" id="toc-probability" class="nav-link" data-scroll-target="#probability"><span class="header-section-number">1.2</span> Probability</a>
  <ul class="collapse">
  <li><a href="#probability-calculations" id="toc-probability-calculations" class="nav-link" data-scroll-target="#probability-calculations"><span class="header-section-number">1.2.1</span> Probability calculations</a></li>
  </ul></li>
  <li><a href="#random-variables" id="toc-random-variables" class="nav-link" data-scroll-target="#random-variables"><span class="header-section-number">1.3</span> Random variables</a>
  <ul class="collapse">
  <li><a href="#sec-indicator" id="toc-sec-indicator" class="nav-link" data-scroll-target="#sec-indicator"><span class="header-section-number">1.3.1</span> Indicator variables</a></li>
  <li><a href="#probability-distributions" id="toc-probability-distributions" class="nav-link" data-scroll-target="#probability-distributions"><span class="header-section-number">1.3.2</span> Probability distributions</a></li>
  <li><a href="#mean-and-variance" id="toc-mean-and-variance" class="nav-link" data-scroll-target="#mean-and-variance"><span class="header-section-number">1.3.3</span> Mean and variance</a></li>
  <li><a href="#bernoulli-distribution" id="toc-bernoulli-distribution" class="nav-link" data-scroll-target="#bernoulli-distribution"><span class="header-section-number">1.3.4</span> Bernoulli distribution</a></li>
  </ul></li>
  <li><a href="#joint-and-marginal-distributions" id="toc-joint-and-marginal-distributions" class="nav-link" data-scroll-target="#joint-and-marginal-distributions"><span class="header-section-number">1.4</span> Joint and marginal distributions</a>
  <ul class="collapse">
  <li><a href="#linear-combinations" id="toc-linear-combinations" class="nav-link" data-scroll-target="#linear-combinations"><span class="header-section-number">1.4.1</span> Linear combinations*</a></li>
  <li><a href="#mean-and-covariance" id="toc-mean-and-covariance" class="nav-link" data-scroll-target="#mean-and-covariance"><span class="header-section-number">1.4.2</span> Mean and covariance*</a></li>
  </ul></li>
  <li><a href="#probability-and-disease-occurrence" id="toc-probability-and-disease-occurrence" class="nav-link" data-scroll-target="#probability-and-disease-occurrence"><span class="header-section-number">1.5</span> Probability and disease occurrence</a>
  <ul class="collapse">
  <li><a href="#prevalence" id="toc-prevalence" class="nav-link" data-scroll-target="#prevalence"><span class="header-section-number">1.5.1</span> Prevalence</a></li>
  <li><a href="#risk-cumulative-incidence" id="toc-risk-cumulative-incidence" class="nav-link" data-scroll-target="#risk-cumulative-incidence"><span class="header-section-number">1.5.2</span> Risk (cumulative incidence)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./probability.html">A. One-Sample Inference for Risks and Rates</a></li><li class="breadcrumb-item"><a href="./probability.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability, Random Variables, and Disease Occurrence</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability, Random Variables, and Disease Occurrence</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- {{< include analyticalepi.qmd >}} -->
<blockquote class="blockquote">
<p>One sees, from this essay, that probability theory is basically common sense reduced to calculation; it makes us appreciate with exactitude that which fair minds sense with a sort of instinct, often without being able to account for it. <span class="citation" data-cites="marquis1820theorie">(<a href="references.html#ref-marquis1820theorie" role="doc-biblioref">Laplace 1820</a>)</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
</blockquote>
<p>To begin at the beginning, we will start with probability. <span class="citation" data-cites="morabia2004epidemiology">Morabia (<a href="references.html#ref-morabia2004epidemiology" role="doc-biblioref">2004</a>)</span> accurately observed that “Epidemiology came late in human history because it had to wait for the emergence of probability.” This is probably the most difficult chapter of the entire book, but it will make all subsequent chapters easier. You can use it as a reference and come back to the difficult parts when you need them. Learning to think clearly about probability will give you a compass to find your way through epidemiologic methods.</p>
<section id="sets-experiments-and-events" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sets-experiments-and-events"><span class="header-section-number">1.1</span> Sets, experiments, and events</h2>
<p>To speak clearly about probabilities, we need some basic notation for sets. If <span class="math inline">\(A\)</span> is a set that contains an object <span class="math inline">\(a\)</span>, we write <span class="math display">\[\begin{equation}
  a \in A
\end{equation}\]</span> to indicate that <span class="math inline">\(a\)</span> is an <strong>element</strong> of <span class="math inline">\(A\)</span>. If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are sets such that every element of <span class="math inline">\(A\)</span> is also an element of <span class="math inline">\(B\)</span>, we write <span class="math display">\[\begin{equation}
  A \subseteq B.
\end{equation}\]</span> to indicate that <span class="math inline">\(A\)</span> is a <strong>subset</strong> of <span class="math inline">\(B\)</span>. Sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are equal if and only if <span class="math inline">\(A \subseteq B\)</span> and <span class="math inline">\(B \subseteq A\)</span>, which means they contain exactly the same elements. The <em>empty set</em> with no elements is denoted <span class="math inline">\(\varnothing\)</span>. For any set <span class="math inline">\(A\)</span>, it is true that <span class="math inline">\(A \subseteq A\)</span> and <span class="math inline">\(\varnothing \subseteq A\)</span>.</p>
<p>We use <span class="math inline">\(\mathbb{R}\)</span> to denote the real numbers. Intervals are subsets of <span class="math inline">\(\mathbb{R}\)</span> that take one of the following forms: <span class="math display">\[\begin{align*}
  [a, b] &amp;= \{x \in \mathbb{R}: a \leq x \leq b\}, \\
  [a, b) &amp;= \{x \in \mathbb{R}: a \leq x &lt; b\}, \\
  (a, b] &amp;= \{x \in \mathbb{R}: a &lt; x \leq b\}, \\
  (a, b) &amp;= \{x \in \mathbb{R}: a &lt; x &lt; b\}.
\end{align*}\]</span> An endpoint with a square bracket is included in the interval and an endpoint with a round bracket is not. We can have <span class="math inline">\(a = -\infty\)</span> or <span class="math inline">\(b = \infty\)</span> as long as we use a round bracket for the corresponding endpoint. For example, it is true that <span class="math inline">\(\mathbb{R} = (-\infty, \infty)\)</span>. However, <span class="math inline">\(\mathbb{R} \neq [-\infty, \infty]\)</span> because <span class="math inline">\(\pm \infty\)</span> are not real numbers.</p>
<section id="experiments-and-events" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="experiments-and-events"><span class="header-section-number">1.1.1</span> Experiments and events</h3>
<p>In probability, an <strong>experiment</strong> is any process that will produce one outcome out of a set possible outcomes. The set of possible outcomes is called the <strong>sample space</strong> and is traditionally denoted <span class="math inline">\(\Omega\)</span>. An experiment produces a single outcome <span class="math inline">\(\omega \in \Omega\)</span>. For example, the sample space for a single coin flip is <span class="math display">\[\begin{equation}
  \Omega = \{H, T\},
\end{equation}\]</span> where <span class="math inline">\(\omega = H\)</span> if we get heads and <span class="math inline">\(\omega = T\)</span> if we get tails.</p>
<p>The outcomes in the sample space must determine everything about the random outcome of the experiment. If we flip a coin twice, the sample space cannot be <span class="math inline">\(\{H, T\}\)</span> because each <span class="math inline">\(\omega \in \Omega\)</span> must specify the outcome of both coin flips. Instead, <span class="math display">\[\begin{equation}
  \Omega = \{HH, HT, TH, TT\}
\end{equation}\]</span> where <span class="math inline">\(\omega = XY\)</span> if we get <span class="math inline">\(X\)</span> on the first flip and <span class="math inline">\(Y\)</span> on the second. This helps us see, for example, that there are two ways to get one <span class="math inline">\(H\)</span> and one <span class="math inline">\(T\)</span> in two coin flips.</p>
<p>The purpose of probability is to summarize uncertainty about the outcomes of experiments. However, the outcomes themselves do not have probabilities. Probabilities are assigned to <strong>events</strong>, which are subsets of the sample space <span class="math inline">\(\Omega\)</span>. If <span class="math inline">\(A\)</span> is an event, then <span class="math inline">\(A\)</span> occurs if and only if the outcome <span class="math inline">\(\omega\)</span> produced by our experiment is an element of <span class="math inline">\(A\)</span> (i.e., if and only if <span class="math inline">\(\omega \in A\)</span>). If we flip a coin twice, the event that we get two heads is <span class="math inline">\(\{HH\}\)</span>, the event that we get one head is <span class="math inline">\(\{HT, TH\}\)</span>, and the event that we get zero heads is <span class="math inline">\(\{TT\}\)</span>. By definition, the event <span class="math inline">\(\Omega\)</span> always occurs and the event <span class="math inline">\(\varnothing\)</span> never occurs.</p>
<p>In experiments with a finite or countably infinite sample space,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> the distinction between the outcome <span class="math inline">\(\omega\)</span> and the event <span class="math inline">\(\{\omega\}\)</span> can be safely ignored. In more complex experiments (e.g., taking a random sample from a standard normal distribution), this distinction is important.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> In all cases, experiments have outcomes and events have probabilities.</p>
<p>In epidemiology, it is often useful to think of the sample space <span class="math inline">\(\Omega\)</span> as being a population and each <span class="math inline">\(\omega \in \Omega\)</span> as an individual in this population. In this context, our experiment is to sample a person from <span class="math inline">\(\Omega\)</span> and ask them questions, take measurements, or follow them over time to ascertain disease occurrence. Events would be subpopulations of <span class="math inline">\(\Omega\)</span>, such as <span class="math inline">\(\{\omega \in \Omega: \omega \text{ lives in Ohio}\}\)</span>. This event occurs if the sampled individual <span class="math inline">\(\omega\)</span> lives in Ohio, and it does not occur if they live somewhere else.</p>
</section>
<section id="set-operations-and-logic" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="set-operations-and-logic"><span class="header-section-number">1.1.2</span> Set operations and logic</h3>
<p>There are three basic set operations that take one or more given sets and define another set: complement, intersection, and union. Each operation has a simple interpretation in terms of logic.</p>
<ul>
<li><p>The <strong>complement</strong> of a set <span class="math inline">\(A\)</span> is <span class="math display">\[\begin{equation}
A^\comp = \{\omega \in \Omega : \omega \not\in A\},
\end{equation}\]</span> which can be interpreted logically as <strong><em>not <span class="math inline">\(A\)</span></em></strong>. If <span class="math inline">\(A\)</span> is an event, then the event <span class="math inline">\(A^\comp\)</span> occurs if <span class="math inline">\(\omega \not\in A\)</span>. For the same reason that “not not A” means “A”, we have <span class="math inline">\((A^\comp)^\comp = A\)</span>.</p></li>
<li><p>The <strong>intersection</strong> of two sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is <span class="math display">\[\begin{equation}
  A \cap B = \{\omega \in \Omega : \omega \in A \text{ and } \omega \in B\},
\end{equation}\]</span> which can be interpreted logically as <strong><em><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span></em></strong>. If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are events, then the event <span class="math inline">\(A \cap B\)</span> occurs if <span class="math inline">\(\omega \in A\)</span> and <span class="math inline">\(\omega \in B\)</span>.</p></li>
<li><p>The <strong>union</strong> of two sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is <span class="math display">\[\begin{equation}
  A \cup B = \{\omega \in \Omega : \omega \in A \text{ or } \omega \in B\},
\end{equation}\]</span> which can be interpreted logically as <strong><em><span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span></em></strong> as long as we use “or” in an inclusive sense (i.e., and/or). If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are events, then the event <span class="math inline">\(A \cup B\)</span> occurs if <span class="math inline">\(\omega \in A\)</span> or <span class="math inline">\(\omega \in B\)</span>.</p></li>
</ul>
<p>If <span class="math inline">\(A \subseteq B\)</span>, then <span class="math inline">\(A \cap B = A\)</span> and <span class="math inline">\(A \cup B = B\)</span>. An important special case is that <span id="eq-AA"><span class="math display">\[
  A \cap A = A \cup A = A.
\tag{1.1}\]</span></span> For the empty set <span class="math inline">\(\varnothing\)</span>, we get <span class="math inline">\(A \cap \varnothing = \varnothing\)</span> and <span class="math inline">\(A \cup \varnothing = A\)</span>. For the sample space <span class="math inline">\(\Omega\)</span>, we get <span class="math inline">\(A \cap \Omega = A\)</span> and <span class="math inline">\(A \cup \Omega = \Omega\)</span>.</p>
<p>Union and intersection are <em>commutative</em> operations like addition and multiplication, so the order of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> does not matter: <span class="math display">\[
  A \cup B = B \cup A
\]</span> and <span class="math display">\[
  A \cap B = B \cap A.
\]</span> Events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong>disjoint</strong> or <em>mutually exclusive</em> when <span class="math inline">\(A \cap B = \varnothing\)</span>. If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint, then at most of one of them can occur in a single experiment. Any set and its complement are disjoint, and the empty set <span class="math inline">\(\varnothing\)</span> is disjoint with itself and all other sets.</p>
<p>If <span class="math inline">\(\Omega\)</span> is a population, these set operations allow us to define subpopulations in terms of multiple traits. If the event <span class="math inline">\(A = \{\omega \in \Omega: \omega \text{ lives in Ohio}\}\)</span>, then its complement <span class="math inline">\(A^\comp\)</span> contains all individuals in <span class="math inline">\(\Omega\)</span> who live outside Ohio. If the event <span class="math inline">\(B = \{\omega \in \Omega: \omega \text{ is 42 years old}\}\)</span>, then the intersection <span class="math inline">\(A \cap B\)</span> contains everyone in <span class="math inline">\(\Omega\)</span> who is 42 years old and lives in Ohio. If <span class="math inline">\(\Omega\)</span> does not contain any such individual, then <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint. The union <span class="math inline">\(A \cup B\)</span> contains everyone in <span class="math inline">\(\Omega\)</span> who lives in Ohio or is 42 years old. This could include both a 24-year-old who lives Ohio and a 42-year-old who lives Michigan.</p>
</section>
<section id="venn-diagrams" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3" class="anchored" data-anchor-id="venn-diagrams"><span class="header-section-number">1.1.3</span> Venn diagrams</h3>
<p>A useful tool for understanding events and set operations is the <strong>Venn diagram</strong>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> An example is shown below.</p>
</section>
<section id="sec-evseq" class="level3" data-number="1.1.4">
<h3 data-number="1.1.4" class="anchored" data-anchor-id="sec-evseq"><span class="header-section-number">1.1.4</span> Sequences of events*</h3>
<p>Intersections can be written for more than two events. The intersection of <span class="math inline">\(A_1, A_2, \ldots, A_n\)</span> is <span id="eq-In"><span class="math display">\[
  I_n = \bigcap_{i = 1}^n A_i.
\tag{1.2}\]</span></span> Because set intersection is commutative and associative, any ordering of <span class="math inline">\(A_1, \ldots, A_n\)</span> produces the same intersection. The event <span class="math inline">\(I_n\)</span> occurs if and only if all of the events <span class="math inline">\(A_1, \ldots, A_n\)</span> occur. Each new event makes the intersection smaller (i.e., never larger) in the sense that <span class="math display">\[
  \bigcap_{i = 1}^{n + 1} A_i \subseteq I_n.
\]</span> whenever <span class="math inline">\(A_{n + 1}\)</span> is another event.</p>
<p>Similarly, unions can be written for more than two events. If <span class="math inline">\(A_1, A_2, \ldots, A_n\)</span> is a set of events, then their union is <span id="eq-Un"><span class="math display">\[
  U_n = \bigcup_{i = 1}^n A_i.
\tag{1.3}\]</span></span> Because set union is commutative and associative, any ordering of <span class="math inline">\(A_1, \ldots, A_n\)</span> produces the same union. The event <span class="math inline">\(U_n\)</span> occurs if and only if at least one of the events <span class="math inline">\(A_i\)</span> occurs. Each new event makes the union bigger (i.e., never smaller) in the sense that <span class="math display">\[
  U_n \subseteq \bigcup_{i = 1}^{n + 1} A_i
\]</span> whenever <span class="math inline">\(A_{n + 1}\)</span> is another event.</p>
<p>Both unions and intersections can be defined for infinite sequences of events.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> To describe this, we let <span class="math inline">\(n = \infty\)</span> in the notation from <a href="#eq-In" class="quarto-xref">Equation&nbsp;<span>1.2</span></a> or <a href="#eq-Un" class="quarto-xref">Equation&nbsp;<span>1.3</span></a>. The union of any finite sequence of events can be turned into the union of an infinite sequence of events by adding an endless sequence of empty sets to the finite sequence. The new sequence is still a sequence of disjoint events, and each empty set <span class="math inline">\(\varnothing\)</span> leaves the union unchanged. If <span class="math inline">\((A_1, A_2, \ldots)\)</span> is an infinite sequence of events such that <span class="math inline">\(A_i = \varnothing\)</span> for all <span class="math inline">\(i &gt; n\)</span>, then <span class="math display">\[
  \bigcup_{i = 1}^\infty A_i = \bigcup_{i = 1}^n A_i.
\]</span> This turns out to be useful when we try to give a mathematically rigorous definition of probability.</p>
</section>
<section id="algebra-of-sets" class="level3" data-number="1.1.5">
<h3 data-number="1.1.5" class="anchored" data-anchor-id="algebra-of-sets"><span class="header-section-number">1.1.5</span> Algebra of sets<span class="math inline">\(^*\)</span></h3>
<p>Unions, intersections, and complements can be combined in complex ways. Fortunately, there are a few basic principles that can be used to simplify these calculations. We have already seen that unions and intersections are commutative. Unions and intersections are also <em>associative</em>, so <span class="math display">\[
  A \cup (B \cup C)
  = (A \cup B) \cup C
\]</span> and <span class="math display">\[
  A \cap (B \cap C)
  = (A \cap B) \cap C
\]</span> for any sets <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span>.</p>
<p><em>De Morgan’s laws</em> describe how complements affect unions and intersections. If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are sets, then <span id="eq-intcomp"><span class="math display">\[
  (A \cap B)^\comp
  = A^\comp \cup B^\comp
\tag{1.4}\]</span></span> because you are outside <span class="math inline">\(A \cap B\)</span> if and only f you are outside <span class="math inline">\(A\)</span> or outside <span class="math inline">\(B\)</span>. Similarly, <span id="eq-unioncomp"><span class="math display">\[
  (A \cup B)^\comp
  = A^\comp \cap B^\comp.
\tag{1.5}\]</span></span> because you are outside <span class="math inline">\(A \cup B\)</span> if and only if you are outside <span class="math inline">\(A\)</span> and outside <span class="math inline">\(B\)</span>. Note that each of these equations implies the other if we replace <span class="math inline">\(A = (A^\comp)^\comp\)</span> with <span class="math inline">\(A^\comp\)</span> and replace <span class="math inline">\(B = (B^\comp)^\comp\)</span> with <span class="math inline">\(B^\comp\)</span>. They are two sides of the same coin, but it is helpful to remember them both.</p>
<p>The <em>distributive properties</em> describe how unions and intersections interact with each other. Recall that multiplication distributes over addition, so <span class="math inline">\(a (b + c) = ab + ac\)</span>. For any sets <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span>, we have the following distributive properties:</p>
<ul>
<li>Intersections distribute over unions, so <span class="math display">\[
  A \cap (B \cup C)
  = (A \cap B) \cup (A \cap C).
\]</span></li>
<li>Unions distribute over intersections, so <span class="math display">\[
  A \cup (B \cap C)
  = (A \cup B) \cap (A \cup C).
\]</span></li>
</ul>
<p>Intersections and unions also distribute over themselves. However, this is a consequence of commutativity, associativity, and <a href="#eq-AA" class="quarto-xref">Equation&nbsp;<span>1.1</span></a>, not a separate property like the distributive rules above.</p>
</section>
</section>
<section id="probability" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="probability"><span class="header-section-number">1.2</span> Probability</h2>
<p>A <em>probability measure</em> is a function that takes an event <span class="math inline">\(A \subseteq \Omega\)</span> and returns a number <span class="math inline">\(\Pr(A) \in [0, 1]\)</span> in any way that conforms to the following rules:</p>
<ul>
<li><span class="math inline">\(\Pr(\Omega) = 1\)</span>.</li>
<li><span class="math inline">\(\Pr(A) \in [0, 1]\)</span> for any event <span class="math inline">\(A \subseteq \Omega\)</span>.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></li>
<li>The <strong>addition rule</strong>: If <span class="math inline">\((A_1, A_2, \ldots)\)</span> is any sequence of disjoint events, then <span class="math display">\[
  \Pr\Biggl( \bigcup_{i = 1}^\infty A_i\Biggr)
  = \sum_{i = 1}^\infty \Pr(A_i).
\]</span> The addition rule is stated in terms of an infinite sequence of disjoint events because this implies the addition rule for any finite sequence of disjoint events (see <a href="#sec-evseq" class="quarto-xref"><span>Section 1.1.4</span></a>).</li>
</ul>
<p>It is useful to think of probability as a generalization of our intuitions about area or volume. When there is no overlap in a set of two-dimensional shapes, we can get the total area they cover by adding up the areas of the individual shapes. Similarly, we can get the total volume taken up by a set of bowling balls by adding up their individual volumes.</p>
<p>There is a lot of debate about the meaning of probability, but its definition does not assume any particular interpretation. Probability calculations are based on the rules above no matter what we think it all means, and any interpretation that is consistent with these rules is valid.</p>
<section id="probability-calculations" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="probability-calculations"><span class="header-section-number">1.2.1</span> Probability calculations</h3>
<p>Several useful properties of probability follow immediately from the definition above. A short proof follows each result. To follow the proofs, it helps to draw Venn diagrams.</p>
<div id="thm-pnull" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.1</strong></span> If <span class="math inline">\(A\)</span> is an event, <span class="math inline">\(\Pr\bigl(A^\comp\bigr) = 1 - \Pr(A)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Because <span class="math inline">\(\Omega = A \cup A^\comp\)</span> and <span class="math inline">\(A\)</span> and <span class="math inline">\(A^\comp\)</span> are disjoint, we have <span class="math display">\[
    \Pr(A) + \Pr\bigl(A^\comp\bigr) = \Pr(\Omega) = 1
  \]</span> by the addition rule. The result follows when we subtract <span class="math inline">\(\Pr(A)\)</span> from both sides.</p>
</div>
<div id="thm-psub" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.2</strong></span> If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are events such that <span class="math inline">\(A \subseteq B\)</span>, then <span class="math inline">\(\Pr(A) = \Pr(B) - \Pr\bigl(B \cap A^\comp\bigr)\)</span>. This implies that <span class="math inline">\(\Pr(A) \leq \Pr(B)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Each element of <span class="math inline">\(B\)</span> either is or is not in <span class="math inline">\(A\)</span>, so <span class="math display">\[
    B = (B \cap A) \cup \big(B \cap A^\comp\big)
    = A \cup \big(B \cap A^\comp\big).
  \]</span> where the second equality follows from the fact that <span class="math inline">\(B \cap A = A\)</span> because <span class="math inline">\(A \subseteq B\)</span>. The two sets on the right-hand side are disjoint, so we have <span class="math display">\[
    \Pr(B) = \Pr(A) + \Pr\bigl(B \cap A^\comp\bigr)
  \]</span> by the addition rule. The result follows if we subtract <span class="math inline">\(\Pr\bigl(B \cap A^\comp\bigr)\)</span> from both sides. This implies that <span class="math inline">\(\Pr(A) \leq \Pr(B)\)</span> because <span class="math inline">\(\Pr\bigl(B \cap A^\comp\bigr) \geq 0\)</span>.</p>
</div>
<div id="thm-punion" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.3</strong></span> If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are events, <span class="math inline">\(\Pr(A \cup B) = \Pr(A) + \Pr(B) - \Pr(A \cap B)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We can break <span class="math inline">\(A \cup B\)</span> into three disjoint sets: elements of <span class="math inline">\(A\)</span> and not <span class="math inline">\(B\)</span>, elements of <span class="math inline">\(B\)</span> and not <span class="math inline">\(A\)</span>, and elements of both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. In set notation, this is <span class="math display">\[
    A \cup B = \big(A \cap B^\comp\big) \cup \big(B \cap A^\comp\big) \cup (A \cap B).
  \]</span> By the addition rule, <span id="eq-punion"><span class="math display">\[
    \Pr(A \cup B) = \Pr\bigl(A \cap B^\comp\bigr) + \Pr\bigl(B \cap A^\comp\bigr) + \Pr(A \cap B).
   \tag{1.6}\]</span></span> By <a href="#thm-psub" class="quarto-xref">Theorem&nbsp;<span>1.2</span></a>, we have <span class="math display">\[
      \Pr\bigl(A \cap B^\comp\bigr)
      = \Pr(A) - \Pr(A \cap B), \\
  \]</span> because <span class="math inline">\(A \cap B \subseteq A\)</span> and <span class="math display">\[
      \Pr\bigl(B \cap A^\comp\bigr)
      = \Pr(B) - \Pr(A \cap B).
  \]</span> because <span class="math inline">\(A \cap B \subseteq B\)</span>. The result follows from substituting these back into <a href="#eq-punion" class="quarto-xref">Equation&nbsp;<span>1.6</span></a> and collecting terms involving <span class="math inline">\(\Pr(A \cap B)\)</span>. Intuitively, <span class="math inline">\(\Pr(A) + \Pr(B)\)</span> includes the overlap <span class="math inline">\(\Pr(A \cap B)\)</span> twice, so we have to subtract out one of them.</p>
</div>
</section>
</section>
<section id="random-variables" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="random-variables"><span class="header-section-number">1.3</span> Random variables</h2>
<p>The outcomes of an experiment are not necessarily numbers. A <strong>random variable</strong> is a real-valued function defined on a sample space <span class="math inline">\(\Omega\)</span>. In other words, a random variable <span class="math inline">\(X\)</span> is a function <span class="math inline">\(X(\omega)\)</span> that gives us a number for each possible outcome <span class="math inline">\(\omega\)</span>. Traditionally, random variables are written as capital letters and possible values are written as lower-case letters, so <span class="math inline">\(\Pr(X = x)\)</span> denotes the probability of the event <span class="math display">\[
  \{\omega \in \Omega : X(\omega) = x\}.
\]</span> For simplicity, random variables are usually written without the <span class="math inline">\(\omega\)</span>.</p>
<p>The distinction between outcomes and random variables is useful because we can define multiple random variables on the same sample space. For example, the height, weight, and age of an individual <span class="math inline">\(\omega\)</span> sampled from a population <span class="math inline">\(\Omega\)</span> are different random variables defined on the same sample space.</p>
<section id="sec-indicator" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="sec-indicator"><span class="header-section-number">1.3.1</span> Indicator variables</h3>
<p>The simplest random variables are <strong>indicator variables</strong>. For an event <span class="math inline">\(A\)</span>, the indicator variable <span class="math display">\[
  \indicator_A(\omega)
  = \begin{cases}
    1 &amp; \text{ if } \omega \in A, \\
    0 &amp; \text{ if } \omega \not\in A.
  \end{cases}
\]</span> When sampling from a population, we could define indicator variables for membership in different subpopulations. Indicator variables are <strong>binary</strong> random variables, which take exactly two values. In practice, these values should be zero and one unless there is a specific reason to do otherwise.</p>
<p>All of the basic set operations above can be expressed in terms of indicator variables for sets.</p>
<ul>
<li>The indicator function for the complement of <span class="math inline">\(A\)</span> is <span id="eq-indcomp"><span class="math display">\[
  \indicator_{A^\comp} = 1 - \indicator_A.
\tag{1.7}\]</span></span></li>
<li>If <span class="math inline">\(B\)</span> is another event and <span class="math inline">\(\indicator_B\)</span> is its indicator variable, then the indicator variable for the intersection <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is <span id="eq-indint"><span class="math display">\[
  \indicator_{A \cap B}
  = \indicator_A \indicator_B,
\tag{1.8}\]</span></span></li>
<li>The indicator variable for the union <span class="math inline">\(A \cup B\)</span> is <span id="eq-indunion"><span class="math display">\[
  \indicator_{A \cup B}
  = 1 - (1 - \indicator_A) (1 - \indicator_B)
  = \indicator_A + \indicator_B - \indicator_{A \cap B}.
\tag{1.9}\]</span></span> This follows from <a href="#eq-unioncomp" class="quarto-xref">Equation&nbsp;<span>1.5</span></a> because <span class="math inline">\(A \cup B = (A^\comp \cap B^\comp)^\comp\)</span>.</li>
</ul>
</section>
<section id="probability-distributions" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="probability-distributions"><span class="header-section-number">1.3.2</span> Probability distributions</h3>
<p>The set of possible values of a random variable <span class="math inline">\(X\)</span> is called the <em>support</em> of <span class="math inline">\(X\)</span> and denoted <span class="math inline">\(\supp(X)\)</span>.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> For example, the support of an indicator variable is <span class="math inline">\(\{0, 1\}\)</span>. In this section, we will focus on <strong>discrete</strong> random variables, which have a support on a finite or countably infinite set. There are two standard ways to describe the distribution of a discrete random variable:</p>
<ul>
<li><p>The <strong>probability mass function</strong> (PMF) of a discrete random variable <span class="math inline">\(X\)</span> is <span class="math display">\[
  f(x) =
  \begin{cases}
    \Pr(X = x) &gt; 0  &amp; \text{ if } x \in \supp(X), \\
    0               &amp; \text{ if } x \not \in \supp(X).
  \end{cases}
\]</span> Because <span class="math inline">\(\Pr(\Omega) = 1\)</span>, we always have <span class="math display">\[
  \sum_{x \in \supp(X)} f(x) = 1.
\]</span></p></li>
<li><p>The <strong>cumulative distribution function</strong> (CDF) of <span class="math inline">\(X\)</span> is <span class="math display">\[
  F(x)
  = \Pr(X \leq x).
\]</span> <span class="math inline">\(F(x)\)</span> is monotonically increasing in <span class="math inline">\(x\)</span>, which means that <span class="math inline">\(F(a) \leq F(b)\)</span> whenever <span class="math inline">\(a &lt; b\)</span>. It has a jump upward of size <span class="math inline">\(f(x)\)</span> at each <span class="math inline">\(x \in \supp(X)\)</span>, and its value at each such <span class="math inline">\(x\)</span> is the value that it jumps to—not the value that it jumps up from. For sufficiently small <span class="math inline">\(x\)</span>, <span class="math inline">\(F(x)\)</span> can be made arbitrarily close to zero. For sufficiently large <span class="math inline">\(x\)</span>, <span class="math inline">\(F(x)\)</span> can be made arbitrarily close to one. More formally, we say that <span class="math inline">\(\lim_{x \downarrow -\infty} F(x) = 0\)</span> and <span class="math inline">\(\lim_{x \uparrow \infty} F(x) = 1\)</span>.</p></li>
</ul>
<p>The PMF and CDF provide equivalent descriptions of the distribution of <span class="math inline">\(X\)</span> in the sense that either of these functions can be used to calculate the other. Given the PMF <span class="math inline">\(f\)</span>, the CDF is defined by <span class="math display">\[
  F(x) = \sum_{\substack{v \in \supp(X): \\ v \leq x}} f(v).
\]</span> where the sum is taken over all <span class="math inline">\(u \in \supp(X)\)</span> such that <span class="math inline">\(u \leq x\)</span>. Given the CDF <span class="math inline">\(F\)</span>, the PMF is defined by <span class="math display">\[
  f(x) = F(x) - \max_{v \leq x} F(v)
\]</span> where the maximum is <span class="math inline">\(F(v)\)</span> for the largest <span class="math inline">\(v \in \supp(X)\)</span> such that <span class="math inline">\(v &lt; x\)</span>.</p>
</section>
<section id="mean-and-variance" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="mean-and-variance"><span class="header-section-number">1.3.3</span> Mean and variance</h3>
<p>The <strong>mean</strong> or <em>expected value</em> of a random variable <span class="math inline">\(X\)</span> is <span class="math display">\[
  \E(X)
  = \sum_{x \in \supp(X)} x \Pr(X = x)
  = \sum_{x \in \supp(X)} x f(x),
\]</span> where <span class="math inline">\(f\)</span> is the PMF of <span class="math inline">\(X\)</span>. The mean is often written <span class="math inline">\(\mu\)</span>, and it is often described as a measure of the “location” or “central tendency” of <span class="math inline">\(X\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> has <span class="math inline">\(\E(X) = \mu\)</span>, then <span class="math inline">\((X - \mu)^2\)</span> is another random variable. The <strong>variance</strong> of <span class="math inline">\(X\)</span> is the expected value of <span class="math inline">\((X - \mu)^2\)</span>: <span class="math display">\[
  \Var(X)
  = \E\big[(X - \mu)^2\big]
  = \sum_{x \in \supp(X)} (x - \mu)^2 f(x).
\]</span> Because <span class="math inline">\((x - \mu)^2 \geq 0\)</span> with equality if and only if <span class="math inline">\(x = \mu\)</span>, we always have <span class="math inline">\(\Var(X) \geq 0\)</span>. We have <span class="math inline">\(\Var(X) = 0\)</span> if and only if <span class="math inline">\(X = \mu\)</span> with probability one. The variance is often written <span class="math inline">\(\sigma^2\)</span>, and it is often described as a measure of the dispersion of <span class="math inline">\(X\)</span> around the mean.</p>
<p>The square root of the variance is called the <strong>standard devation</strong>, which is often written <span class="math inline">\(\sigma\)</span>. If a random variable <span class="math inline">\(X\)</span> has units (e.g., length, weight, or time), the mean and the standard deviation have the same units as <span class="math inline">\(X\)</span>. For example, the mean and standard deviation of a length in meters both have units of meters, but the variance has units of <span class="math inline">\(\text{meters}^2\)</span>.</p>
</section>
<section id="bernoulli-distribution" class="level3" data-number="1.3.4">
<h3 data-number="1.3.4" class="anchored" data-anchor-id="bernoulli-distribution"><span class="header-section-number">1.3.4</span> Bernoulli distribution</h3>
<p>The distribution of an indicator variable is called the <strong>Bernoulli distribution</strong>.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> A random variable with the Bernoulli(<span class="math inline">\(p\)</span>) distribution has the PMF <span class="math display">\[
  f(x) =
  \begin{cases}
    1 - p &amp;\text{if } x = 0\\
    p     &amp;\text{if } x = 1.
  \end{cases}
\]</span> If a random variable <span class="math inline">\(X\)</span> has a Bernoulli(<span class="math inline">\(p\)</span>) distribution, we write <span class="math inline">\(X \sim \text{Bernoulli}(p)\)</span>. The indicator variable for an event <span class="math inline">\(A\)</span> has a Bernoulli distribution with <span class="math inline">\(p = \Pr(A)\)</span>.</p>
<p>If <span class="math inline">\(X \sim \Bernoulli(p)\)</span>, then it has mean <span class="math display">\[
  \begin{align}
    \E(X)
    &amp;= 0 \times (1 - p) + 1 \times p \\
    &amp;= p
  \end{align}
\]</span> and variance <span class="math display">\[
  \begin{align}
    \Var(X)
    &amp;= (0 - p)^2(1 - p) + (1 - p)^2 p \\
    &amp;= p (1 - p).
  \end{align}
\]</span> The standard deviation is <span class="math inline">\(\sqrt{p (1 - p)}\)</span>. This standard deviation is greater than zero unless <span class="math inline">\(p = 0\)</span> or <span class="math inline">\(p = 1\)</span>. If <span class="math inline">\(p = 0\)</span>, then <span class="math inline">\(X = 0\)</span> with probability one. If <span class="math inline">\(p = 1\)</span>, then <span class="math inline">\(X = 1\)</span> with probability one.</p>
</section>
</section>
<section id="joint-and-marginal-distributions" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="joint-and-marginal-distributions"><span class="header-section-number">1.4</span> Joint and marginal distributions</h2>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables defined on the same probability space, then their <strong>joint</strong> probability mass function is <span class="math display">\[
  f(x, y)
  = \Pr(X = x \text{ and } Y = y)
  = \Pr\big(\{\omega: X(\omega) = x \text{ and } Y(\omega) = y\}\big).
\]</span> The <strong>marginal</strong> probability mass functions are the PMFs of <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> individually, which can be calculated from the joint PMF. The marginal PMF of <span class="math inline">\(X\)</span> is <span class="math display">\[
  f_X(x) = \sum_{y \in \supp(Y)} f(x, y),
\]</span> and the marginal PMF of <span class="math inline">\(Y\)</span> is <span class="math display">\[
  f_Y(y) = \sum_{x \in \supp(X)} f(x, y).
\]</span> These are called marginal distributions by analogy to the margins of a table.</p>
<p>Joint distributions can be defined for more than two random variables. If <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> are random variables defined on the same sample space, then their joint PMF is <span class="math display">\[
  f(x_1, x_2, \ldots, x_n) = \Pr(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n).
\]</span> The marginal distribution of each <span class="math inline">\(X_i\)</span> can be found by adding up the PMF over the support of all the other random variables. For example, <span class="math display">\[
  f_{X_2}(x_2) = \sum_{x_1 \in \supp(X_1)} \sum_{x_3 \in \supp(X_3)} f(x_1, x_2, x_3).
\]</span> when <span class="math inline">\(n = 3\)</span>. For larger <span class="math inline">\(n\)</span>, the formula gets uglier but the idea is the same.</p>
<p>The distinction between joint and marginal distributions is extremely important in epidemiology (and other applications of probability). The joint distribution can be used to find any of the the marginal distributions, but the marginal distributions cannot be used to find the joint distribution—unless we make additional assumptions.</p>
<section id="linear-combinations" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="linear-combinations"><span class="header-section-number">1.4.1</span> Linear combinations*</h3>
<p>If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, then <span class="math inline">\(a X + b Y\)</span> is another random variable on <span class="math inline">\(\Omega\)</span>. It is called a <em>linear combination</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Linear combinations can be defined for more than two random variables. If <span class="math inline">\(X_1, \ldots, X_n\)</span> are random variables defined on a sample space and <span class="math inline">\(a_1, \ldots, a_n\)</span> are constants, then <span class="math display">\[
  \sum_{i = 1}^n a_i X_i = a_1 X_1 + a_2 X_2 + \cdots + a_n X_n
\]</span> is a linear combination of <span class="math inline">\(X_1, \ldots, X_n\)</span>. The constants can be any real numbers, including one and zero.</p>
<p><a href="#sec-indicator" class="quarto-xref"><span>Section 1.3.1</span></a> contains both examples and non-examples of linear combinations of random variables.</p>
<ul>
<li><p>The indicator function for <span class="math inline">\(A^\comp\)</span> in <a href="#eq-indcomp" class="quarto-xref">Equation&nbsp;<span>1.7</span></a> is a linear combination of <span class="math inline">\(\indicator_A\)</span> and the random variable <span class="math inline">\(\indicator_\Omega\)</span>, which equals one for all <span class="math inline">\(\omega \in \Omega\)</span>.</p></li>
<li><p>The indicator function for <span class="math inline">\(A \cup B\)</span> in <a href="#eq-indunion" class="quarto-xref">Equation&nbsp;<span>1.9</span></a> is linear combination of the indicator variables <span class="math inline">\(\indicator_A\)</span>, <span class="math inline">\(\indicator_B\)</span>, and <span class="math inline">\(\indicator_{A \cap B}\)</span>.</p></li>
<li><p>The indicator function for <span class="math inline">\(A \cap B\)</span> in <a href="#eq-indint" class="quarto-xref">Equation&nbsp;<span>1.8</span></a> is not a linear combination of <span class="math inline">\(\indicator_A\)</span> and <span class="math inline">\(\indicator_B\)</span> because we have to multiply these two variables.</p></li>
</ul>
</section>
<section id="mean-and-covariance" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="mean-and-covariance"><span class="header-section-number">1.4.2</span> Mean and covariance*</h3>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables defined on the same sample space and <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, the mean of the linear combination <span class="math inline">\(a X + b Y\)</span> is <span class="math display">\[
  \E(a X + b Y) = a \E(X) + b \E(Y).
\]</span> This is a direct consequence of the definition of expected value: <span class="math display">\[
  \begin{aligned}
    \E(a X + b Y)
    &amp;= \sum_{x \in \supp(X)} \sum_{y \in \supp(Y)} (a x + b y) f(x, y) \\
    &amp;= a \sum_{x \in \supp(X)} \bigg(x \sum_{y \in \supp(Y)} f(x, y)\bigg)
      + b \sum_{y \in \supp(Y)} \bigg(y \sum_{x \in \supp(X)} f(x, y)\bigg) \\
    &amp;= a \sum_{x \in \supp(X)} x f_X(x) + b \sum_{y \in \supp(Y)} y f_Y(y).
  \end{aligned}
\]</span> The algebra is not pretty, but the logic is straightforward. We split up the sum into parts depending only on <span class="math inline">\(x\)</span> and only on <span class="math inline">\(y\)</span> outside the joint PMF. In each part, we factor out a constant and find the marginal PMF. This same logic extends to a linear combination of any number of random variables.</p>
<p>The variance of <span class="math inline">\(a X + b Y\)</span> is <span class="math display">\[
  \Var(a X + b Y) = a^2 \Var(X) + b^2 \Var(Y) + 2 a b \Cov(X, Y)
\]</span> where <span class="math display">\[
  \Cov(X, Y) = \E\bigl[\big(X - \E(X)\big) \big(Y - \E(Y)\big)\bigr]
\]</span> is called the <strong>covariance</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Note that <span class="math inline">\(\Cov(X, Y) = \Cov(Y, X)\)</span>. Because <span class="math inline">\(\Var(X) = \Cov(X, X)\)</span>, variance is a special case of covariance.</p>
<p>The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> has a <strong>covariance matrix</strong> which is <span class="math display">\[
  \begin{bmatrix}
    \Var(X)     &amp; \Cov(X, Y) \\
    \Cov(X, Y)  &amp; \Var(Y)
  \end{bmatrix}
\]</span> The variances are along the diagonal of the matrix, and the covariances appear off the diagonal. Because <span class="math inline">\(\Cov(X, Y) = \Cov(Y, X)\)</span>, covariance matrices are always symmetric (i.e., symmetric across the diagonal). Covariance matrices are an extremely useful tool for calculating the variances of linear combinations of random variables. For example: <span class="math display">\[
  \Var(a X + b Y)
  = \begin{pmatrix}
      a &amp; b
    \end{pmatrix}
    \begin{bmatrix}
      \Var(X)     &amp; \Cov(X, Y) \\
      \Cov(X, Y)  &amp; \Var(Y)
    \end{bmatrix}
    \begin{pmatrix}
      a \\ b
    \end{pmatrix}
\]</span> in matrix and vector notation from linear algebra. This logic extends to linear combinations of any number of random variables.</p>
<p>The covariance is the numerator of the <em>Pearson correlation coefficient</em>,<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> which is <span class="math display">\[
  \rho_{XY} = \rho_{YX}
  = \frac{\Cov(X, Y)}{\sqrt{\Var(X) \Var(Y)}}.
\]</span> Because of the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy-Schwarz inequality</a>, it turns out that <span class="math inline">\(\rho_{XY} \in [-1, 1]\)</span>.</p>
<ul>
<li><p>We get <span class="math inline">\(\rho_{XY} = -1\)</span> if and only if <span class="math inline">\(Y = c X\)</span> for some negative constant <span class="math inline">\(c\)</span>.</p></li>
<li><p>We get <span class="math inline">\(\rho_{XY} = 1\)</span> if and only if <span class="math inline">\(Y = c X\)</span> for some positive constant <span class="math inline">\(c\)</span>. For example, <span class="math inline">\(\rho_{XX} = 1\)</span> for any random variable <span class="math inline">\(X\)</span>.</p></li>
<li><p>We get <span class="math inline">\(\rho_{XY} = 0\)</span> if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <em>independent</em> in the sense that the value of one tells us nothing about the value of the other.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> However, it is possible to have <span class="math inline">\(\rho_{XY} = 0\)</span> when <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not independent.</p></li>
</ul>
<p>If we divide each entry <span class="math inline">\(\Cov(X, Y)\)</span> in a covariance matrix by <span class="math inline">\(\sqrt{\Var(X) \Var(Y)}\)</span>, when we get a <em>correlation matrix</em>. Any correlation matrix is symmetric, and the entries along its diagonals are all ones.</p>
</section>
</section>
<section id="probability-and-disease-occurrence" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="probability-and-disease-occurrence"><span class="header-section-number">1.5</span> Probability and disease occurrence</h2>
<p>In epidemiology, there are two fundamental measures of disease occurrence that are probabilities: <strong>prevalence</strong> and <strong>risk</strong>. In both cases, our experiment is to sample an individual <span class="math inline">\(\omega\)</span> from a population <span class="math inline">\(\Omega\)</span>. The <strong>disease outcome</strong> is a binary random variable <span class="math display">\[
  D(\omega) =
  \begin{cases}
    1 &amp; \text{if } \omega \text{ has the disease outcome}, \\
    0 &amp; \text{otherwise}.
  \end{cases}
\]</span> The set of individuals in <span class="math inline">\(\Omega\)</span> who have <span class="math inline">\(D(\omega) = 1\)</span> is an event in <span class="math inline">\(\Omega\)</span>, and the prevalence or risk is its probability <span class="math display">\[
  \Pr(\{\omega \in \Omega: D(\omega) = 1\}).
\]</span> The most important difference between prevalence and risk is the role of time in the definition of <span class="math inline">\(D\)</span>.</p>
<p>There is an important technicality to remember when we talk about disease onset and recovery. When a person has disease onset at time <span class="math inline">\(\tonset\)</span> and recovers at time <span class="math inline">\(\trec\)</span>, they have disease for each <span class="math inline">\(t \in [\tonset, \trec)\)</span>. We assume that <span class="math inline">\(\trec &gt; \tonset\)</span> so this interval is nonempty. We let the onset and recovery times for person <span class="math inline">\(i\)</span> be <span class="math inline">\(\tonset_i\)</span> and <span class="math inline">\(\trec_i\)</span>, respectively. If a person has multiple episodes of the disease, each episode has its own <span class="math inline">\(\tonset\)</span> and <span class="math inline">\(\trec\)</span>. For example, the <span class="math inline">\(j^\text{th}\)</span> episode in person <span class="math inline">\(i\)</span> would have onset time <span class="math inline">\(\tonset_{ij}\)</span> and recovery time <span class="math inline">\(\trec_{ij}\)</span>.</p>
<section id="prevalence" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="prevalence"><span class="header-section-number">1.5.1</span> Prevalence</h3>
<p>For prevalence, the disease outcome is defined by choosing a time <span class="math inline">\(t\)</span> and letting <span class="math display">\[
  D(\omega) =
  \begin{cases}
    1 &amp; \text{if } \omega \text{ has disease at time } t, \\
    0 &amp; \text{otherwise}.
  \end{cases}
\]</span> In other words, it is the proportion of the population <span class="math inline">\(\Omega\)</span> that disease at time <span class="math inline">\(t\)</span>. This includes individuals who have disease onset at time <span class="math inline">\(\tonset = t\)</span> but not individuals who recover from disease at time <span class="math inline">\(\trec = t\)</span>. This is often called the <strong>point prevalence</strong> at time <span class="math inline">\(t\)</span>.</p>
<p>Another version of prevalence is <strong>period prevalence</strong>. For period prevalence, we choose a nonempty time interval <span class="math inline">\((t_a, t_b]\)</span> and define <span class="math display">\[
  D(\omega) =
  \begin{cases}
    1 &amp; \text{if } \omega \text{ has disease at any time } t \in (t_a, t_b], \\
    0 &amp; \text{otherwise}.
  \end{cases}
\]</span> In other words, it is the proportion of the population that has disease at any time in the interval <span class="math inline">\((t_a, t_b]\)</span>. This can include people who had disease onset at a time <span class="math inline">\(\tonset \leq t_a\)</span> as long as <span class="math inline">\(\trec &gt; t_a\)</span>. In particular, it includes all individuals who have disease onset at time <span class="math inline">\(t_a\)</span>, who must have <span class="math inline">\(\trec &gt; t_a\)</span>.</p>
</section>
<section id="risk-cumulative-incidence" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="risk-cumulative-incidence"><span class="header-section-number">1.5.2</span> Risk (cumulative incidence)</h3>
<p>To define <strong>risk</strong> or <strong>cumulative incidence</strong>, we first choose an nonempty time interval <span class="math inline">\((t_a, t_b]\)</span> and let the population <span class="math inline">\(\Omega\)</span> consist only of individuals who do not have disease at time <span class="math inline">\(t_a\)</span> and are at risk of disease onset after that. The disease outcome is defined as <span class="math display">\[
  D(\omega) =
  \begin{cases}
    1 &amp; \text{if } \omega \text{ has } \tonset \in (t_a, t_b], \\
    0 &amp; \text{otherwise}.
  \end{cases}
\]</span> In the population that is disease-free and at risk of disease at time <span class="math inline">\(t_a\)</span>, it is the proportion who have disease onset at <span class="math inline">\(\tonset \leq t_b\)</span>.</p>
<p>Point and interval prevalence are affected by both the frequency of disease onset and the duration of disease, the risk in any time interval depends only on the frequency of disease onset (see Figure~<span class="math inline">\(\ref{fig:prevdur}\)</span>). This makes risk a more useful measure for identifying the causes of disease, which is the primary goal of <em>analytic epidemiology</em>.</p>
<p>Another advantage of risk is that it can be used for outcomes that begin and end very quickly (e.g., traffic accidents or being hit by lightning) or outcomes that remove individuals from the population (e.g., emigration or death). Point prevalence would not be a useful measure of the impact of these outcomes on public health, and period prevalence would be nearly identical to the risk over the same period.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-marquis1820theorie" class="csl-entry" role="listitem">
Laplace, Pierre Simon. 1820. <em>Th<span>é</span>orie Analytique Des Probabilit<span>é</span>s</em>. Vol. 7. Courcier.
</div>
<div id="ref-morabia2004epidemiology" class="csl-entry" role="listitem">
Morabia, Alfredo. 2004. <span>“Epidemiology: An Epistemological Perspective.”</span> In <em>A History of Epidemiologic Methods and Concepts</em>, edited by Alfredo Morabia, 3–125. Springer.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p> <a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">Pierre-Simone, marquis de Laplace</a> (1749-1827) is often called the Newton of France. He proved that the solar system is stable, developed theories of ocean tides and gravitational potential, proved one of the first general versions of the central limit theorem, and pioneered the Bayesian interpretation of probability. For just six weeks in 1799, he was Minister of the Interior for France under Napoleon. His is one of the 72 names on the Eiffel Tower. <a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p> The natural numbers $ = {0, 1, 2, } are <em>countably infinite</em>, as are the integers <span class="math inline">\(\mathbb{Z}\)</span> and the rational numbers <span class="math inline">\(\mathbb{Q}\)</span>. The real numbers <span class="math inline">\(\mathbb{R}\)</span> are <em>uncountably infinite</em>, as are the real numbers in any interval <span class="math inline">\((a, b)\)</span> with <span class="math inline">\(a &lt; b\)</span> and the irrational numbers. Uncountably infinite sets are infinitely larger than countably infinite sets. This distinction was discovered in the 1870s by the German mathematician <a href="https://en.wikipedia.org/wiki/Georg_Cantor">Georg Cantor</a> (1845–1918). It was considered shocking at the time, but it has become a cornerstone of modern mathematics.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p> In experiments with uncountably infinite sample spaces, the probability of an event <span class="math inline">\(A\)</span> cannot always be calculated by adding up the probabilities of <span class="math inline">\(\{\omega\}\)</span> for all <span class="math inline">\(\omega \in A\)</span>. For example: If we choose a number at uniformly at random in <span class="math inline">\([0, 1]\)</span>, the probability of getting any particular number <span class="math inline">\(\omega\)</span> is zero. The sum of the probabilities of all <span class="math inline">\(\{\omega\} \subseteq A\)</span> is zero (if <span class="math inline">\(A\)</span> is countable) or undefined (if <span class="math inline">\(A\)</span> is uncountable). By maintaining a distinction between outcomes and events and by limiting probability calculations to countable (i.e., finite or countably infinite) sums, we end up with something coherent and useful.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p> Named after <a href="https://en.wikipedia.org/wiki/John_Venn">John Venn</a> (1834-1923), an English logician and philosopher who was one of the pioneers of the frequentist interpretation of probability. He was ordained as an Anglican priest in 1859 but resigned from the church in 1883. He was a prize-winning gardener of roses and white carrots and a prominent supporter of women’s right to vote. From 1903 until his death, he was President of Fellows in Gonville and Caius College at the University of Cambridge, where he is commemorated with a Venn diagram in a stained glass window.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p> In probability, we only consider unions and intersections of finite or countably infinite sets of events. Although unions and intersections can be defined for uncountably infinite sets of events, it can be impossible to assign probabilities to the resulting sets (see the <a href="https://en.wikipedia.org/wiki/Banach_Tarski_paradox">Banach-Tarski paradox</a>). As an epidemiologist, this should not keep you up at night.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p> Technically, we assign probabilities only to events in a set <span class="math inline">\(\mathcal{F}\)</span> of subsets of <span class="math inline">\(\Omega\)</span> that is required to contain <span class="math inline">\(\Omega\)</span> and to be closed under complements and countable unions. “Closed under complements” means that <span class="math inline">\(A^\comp \in \mathcal{F}\)</span> whenever <span class="math inline">\(A \in \mathcal{F}\)</span>. For example, <span class="math inline">\(\varnothing = \Omega^\comp\)</span> must be in <span class="math inline">\(\mathcal{F}\)</span> because <span class="math inline">\(\Omega \in \mathcal{F}\)</span>. “Closed under countable unions” means that <span class="math inline">\(\bigcup_{i = 1}^\infty A_i \in \mathcal{F}\)</span> whenever <span class="math inline">\((A_1, A_2, \ldots)\)</span> is a sequence of events in <span class="math inline">\(\mathcal{F}\)</span>. The set <span class="math inline">\(\mathcal{F}\)</span> is called a <span class="math inline">\(\sigma\)</span>-algebra, and this restriction on the domain of probability helps avoid internal contradictions like the mind-blowing <a href="https://en.wikipedia.org/wiki/Banach_Tarski_paradox">Banach-Tarski paradox</a>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p> Technically, the support of <span class="math inline">\(X\)</span> is the smallest closed set <span class="math inline">\(S_X\)</span> such that <span class="math inline">\(\Pr(X \in S_X) = 1\)</span>. For a discrete random variable with support on a finite set, it is just the set of possible values. For a discrete random variable with support on a countably infinite set, it can include points whose probability mass is zero—a pathological case that we can safely ignore. For a continuous random variable, it can include values whose probability density is zero—a case that is not unusual or pathological.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p> Named after <a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Jacob Bernoulli</a> (1655-1705), a Swiss mathematician who derived the first version of the law of large numbers and discovered the constant <span class="math inline">\(e \approx 2.718281828\)</span>, which is the base for natural logarithms. He and his younger brother Johann Bernoulli (1667-1748) were some of the first mathematicians to try to understand and apply calculus, but their relationship eventually curdled into a jealous rivalry. A lunar impact crater called Bernoulli is named jointly after them.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p> Named after <a href="https://en.wikipedia.org/wiki/Karl_Pearson">Karl Pearson</a> (1857-1936), an English mathematician who founded the modern discipline of mathematical statistics. In 1911, he started the world’s first university department of statistics at University College London. He was an outspoken socialist and supporter of women’s rights, but he was also a vocal proponent of social Darwinism and eugenics who opposed Jewish immigration into Britain.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p> We will define independence of random variables more rigorously when we discuss conditional probabilities in <a href="condprob.html" class="quarto-xref"><span>Chapter 2</span></a>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Preface">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./condprob.html" class="pagination-link" aria-label="Conditional Probability and Diagnostic Tests">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Probability and Diagnostic Tests</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>